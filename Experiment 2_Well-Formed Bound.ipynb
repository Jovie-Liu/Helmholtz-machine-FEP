{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 2</span> \n",
    "\n",
    "# Helmholtz Machine on Well-formed Bound\n",
    "\n",
    "This notebook examines the capacity of the vanilla Helmholtz machine and gives a light introduction to the mechanism of **active sampling**. Although it's not my original motivation of incorporating active sampling to a machine learning model, I guess this may be an easier way for people to understand the meaning and necessity of it and what kind of problem it could solve. I will shed light on more elaborated explanations later on but let's get started light-heartedly...\n",
    "\n",
    "*Updated:* December 7, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previously on...\n",
    "\n",
    "It's been 8 months since we last updated on the Helmholtz series (oops..) lots of things happened, I guess I first deviated to the active inference conference, since I never atteded a conference before so I had to hurry up and wrap everything up in one week to manage the deadline. The results were unexpected, I not only made it to Ghent, but also made it to Paris with the same kind of work! Then I deviated a little bit to music generation, as that's a continuing endeavor of my career and I have to lead it to fruition somehow... I also did a lot of travelling in summer! China, Japan, France, Belgium, etc. But the contemplation on the Helmholtz machine never terminates, so let me recap what we were left half a year ago...\n",
    "\n",
    "The idea is still on well-formed and preference sets, and the data is generated by the rules accordingly.\n",
    "\n",
    "<img src=\"Venn_1.jpg\" style=\"width:400px;height:350px;\">\n",
    "<caption><center> **Figure 1**: Venn Diagram for Ground Truth Data  </center></caption>\n",
    "\n",
    "Let's continue with the first preliminary experiment (which was really done in a hurry, but thanks to it, I made it to IWAI conference!). In this experiment, we use the well-formed set as the pre-given dataset (ignoring the preference set here) and firstly use this dataset to test the performance of the Helmholtz machine. We agree on that a dataset should be some regularity over all possibilities of the given space, like in image generation there is always a difference between image and noise, which deliminates the given dataset from the rest of the possible generations (which is considered as noise, or false examples if not considering exposure bias). In our example, the well-formed set bounds the regularity of patterns, which means anything not included in the well-formed set is a false instance, thus should not be generated by the model. **Our goal is that, by training the model, we want to eliminate any possibility of generating false instances**. If you have biological inclined thoughts, you can make anology of the well-formed set to the survival status of a phenotype. Once this bound is transgressed, the phenotype's survival status is challenged, like extreme climates, low oxygen proportions, shortage of water, etc., thus the agent would die in these states. Therefore, any of such negative state could not be sampled by the model. If you have more machine learning type of logic, you can make analogy of this situation to the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) in classification problem:\n",
    "\n",
    "<img src=\"FP.jpg\" style=\"width:550px\">\n",
    "<caption><center> **Figure 2**: Confusion Matrix  </center></caption>\n",
    "\n",
    "In our case, we assign infinite punishment on the false positive case, which means we give zero tolerence to the false alarms which cause the agent's life in danger; on the other hand, for the false negative case, we are quite generous since the agent doesn't need to know every corner of the world to live a life; it can choose its habitat and live comfortably within it as long as it wants to. This situation is visualized as the graph below:\n",
    "\n",
    "<img src=\"set_show.jpg\" style=\"width:550px\">\n",
    "<caption><center> **Figure 3**: Illustration of the Generative Set  </center></caption>\n",
    "\n",
    "Unlike the classification problem, now we are assessing a generative model, thus the goal is to generate a distribution within the regularity bound (well-formed set) thus no FP occurs. Let's see how the Helmholtz machine performs.\n",
    "\n",
    "$$\n",
    "D_{KL}(Q || P) = E_Q [\\log \\frac{Q}{P}]\n",
    "$$\n",
    "\n",
    "KL-divergence is unsymmetric, which penalizes for false positive but not for false negative (Figure 3). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: A test on the generative capacity of vanilla Helmholtz machine\n",
    "\n",
    "The dataset we use is the well-formed set generated with the well-formedness rules:\n",
    "\n",
    "1. Start with 1\n",
    "2. Forbid 00100 (no 100, 001 on the boundary)\n",
    "3. Forbid 0000\n",
    "\n",
    "We use a 10-node pattern with binary values thus there are 1024 combinations of all patterns, of which the well-formed set contains 256 instances. The dataset is generated as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       [ 1., -1.,  1., ...,  1., -1.,  1.],\n",
       "       [ 1.,  1., -1., ...,  1.,  1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "well_formed_set = np.zeros([1,n])\n",
    "well_formed_set[0,0] = 1\n",
    "\n",
    "for i in range(1,n):\n",
    "    for j in range(np.shape(well_formed_set)[0]):\n",
    "        if i == 2 and np.array_equal(well_formed_set[j,i-2:i], [1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-3:i], [0,0,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-4:i], [0,0,1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        else:\n",
    "            well_formed_set = np.append(well_formed_set, well_formed_set[j:j+1,:], axis=0)\n",
    "            well_formed_set[j,i] = 1\n",
    "            \n",
    "ind = np.array([], dtype=np.int8)\n",
    "for i in range(well_formed_set.shape[0]):\n",
    "    if np.array_equal(well_formed_set[i,-3:], [0,0,1]):\n",
    "        ind = np.append(ind,i)\n",
    "\n",
    "well_formed_set = np.delete(well_formed_set,ind,0)\n",
    "well_formed_set = (well_formed_set - 0.5)*2\n",
    "well_formed_set = np.transpose(well_formed_set)\n",
    "well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Helmholtz machine structure is given as below:\n",
    "\n",
    "<img src=\"Helmz.jpg\" style=\"width:550px\">\n",
    "<caption><center> **Figure 4**: The Helmholtz Machine  </center></caption>\n",
    "\n",
    "I previously wrote in Document 2 about the variational objective function and deduction of the parameter updating rules of the Helmholtz machine, and I will give a more systematic analysis on these matters in upcoming notebooks, so here let's skip these steps and say we know the parameter updating local delta rule is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{F}}{\\partial \\phi_{k,n}^{m-1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m-1}(1-q_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m-1} \\centerdot q_n^m & \\text{if } s_n^m = -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial \\theta_{k,n}^{m+1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m = -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Let's write the functions to run this model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always have the bottom layer as data input, denoted as $d_0$. In our case illustrated in Figure 4, we have $m = 4$ layers, with various numbers of neurons, $n_{d_0} = 10$, $n_{z_1} = 8$, $n_{z_2} = 5$, $n_{z_3} = 3$. These are the hyperparameters to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(init_type,n_dz):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (m, ), where m is the number of layers\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \"\"\"\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    m = len(n_dz)\n",
    "    if init_type == \"zero\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((n_dz[i+1],n_dz[i]+1))\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.zeros((n_dz[i],n_dz[i+1]+1))\n",
    "        Theta[\"Theta_k\"] = np.zeros((n_dz[-1],1))\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(n_dz[i+1],n_dz[i]+1)\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.random.randn(n_dz[i],n_dz[i+1]+1)\n",
    "        Theta[\"Theta_k\"] = np.random.randn(n_dz[-1],1)\n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "    return Phi, Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- probability of each neuron (taking value 1), Python dictionary of same length as Phi with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = {}\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0)))\n",
    "        S = ((q > np.random.rand(len(q),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        Q[\"q\"+str(i+1)] = q\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = well_formed_set[:,5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, Alpha_Q = wake_forward(d0,Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- probability of each neuron (taking value 1), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    \n",
    "    m = len(Theta)\n",
    "    P = {\"p\"+str(m-1):p}\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        P[\"p\"+str(i-1)] = p\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    return P, Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Alpha_P = sleep_forward(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    for i in range(n):\n",
    "        S_bias = np.append(Alpha_P[\"z\"+str(i)],[[1]], axis=0)\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S_bias))\n",
    "        gain = q - (1+Alpha_P[\"z\"+str(i+1)])/2\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, info_gain_wake,error_Q = wake_update_delta(Phi,Alpha_P,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information gain, we can read the error of the given assignment *Alpha_P* for parameters *Phi*, on each neuron it computes. The value of info_gain is within $[-1,1]$, where positive value indicates the current neuron takes value $-1$, while negative value indicates that the current neuron takes value $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - (1+Alpha_Q[\"z\"+str(n-1)])/2\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,0,-1):\n",
    "        S_bias = np.append(Alpha_Q[\"z\"+str(i)],[[1]], axis=0)\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S_bias))\n",
    "        gain = p - (1+Alpha_Q[\"z\"+str(i-1)])/2\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta, info_gain_sleep, error_P = sleep_update_delta(Theta,Alpha_Q,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "well_formed_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like stochastic gradient descent, we update the parameters for each data input (no batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15901168833946058 0.03421493060559103\n",
      "0.15983237228741018 0.042337448041664415\n",
      "0.16089064198945544 0.037757960873685444\n",
      "0.1602820853040595 0.043463366422903986\n",
      "0.162436147332215 0.045190349067424675\n",
      "0.16257998694032066 0.04658391600957921\n",
      "0.16106838165648743 0.04114501347645634\n",
      "0.15999348229902471 0.04330094169931256\n",
      "0.1584051683663183 0.04294083195459657\n",
      "0.16345340897528995 0.0469827252502302\n",
      "0.15954049624880587 0.04614623403714955\n",
      "0.15901594227101018 0.043286068320060475\n",
      "0.1614373295494023 0.0482873288450563\n",
      "0.16343093765649377 0.04968456863149329\n",
      "0.16324437340257036 0.047771559750944236\n",
      "0.16257391740114227 0.04028638985493899\n",
      "0.16285101059290302 0.048896744479330075\n",
      "0.16046760106335872 0.047016229855362385\n",
      "0.1617716259513267 0.04500475671167798\n",
      "0.16267413311220677 0.046086984290861464\n",
      "0.16525616759754688 0.04369494757312913\n",
      "0.16414483433822427 0.043707828461685826\n",
      "0.16447360555760385 0.052850497618348515\n",
      "0.16368339302449905 0.05146749276637143\n",
      "0.16510679902349978 0.04816218676688907\n",
      "0.1631907250586158 0.04446401582689766\n",
      "0.16460867485133682 0.05016000718012976\n",
      "0.16519893367349836 0.050832944548967376\n",
      "0.16554981533068944 0.04216988312073981\n",
      "0.1639326213941687 0.04701652940716546\n",
      "0.165086158896818 0.04817694324395113\n",
      "0.16379265616680241 0.04014461666484187\n",
      "0.1634785105691189 0.04131701442158304\n",
      "0.16260966700934745 0.043081438730397215\n",
      "0.1624230095397565 0.04518015308216705\n",
      "0.16486056110015984 0.04433445522765646\n",
      "0.16230057436755774 0.04433686915976567\n",
      "0.16080692901554988 0.042586883268273416\n",
      "0.15893645699657571 0.04533408415154614\n",
      "0.15385153223116754 0.033749162609078856\n",
      "0.1545683489030985 0.03719252355656623\n",
      "0.15403519184725467 0.038134285558712684\n",
      "0.15009199146885027 0.030867389014885522\n",
      "0.1501224268138259 0.030786801776151555\n",
      "0.1498660249505234 0.029842563668128597\n",
      "0.14798400447234822 0.02847866423015148\n",
      "0.15078763803818826 0.03401905760539793\n",
      "0.1493949886647169 0.029681993589443283\n",
      "0.15053947790917063 0.030711442337561012\n",
      "0.15138270237656085 0.03227776449329322\n",
      "0.14854547027111364 0.02747176055364439\n",
      "0.15082062449591838 0.029398739584044566\n",
      "0.14849785175628769 0.03047973879401307\n",
      "0.14831294293627106 0.028145919605214866\n",
      "0.14909632709682924 0.029766603555493466\n",
      "0.14982321563079903 0.029009038864644017\n",
      "0.15007781722504834 0.030352479869523972\n",
      "0.15033187113009064 0.033906484098154756\n",
      "0.1496003887034191 0.028061013887929157\n",
      "0.15003598082330155 0.03234140344855989\n",
      "0.1483096407372524 0.026261619587583145\n",
      "0.15189043020393017 0.03243525383575896\n",
      "0.151396618405943 0.03444818980830284\n",
      "0.1540867220535995 0.03404404332951813\n",
      "0.1564899049217538 0.037578498101221186\n",
      "0.15430154509429245 0.0358863798095482\n",
      "0.15065554991684277 0.032343425437744405\n",
      "0.15071050425290947 0.02503950536843937\n",
      "0.15357950930596811 0.031830170961545834\n",
      "0.14967641539287338 0.03012418249949305\n",
      "0.14925605729147706 0.022214234780972324\n",
      "0.1478735206780105 0.03248063155480826\n",
      "0.14663426478252373 0.026055559919363994\n",
      "0.14739138191139095 0.022685217008814627\n",
      "0.14667835177816446 0.02440478193999924\n",
      "0.14928444386495834 0.025770759344633375\n",
      "0.1455365493552105 0.020527824185063378\n",
      "0.14546820640516692 0.023374324330830018\n",
      "0.14688280492888534 0.0241092789061137\n",
      "0.14510083507111132 0.02227252887983141\n",
      "0.1443352919323851 0.021208293103153676\n",
      "0.14791274629522272 0.02269759215999813\n",
      "0.1470944853505555 0.024517815928070473\n",
      "0.14921529624577534 0.02955563401698622\n",
      "0.14867114877101884 0.028035391955130946\n",
      "0.15037149947160405 0.028667417342416526\n",
      "0.14742324898391065 0.024388490582571417\n",
      "0.15028561995208298 0.030197163457720975\n",
      "0.1491986255620543 0.027367984820017018\n",
      "0.14791345708855963 0.03215742474788862\n",
      "0.14775375983440153 0.025105538585704343\n",
      "0.14744600484875267 0.029507702908401025\n",
      "0.14750815602158454 0.028665098825651\n",
      "0.1489578431669169 0.03167556672328345\n",
      "0.14845399665521994 0.025550779458967443\n",
      "0.14924282540898604 0.03543612605445475\n",
      "0.1505386912965876 0.031535816678906314\n",
      "0.15157187116275342 0.03281108695178359\n",
      "0.15106159156730564 0.028108162509199605\n",
      "0.15220705965378264 0.034483926683486855\n",
      "0.14915517226851419 0.032054582451328806\n",
      "0.1478561396662817 0.03061425996437897\n",
      "0.15026640516904688 0.02972623179775756\n",
      "0.14891620826992985 0.029111788598777368\n",
      "0.1476895605833717 0.028230666925347637\n",
      "0.14905055892752989 0.02951718844963395\n",
      "0.14948357178325522 0.03106125271560467\n",
      "0.1473543701488685 0.029839056187432204\n",
      "0.1483107677793629 0.030329628648573047\n",
      "0.1516435882187755 0.03331523420974824\n",
      "0.14999499484034748 0.02942350461796625\n",
      "0.15028796664369998 0.03089297334841998\n",
      "0.1475976181669604 0.03141587008268393\n",
      "0.1490676569918795 0.03458058628791925\n",
      "0.15046580097125895 0.03216526605342778\n",
      "0.14613888772412909 0.03157444535678679\n",
      "0.14851302569223987 0.02845900306413711\n",
      "0.15049527652557096 0.033835928764959285\n",
      "0.14955656904320655 0.031224495325781833\n",
      "0.15051833367724526 0.037660090166781617\n",
      "0.15058114910584286 0.030178862312619383\n",
      "0.14857586632148745 0.03272302383654764\n",
      "0.1492119031636427 0.03058348158492763\n",
      "0.14866049545358032 0.03324911942762764\n",
      "0.15221442871866914 0.03850800071375665\n",
      "0.14975524168093704 0.03486186277956502\n",
      "0.1499873658146596 0.03253214670646699\n",
      "0.1472499361258369 0.03113625569026948\n",
      "0.14914845955137948 0.030665967996399583\n",
      "0.15006414057012946 0.031067572206467175\n",
      "0.14849855083366825 0.03402724857160695\n",
      "0.14750351698981035 0.030029176570296016\n",
      "0.14826585909909448 0.03251853542613033\n",
      "0.14600755576724445 0.02881583627684869\n",
      "0.14708115377062192 0.027554419767429294\n",
      "0.14556959891424845 0.027397087027712017\n",
      "0.14645100620638024 0.02680353084124095\n",
      "0.1465701381861191 0.02630339909349449\n",
      "0.14613228422467084 0.028616318374712246\n",
      "0.14790585883696708 0.028724265778215324\n",
      "0.14737978938637347 0.02433577870079318\n",
      "0.14723813600929514 0.030605759596302635\n",
      "0.1457197663444782 0.028022564837926183\n",
      "0.14686212799770706 0.024810385642263948\n",
      "0.14433666495845765 0.025789254218527586\n",
      "0.14533875722023676 0.0226925405916455\n",
      "0.14443099890749356 0.023291069337795768\n",
      "0.1420420553497424 0.02477569319458469\n",
      "0.1440817078209712 0.024319534695993864\n",
      "0.1419382869803563 0.022421058054161575\n",
      "0.14433531654869028 0.027127030256583765\n",
      "0.14579524549408224 0.029341384207161796\n",
      "0.1503706050625633 0.030103238164004002\n",
      "0.15041559369390822 0.030723119603011217\n",
      "0.14965198098236468 0.028078219529045115\n",
      "0.14774615711604186 0.030312924399373643\n",
      "0.14768369730594708 0.028940801740652184\n",
      "0.14772320287997442 0.030369454120307997\n",
      "0.1476804806637413 0.03197767384808481\n",
      "0.14928787485786887 0.030298481626145717\n",
      "0.14998579021816502 0.028565241335459082\n",
      "0.15024090117518724 0.028579972072839573\n",
      "0.14948172613890373 0.03095866324995864\n",
      "0.15024057390700135 0.026730672117708937\n",
      "0.15198290726461255 0.02930401564275661\n",
      "0.15017581166330757 0.02954409707138949\n",
      "0.15048256643604238 0.03146060828641561\n",
      "0.1488739044363304 0.027640942045410264\n",
      "0.15054071939517122 0.027695989523655706\n",
      "0.14931350811354382 0.028126095402167317\n",
      "0.14956580449661766 0.022335918535336573\n",
      "0.14923428210263434 0.028408899341698032\n",
      "0.1480311545045053 0.02812968134203525\n",
      "0.1480340015774163 0.024613995176600505\n",
      "0.14583242662815343 0.02586049035033801\n",
      "0.1466611370642142 0.024644579416015427\n",
      "0.14546576361907043 0.02287047639070406\n",
      "0.14748735559593806 0.023008639411584784\n",
      "0.1469494389097864 0.02469144532913201\n",
      "0.14708678299487402 0.02486346036525573\n",
      "0.14781583810036808 0.026136454600422996\n",
      "0.14685098803615682 0.026887485138743013\n",
      "0.14962501258663183 0.028400745965879586\n",
      "0.1486802232780174 0.029483724369421756\n",
      "0.14968417264922185 0.027348845257522816\n",
      "0.15227177702399045 0.033056448654014116\n",
      "0.1498837927773875 0.0314872628130534\n",
      "0.14980801503509958 0.03038058188521519\n",
      "0.1493229373648069 0.03139090376893213\n",
      "0.15015682803350083 0.026699419732239358\n",
      "0.15146095822198988 0.030088981591386687\n",
      "0.1506195519272704 0.03345965806551148\n",
      "0.15278138460278465 0.03660856434268616\n",
      "0.15043986915280388 0.03582246107055638\n",
      "0.14940365485694884 0.029583162713541937\n",
      "0.1506764205972838 0.030703427232809673\n",
      "0.15199016021841022 0.03082392374443173\n",
      "0.15284447721125255 0.03428235610721003\n",
      "0.15388190479913622 0.03022663980014967\n",
      "0.15278756007917285 0.03315766051783891\n",
      "0.1526561110817609 0.03351785315847127\n",
      "0.15140028135816094 0.030729264746984137\n",
      "0.1518964970741806 0.03170425768937\n",
      "0.15458220933101016 0.033629706792633704\n",
      "0.152976783331493 0.03348273748431682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15245652588906716 0.03198774311547361\n",
      "0.15286715996775388 0.03326996890171346\n",
      "0.15412192116190543 0.029019132488549183\n",
      "0.15419642957633004 0.035434682344561985\n",
      "0.15433995495862807 0.03709293628468684\n",
      "0.15574835034118648 0.03303354965457896\n",
      "0.15505326982168643 0.03551112862059931\n",
      "0.15937608099891723 0.03528630689334713\n",
      "0.15716076756459138 0.03377466531832348\n",
      "0.1568022193613375 0.031138509556607505\n",
      "0.15322360011465833 0.032304248000965693\n",
      "0.15700719299597318 0.035276899818700184\n",
      "0.1577819749968263 0.03755814813086247\n",
      "0.15801757475467115 0.039539101646228966\n",
      "0.1608077651621636 0.0418057565195051\n",
      "0.15899719295864057 0.04812908981959807\n",
      "0.15972925338345884 0.04630273973598578\n",
      "0.15976349100337156 0.042706536129753565\n",
      "0.1601695969240812 0.04506974981368085\n",
      "0.15791579507694029 0.042467408628736394\n",
      "0.15893789816076473 0.04299895972559209\n",
      "0.1627490313455255 0.04587128563040534\n",
      "0.16133112345924458 0.045591503409809116\n",
      "0.15971269110689995 0.038719820864645114\n",
      "0.159417567900173 0.04087978721961302\n",
      "0.15735025252438806 0.036269463049093224\n",
      "0.15622362339865176 0.0394032260700721\n",
      "0.15593376414621424 0.037516879751864476\n",
      "0.15233525140613577 0.03264960512923981\n",
      "0.15600592032051003 0.03857221832841604\n",
      "0.15212326870875806 0.029788520809994924\n",
      "0.1524955432200384 0.0313765097789206\n",
      "0.15481242410427437 0.03418407831313969\n",
      "0.1574870185382591 0.041980125401719644\n",
      "0.1570283160787087 0.044987350796263016\n",
      "0.15592622165968276 0.036252517799002355\n",
      "0.15328830559821374 0.03171469807109435\n",
      "0.15111127368795535 0.030409870012634087\n",
      "0.1537678364559478 0.03056052780408548\n",
      "0.15739685644688156 0.038773316548219326\n",
      "0.15846785477599723 0.038551291514246355\n",
      "0.15861300596746114 0.045244755284646256\n",
      "0.15767956804268904 0.03910155888684241\n",
      "0.16027125782654278 0.05248580597563925\n",
      "0.15878462671790639 0.04398178427269472\n",
      "0.15881846163574923 0.041061822222030905\n",
      "0.15921308167188716 0.04262524285454274\n",
      "0.16030616510436815 0.04216050160037255\n",
      "0.16170232517077912 0.04547071036227155\n",
      "0.16171859782639003 0.045456331690829584\n",
      "0.16113732110907747 0.04280145489339036\n",
      "0.16277454608933206 0.0454083610746157\n",
      "0.1621139702933361 0.04436867360906278\n",
      "0.15633305692300545 0.04082913638542335\n",
      "0.15714747308383087 0.04250562632317109\n",
      "0.1622214097876666 0.04153171283721516\n",
      "0.15848359775041254 0.04716275300132954\n",
      "0.1603100740732779 0.04183571444823483\n",
      "0.15961086922966497 0.04127854603664778\n",
      "0.16058108455215428 0.042038673952864335\n",
      "0.16099271092609063 0.04268199662746114\n",
      "0.16016845129830007 0.0432376148201032\n",
      "0.15886973577309949 0.04284685517713828\n",
      "0.15883584229987768 0.040941949526167476\n",
      "0.15596868879529974 0.03930449457841899\n",
      "0.15622216140583833 0.03723367894450377\n",
      "0.15607549980819968 0.033702543617040934\n",
      "0.15518835803351505 0.03784813567444764\n",
      "0.1540450029554508 0.027692528053047467\n",
      "0.15425460529160848 0.028348126384618307\n",
      "0.15339674067856357 0.03110416214073507\n",
      "0.15383017176464062 0.028927517012804438\n",
      "0.1505786328257135 0.03280511108709804\n",
      "0.15045641153188025 0.026572480216431778\n",
      "0.15209954090451755 0.031867992576870924\n",
      "0.15154947041007474 0.030340496734348764\n",
      "0.15308308550432614 0.028997366167453485\n",
      "0.15338034470185094 0.03399094098866266\n",
      "0.15288189233828736 0.027557457487433313\n",
      "0.15279818389193892 0.030124416656848244\n",
      "0.1516427625658864 0.028027745714990088\n",
      "0.1525092437292663 0.025989257148852925\n",
      "0.15304747658635548 0.03051834427004714\n",
      "0.1522131915094121 0.02871459038728074\n",
      "0.15040698207242564 0.025206536562809048\n",
      "0.15014004861116748 0.02391030845672856\n",
      "0.14835722227997528 0.023706498260458116\n",
      "0.14924568321046733 0.024005939415551156\n",
      "0.14915147793426414 0.024847668036033756\n",
      "0.14765703029216654 0.022583869178364725\n",
      "0.14833787087537886 0.02167550775078977\n",
      "0.1475870403256748 0.0226133591746957\n",
      "0.14651812237870993 0.022859962665469562\n",
      "0.14741416094863286 0.021438244546555525\n",
      "0.1505033797908127 0.024563672381786517\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set[:,i:i+1]\n",
    "        Q, Alpha_Q = wake_forward(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_delta(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        P, Alpha_P = sleep_forward(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_delta(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate with this model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to examine the distribution of generation. We let the model generate, say 10000 instances, then use the samples to approximate its generative distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know there are 1024 combinations in total, which gives 1024 categories. Here we use integer numbers 0-1023 to represent these combinations and for visualization, we assign 0-255 as our well-formed set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  1., ..., -1.,  1., -1.],\n",
       "       [ 1.,  1., -1., ...,  1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "entire_set = np.zeros((2,10))\n",
    "entire_set[0,0] = 1\n",
    "for i in range(1,n):\n",
    "    for j in range(entire_set.shape[0]):\n",
    "        entire_set = np.append(entire_set, entire_set[j:j+1,:], axis=0)\n",
    "        entire_set[j,i] = 1\n",
    "entire_set = (entire_set - 0.5)*2\n",
    "entire_set = np.transpose(entire_set)\n",
    "entire_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., ...,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       ...,\n",
       "       [-1., -1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1., -1., ..., -1., -1., -1.],\n",
       "       [-1.,  1., -1., ..., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_set = np.unique(entire_set, axis=1)\n",
    "well_formed_set = np.unique(well_formed_set, axis=1)\n",
    "reordered_set = np.zeros(entire_set.shape)\n",
    "reordered_set[:,:well_formed_set.shape[1]] = well_formed_set\n",
    "\n",
    "k = well_formed_set.shape[1]\n",
    "for i in range(entire_set.shape[1]):\n",
    "    flag = 0\n",
    "    for j in range(well_formed_set.shape[1]):\n",
    "        if np.array_equal(entire_set[:,i], well_formed_set[:,j]):\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 0:\n",
    "        reordered_set[:,k] = entire_set[:,i]\n",
    "        k += 1\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reordered_set, we set the first 256 columns as the well-formed set and the rest as negative samples outside of well-formed bound. Now we calculate the distribution of generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([243, 237, 247, ..., 211, 244, 253])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    6,   13,   14,   15,   17,   18,   19,   22,   25,   26,\n",
       "          28,   29,   30,   31,   32,   33,   34,   35,   36,   38,   39,\n",
       "          40,   41,   45,   46,   47,   48,   50,   51,   52,   53,   54,\n",
       "          55,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n",
       "          67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n",
       "          78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n",
       "          90,   91,   94,   95,   96,   97,   98,  100,  103,  104,  105,\n",
       "         106,  107,  109,  110,  112,  113,  114,  115,  116,  117,  118,\n",
       "         119,  120,  121,  123,  125,  126,  127,  128,  129,  130,  131,\n",
       "         132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,\n",
       "         143,  144,  146,  147,  148,  149,  150,  151,  152,  153,  154,\n",
       "         155,  156,  157,  158,  159,  160,  161,  162,  163,  164,  165,\n",
       "         166,  167,  168,  169,  170,  171,  172,  173,  174,  175,  176,\n",
       "         177,  178,  179,  180,  181,  182,  183,  184,  185,  186,  187,\n",
       "         188,  189,  190,  191,  192,  193,  194,  195,  196,  197,  198,\n",
       "         199,  200,  201,  202,  203,  204,  205,  206,  207,  208,  209,\n",
       "         210,  211,  212,  213,  214,  215,  216,  217,  218,  219,  220,\n",
       "         221,  222,  223,  224,  225,  226,  227,  228,  229,  230,  231,\n",
       "         232,  233,  234,  235,  236,  237,  238,  239,  240,  241,  242,\n",
       "         243,  244,  245,  246,  247,  248,  249,  250,  251,  252,  253,\n",
       "         254,  255,  901,  921,  923,  930,  948,  960,  961,  963,  970,\n",
       "         971,  972,  973,  980,  988,  989,  990,  991,  992,  993,  997,\n",
       "         998, 1002, 1009, 1010, 1011, 1012, 1016, 1019, 1020, 1021],\n",
       "       [   1,    1,    1,    3,    1,    1,    1,    1,    2,    2,    1,\n",
       "           3,    1,    1,    3,    3,    1,    4,    2,    3,    2,    2,\n",
       "           1,    3,    1,    2,    1,    1,    3,    2,    1,    2,    4,\n",
       "           3,    2,    6,    2,    2,    9,    8,    5,    3,    5,    7,\n",
       "           5,    3,    8,    3,   15,    4,   12,    3,    9,   17,   13,\n",
       "          10,   14,   12,    5,   15,   20,   17,   10,   15,   13,   15,\n",
       "           6,    1,    2,    1,    1,    6,    1,    2,    2,    1,    1,\n",
       "           1,    1,    3,    7,    1,    2,    3,    2,    8,    5,    2,\n",
       "           3,    4,    1,    4,    6,    3,    2,    1,    4,    7,    7,\n",
       "          14,    8,    9,   13,   11,   10,   22,   26,   17,   11,    4,\n",
       "           3,    5,    7,    9,   10,   13,   13,   13,    7,    6,    9,\n",
       "          11,   24,   36,   18,   21,   27,   21,   25,   30,   36,   29,\n",
       "          24,    2,    4,    1,    6,    9,    5,   16,   34,    9,   13,\n",
       "           7,   15,   14,   22,   29,   24,   18,    8,    4,    8,    2,\n",
       "           9,   20,   18,   23,   13,   27,   22,   28,   23,   28,   33,\n",
       "          50,   29,   41,   59,   39,   48,   64,   56,   55,   48,   29,\n",
       "          12,   24,   26,   39,   22,   49,   42,   42,   72,   67,   70,\n",
       "         120,   86,   94,   51,  113,  103,  127,  178,  135,  132,   94,\n",
       "          99,  113,  104,  151,  123,  184,  263,  148,  204,  238,  286,\n",
       "         211,  221,  185,  276,  322,  317,  262,  408,  359,  342,  320,\n",
       "         502,  494,    1,    1,    1,    3,    1,    2,    2,    1,    1,\n",
       "           1,    1,    1,    4,    1,    3,    3,    1,    4,    1,    5,\n",
       "           9,   11,   11,   11,   17,    8,    2,   55,    1,    1]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.zeros((2,counts.size),dtype=int)\n",
    "statistics[0,:] = values\n",
    "statistics[1,:] = counts\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16,  14,  10,  12,  16,  17,  21,  14,  18,  32,  28,  23,  19,\n",
       "        19,  18,  19,  17,  36,   8,   1,   6,   3,   1,   1,   1,   1,\n",
       "         1,   4,   1,   1,   3,  12,   6,   2,   5,   3,   2,   5,   7,\n",
       "         8,   8,   4,   3,   1,   5,   5,   5,   2,   6,   2,   7,   5,\n",
       "         3,   8,  13,   4,  10,   5,   4,   9,  13,  22,  13,   4,   2,\n",
       "        10,   9,   4,  16,   7,   5,  10,  22,  22,   7,  17,  14,  23,\n",
       "        23,  36,  18,  24,  15,  19,  14,  32,  26,  31,  18,   3,   6,\n",
       "         2,  10,   6,   4,  11,  16,  11,  13,  14,  13,  14,  22,  32,\n",
       "        14,  12,   8,  17,  26,  13,  34,  27,  20,  24,  36,  46,  30,\n",
       "        27,  25,  33,  49,  88,  48,  58,  47,  54,  40,  64,  70,  71,\n",
       "        41,  18,  23,  48,  28,  42,  77,  68,  74,  87,  60,  55,  55,\n",
       "       125, 101,  91,  97, 120, 121, 135, 132, 176, 118,  68, 104, 115,\n",
       "        73, 206, 140, 123, 198, 251, 142, 245, 329, 187, 212, 137, 293,\n",
       "       310, 380, 198, 318, 250, 260, 195, 442, 236,   1,   2,   1,   2,\n",
       "         3,   1,   2,   1,   1,   1,   2,   1,   1,   1,   1,   3,   2,\n",
       "         1,   1,   1,   1,   3,   2,   3,   1,   7,   1,   2,   1,   2,\n",
       "         6,   2,   7,   1,   5,   7,   1,   4,   1,   8,   2,   2,   2,\n",
       "        12,  13,   4,  39,   4,   2,   2,   3,  46,  22,  11,   8],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values > 69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set[:,1019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': array([[ 0.03542588, -0.40604692,  0.3781762 , -0.49153731, -0.08397746,\n",
       "         -0.2010533 ,  0.33549308,  0.07632863,  9.5571646 ],\n",
       "        [ 0.01119892,  0.08971083, -0.27453663, -3.37715804, -7.07492649,\n",
       "         -0.33290296, -0.24189164, -0.43415471, -2.5426537 ],\n",
       "        [ 0.59657965,  0.02172891,  1.09085874,  1.87806213,  2.99995893,\n",
       "          1.42338677, -0.25050324,  0.97784309,  6.23538604],\n",
       "        [ 0.02987976,  0.02188166, -1.03303348,  0.0451028 , -0.07219841,\n",
       "         -0.70036023, -0.16251728, -1.88482923,  3.95338621],\n",
       "        [ 0.04544263, -0.45661886, -1.00715182,  0.72660779, -0.04020447,\n",
       "         -1.10307841, -0.43866377,  4.04857463, -1.87613424],\n",
       "        [ 0.44621878,  0.13255316, -0.15442986,  1.17574402, -0.44753401,\n",
       "         -0.75127768,  4.35156392,  0.22097469,  0.84495479],\n",
       "        [ 0.2386661 , -2.66350125,  0.15657567, -0.76012247,  0.06498884,\n",
       "         -2.37754893, -0.1609274 , -0.01633704, -0.35521378],\n",
       "        [-0.04039548,  4.80963361,  0.0431661 , -1.46310335, -0.04339565,\n",
       "          0.60890725,  0.0476896 ,  0.034695  ,  2.93487061],\n",
       "        [ 0.26493845, -3.76745832, -3.06309888, -0.14424858,  0.48821655,\n",
       "          6.20526476,  0.8079419 , -0.35739714, -0.24027199],\n",
       "        [-0.84038056,  3.01823361,  0.22497551, -0.21383416, -0.07563949,\n",
       "          3.0026332 , -0.08911153, -0.03091262,  0.8664792 ]]),\n",
       " 'Theta_21': array([[ 0.13834757, -1.1032342 , -0.65702843, -0.93864532, -6.39386718,\n",
       "          1.00006953],\n",
       "        [ 2.63670513, -0.04615699, -0.55150957,  1.42832412,  0.29474524,\n",
       "         -0.3618158 ],\n",
       "        [ 3.18182444,  0.43928229, -0.15038067,  0.51691047,  0.26598141,\n",
       "          0.84808502],\n",
       "        [ 0.17590818,  1.56807153,  5.75201499,  0.95979295,  0.91105035,\n",
       "         -0.99199313],\n",
       "        [ 0.07649044, -0.41097181,  4.0329676 ,  0.85659803,  0.73371237,\n",
       "          1.63540575],\n",
       "        [-6.98240829, -0.05079599, -0.09689674,  0.39759329, -0.33359964,\n",
       "          0.36169848],\n",
       "        [ 0.07445349,  0.09558038,  0.7262562 , -0.27252009, -0.51426793,\n",
       "          0.95490737],\n",
       "        [ 0.2058662 , -0.82083136,  0.17567063, -0.36134698,  0.04116515,\n",
       "          2.4368956 ]]),\n",
       " 'Theta_32': array([[-4.37708958, -0.70444629, -0.56823187, -2.08273235],\n",
       "        [ 0.54560214, -1.48861031, -1.22793189, -4.93108175],\n",
       "        [ 0.21050672,  0.5851406 , -1.3993201 , -3.77016032],\n",
       "        [-0.63171673, -1.04833937, -1.65347785, -4.378647  ],\n",
       "        [-0.77581499, -1.37411683, -1.45616558, -2.98527544]]),\n",
       " 'Theta_k': array([[0.03705406],\n",
       "        [6.15761987],\n",
       "        [7.25959567]])}"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
