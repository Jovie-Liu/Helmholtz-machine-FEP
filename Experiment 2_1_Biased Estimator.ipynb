{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 2</span> \n",
    "\n",
    "# Helmholtz Machine as Biased Generator\n",
    "\n",
    "By experiment, I found that the Helmholtz machine is a biased generator under the original training mechanism. In this notebook,  I will further examine the property of the Helmholtz machine and hope to train a generator that recovers not only the data but also its distribution by modifying the local delta rule.\n",
    "\n",
    "*Updated:* December 9, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       [ 1., -1.,  1., ...,  1., -1.,  1.],\n",
       "       [ 1.,  1., -1., ...,  1.,  1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "well_formed_set = np.zeros([1,n])\n",
    "well_formed_set[0,0] = 1\n",
    "\n",
    "for i in range(1,n):\n",
    "    for j in range(np.shape(well_formed_set)[0]):\n",
    "        if i == 2 and np.array_equal(well_formed_set[j,i-2:i], [1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-3:i], [0,0,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-4:i], [0,0,1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        else:\n",
    "            well_formed_set = np.append(well_formed_set, well_formed_set[j:j+1,:], axis=0)\n",
    "            well_formed_set[j,i] = 1\n",
    "            \n",
    "ind = np.array([], dtype=np.int8)\n",
    "for i in range(well_formed_set.shape[0]):\n",
    "    if np.array_equal(well_formed_set[i,-3:], [0,0,1]):\n",
    "        ind = np.append(ind,i)\n",
    "\n",
    "well_formed_set = np.delete(well_formed_set,ind,0)\n",
    "well_formed_set = (well_formed_set - 0.5)*2\n",
    "well_formed_set = np.transpose(well_formed_set)\n",
    "well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Helmholtz machine structure is given as below:\n",
    "\n",
    "<img src=\"Helmz.jpg\" style=\"width:550px\">\n",
    "<caption><center> **Figure 4**: The Helmholtz Machine  </center></caption>\n",
    "\n",
    "I previously wrote in Document 2 about the variational objective function and deduction of the parameter updating rules of the Helmholtz machine, and I will give a more systematic analysis on these matters in upcoming notebooks, so here let's skip these steps and say we know the parameter updating local delta rule is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{F}}{\\partial \\phi_{k,n}^{m-1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m-1}(1-q_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m-1} \\centerdot q_n^m & \\text{if } s_n^m = -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial \\theta_{k,n}^{m+1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m = -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Let's write the functions to run this model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always have the bottom layer as data input, denoted as $d_0$. In our case illustrated in Figure 4, we have $m = 4$ layers, with various numbers of neurons, $n_{d_0} = 10$, $n_{z_1} = 8$, $n_{z_2} = 5$, $n_{z_3} = 3$. These are the hyperparameters to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(init_type,n_dz):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (m, ), where m is the number of layers\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \"\"\"\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    m = len(n_dz)\n",
    "    if init_type == \"zero\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((n_dz[i+1],n_dz[i]+1))\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.zeros((n_dz[i],n_dz[i+1]+1))\n",
    "        Theta[\"Theta_k\"] = np.zeros((n_dz[-1],1))\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(n_dz[i+1],n_dz[i]+1)\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.random.randn(n_dz[i],n_dz[i+1]+1)\n",
    "        Theta[\"Theta_k\"] = np.random.randn(n_dz[-1],1)\n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "    return Phi, Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- Probability of sampled recognition assignment, number in (0,1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = 1\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0)))\n",
    "        S = ((q > np.random.rand(len(q),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        Q = Q * np.cumprod(S*q + (1-S)/2)[-1] * 2**q.size  # S takes {-1,1}\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353.1439503620823"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, Alpha_Q = wake_forward(d0,Phi)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward_expectation(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- Probability of sampled recognition assignment, number in (0,1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0))) \n",
    "        S = q  # use q as expectation assignment, which indicates the hidden units binary values take {0,1}\n",
    "        Alpha_Q[\"z\"+str(i+1)] = q\n",
    "    return Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = well_formed_set[:,5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha_Q = wake_forward_expectation(d0,Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- Probability of sampled generative assignment, number in (0,1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    \n",
    "    m = len(Theta)\n",
    "    P = np.cumprod(S*p + (1-S)/2)[-1] * 2**p.size\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        P = P * np.cumprod(S*p + (1-S)/2)[-1] * 2**p.size\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    return P, Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.73143630509956"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, Alpha_P= sleep_forward(Theta)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward_expectation(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- Probability of sampled generative assignment, number in (0,1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = p # use p as expectation assignment, which indicates the hidden units binary values take {0,1}\n",
    "    \n",
    "    m = len(Theta)\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,1,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = p # use p as expectation assignment, which indicates the hidden units binary values take {0,1}\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    p = sigmoid(np.matmul(Theta[\"Theta_10\"],np.append(S,[[1]], axis=0)))\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling for output\n",
    "    Alpha_P[\"z0\"] = S\n",
    "    return Alpha_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Expectation rule**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{F}}{\\partial \\phi_{k,n}^{m-1,m}} = s_k^{m-1}(q_n^m - p_n^m)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial \\theta_{k,n}^{m+1,m}} = s_k^{m+1}(p_n^m - q_n^m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    for i in range(n):\n",
    "        S_bias = np.append(Alpha_P[\"z\"+str(i)],[[1]], axis=0)\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S_bias))\n",
    "        gain = q - (1+Alpha_P[\"z\"+str(i+1)])/2\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, info_gain_wake,error_Q = wake_update_delta(Phi,Alpha_P,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information gain, we can read the error of the given assignment *Alpha_P* for parameters *Phi*, on each neuron it computes. The value of info_gain is within $[-1,1]$, where positive value indicates the current neuron takes value $-1$, while negative value indicates that the current neuron takes value $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_expectation(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        S_bias = np.append(Alpha_P[\"z\"+str(i)],[[1]], axis=0)\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S_bias))\n",
    "        gain = q - Alpha_P[\"z\"+str(i+1)]\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, info_gain_wake,error_Q = wake_update_expectation(Phi,Alpha_P,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - (1+Alpha_Q[\"z\"+str(n-1)])/2\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,0,-1):\n",
    "        S_bias = np.append(Alpha_Q[\"z\"+str(i)],[[1]], axis=0)\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S_bias))\n",
    "        gain = p - (1+Alpha_Q[\"z\"+str(i-1)])/2\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_expectation(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - Alpha_Q[\"z\"+str(n-1)]\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,1,-1):\n",
    "        S_bias = np.append(Alpha_Q[\"z\"+str(i)],[[1]], axis=0)\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S_bias))\n",
    "        gain = p - Alpha_Q[\"z\"+str(i-1)]\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "        \n",
    "    S_bias = np.append(Alpha_Q[\"z1\"],[[1]], axis=0)\n",
    "    p = sigmoid(np.matmul(Theta[\"Theta_10\"],S_bias))\n",
    "    gain = p - (1+Alpha_Q[\"z0\"])/2\n",
    "    Theta[\"Theta_10\"] -= lr * np.outer(gain,S_bias)\n",
    "    info_gain_sleep[\"z0\"] = gain # positive(s=-1) & negative(s=1)\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "well_formed_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like stochastic gradient descent, we update the parameters for each data input (no batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22976467598426914 0.0998830132602375\n",
      "0.16198684112214215 0.02003261412546685\n",
      "0.1488447123489608 0.00557447666957883\n",
      "0.14532844522800833 0.0022320495561867775\n",
      "0.1445487361742158 0.0012723708157633157\n",
      "0.14389617260390217 0.00036119492996504216\n",
      "0.14386894592540866 0.0004815711459145259\n",
      "0.14379846219334358 0.0002378820664904722\n",
      "0.14356088925989383 6.383666025583678e-05\n",
      "0.14381797336787555 0.00048419803399050814\n",
      "0.14365522276394863 0.00014742970336214986\n",
      "0.14351956539466196 4.614003318067569e-05\n",
      "0.14347577968748187 1.5580761305048133e-05\n",
      "0.14345795393688685 6.231388974197892e-06\n",
      "0.14344851990459864 2.276146520173093e-06\n",
      "0.14379289093023415 0.0004849002515404571\n",
      "0.14353513530156956 8.577231936732029e-05\n",
      "0.14347466891231292 3.126958523844601e-05\n",
      "0.14345388422557961 1.0778200474256022e-05\n",
      "0.14344634678927784 5.4506945984488236e-06\n",
      "0.14388741811966804 0.0007333641424032892\n",
      "0.1437289888396802 0.00020758129454119696\n",
      "0.14352101986706198 5.226815254661716e-05\n",
      "0.1434666759976685 1.676668778443905e-05\n",
      "0.14344941306306694 5.547299421806027e-06\n",
      "0.14344321848809424 2.4292798847980025e-06\n",
      "0.14344012221674962 9.833011141319948e-07\n",
      "0.14343840015093356 4.912762625658353e-07\n",
      "0.14355694117918763 0.0002644422295583099\n",
      "0.1437231777475556 0.00024154930851958485\n",
      "0.1435156597840189 6.340430019088641e-05\n",
      "0.1434611611876079 1.9773358371183093e-05\n",
      "0.1434441847504216 6.062952967435728e-06\n",
      "0.14343819974583102 2.2070084555607698e-06\n",
      "0.1434354041408284 8.619070355800646e-07\n",
      "0.1434340452464969 3.489680393590786e-07\n",
      "0.14343319942454788 1.45690969853782e-07\n",
      "0.1434325306185121 7.046067165935221e-08\n",
      "0.14343195843014173 2.8532797022877605e-08\n",
      "0.14343143773259248 1.435379352392695e-08\n",
      "0.14343095184617435 7.228520640583725e-09\n",
      "0.1435926800491234 0.00032184915040530814\n",
      "0.14363344188542723 0.00016058343944483637\n",
      "0.14349086407168202 3.618177204888555e-05\n",
      "0.14345597966356813 1.0955247328003258e-05\n",
      "0.14344433623180697 4.026466823315401e-06\n",
      "0.14344056985252057 1.479181692710882e-06\n",
      "0.14343891464708977 5.859842208796097e-07\n",
      "0.1434380675243579 2.15410248543555e-07\n",
      "0.1434375739552588 9.368308702557002e-08\n",
      "0.14343720419035735 4.7768071481797106e-08\n",
      "0.1434368834974635 2.047515806747035e-08\n",
      "0.1434365895783744 1.0623214229261416e-08\n",
      "0.1434363106794477 5.220059994597514e-09\n",
      "0.14343604562801213 2.2939862121757804e-09\n",
      "0.14343579191310968 1.0285665525883495e-09\n",
      "0.14343554791694815 4.722716679330546e-10\n",
      "0.14343531282780364 2.0987594150247537e-10\n",
      "0.14343508599349877 9.643484191664562e-11\n",
      "0.1434348669030074 4.3061625520522586e-11\n",
      "0.1434346551284248 2.280687896432645e-11\n",
      "0.14343445028886548 1.1532047560804115e-11\n",
      "0.14343425204006705 6.087772050789725e-12\n",
      "0.14343406006886458 2.700275990492624e-12\n",
      "0.1434338740802489 1.2537017119438644e-12\n",
      "0.1434336937974653 5.529188286152072e-13\n",
      "0.1434335189616707 2.8588868333226033e-13\n",
      "0.14343334932935803 1.2881134960757157e-13\n",
      "0.1434331846716002 5.842160387186865e-14\n",
      "0.14343302477270625 2.5298651520032356e-14\n",
      "0.14343286942928676 1.1527996439386623e-14\n",
      "0.1434327184494021 4.292071797799835e-15\n",
      "0.14343257165175383 2.0227688881518935e-15\n",
      "0.14343242886494686 9.667763009417463e-16\n",
      "0.14343228992682755 5.038694071354821e-16\n",
      "0.14343215468386744 2.36166338221655e-16\n",
      "0.14343202299059687 1.394148468544798e-16\n",
      "0.14343189470908252 7.488448880085818e-17\n",
      "0.14343176970844623 3.966147072223536e-17\n",
      "0.1434316478644185 3.237021111227034e-17\n",
      "0.1434315290589279 3.469446951953614e-17\n",
      "0.14343141317971875 3.6422416731934915e-17\n",
      "0.14343130011999705 4.046106982444342e-17\n",
      "0.14343118977810296 4.352394096171497e-17\n",
      "0.14343108205720595 4.447939412621782e-17\n",
      "0.14343097686502063 4.5380637182096395e-17\n",
      "0.14343087411354372 4.540774223640853e-17\n",
      "0.14343077371880894 4.2412633734917327e-17\n",
      "0.1434306756006576 3.4328551286322284e-17\n",
      "0.14343057968252518 3.264803791896975e-17\n",
      "0.14374160803201022 0.00043216888423595044\n",
      "0.14350008878050083 6.0003596473101604e-05\n",
      "0.14345721585264454 1.965758570804112e-05\n",
      "0.14344314184731047 7.968518108204264e-06\n",
      "0.1434379873053137 3.434840480445374e-06\n",
      "0.14343574697943015 1.4470406188245828e-06\n",
      "0.1434347269737445 6.824051371434017e-07\n",
      "0.1434342204397644 3.1291260262808994e-07\n",
      "0.1434339181077455 1.4648060615144224e-07\n",
      "0.14343373994712266 6.795732275031958e-08\n",
      "0.1434336174874511 3.104323706066051e-08\n",
      "0.14343352078428753 1.6282525703225584e-08\n",
      "0.14343343626730629 6.48988838911806e-09\n",
      "0.14343335998538334 3.032644249099741e-09\n",
      "0.14343328766778296 1.3694486008782123e-09\n",
      "0.14343321765780448 6.283228199004446e-10\n",
      "0.14343314953606165 2.80497186255454e-10\n",
      "0.14343308293346352 1.3579538158552985e-10\n",
      "0.1434330176783865 5.557743969089677e-11\n",
      "0.14373657055106664 0.0003699857820436714\n",
      "0.14350702312069585 5.896934518139471e-05\n",
      "0.143462500711852 1.56630766155006e-05\n",
      "0.14344775720354708 6.547369041336678e-06\n",
      "0.1434425436035069 2.5496529802599023e-06\n",
      "0.14344056912036918 9.368710894714822e-07\n",
      "0.14343978338369265 3.442438489967289e-07\n",
      "0.14343948538300488 1.423379454423547e-07\n",
      "0.14343934444930415 6.475898534219748e-08\n",
      "0.1434392548451559 3.060589869751827e-08\n",
      "0.14343918423439853 1.5025870443142965e-08\n",
      "0.1434391213876788 7.445689714099765e-09\n",
      "0.14343906396772416 3.5290222349674476e-09\n",
      "0.14343901037566203 1.6995422005695155e-09\n",
      "0.14343895881904803 7.08888630359273e-10\n",
      "0.14343890880779805 3.4292154141665364e-10\n",
      "0.14343885985202867 1.6696344485336813e-10\n",
      "0.14343881179485254 7.102039736742258e-11\n",
      "0.14343876457293073 3.4033398190293825e-11\n",
      "0.14343871811406966 1.6198435869279725e-11\n",
      "0.14343867238927352 7.230663157521515e-12\n",
      "0.1434386273754454 3.1123244985994253e-12\n",
      "0.14343858305161716 1.3970347705820167e-12\n",
      "0.1434385394010823 5.71220249378189e-13\n",
      "0.14343849640745168 2.4974111862731407e-13\n",
      "0.1434384540557527 1.1482949194372566e-13\n",
      "0.1434384123315058 5.181944961020826e-14\n",
      "0.14343837122083625 2.3767473449412546e-14\n",
      "0.14343833071026926 9.939125260663428e-15\n",
      "0.143438290786756 4.504467003389745e-15\n",
      "0.1434382514376184 2.2157704273814693e-15\n",
      "0.14343821265054824 9.520175988687873e-16\n",
      "0.14343817441358706 3.95123929235186e-16\n",
      "0.14343813671511402 1.75105427119987e-16\n",
      "0.14343809954383416 8.533348723818723e-17\n",
      "0.1435243408303478 0.00022527372405497668\n",
      "0.14364145490911845 0.00022042667919906287\n",
      "0.14347841566264793 3.816168545905549e-05\n",
      "0.1434526687838106 1.2134033175963092e-05\n",
      "0.1434449243219349 4.17556286093325e-06\n",
      "0.1434424270398312 1.8537694539221724e-06\n",
      "0.14344118080425944 7.673060258854946e-07\n",
      "0.14344069857960165 3.7726043683377415e-07\n",
      "0.14344041515880346 1.698467243987231e-07\n",
      "0.1434402704281484 7.886676429349218e-08\n",
      "0.14344018086786575 3.421443973767506e-08\n",
      "0.14344012575739018 1.5678440586559567e-08\n",
      "0.14344008284688278 7.3566224121786295e-09\n",
      "0.14344004706685906 3.6074896671517507e-09\n",
      "0.1434400138268064 1.6387677603609701e-09\n",
      "0.14343998283165946 8.258494463170307e-10\n",
      "0.14343995258533068 4.0876088794425825e-10\n",
      "0.143439922926008 2.1179375066888648e-10\n",
      "0.143439893814248 1.0089083039876209e-10\n",
      "0.14368465825872698 0.00034995192786837657\n",
      "0.14356518075896718 9.944767470270352e-05\n",
      "0.14347389576857988 2.1443420098106656e-05\n",
      "0.14345435048738 6.17285520626825e-06\n",
      "0.14344868155396415 1.9008912770115911e-06\n",
      "0.14344724276121154 7.612808938660233e-07\n",
      "0.14344663864473028 3.4708360907100314e-07\n",
      "0.14344639229463607 1.4798542911851058e-07\n",
      "0.14344626715851308 7.21305440411755e-08\n",
      "0.14344619764708402 3.451221146248061e-08\n",
      "0.14344614909741865 1.5262110808385027e-08\n",
      "0.14344611504986113 8.704057707239706e-09\n",
      "0.14344608325480776 3.8851716576649375e-09\n",
      "0.14344605608353656 2.0544466283896855e-09\n",
      "0.14344603016601482 8.549776900386853e-10\n",
      "0.14344600548581882 3.8877941691732934e-10\n",
      "0.1434459813587261 1.6465638006296678e-10\n",
      "0.14344595772196098 8.028270531549621e-11\n",
      "0.1434459344054593 3.584395484552494e-11\n",
      "0.14344591138878124 1.9157854983127814e-11\n",
      "0.14344588863508867 8.26908714976959e-12\n",
      "0.1434458661372217 3.716827511729675e-12\n",
      "0.14344584388683343 1.505069289683969e-12\n",
      "0.14344582187814858 7.619183468822108e-13\n",
      "0.14344580010559532 3.2888751807678626e-13\n",
      "0.14344577856540233 1.377528936730675e-13\n",
      "0.14344575725356776 6.686230074378491e-14\n",
      "0.1434457361664253 3.0698540768887145e-14\n",
      "0.14344571530038822 1.5936972336434707e-14\n",
      "0.14344569465199233 6.977410186084776e-15\n",
      "0.14344567421784066 2.9404985933158267e-15\n",
      "0.14344565399461454 1.3643396850650247e-15\n",
      "0.14344563397906254 6.521882643314991e-16\n",
      "0.14362177451306485 0.00029651266844537427\n",
      "0.14359993351334205 0.00015374456629331978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14349332501856965 4.396973690934385e-05\n",
      "0.1434638119813436 1.4717102412614632e-05\n",
      "0.14345319555211922 6.025328880621728e-06\n",
      "0.14344888406130113 2.5237906841068345e-06\n",
      "0.14344703199258205 1.2011400551109117e-06\n",
      "0.1436610031848033 0.0002904127691555871\n",
      "0.14361101938587612 0.00011824404931537442\n",
      "0.1434958773955706 3.117049062912567e-05\n",
      "0.14346406338654177 8.601230814547875e-06\n",
      "0.14345579974999909 2.796285700143378e-06\n",
      "0.14345321083002574 9.264992701183013e-07\n",
      "0.143452415370206 3.6278273525267913e-07\n",
      "0.14345205026410707 1.6103326872883014e-07\n",
      "0.14345189906164685 7.542820404030223e-08\n",
      "0.14345182358989172 2.9469277950058977e-08\n",
      "0.1434517852026696 1.3208630784707737e-08\n",
      "0.1434517588590751 6.140256783536131e-09\n",
      "0.1434517391461197 3.1344722368597654e-09\n",
      "0.14345172081318724 1.3442331415800088e-09\n",
      "0.14345170372522822 5.848049042093441e-10\n",
      "0.14345168728080057 3.0287337951349783e-10\n",
      "0.1434516711808711 1.3423515198566138e-10\n",
      "0.1434516553300886 6.283019016086605e-11\n",
      "0.1434516396655082 3.0454809037312475e-11\n",
      "0.14345162417752988 1.3514866214537659e-11\n",
      "0.1434516088370514 7.024517035755795e-12\n",
      "0.1434515936386084 3.2577933135446296e-12\n",
      "0.14345157857802887 1.5097974885819147e-12\n",
      "0.14345156365232142 7.177196459221859e-13\n",
      "0.1434515488589608 3.1474132246864586e-13\n",
      "0.1434515341960152 1.4179119439986995e-13\n",
      "0.14345151966152322 6.399516875622846e-14\n",
      "0.1434515052537546 2.7956701894888553e-14\n",
      "0.14345149097101717 1.3093611481510004e-14\n",
      "0.1434514768116803 5.8800281809399946e-15\n",
      "0.14345146277414628 2.518012111752538e-15\n",
      "0.14345144885684727 1.2178368665079209e-15\n",
      "0.14345143505824395 6.179681332624254e-16\n",
      "0.1434514213768237 2.7327993383854943e-16\n",
      "0.1434514078110999 1.4832563345959504e-16\n",
      "0.14345139435961127 7.686315776564423e-17\n",
      "0.14345138102092028 5.975309223110736e-17\n",
      "0.1434513677936145 5.673087867530402e-17\n",
      "0.1434513546763046 5.566700529355262e-17\n",
      "0.14345134166762408 5.553148002199193e-17\n",
      "0.14345132876622915 5.471155212904977e-17\n",
      "0.14345131597079822 5.697482416411326e-17\n",
      "0.1434513032800311 5.262446294701517e-17\n",
      "0.1434512906926486 5.860790368641955e-17\n",
      "0.14345127820739284 5.839106325192245e-17\n",
      "0.143451265823026 5.4874182454922593e-17\n",
      "0.1434512535383299 5.480641981914225e-17\n",
      "0.14345124135210652 5.4894511245656696e-17\n",
      "0.1434512292631763 5.7909948537882e-17\n",
      "0.14345121727037874 5.8302971825408e-17\n",
      "0.14345120537257192 5.782863337494559e-17\n",
      "0.14345119356863134 5.804547380944269e-17\n",
      "0.1434511818574512 5.569411034786476e-17\n",
      "0.1434511702379417 5.598548968172024e-17\n",
      "0.1434511587090312 5.4338857632257875e-17\n",
      "0.1434511472696641 5.516556178877807e-17\n",
      "0.14345113591880115 5.569411034786476e-17\n",
      "0.14345112465541918 5.572121540217689e-17\n",
      "0.1434511134785109 5.341050952206716e-17\n",
      "0.14345110238708436 5.141828803012505e-17\n",
      "0.14345109138016246 5.720521712576643e-17\n",
      "0.14345108045678326 5.659535340374333e-17\n",
      "0.143451069615999 5.4677670811159595e-17\n",
      "0.14345105885687706 5.841816830623459e-17\n",
      "0.1434510481784974 5.5429836068321414e-17\n",
      "0.14345103757995523 5.357991611151802e-17\n",
      "0.14345102706035806 5.311235392463365e-17\n",
      "0.1434510166188274 5.5416283541165345e-17\n",
      "0.14345100625449733 5.72797560251248e-17\n",
      "0.14345099596651487 5.923131993559871e-17\n",
      "0.14345098575403928 5.633785538777802e-17\n",
      "0.14345097561624312 5.410168840702667e-17\n",
      "0.14345096555230943 5.4725104656205836e-17\n",
      "0.1434509555614342 5.2766764482153894e-17\n",
      "0.14345094564282485 5.578897803795724e-17\n",
      "0.14345093579570037 5.377642775528102e-17\n",
      "0.14345092601929021 5.385774291821743e-17\n",
      "0.14345091631283594 5.99970377199166e-17\n",
      "0.14345090667558885 5.900770323752358e-17\n",
      "0.14345089710681186 5.3430838312801265e-17\n",
      "0.1434508876057776 5.412879346133881e-17\n",
      "0.14345087817176944 5.601259473603237e-17\n",
      "0.14345086880408042 5.4061030825558465e-17\n",
      "0.1434508595020139 5.417622730638505e-17\n",
      "0.1434508502648826 5.919066235413051e-17\n",
      "0.14345084109200895 5.5294310796760726e-17\n",
      "0.1434508319827249 5.4447277849506426e-17\n",
      "0.14345082293637135 5.303781502527527e-17\n",
      "0.14345081395229858 5.403392577124633e-17\n",
      "0.1434508050298655 5.319366908757006e-17\n",
      "0.1434507961684401 5.845204962412476e-17\n",
      "0.14345078736739872 5.4833524873454387e-17\n",
      "0.1434507786261262 5.616844879832716e-17\n",
      "0.14345076994401582 5.329531304124058e-17\n",
      "0.14345076132046894 5.5348520905385e-17\n",
      "0.1435826353600536 0.0002493819873378503\n",
      "0.14363364546776314 0.00016250526949460414\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set[:,i:i+1]\n",
    "        Alpha_Q = wake_forward_expectation(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_expectation(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        Alpha_P = sleep_forward_expectation(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_expectation(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate with this model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expectation(Theta, gen_type):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    if gen_type == \"sampling\":\n",
    "        S = (p > np.random.rand(len(p),1)).astype(int)\n",
    "    else:\n",
    "        S = p\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,1,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        if gen_type == \"sampling\":\n",
    "            S = (p > np.random.rand(len(p),1)).astype(int)   # rejection sampling\n",
    "        else:\n",
    "            S = p\n",
    "    p = sigmoid(np.matmul(Theta[\"Theta_10\"],np.append(S,[[1]], axis=0)))\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to examine the distribution of generation. We let the model generate, say 10000 instances, then use the samples to approximate its generative distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate_expectation(Theta, \"sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know there are 1024 combinations in total, which gives 1024 categories. Here we use integer numbers 0-1023 to represent these combinations and for visualization, we assign 0-255 as our well-formed set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  1., ..., -1.,  1., -1.],\n",
       "       [ 1.,  1., -1., ...,  1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "entire_set = np.zeros((2,10))\n",
    "entire_set[0,0] = 1\n",
    "for i in range(1,n):\n",
    "    for j in range(entire_set.shape[0]):\n",
    "        entire_set = np.append(entire_set, entire_set[j:j+1,:], axis=0)\n",
    "        entire_set[j,i] = 1\n",
    "entire_set = (entire_set - 0.5)*2\n",
    "entire_set = np.transpose(entire_set)\n",
    "entire_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., ...,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       ...,\n",
       "       [-1., -1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1., -1., ..., -1., -1., -1.],\n",
       "       [-1.,  1., -1., ..., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_set = np.unique(entire_set, axis=1)\n",
    "well_formed_set = np.unique(well_formed_set, axis=1)\n",
    "reordered_set = np.zeros(entire_set.shape)\n",
    "reordered_set[:,:well_formed_set.shape[1]] = well_formed_set\n",
    "\n",
    "k = well_formed_set.shape[1]\n",
    "for i in range(entire_set.shape[1]):\n",
    "    flag = 0\n",
    "    for j in range(well_formed_set.shape[1]):\n",
    "        if np.array_equal(entire_set[:,i], well_formed_set[:,j]):\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 0:\n",
    "        reordered_set[:,k] = entire_set[:,i]\n",
    "        k += 1\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reordered_set, we set the first 256 columns as the well-formed set and the rest as negative samples outside of well-formed bound. Now we calculate the distribution of generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([243, 252, 201, ...,  32, 229, 209])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values < 256].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   0,   1,   0,   1,   0,  11,   6,   7,   5,   2,   5,   4,\n",
       "         6,   4,   6,   4,   6,   0,   1,   3,   1,   3,   4,   2,   2,\n",
       "         0,   4,   3,  21,  12,  11,  10,  13,  10,   7,   4,   5,   9,\n",
       "         4,  10,   2,   1,   7,   3,  16,   7,   7,   6,  11,   3,  19,\n",
       "        16,  34,  30,  40,  15,  27,  23,  11,  25,  18,  33,  22,   9,\n",
       "         5,   6,  12,   6,   6,  16,  12,   3,   8,  25,   9,  38,  15,\n",
       "        23,  24,  28,  23,  13,  25,  17,  22,  23,  31,  17,   2,   1,\n",
       "         0,   0,   1,   1,   4,   1,   4,   1,   2,   0,   5,   1,   2,\n",
       "         2,   2,   6,   3,   2,   7,   4,   5,   5,   1,   5,   3,   2,\n",
       "         9,   3,   6,   8,   8,   6,   7,   8,  11,  24,  15,  13,   6,\n",
       "         5,  10,   9,   7,   8,   6,   7,   7,  16,  12,  10,  10,   6,\n",
       "         8,   6,  13,  21,   7,  15,  33,  14,  30,  27,  46,   8,   8,\n",
       "        11,  14,  13,  17,  10,   9,  31,   9,  20,  17,  19,   6,   6,\n",
       "         3,   2,   5,  10,  10,  11,  16,   9,  11,  18,  16,   6,  15,\n",
       "        18,  14,  24,  15,  17,  18,  29,  11,  34,  31,   8,  25,  56,\n",
       "        42,  29,  30,  42,  35,  53,  45,  29,  43,  40,  31,  38,  65,\n",
       "        57,  32,  45,  35,  43,  54,  48,  68,  95,  98,  59,  27,  52,\n",
       "        59,  62,  61,  43,  96,  87,  61, 105, 111, 130,  91,  72,  64,\n",
       "       138, 128,  63, 186, 206,  91, 158, 314, 328, 121,  82, 150, 128,\n",
       "       189, 167,  87, 217, 223, 164, 213, 319, 327], dtype=int64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.array([values, counts])\n",
    "for i in range(values.size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        statistics = np.append(statistics, np.array([[values[i]+j],[0]]),axis = 1)\n",
    "statistics = np.unique(statistics,axis = 1)\n",
    "statistics[1,0:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': array([[ 0.48353902,  1.62643856,  1.97819237,  1.672384  ,  0.8772383 ,\n",
       "          2.82323708,  0.12645715,  0.65330562,  3.03327121],\n",
       "        [ 1.20807793,  0.78771601,  2.68433376,  1.05228895,  1.2821227 ,\n",
       "         -1.96009945,  2.27746462,  0.52836295, -0.73190118],\n",
       "        [-0.14412266,  0.56010536,  0.77658523,  0.94079198,  1.07937709,\n",
       "          0.05195837, -0.39628455, -1.37831183,  0.99075065],\n",
       "        [ 0.17174982,  0.78778258,  0.36371727, -1.54016735,  0.52372244,\n",
       "         -0.19732077,  0.52564788,  0.2706051 ,  1.58925647],\n",
       "        [ 0.66079355, -1.54666492,  1.34478288,  2.16108398,  0.14279446,\n",
       "          0.10313312,  0.30086041, -0.08404553, -0.50601995],\n",
       "        [ 0.18979941,  1.86013072, -1.08473127,  1.00194203, -0.35596614,\n",
       "          0.36210117, -2.01470296, -0.39067427,  0.06588787],\n",
       "        [ 0.88308429,  0.05063965,  1.41195692, -0.79069707,  1.06200532,\n",
       "         -0.68721921, -0.78442672,  0.37673018,  0.29362181],\n",
       "        [ 0.20635206,  0.6845116 ,  0.8916963 , -0.28118957, -1.05050301,\n",
       "          0.27270374,  0.32717671, -0.49946687, -0.20458323],\n",
       "        [ 0.38967174, -0.94182542,  0.58661511,  0.07603149,  1.06191459,\n",
       "          0.71215902,  0.0258397 , -0.15864065, -0.28279879],\n",
       "        [ 1.43332648,  0.05679893, -0.30634886, -0.31028173, -0.80693876,\n",
       "         -1.85379711,  0.14889648, -0.17473575,  1.17248446]]),\n",
       " 'Theta_21': array([[ 0.95463424,  1.9825875 ,  0.10743911, -0.65998499, -0.57514815,\n",
       "         -0.13467471],\n",
       "        [ 1.75602184, -1.55407146, -0.06402794,  0.77901499,  0.25269026,\n",
       "          0.33369708],\n",
       "        [ 1.46904788,  1.21973593, -2.14345773, -0.30723101, -0.02401981,\n",
       "          0.54538876],\n",
       "        [ 0.41054204, -0.84860358,  1.79584383,  1.84699929, -0.25552515,\n",
       "         -1.08103073],\n",
       "        [-0.55600551,  0.39705916,  0.47522989, -0.23268085, -1.42720307,\n",
       "          0.69419876],\n",
       "        [ 0.80963287, -0.44684575, -0.27898946,  0.04686564,  0.94851321,\n",
       "          0.35040838],\n",
       "        [ 1.43180242,  1.56161535,  1.15836735, -0.79807539,  0.50118136,\n",
       "         -1.48431943],\n",
       "        [-0.13714773, -0.08325532,  0.11198738, -0.0551511 , -0.92931084,\n",
       "         -1.23998047]]),\n",
       " 'Theta_32': array([[-1.16442082,  1.33602295, -0.45840948, -0.17007644],\n",
       "        [-1.20510497, -1.26885114,  0.99161715, -0.08061362],\n",
       "        [ 0.13633545,  1.0516591 , -0.9356697 ,  0.11753533],\n",
       "        [-0.84679208, -0.0146821 ,  1.07295736,  0.02858046],\n",
       "        [ 0.43852265,  0.31644698, -0.85968847,  0.34735128]]),\n",
       " 'Theta_k': array([[-0.73991842],\n",
       "        [ 1.9889549 ],\n",
       "        [-0.18649867]])}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Training Method\n",
    "\n",
    "**Weighted local delta rule**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial \\theta_{k,n}^{m+1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-\\sum_\\alpha Q_\\phi(\\alpha|d_0)s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "\\sum_\\alpha Q_\\phi(\\alpha|d_0)s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m = -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\tilde{F}}{\\partial \\phi_{k,n}^{m-1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "- \\sum_{\\alpha} P_\\theta(\\alpha,d_0)s_k^{m-1}(1-q_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "\\sum_{\\alpha} P_\\theta(\\alpha,d_0)s_k^{m-1} \\centerdot q_n^m & \\text{if } s_n^m = -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "It's impossible to sum over the complete assignments. Instead, we sample a subset of assignments and train with its weighted summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward_sample(d0,Phi,k):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    k -- number of instances sampled\n",
    "    \n",
    "    Returns:\n",
    "    Q -- Probability of sampled recognition assignment, number in (0,1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = 1\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0)))\n",
    "        S = ((q > np.random.rand(len(q),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        Q = Q * np.cumprod(S*q + (1-S)/2)[-1] * 2**q.size  # S takes {-1,1}\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
