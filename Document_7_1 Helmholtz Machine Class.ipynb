{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af70d803",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Document 7</span> \n",
    "\n",
    "# Helmholtz Machine Class\n",
    "\n",
    "In this version, we made some improvements based on previous development and experiments:\n",
    "\n",
    "- We struggled with adding bias to different types of layers, including instantiation layer, MLP hidden layer, data layer. Based on our first round of experiments, the effect of having bias seems ambiguous, only data bias improved the model performance. In this version, we got rid of the confusing and redundant bias choice, integrating it into a more scientific framework -- batch normalization. By performing batch normalization, we recentered the pre-activation term in each layer which gives better structured layer values and derivatives. By adaptive scaling and bias, we attain some control over the mean and variance of all layers.\n",
    "- In the previous version, we pointed out the effect and variability of binary outcomes of sampling layers. We tried to change the classical binary outcomes {0,1} to {-1,1}, and hoped for better performance. However, the modified values seem down-performed the vanillas version for certain reasons. In this version, we add an option to parameterize the binary outcomes of each sampling layer, which may improve the results comparing to predefined values.\n",
    "- We retained the deep structure of hidden layers, although this feature didn't make much difference in the control experiments. As means to promote the correlation between two adjacent sampling layers, we believe this feature will take effect in following experiments.\n",
    "\n",
    "Something new:\n",
    "\n",
    "- **Adaptive learning rate.** In the previous experiments, we set the learning rate manually, which is quite inconvenient and confusing, since it's difficult to know what learning rate is suitable and when to change it. Due to the nature of wake-sleep algorithm, it's infeasible to train a decreasing loss curve and use it to evaluate the model performance. In this version we take the advantage of Taylor expansion and designed some mechanism to modify the learning rate. This feature is applicable to parameter-specific learning rate design (instead of one learning rate for all parameters), but this feature is still under test.\n",
    "\n",
    "- **Layer normalization learning rate.** In Document 6, we pointed out that the values of derivatives are not necessary for achieving decreasing loss, only its sign (+ or -) matters. In this version we made our first attempt on this feature. For the parameter matrix $W$, its rows are normalized by batch normalization, while the inside row distribution is not modifiable. We designed a layer-norm learning rate, which adjusts the learning rate of each parameter within a row separately to diversify the row distribution, namely increasing its entropy.\n",
    "\n",
    "- **MI (mutual information) diversifying training algorithm.** Experimented on the wake-sleep algorithm in previous experiments, it produces too many outliers due to its training on stochastic sampling. It works in certain cases but not applicable to any data distribution. Therefore, based on my understanding, I proposed a new traning algorithm based on maximizing mutual information of adjacent sampling layers, and hope this algorithm could improve model performance and training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96291179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904cb37",
   "metadata": {},
   "source": [
    "The model structure looks like below:\n",
    "\n",
    "<img src=\"Modern.jpg\" style=\"width:800px\">\n",
    "<caption><center> **Figure 1**: Modern Helmholtz Machine. Blue neurons represent sampling layers, where the neurons take binary values. Orange neurons indicate the inserted activations, where the neurons take value in real numbers in given range.    </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a31b6",
   "metadata": {},
   "source": [
    "The skeleton of the model is given by the sampling layers (blue neurons) where each neuron represents a Bernoulli variable. \n",
    "\n",
    "<img src=\"Helm_global.jpg\" style=\"width:700px\">\n",
    "<caption><center> **Figure 2**: External structure of the Helmholtz Machine, composed of sampling layers. Each adjacent pair may be concatenated by hidden activation layers.    </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d965bf",
   "metadata": {},
   "source": [
    "The internal structure of adjacent sampling layers:\n",
    "\n",
    "<img src=\"muti-bernoulli.jpg\" style=\"width:800px\">\n",
    "<caption><center> **Figure 3**: Multivariate Bernoulli Distribution. In wake phase, we go from input $\\mathbf{x}$ to output $\\mathbf{y}$ by weight $\\Phi$. Blue neurons represent sampling layers, where each neuron takes binary value and is computed as an independent Bernoulli variable. Orange neurons are inserted activations, which transform a shallow neural network with one-step prameter updating to deep neural network with backpropogation.\n",
    "</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255f1cb",
   "metadata": {},
   "source": [
    "The computation and parameter updation of the Helmholtz machine is layer-separated by instance sampling. Therefore, the programming architecture of this model is modular -- we perform all functions on the submodule in Figure 3 then assemble them as the skeleton in Figure 2 to achieve the Helmholtz machine as shown in Figure 1. The `Helmholtz_machine class` describes this skeleton in Figure 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b1e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161be893",
   "metadata": {},
   "source": [
    "#### Define model structure\n",
    "\n",
    "n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "n is the maximum number of inserted layers between adjacent instantiation layers.\n",
    "\n",
    "A neural network in Figure 1 has structure ndarray:\n",
    "$$\n",
    "n_{dz} = \n",
    "\\begin{pmatrix}\n",
    "10 & 8 & 6 & 3 & 1 \\\\\n",
    "9 & 0 & 5 & 0 & 0 \\\\\n",
    "0 & 0 & 4 & 0 & 0  \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The first row gives the number of neurons in each instantiation layer. The second row represents the first inserted layer, and there is a 9-neuron layer inserted between $d_0$ and $z_1$, a 5-neuron layer inserted between $z_2$ and $z_3$. The third row represents the second inserted layer, and there is only one 4-neuron layer inserted between $z_2$ and $z_3$ above the first 5-neuron inserted layer. Obviously, $n_{dz}(i,k) \\ne 0$ if and only if $n_{dz}(i,j) \\ne 0, \\forall j < k$; the last column of  $n_{dz}$ is all zero except for the value in the first row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca19f5",
   "metadata": {},
   "source": [
    "In this class, we have following quantities for computation:\n",
    "\n",
    "- Parameters\n",
    "    - Matrix $\\phi$, of shape $(n_l,n_x+2)$. The first $(n_l,n_x)$ is weight matrix from layer $x$ to $l$, penultimate column $(n_l,1)$ is adaptive bias, last column $(n_l,1)$ is adaptive scale. Matrix $\\phi$'s are for every concatenated layers, dictionary of dictionary.\n",
    "    - Binary outcomes of every sampling layer, matrix $Scalar$ of shape $(n_{layer},2)$, in this example $n_{layer}=5$. This matrix, the sampled binary values, are shared between wake and sleep phases.\n",
    "    \n",
    "- Statistics\n",
    "    - Batch normalization statistics, statistical mean and variance of each layer, matrix $norm$ of shape $(n_l,3)$, same length as parameter set. First column is batch summation $\\sum_i z_i$; second column is batch square sum $\\sum_i z_i^2$; third column is fixed bias placeholder that pulls the pre-activition mean to $0$. Separate statistics for wake and sleep phases, dictionary of dictionary.\n",
    "    - Learning rate update criteria. \n",
    "    - Latent assignment distribution\n",
    "- Control variables\n",
    "    - add_norm: control whether to add the current instance to batch normalization counts.\n",
    "    - \n",
    "    - learning rate: assign different learning rate to each submodule, matrix of shape $(2,n_{layer}-1)$. First row stores lr for wake phase modules (last entry is idle), 3 modules; second row stores lr for sleep phase modules, 4 modules.\n",
    "    \n",
    "- Derivatives\n",
    "    - Gradient\n",
    "    - Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0206308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_para_init(n_dz_slice,init_type):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz_slice -- 2 columns of n_dz\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "\n",
    "    Returns:\n",
    "    Phi, Theta -- -2 column bias, -1 column scale\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row is counts, column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    \"\"\"\n",
    "    l = np.where(n_dz_slice[:,0] != 0)[0].size  # number of layers\n",
    "    layer_vt = np.append(n_dz_slice[:l,0],n_dz_slice[0,1])\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    norm_wake = {}\n",
    "    norm_sleep = {}\n",
    "    for i in range(l):\n",
    "        if init_type == \"zero\":\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((layer_vt[i+1],layer_vt[i]+2))\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.zeros((layer_vt[l-i-1],layer_vt[l-i]+2))\n",
    "        elif init_type == \"random\":\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(layer_vt[i+1],layer_vt[i]+2)\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.random.randn(layer_vt[l-i-1],layer_vt[l-i]+2)\n",
    "        else:\n",
    "            raise Exception(\"Wrong Init Type\")\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)][:,-1] = 1          #scale\n",
    "        Theta[\"Theta_\" + str(l-i) + str(l-i-1)][:,-1] = 1\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)][:,-2] = 0              #bias\n",
    "        Theta[\"Theta_\" + str(l-i) + str(l-i-1)][:,-2] = 0\n",
    "        \n",
    "        norm_wake[\"l_\" + str(i+1)] = np.zeros((layer_vt[i+1]+1,3))\n",
    "        norm_sleep[\"l_\" + str(l-i-1)] = np.zeros((layer_vt[l-i-1]+1,3))\n",
    "        \n",
    "    return Phi,Theta,norm_wake,norm_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89431a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_forward(x,activation_type,parameter_set,norm_set,add_norm):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input instantiation layer, numpy array of shape (n,1)\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l. The keys are ordered sequentially from layer x to y.\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    add_norm -- True or False. Update norm matrix or not\n",
    "    \n",
    "    Returns:\n",
    "    G -- activation of each layer including x\n",
    "    q -- probability of layer y\n",
    "    \"\"\"\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    G = {'z0': x}\n",
    "    g = x\n",
    "    \n",
    "    for i in range(l):\n",
    "        phi = parameter_set[p_keys[i]]\n",
    "        norm = norm_set[n_keys[i]]\n",
    "        \n",
    "        pre_ac = np.matmul(phi[:,:-2],g)+norm[:-1,2:]  # pre-activation; bias placeholder\n",
    "        K = phi[:,-1:] * pre_ac + phi[:,-2:-1]  # rescale linear term\n",
    "        if activation_type == \"sigmoid\":\n",
    "            g = sigmoid(K)\n",
    "        elif activation_type == \"tanh\":\n",
    "            g = np.tanh(K)\n",
    "        if i == l-1:\n",
    "            g = sigmoid(K)\n",
    "        G['z'+str(i+1)] = g\n",
    "        \n",
    "        if add_norm == True:\n",
    "            norm[:-1,0:1] += pre_ac\n",
    "            norm[-1,0] += 1\n",
    "            norm[:-1,1:2] += pre_ac**2\n",
    "            norm[-1,1] += 1\n",
    "            norm_set[n_keys[i]] = norm        \n",
    "    return G,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7f1e8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_update(x,y,G,activation_type,parameter_set,value_set,norm_set,lr,\\\n",
    "                    fz_binary,fz_scale,fz_W,layer_norm,layer_norm_rate,check_lr):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input sampling layer, numpy array of shape (n,1)\n",
    "    y -- target sampling layer, numpy array of shape (m,1)\n",
    "    G -- output of one_step_forward_norm, activation of each layer including x\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    parameter_set -- parameters from x to y. Each array phi: -2 column bias, -1 column scale\n",
    "    value_set -- list or array [[a_x,b_x],[a_y,b_y]], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,3), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    lr -- learning rate, decimals\n",
    "    fz_binary -- True or Flase. If True, [a_x,b_x] are fixed values; if False, update [a_x,b_x]\n",
    "    fz_scale -- True or Flase. If True, last 2 columns of phi are fixed; if False, update adaptive scale and bias\n",
    "    fz_W -- True or Flase. If True, freeze weights; if False, update weights\n",
    "    layer_norm -- True or Flase, whether to adjust row distribution of W (more evenly distributed) by adjusting the \n",
    "    updating rate \"layer_norm_rate\" of dW\n",
    "    check_lr -- True or Flase. If True, check current lr and update it as needed\n",
    "    \n",
    "    Returns:\n",
    "    parameter_set -- updated parameters\n",
    "    loss -- value of loss function before updating, a number\n",
    "    grad_set -- gradients of parameters\n",
    "    lr -- updated learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    a_x = value_set[0,0]\n",
    "    b_x = value_set[0,1]\n",
    "    a_y = value_set[1,0]\n",
    "    b_y = value_set[1,1]\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    grad_set = {}\n",
    "    \n",
    "    # derivatives\n",
    "    q = G['z'+str(l)]\n",
    "    u = q - (y-b_y)/(a_y-b_y)   #[n_y,1]\n",
    "    loss = -np.sum(((y-b_y)*np.log(q) + (a_y-y)*np.log(1-q))/(a_y-b_y))  # layer entropy loss\n",
    "    if np.where(np.abs(y - a_y) < 1e-8)[0].size + np.where(np.abs(y - b_y) < 1e-8)[0].size != y.size:\n",
    "        raise Exception(\"Incorrect output layer\" + str(a_y)+ \" \"+str(b_y)+ \" \"+str(y))\n",
    "    # a = g(scale * [(Wz+b-mean)/deviation] + bias)\n",
    "    for i in range(l-1,-1,-1):\n",
    "        phi = parameter_set[p_keys[i]]\n",
    "        grad_set['d_'+p_keys[i]] = np.zeros(phi.shape)\n",
    "        W = phi[:,:-2]        #[n_y,n_z]\n",
    "        bias = phi[:,-2:-1]   #[n_y,1]\n",
    "        scale = phi[:,-1:]    #[n_y,1]\n",
    "        \n",
    "        z = G['z'+str(i)]      #[n_z,1]\n",
    "        \n",
    "        if fz_W == False:\n",
    "            dW = np.outer(u*scale,z)     #[n_y,n_z]\n",
    "            grad_set['d_'+p_keys[i]][:,:-2] = dW\n",
    "            # update weights\n",
    "            if layer_norm == True:\n",
    "                dW = layer_lr(W, dW, rate=layer_norm_rate)\n",
    "            parameter_set[p_keys[i]][:,:-2] -= lr * dW\n",
    "        \n",
    "        if fz_scale == False:\n",
    "            norm = norm_set[n_keys[i]]      #[n_y,3]\n",
    "            b = norm[:-1,2:]                #[n_y,1]\n",
    "            N = np.matmul(W,z)+b   #[n_y,1]\n",
    "            d_scale = u * N      #[n_y,1]\n",
    "            d_bias = u             #[n_y,1]\n",
    "            parameter_set[p_keys[i]][:,-1:] -= lr * d_scale\n",
    "            parameter_set[p_keys[i]][:,-2:-1] -= lr * d_bias\n",
    "            grad_set['d_'+p_keys[i]][:,-1:] = d_scale\n",
    "            grad_set['d_'+p_keys[i]][:,-2:-1] = d_bias\n",
    "        \n",
    "        dz = np.matmul(W.T,u*scale)  #[n_z,1]\n",
    "        if i > 0:\n",
    "            if activation_type == \"sigmoid\":\n",
    "                u = dz * z * (1-z)\n",
    "            elif activation_type == \"tanh\":\n",
    "                u = dz * (1-z**2)\n",
    "        else:\n",
    "            # input layer, parameters a_x, b_x\n",
    "            if fz_binary == False:\n",
    "                d_ax = np.mean(dz[np.where(np.abs(x - a_x) < 1e-8)[0]]) # use mean instead of sum\n",
    "                d_bx = np.mean(dz[np.where(np.abs(x - b_x) < 1e-8)[0]])\n",
    "                if np.where(np.abs(x - a_x) < 1e-8)[0].size + np.where(np.abs(x - b_x) < 1e-8)[0].size != dz.size:\n",
    "                    raise Exception(\"Incorrect input layer\" + str(a_x)+ \" \"+str(b_x)+ \" \"+str(x))\n",
    "                a_x -= lr * d_ax\n",
    "                b_x -= lr * d_bx\n",
    "            x_values = [a_x,b_x]\n",
    "    \n",
    "    if check_lr == True:\n",
    "        G,q = one_step_forward(x,activation_type,parameter_set,norm_set,add_norm=False)\n",
    "        loss_new = -np.sum(((y-b_y)*np.log(q) + (a_y-y)*np.log(1-q))/(a_y-b_y))  # layer entropy loss\n",
    "        delta_loss = loss - loss_new\n",
    "        delta_f = 0\n",
    "        for keys in grad_set:\n",
    "            delta_f += np.sum(grad_set[keys]**2)\n",
    "        delta_f *= lr\n",
    "        print(\"delta_loss: \"+ str(delta_loss), \"delta_f: \"+ str(delta_f))\n",
    "        if delta_loss < delta_f:\n",
    "            lr /= delta_f/delta_loss\n",
    "            print(\"learning rate change: \" + str(lr))\n",
    "    \n",
    "    return x_values,lr,loss,grad_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e988f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_lr(W, dW, rate):\n",
    "    \"\"\"\n",
    "    adjust row distribution of W (more evenly distributed) by adjusting the updating rate of dW\n",
    "    Arguments:\n",
    "    W -- phi[:,:-2], numpy array of sahpe (n_z,n_x)\n",
    "    dW -- derivatives of W\n",
    "    rate -- 10 folds, x rate; 100 folds, x 2 rate\n",
    "    \n",
    "    Returns:\n",
    "    layer_lr\n",
    "    \"\"\"\n",
    "    multiple = np.log10(np.abs(W)/np.min(np.abs(W),axis=1,keepdims=True)).astype(int)\n",
    "    index = np.where((multiple > 0) & (W*dW > 0))\n",
    "    dW[index] *= rate\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_norm_update(parameter_set,norm_set):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l. The keys are ordered sequentially from layer x to y.\n",
    "    Each array phi has: -2 column bias, -1 column scale\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    \n",
    "    Returns:\n",
    "    parameter_set -- updated parameters\n",
    "    norm_set -- reset mean, variance; update bias placeholder\n",
    "    \"\"\"\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    \n",
    "    for i in range(l):\n",
    "        phi = parameter_set[p_keys[i]]  #[n_z,n_x+2]\n",
    "        norm = norm_set[n_keys[i]]      #[n_z,1]\n",
    "        mean = norm[:-1,0:1]/norm[-1,0]\n",
    "        variance = norm[:-1,1:2]/norm[-1,1] - mean**2\n",
    "        print(\"mean: \", mean)\n",
    "        print(\"variance: \", variance)\n",
    "        \n",
    "        parameter_set[p_keys[i]][:,:-2] = phi[:,:-2]/np.sqrt(variance)  # scale every row of W by deviations\n",
    "        b = norm[:-1,2:]\n",
    "        norm_set[n_keys[i]][:-1,2:] = (b-mean)/np.sqrt(variance) # update bias placeholder\n",
    "        norm_set[n_keys[i]][:,:2] = 0\n",
    "    return parameter_set,norm_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70349164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimalize(bnr):\n",
    "    \"\"\"\n",
    "    Arguments: bnr -- numpy array of binary values {0,1}, of shape (width,n)\n",
    "    Returns: dcm -- a decimalized intger array in range [0,2**width), shape (n,)\n",
    "    \"\"\"\n",
    "    width = bnr.shape[0]\n",
    "    n = bnr.shape[1]\n",
    "    dcm = np.zeros(n,dtype=int)\n",
    "    for i in range(n):\n",
    "        binary = ''.join(bnr[:,i].reshape(width,).astype(str))\n",
    "        dcm[i] = int(binary,2)\n",
    "    return dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6772442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarilize(dcm,width):\n",
    "    \"\"\"\n",
    "    Arguments: dcm -- a decimalized intger array in range [0,2**width), shape (n,)\n",
    "    Returns:   bnr -- numpy array of binary values {0,1}, of shape (width,n)\n",
    "    \"\"\"\n",
    "    n = len(dcm)\n",
    "    bnr = np.zeros((width,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        bnr[:,i:i+1] = np.array(list(np.binary_repr(dcm[i], width=width))).astype(int).reshape(-1,1)\n",
    "    return bnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d27075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_prob_matrix(activation_type,n_x,parameter_set,norm_set):\n",
    "    x_ind = np.arange(2**n_x)\n",
    "    X = binarilize(x_ind,n_x)  #(n_x,2**n_x)\n",
    "    G,q = one_step_forward(X,activation_type,parameter_set,norm_set,add_norm=False)\n",
    "    \n",
    "    n_y = q.shape[0]           # q:(n_y,2**n_x)\n",
    "    y_ind = np.arange(2**n_y)\n",
    "    Y = binarilize(y_ind,n_y)  #(n_y,2**n_y)\n",
    "    \n",
    "    prob_mtx = np.zeros((2**n_x+1,2**n_y+1)) \n",
    "    # probability from X to Y, last row unconditioned probability, last column is entropy H(Y|x), H(Y|X)\n",
    "    for i in range(2**n_x):\n",
    "        prob_mtx[i,:-1] = np.prod(q[:,i:i+1]**Y * (1-q[:,i:i+1])**(1-Y),axis=0)\n",
    "    prob_mtx[:-1,-1] = -np.sum(prob_mtx[:-1,:-1]*np.log(prob_mtx[:-1,:-1]),axis=1)\n",
    "    return prob_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dcea8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(arr):\n",
    "    \"\"\"\n",
    "    row entropy\n",
    "    \"\"\"\n",
    "    p = arr/arr.sum(axis=1,keepdims=True)\n",
    "    H = -(p*np.log(p)).sum(axis=1)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cc8735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_itr(H_prev,s,a):\n",
    "    \"\"\"\n",
    "    Iterative formula for entropy:\n",
    "    H_+1 = (s(H-log(s)) + (s+1)log(s+1) + [alog(a) - (a+1)log(a+1)])/(s+1)\n",
    "    \n",
    "    Arguments:\n",
    "    H_prev -- previous entropy\n",
    "    s -- total counts\n",
    "    a -- counts for the category being modified\n",
    "     \n",
    "    Returns:\n",
    "    H -- updated entropy\n",
    "    \"\"\"\n",
    "    H = (s*(H_prev-np.log(s)) + (s+1)*np.log(s+1) + [a*np.log(a) - (a+1)*np.log(a+1)])/(s+1)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51defa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_M(Prob_mtx_wake,fc=10):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+2), last row: p(y); -2 column: summation; \n",
    "    last column: row entropy H(Y|x) & H(Y)\n",
    "    fc -- scaling factor, integer\n",
    "    \"\"\"\n",
    "    MI_mtx = {}\n",
    "    for m in Prob_mtx_wake:\n",
    "        factor = (Prob_mtx_wake[m].shape[1]-1)*fc     # 2**n_y\n",
    "#         factor = 1\n",
    "        \n",
    "        MI_mtx[\"MI_\" + m] = np.zeros((Prob_mtx_wake[m].shape[0],Prob_mtx_wake[m].shape[1]+1))\n",
    "        MI_mtx[\"MI_\" + m][:,:-2] = Prob_mtx_wake[m][:,:-1]\n",
    "        MI_mtx[\"MI_\" + m][:,:-2] *= factor\n",
    "        MI_mtx[\"MI_\" + m][:,-2] = factor\n",
    "        MI_mtx[\"MI_\" + m][:-1,-1] = Prob_mtx_wake[m][:-1,-1]\n",
    "        MI_mtx[\"MI_\" + m][-1,-1] = entropy(Prob_mtx_wake[m][-1:,:-1])\n",
    "    return MI_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06407f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_M(M):\n",
    "    flag = True\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    if np.any(np.abs(np.sum(M[:,:-2],axis=1) - M[:,-2]) > epsilon):\n",
    "        flag = False\n",
    "        raise Exception(\"column addition wrong\")\n",
    "    elif np.any(entropy(M[:,:-2]) - M[:,-1] > epsilon):\n",
    "        flag = False\n",
    "        raise Exception(\"entropy computation wrong\")\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bfa090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(Input):\n",
    "    \"\"\"\n",
    "    Find y for given x that increases the layer-wise accumulative mutual information\n",
    "    Iterative formula for entropy:\n",
    "    H_+1 = (s(H-log(s)) + (s+1)log(s+1) + [alog(a) - (a+1)log(a+1)])/(s+1)\n",
    "    \n",
    "    s = 1 -> H_+1 = H/2 + log(2) + [alog(a) - (a+1)log(a+1)]/2\n",
    "    \n",
    "    Arguments:\n",
    "    Input -- numpy array of shape (2,m+2), with each row comprises \n",
    "    [counts for m categories separately, sum of counts, entropy of this row]\n",
    "    first row: H(Y|x)    second row: H(Y) [summation over x]\n",
    "    \n",
    "    Returns:\n",
    "    MI_index -- list of indices of Y where the mutual information increases when this category counts +1, numpy array of shape (k, )\n",
    "    MI -- mutual information of each choice, numpy array of shape (k, )\n",
    "    \"\"\"\n",
    "    s = Input[:,-2:-1]   #(2,1)\n",
    "    H = Input[:,-1:]     #(2,1)\n",
    "    C = s*(H - np.log(s)) + (s+1)*np.log(s+1)   #(2,1)\n",
    "    \n",
    "    M = Input[:,:-2]     #(2,m)\n",
    "    D = M*np.log(M) - (M+1)*np.log(M+1)     #(2,m)\n",
    "    H_new = (C + D)/(s+1)      #(2,m)\n",
    "\n",
    "    MI = H_new[1,:] - H_new[0,:]  # enlarge H(Y)\n",
    "    \n",
    "    return MI,H_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b8677ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_samples(q,precision):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    q -- sigmoid output of a given layer\n",
    "    precision -- a decimal number in [0,0.5)\n",
    "    \n",
    "    Returns:\n",
    "    p_index -- numpy array of shape (2**num, ), categorical number of possible y's\n",
    "    probability -- numpy array of shape (2**num, ), probability of each category\n",
    "    num -- max number of neurons being changed\n",
    "    \"\"\"\n",
    "    \n",
    "    var_index = np.where(np.abs(q.reshape(-1,) - 0.5) < precision)[0]\n",
    "    num = len(var_index)\n",
    "    comb = binarilize(np.arange(2**num),num)   #(num, 2**num)\n",
    "    prob = q[var_index]\n",
    "    probability = np.prod(prob**comb * (1-prob)**(1-comb),axis=0)  #(2**num, )\n",
    "    \n",
    "    z = (q+0.5).astype(int)\n",
    "    z_all = np.repeat(z, 2**num, axis=1)  #(n_q, 2**num)\n",
    "    z_all[var_index,:] = comb\n",
    "    p_index = decimalize(z_all)   #(2**num, )\n",
    "        \n",
    "    return p_index,probability,num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de6d3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_y(M,cat_x,q,typ,precision):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+2), last row: summation; -2 column: summation; last column: entropy\n",
    "    cat_x -- categorical number of given x\n",
    "    q -- sigmoid output of input x\n",
    "    precision -- a decimal number in [0,0.5)\n",
    "    \n",
    "    Returns:\n",
    "    M -- updated count matrix\n",
    "    \"\"\"\n",
    "    M_pick = M[[cat_x,-1],:]\n",
    "    MI,H_new = mutual_info(M_pick)\n",
    "    p_index,probability,num = top_samples(q,precision)\n",
    "    \n",
    "    # type 1,4: MI main; type 2,3: prob main\n",
    "    if typ == 1:\n",
    "        cat_y = np.argsort(MI)[::-1][0]\n",
    "        \n",
    "    elif typ == 2:\n",
    "        ind = np.argsort(MI[p_index])[::-1][0]\n",
    "        cat_y = p_index[ind]\n",
    "        \n",
    "    elif typ == 3:\n",
    "        ind = np.argsort(MI[p_index] * np.log(1+probability))[::-1][0]\n",
    "        cat_y = p_index[ind]\n",
    "        \n",
    "    elif typ == 4:\n",
    "        p_ind = np.argsort(MI)[::-1][0:10]      # top 10 max MI\n",
    "        comb = binarilize(p_ind,width=len(q))  # (2**n_y,5)\n",
    "        probability = np.prod(q**comb * (1-q)**(1-comb),axis=0)  #(5, )\n",
    "        ii = np.argsort(probability * MI[p_ind])[::-1][0]\n",
    "        cat_y = p_ind[ii]\n",
    "    else:\n",
    "        raise Exception(\"Wrong type\")\n",
    "        \n",
    "    # update count matrix\n",
    "    M[cat_x,-1] = entropy_itr(M[cat_x,-1],M[cat_x,-2],M[cat_x,cat_y])\n",
    "    M[-1,-1] = entropy_itr(M[-1,-1],M[-1,-2],M[-1,cat_y])\n",
    "    M[cat_x,cat_y] += 1\n",
    "    M[-1,cat_y] += 1\n",
    "    M[cat_x,-2] += 1\n",
    "    M[-1,-2] += 1    \n",
    "    \n",
    "    return cat_y,M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "15ed00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helmholtz_machine:\n",
    "\n",
    "    def __init__(self, n_dz, activation_type, init_lr = 0.1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "        n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "        activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "        init_lr -- initial learning rate, decimal number\n",
    "        \n",
    "        Returns:\n",
    "        n_layer -- number of instantiation layers\n",
    "        lr -- different learning rate for each submodule, numpy array of shape (2,m-1)\n",
    "        \"\"\"\n",
    "        self.n_dz = n_dz\n",
    "        self.n_layer = self.n_dz.shape[1]\n",
    "        self.n_d = n_dz[0,0]\n",
    "        self.ac = activation_type\n",
    "        self.lr = np.ones((2,self.n_layer-1))*init_lr\n",
    "        \n",
    "    def parameter_init(self,init_type,value_set):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "        value_set -- numpy array [a,b], binary outcomes as {positive, negative}\n",
    "\n",
    "        Returns:\n",
    "        Phi, Theta -- -2 column bias, -1 column scale. \n",
    "        Eg. {Phi_01:{Phi_01,Phi_12}, Phi_12:{Phi_01,Phi_12,Phi_23}}, dictionary of dictionary\n",
    "        Norm_wake, Norm_sleep -- statistical mean and variance of each pre-activation layer. Norm_wake's last dic is idle.\n",
    "        Dictionary of dictionary, numpy array of shape (n_neuron+1,3), \n",
    "        last row is counts, column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "        Scalar -- numpy array of shape (n_layer,2), binary outcomes of every sample layer, shared between wake and sleep phases\n",
    "        \"\"\"\n",
    "        Phi = {}\n",
    "        Theta = {}\n",
    "        Norm_wake = {}\n",
    "        Norm_sleep = {}\n",
    "        \n",
    "        for i in range(self.n_layer-1):\n",
    "            Phi[\"Phi_\"+str(i)+str(i+1)],Theta[\"Theta_\"+str(i+1)+str(i)],Norm_wake['sl_'+str(i+1)], Norm_sleep['sl_'+str(i)] \\\n",
    "            = one_step_para_init(self.n_dz[:,i:i+2],init_type)\n",
    "            \n",
    "        self.Phi = Phi\n",
    "        self.Theta = Theta\n",
    "        self.Norm_wake = Norm_wake\n",
    "        self.Norm_sleep = Norm_sleep\n",
    "        self.Scalar = np.repeat(value_set.reshape(1,-1),self.n_layer,axis=0)\n",
    "    \n",
    "    def set_control(self,Fz_binary,fz_scale=True,fz_W=False,layer_norm=False,layer_norm_rate=2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Fz_binary -- Boolean list of size n_layer, with last entry always as True\n",
    "        Fz_scale -- Boolean list of size n_layer-1, with last entry always as True. \n",
    "        If True, last 2 columns of phi are fixed; if False, update adaptive scale and bias\n",
    "        fz_W -- True or Flase. If True, freeze weights; if False, update weights\n",
    "        layer_norm -- True or Flase, whether to adjust row distribution of W (more evenly distributed) by adjusting the \n",
    "        updating rate \"layer_norm_rate\" of dW\n",
    "        \"\"\"\n",
    "        self.Fz_binary = Fz_binary\n",
    "        self.Fz_scale = Fz_scale\n",
    "        self.fz_W = fz_W\n",
    "        self.layer_norm = layer_norm\n",
    "        self.ln_rate = layer_norm_rate\n",
    "        if np.any(np.array(Fz_binary) == False):\n",
    "            self.Scalar = self.Scalar.astype(float)\n",
    "    \n",
    "    def wake_sample(self,d0,add_norm=True):\n",
    "        \"\"\"\n",
    "        Stochastic sample in wake phase\n",
    "        Arguments:\n",
    "        d0 -- input pattern in {0,1}, numpy array of shape (n_d, 1)\n",
    "\n",
    "        Returns:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "        Norm -- updated mean and variance on every sampling\n",
    "        \"\"\"\n",
    "        a = self.Scalar[0,0]\n",
    "        b = self.Scalar[0,1]\n",
    "        S = d0*(a-b)+b  # input layer\n",
    "        Alpha_Q = {\"z0\":S}\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(S,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            S = ((q > np.random.rand(len(q),1)).astype(int))*(a-b)+b\n",
    "            Alpha_Q[\"z\"+str(i+1)] = S\n",
    "        Alpha_Q[\"z\"+str(self.n_layer-1)] = [[1]]\n",
    "        return Alpha_Q\n",
    "    \n",
    "    def sleep_sample(self,add_norm=True):\n",
    "        \"\"\"\n",
    "        Stochastic sample in sleep phase\n",
    "        Returns:\n",
    "        Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "        \"\"\"\n",
    "        S = [[1]]\n",
    "        Alpha_P = {\"z\"+str(self.n_layer-1):S}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            G,p = one_step_forward(S,self.ac,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)],add_norm)\n",
    "            a = self.Scalar[i-1,0]\n",
    "            b = self.Scalar[i-1,1]\n",
    "            S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "            Alpha_P[\"z\"+str(i-1)] = S\n",
    "        return Alpha_P\n",
    "    \n",
    "    def wake_update(self,Alpha_P,add_norm=True,check_lr=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "\n",
    "        Returns:\n",
    "        Phi, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        \"\"\"\n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "        for i in range(self.n_layer-2):\n",
    "            x = Alpha_P['z'+str(i)]\n",
    "            y = Alpha_P['z'+str(i+1)]\n",
    "            G,q = one_step_forward(x,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)    \n",
    "            \n",
    "            self.Scalar[i,:],self.lr[0,i],loss,Grad_set[\"grad_Phi_\"+str(i)+str(i+1)] = one_step_update(x,y,G,self.ac,\\\n",
    "             self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Scalar[[i,i+1],:],self.Norm_wake['sl_'+str(i+1)],self.lr[0,i],\\\n",
    "                self.Fz_binary[i],self.Fz_scale[i],self.fz_W,self.layer_norm,self.ln_rate,check_lr)\n",
    "            Loss[i] = loss\n",
    "            Loss[-1] += loss\n",
    "        return Loss,Grad_set\n",
    "    \n",
    "    def sleep_update(self,Alpha_Q,add_norm=True,check_lr=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "\n",
    "        Returns:\n",
    "        Theta, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        \"\"\"\n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            x = Alpha_Q['z'+str(i)]\n",
    "            y = Alpha_Q['z'+str(i-1)]\n",
    "            G,p = one_step_forward(x,self.ac,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)],add_norm)\n",
    "            \n",
    "            self.Scalar[i,:],self.lr[1,i-1],loss,Grad_set[\"grad_Theta_\"+str(i)+str(i-1)] = one_step_update(x,y,G,self.ac,\\\n",
    "                self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Scalar[[i,i-1],:],self.Norm_sleep['sl_'+str(i-1)],self.lr[1,i-1],\\\n",
    "                self.Fz_binary[i],self.Fz_scale[i-1],self.fz_W,self.layer_norm,self.ln_rate,check_lr)\n",
    "            Loss[i-1] = loss\n",
    "            Loss[-1] += loss\n",
    "        return Loss,Grad_set\n",
    "    \n",
    "    # update statistics\n",
    "    def norm_update(self):\n",
    "        \"\"\"\n",
    "        After eg.1000 steps,\n",
    "        Normalize layer pre-activation linear term to mean 0, variance 1 and offset norm matrices to count 0\n",
    "        Check current learning rate, and update it if needed\n",
    "        Check parameters, see if each matrix rows are evenly distributed, consider turning layer_norm on\n",
    "        \n",
    "        After enough training,\n",
    "        For last 2 columns of parameter matrix, adaptive scale and bias, may turn it on\n",
    "        For Scalar, binary values for each layer, could modify it by training\n",
    "        \"\"\"\n",
    "        for i in range(self.n_layer-2):\n",
    "            one_step_norm_update(self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)])\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            one_step_norm_update(self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)])\n",
    "            \n",
    "    def check_learning_rate(self,Alpha_P,Alpha_Q):\n",
    "        self.wake_update(Alpha_P,add_norm=False,check_lr=True) # see self.lr\n",
    "        self.sleep_update(Alpha_Q,add_norm=False,check_lr=True)\n",
    "    \n",
    "    # Compute latent statistics\n",
    "    def wake_sample_batch(self,D,add_norm=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        D -- data distriution represented in {0,1}, numpy array of shape (n_d, n_data)\n",
    "        Returns:\n",
    "        Alpha_Q_bnr -- assignment of each neuron as {1,0}, Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], n_data),i = 0,...m-1\n",
    "        Norm -- updated mean and variance on every sampling\n",
    "        \"\"\"\n",
    "        n_data = D.shape[1]\n",
    "        a = self.Scalar[0,0]\n",
    "        b = self.Scalar[0,1]\n",
    "        S = D*(a-b)+b  # input layer\n",
    "        Alpha_Q_bnr = {\"z0\":D}\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(S,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            S_bnr = (q > np.random.rand(len(q),n_data)).astype(int)\n",
    "            S = S_bnr*(a-b)+b\n",
    "            Alpha_Q_bnr[\"z\"+str(i+1)] = S_bnr\n",
    "        return Alpha_Q_bnr\n",
    "    \n",
    "    def sleep_sample_batch(self,n_sample,add_norm=False):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        Alpha_P_bnr -- assignment of each neuron (binary value) as {1,0}, Python dictionary of length m with each key-value pair \n",
    "        being a numpy array of shape (n_dz[0,i], n_data),i = m-1,...,0\n",
    "        \"\"\"\n",
    "        S = np.ones((1,n_sample))\n",
    "        Alpha_P_bnr = {\"z\"+str(self.n_layer-1):S}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            G,p = one_step_forward(S,self.ac,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)],add_norm)\n",
    "            a = self.Scalar[i-1,0]\n",
    "            b = self.Scalar[i-1,1]\n",
    "            S_bnr = (p > np.random.rand(len(p),n_sample)).astype(int)\n",
    "            S = S_bnr*(a-b)+b   # rejection sampling as a or b\n",
    "            Alpha_P_bnr[\"z\"+str(i-1)] = S_bnr\n",
    "        return Alpha_P_bnr\n",
    "    \n",
    "    # Compute latent distribution\n",
    "    def prob_matrix(self,data_dist):\n",
    "        # H(Y|x) & H(Y|X)\n",
    "        self.Prob_mtx_wake = {}\n",
    "        self.Prob_mtx_sleep = {}\n",
    "        dist = data_dist\n",
    "        for i in range(self.n_layer-2):\n",
    "            self.Prob_mtx_wake[\"Prob_\"+str(i)+str(i+1)] = one_step_prob_matrix(\\\n",
    "                self.ac,self.n_dz[0,i],self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)])\n",
    "            \n",
    "            self.Prob_mtx_wake[\"Prob_\"+str(i)+str(i+1)][-1,:] = \\\n",
    "            np.sum(self.Prob_mtx_wake[\"Prob_\"+str(i)+str(i+1)][:-1,:] * dist.reshape(-1,1),axis=0)\n",
    "            dist = self.Prob_mtx_wake[\"Prob_\"+str(i)+str(i+1)][-1,:-1]\n",
    "            \n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            self.Prob_mtx_sleep[\"Prob_\"+str(i)+str(i-1)] = one_step_prob_matrix(\\\n",
    "                self.ac,self.n_dz[0,i],self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)])\n",
    "            \n",
    "    def MI_wake(self,d0,MI_matrix,typ,precision=0.2,add_norm=True):\n",
    "        \"\"\"\n",
    "        Sample wake phase by maximize layer-wise mutual information\n",
    "        Arguments:\n",
    "        d0 -- input pattern in {0,1}, numpy array of shape (n_d, 1)\n",
    "\n",
    "        Returns:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "        Norm_wake, Phi, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        Nums -- numpy array of shape (3,m-2), row1: number of picked probabilities; row2: number of MI increase; \n",
    "        row3: number of intersection of two sets\n",
    "        \"\"\"\n",
    "        a = self.Scalar[0,0]\n",
    "        b = self.Scalar[0,1]\n",
    "        cat_x = decimalize(d0)[0]\n",
    "        x = d0*(a-b)+b  # input layer\n",
    "        Alpha_Q = {\"z0\":x}\n",
    "        \n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "#         Num = np.zeros(self.n_layer-2)\n",
    "        keys = [*MI_matrix]\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(x,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            \n",
    "            cat_y,MI_matrix[keys[i]] = determine_y(MI_matrix[keys[i]],cat_x,q,typ,precision)\n",
    "            y_bnr = binarilize([cat_y],width=self.n_dz[0,i+1])\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            y = y_bnr*(a-b)+b\n",
    "            \n",
    "            self.Scalar[i,:],self.lr[0,i],loss,Grad_set[\"grad_Phi_\"+str(i)+str(i+1)] = one_step_update(x,y,G,self.ac,\\\n",
    "             self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Scalar[[i,i+1],:],self.Norm_wake['sl_'+str(i+1)],self.lr[0,i],\\\n",
    "                self.Fz_binary[i],self.Fz_scale[i],self.fz_W,self.layer_norm,self.ln_rate,check_lr=False)\n",
    "            Loss[i] = loss\n",
    "            Loss[-1] += loss\n",
    "            \n",
    "            Alpha_Q[\"z\"+str(i+1)] = y\n",
    "            cat_x = cat_y\n",
    "            x = y\n",
    "        Alpha_Q[\"z\"+str(self.n_layer-1)] = [[1]]\n",
    "        \n",
    "        return Alpha_Q,MI_matrix,Loss,Grad_set\n",
    "    \n",
    "    def wake_sample_MI(self,d0,MI_matrix,add_norm=True):\n",
    "        \"\"\"\n",
    "        Stochastic sample in wake phase\n",
    "        Arguments:\n",
    "        d0 -- input pattern in {0,1}, numpy array of shape (n_d, 1)\n",
    "        MI_matrix -- count matrix in wake phase\n",
    "\n",
    "        Returns:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "        Norm -- updated mean and variance on every sampling\n",
    "        \"\"\"\n",
    "        a = self.Scalar[0,0]\n",
    "        b = self.Scalar[0,1]\n",
    "        S = d0*(a-b)+b  # input layer\n",
    "        Alpha_Q = {\"z0\":S}\n",
    "        keys = [*MI_matrix]\n",
    "        cat_x = decimalize(d0)[0]\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(S,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            S_bnr = (q > np.random.rand(len(q),1)).astype(int)\n",
    "            cat_y = decimalize(S_bnr)[0]\n",
    "            S = S_bnr*(a-b)+b\n",
    "            Alpha_Q[\"z\"+str(i+1)] = S\n",
    "            \n",
    "            # update count matrix\n",
    "            M = MI_matrix[keys[i]]\n",
    "            M[cat_x,-1] = entropy_itr(M[cat_x,-1],M[cat_x,-2],M[cat_x,cat_y])\n",
    "            M[-1,-1] = entropy_itr(M[-1,-1],M[-1,-2],M[-1,cat_y])\n",
    "            M[cat_x,cat_y] += 1\n",
    "            M[-1,cat_y] += 1\n",
    "            M[cat_x,-2] += 1\n",
    "            M[-1,-2] += 1\n",
    "            MI_matrix[keys[i]] = M\n",
    "            cat_x = cat_y\n",
    "            \n",
    "        Alpha_Q[\"z\"+str(self.n_layer-1)] = [[1]]\n",
    "        \n",
    "        return Alpha_Q,MI_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c7acd",
   "metadata": {},
   "source": [
    "#### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3094409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  6,  3,  1],\n",
       "       [ 9,  0,  5,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[10,8,6,3,1],\n",
    "             [9, 0,5,0,0],\n",
    "             [0, 0,0,0,0]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5b4fbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM = Helmholtz_machine(n_dz,'sigmoid',init_lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "303a8f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_set = np.array([1,0])\n",
    "value_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "21d59290",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM.parameter_init('random',value_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6a2594d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fz_binary = [True]*HM.n_layer\n",
    "Fz_scale = [True]*(HM.n_layer-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "84375bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM.set_control(Fz_binary,Fz_scale,fz_W=False,layer_norm=False,layer_norm_rate=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f8f97",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffb7ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_formed_generate(n_d):\n",
    "    \"\"\"\n",
    "    Well-formedness rules:\n",
    "        1. Start with 1\n",
    "        2. Forbid 00100 (no 100, 001 on the boundary)\n",
    "        3. Forbid 0000\n",
    "        \n",
    "    Arguments:\n",
    "    n_d -- length of input layer (single data point)\n",
    "    \n",
    "    Returns:\n",
    "    well_formed_set -- a dataset obeys the well-formedness rules, numpy array of shape (n_d,n_data) in {0,1}\n",
    "    n_data is the number of datapoints in the generated dataset\n",
    "    \"\"\"\n",
    "    well_formed_set = np.zeros([1,n_d],dtype=int)\n",
    "    well_formed_set[0,0] = 1\n",
    "\n",
    "    for i in range(1,n_d):\n",
    "        for j in range(np.shape(well_formed_set)[0]):\n",
    "            if i == 2 and np.array_equal(well_formed_set[j,i-2:i], [1,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            elif i > 3 and np.array_equal(well_formed_set[j,i-3:i], [0,0,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            elif i > 3 and np.array_equal(well_formed_set[j,i-4:i], [0,0,1,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            else:\n",
    "                well_formed_set = np.append(well_formed_set, well_formed_set[j:j+1,:], axis=0)\n",
    "                well_formed_set[j,i] = 1\n",
    "\n",
    "    ind = np.array([], dtype=np.int8)\n",
    "    for i in range(well_formed_set.shape[0]):\n",
    "        if np.array_equal(well_formed_set[i,-3:], [0,0,1]):\n",
    "            ind = np.append(ind,i)\n",
    "\n",
    "    well_formed_set = np.delete(well_formed_set,ind,0)\n",
    "    well_formed_set = np.transpose(well_formed_set)\n",
    "    \n",
    "    return well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f08793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 0, 1, ..., 1, 0, 1],\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set = well_formed_generate(HM.n_d)\n",
    "well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5f20588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 1, ..., 0, 1, 1],\n",
       "       [0, 1, 0, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_set = binarilize(np.arange(2**HM.n_d),HM.n_d)\n",
    "entire_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac35ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generate(k,n,n_data):\n",
    "    \"\"\"\n",
    "    The dataset is generated in a favor of Bayesian mixure of Gaussians. Given k mixture Gaussian components, we sample their \n",
    "    means u_1...u_k uniformly from [0,1]. Then we randomly assign each data to one of the components, and sample from its \n",
    "    Gaussian distribution (u_k, sigma). sigma is a hyperparameter, we default it to 1.\n",
    "    \n",
    "    The \"Bayesian mixure of Gaussians\" generation is just a way to generate dataset with non-singular distributions. The \n",
    "    generated data distribution is not identified with the mixure of Gaussian distributions that generated it. In other words, \n",
    "    the data is treated as sole evidence without any prior on how it's been generated thus its reconstruction is not convolved \n",
    "    with it's generative distributions, which is a major difference from varietional inference.\n",
    "        \n",
    "    Arguments:\n",
    "    k -- number of Gaussian components\n",
    "    n -- length of input layer (single data point)\n",
    "    n_data -- number of datapoints to generate\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    \n",
    "    Returns:\n",
    "    random_set -- generated dataset, numpy array of shape (n,n_data), n_data is the number of datapoints in the generated dataset\n",
    "    \"\"\"\n",
    "    u = np.random.rand(n,k)\n",
    "    c = np.random.randint(k, size=(n_data,))\n",
    "    data_mean = u[:,c]\n",
    "    prob = np.random.randn(n,n_data) + data_mean\n",
    "    random_set = (prob>0.5).astype(int)\n",
    "    \n",
    "    return random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a6cc913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [0, 0, 1, ..., 0, 0, 1],\n",
       "       [1, 0, 1, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 1, 0, 1],\n",
       "       [1, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "n_data = 300\n",
    "random_set = random_generate(k,HM.n_d,n_data)\n",
    "random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12eadddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 262)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(random_set,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f3ce388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = random_set\n",
    "n_data = dataset.shape[1]\n",
    "data_dcm = decimalize(dataset)\n",
    "values_d,counts_d = np.unique(data_dcm,return_counts=True)\n",
    "data_dist = np.zeros(2**HM.n_d)\n",
    "data_dist[values_d]=counts_d/n_data\n",
    "data_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2dcd7",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0d08a",
   "metadata": {},
   "source": [
    "##### wake-sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6efc5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "3cd23238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range (epoch):\n",
    "#     index = np.random.permutation(n_data)\n",
    "#     Loss_Q_total = np.zeros(HM.n_layer)\n",
    "#     Loss_P_total = np.zeros(HM.n_layer)\n",
    "#     for i in range(n_data):\n",
    "#         d0 = dataset[:,index[i]:index[i]+1]\n",
    "#         Alpha_Q = HM.wake_sample(d0)\n",
    "#         Loss_P,Grad_P = HM.sleep_update(Alpha_Q)\n",
    "#         Alpha_P = HM.sleep_sample()\n",
    "#         Loss_Q,Grad_Q = HM.wake_update(Alpha_P)\n",
    "        \n",
    "#         Loss_Q_total += Loss_Q\n",
    "#         Loss_P_total += Loss_P\n",
    "#     Loss_Q_total = Loss_Q_total/n_data\n",
    "#     Loss_P_total = Loss_P_total/n_data\n",
    "#     print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))\n",
    "#     if e % 100 == 0:\n",
    "#         HM.norm_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48d1ca",
   "metadata": {},
   "source": [
    "##### Wake-sleep & MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "aba45e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "e58e318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.74 2.7  1.98 2.05 8.47]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.7  2.61 1.95 2.07 8.32]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.76 2.74 1.97 2.05 8.52]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.68 2.   2.05 8.55]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.83 2.73 1.98 2.06 8.61]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.6  1.99 2.06 8.38]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.74 1.92 2.04 8.52]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.76 2.67 2.01 2.05 8.49]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.8  2.64 2.02 2.07 8.53]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.56 1.99 2.06 8.43]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.85 2.62 2.   2.06 8.53]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.89 2.72 2.   2.05 8.65]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.75 2.64 1.96 2.06 8.4 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.75 2.78 2.04 2.06 8.62]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.86 2.65 2.   2.05 8.56]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.71 2.53 2.   2.06 8.3 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.76 2.75 1.97 2.06 8.53]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.83 2.6  2.01 2.05 8.5 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.71 2.66 1.96 2.06 8.39]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.86 2.65 1.97 2.05 8.53]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.89 2.53 2.01 2.06 8.48]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.74 2.63 1.96 2.05 8.38]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.77 2.6  2.04 2.06 8.47]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.85 2.51 2.04 2.05 8.45]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.7  2.02 2.04 8.49]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.79 2.64 1.99 2.06 8.48]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.6  1.99 2.05 8.38]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.89 2.65 2.   2.06 8.6 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.72 2.72 1.96 2.05 8.45]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.7  2.61 1.98 2.06 8.34]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.56 2.   2.04 8.33]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.7  2.57 2.   2.05 8.32]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.64 1.96 2.05 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.76 2.71 1.96 2.06 8.5 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.52 2.03 2.05 8.38]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.6  1.99 2.05 8.38]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.77 2.56 1.97 2.06 8.36]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.79 2.58 2.02 2.06 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.79 2.61 1.99 2.05 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.79 2.74 2.   2.06 8.59]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.74 2.62 2.02 2.06 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.86 2.72 1.98 2.05 8.61]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.72 2.67 2.   2.05 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.62 1.95 2.06 8.41]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.54 1.96 2.07 8.3 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.66 1.98 2.07 8.48]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.86 2.52 1.98 2.07 8.42]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.74 2.76 2.03 2.06 8.59]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.69 2.76 1.96 2.05 8.46]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.85 2.7  1.98 2.06 8.59]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.85 2.75 1.99 2.06 8.64]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.49 1.99 2.06 8.32]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.97 2.82 1.94 2.04 8.76]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.81 2.53 2.02 2.06 8.43]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.87 2.69 1.99 2.05 8.59]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.79 2.57 2.01 2.07 8.43]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.77 2.57 1.96 2.06 8.37]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.83 2.78 2.   2.04 8.65]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.67 1.97 2.04 8.46]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.69 2.   2.04 8.5 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.57 1.99 2.05 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.56 2.01 2.06 8.36]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.96 2.59 2.01 2.06 8.62]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.85 2.61 2.05 2.05 8.55]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.73 2.51 2.   2.06 8.3 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.83 2.61 1.96 2.06 8.45]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.54 1.98 2.05 8.4 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.83 2.64 1.98 2.06 8.51]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.76 2.61 1.95 2.05 8.38]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.69 2.5  1.96 2.05 8.2 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.65 1.98 2.05 8.46]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.76 2.59 2.02 2.05 8.42]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.92 2.71 2.02 2.07 8.71]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.49 2.   2.06 8.37]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.68 2.68 1.98 2.04 8.38]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.68 2.55 2.01 2.07 8.32]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.75 2.51 2.01 2.06 8.33]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.74 2.67 2.   2.05 8.47]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.8  2.43 2.   2.04 8.26]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.83 2.55 1.98 2.06 8.42]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.77 2.66 2.01 2.06 8.5 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.86 2.68 2.   2.04 8.58]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.74 2.69 1.96 2.05 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.66 2.62 1.99 2.05 8.33]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.7  2.01 2.06 8.58]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.87 2.59 2.01 2.05 8.52]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.82 2.44 1.98 2.06 8.31]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.84 2.61 2.01 2.06 8.52]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.81 2.56 2.08 2.05 8.5 ]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.72 2.59 1.99 2.06 8.36]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.74 2.62 1.99 2.05 8.41]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.85 2.57 2.01 2.07 8.49]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.77 2.54 1.99 2.06 8.36]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.85 2.74 1.99 2.03 8.62]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.92 2.76 1.97 2.07 8.71]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.71 2.67 1.97 2.04 8.39]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.72 2.59 1.99 2.05 8.35]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.86 2.79 1.96 2.03 8.64]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.78 2.62 1.98 2.05 8.44]\n",
      "Loss_Q: [0.03 0.19 0.   0.   0.22] Loss_P: [1.76 2.61 1.99 2.04 8.4 ]\n",
      "delta_loss: 5.461584021947824e-05 delta_f: 5.469111832597065e-05\n",
      "learning rate change: 0.07778809074243233\n",
      "delta_loss: 0.005291199079331704 delta_f: 0.005375205373427814\n",
      "learning rate change: 0.08615127861330774\n",
      "delta_loss: 2.7492855093360323e-07 delta_f: 2.7503538832461014e-07\n",
      "learning rate change: 0.09784568538686658\n",
      "delta_loss: 0.05296581160680125 delta_f: 0.05353316053834726\n",
      "learning rate change: 0.08789331159450615\n",
      "delta_loss: 0.12240966732752145 delta_f: 0.12476505280403381\n",
      "learning rate change: 0.0718280732423221\n",
      "delta_loss: 0.022025874615063 delta_f: 0.02235462485755971\n",
      "learning rate change: 0.07282739534154947\n",
      "delta_loss: 0.1287785482060848 delta_f: 0.13195931227752467\n",
      "learning rate change: 0.05177994802280325\n"
     ]
    }
   ],
   "source": [
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(HM.n_layer)\n",
    "    Loss_P_total = np.zeros(HM.n_layer)\n",
    "#     Nums_total = np.zeros((HM.n_layer-2))\n",
    "    \n",
    "#     HM.prob_matrix(data_dist)\n",
    "#     MI_mtx = init_M(HM.Prob_mtx_wake,fc = 10)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        \n",
    "        # MI & wake-sleep\n",
    "#         if i % 2 == 0:\n",
    "#             # type 1(MI): only MI; type 2(p): MI in top 2**num samples; type 3(p): MI * log(1+p); type 4(MI): top 10 MI ordered by p\n",
    "#             Alpha_Q,MI_mtx,Loss_Q,Grad_Q = HM.MI_wake(d0,MI_mtx,typ=1,precision=0.3,add_norm=False) \n",
    "#             Loss_P,Grad_P = HM.sleep_update(Alpha_Q,add_norm=False)\n",
    "#         else:\n",
    "#             Alpha_Q,MI_mtx = HM.wake_sample_MI(d0,MI_mtx,add_norm=False)\n",
    "#             Loss_P,Grad_P = HM.sleep_update(Alpha_Q,add_norm=False)\n",
    "#             Alpha_P = HM.sleep_sample(add_norm=False)\n",
    "#             Loss_Q,Grad_Q = HM.wake_update(Alpha_P,add_norm=False)\n",
    "        \n",
    "#         # MI\n",
    "#         Alpha_Q,MI_mtx,Loss_Q,Grad_Q = HM.MI_wake(d0,MI_mtx,typ=2,precision=0.3)\n",
    "#         Loss_P,Grad_P = HM.sleep_update(Alpha_Q)\n",
    "\n",
    "        # wake-sleep\n",
    "        Alpha_Q,MI_mtx = HM.wake_sample_MI(d0,MI_mtx,add_norm=False)\n",
    "        Loss_P,Grad_P = HM.sleep_update(Alpha_Q,add_norm=False)\n",
    "#         Alpha_P = HM.sleep_sample(add_norm=False)\n",
    "#         Loss_Q,Grad_Q = HM.wake_update(Alpha_P,add_norm=False)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "        \n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    \n",
    "#     if e % 100 == 0:\n",
    "#         HM.norm_update()\n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))\n",
    "    \n",
    "HM.check_learning_rate(Alpha_P,Alpha_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "fed80379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MI_Prob_01': array([[2.54835656e+00, 2.10930845e+00, 1.11711305e+01, ...,\n",
       "         5.83826742e+00, 2.56000000e+03, 4.71813361e+00],\n",
       "        [8.40204665e-04, 2.20397954e-05, 7.70525030e-02, ...,\n",
       "         1.03584376e-05, 2.56000000e+03, 9.02219826e-01],\n",
       "        [8.46567123e-03, 1.21279196e-04, 5.37072982e-05, ...,\n",
       "         1.80832800e-05, 2.56000000e+03, 1.26090352e+00],\n",
       "        ...,\n",
       "        [6.64200205e-07, 5.45006013e-05, 3.09555218e-08, ...,\n",
       "         1.13586145e-05, 2.56000000e+03, 4.33297547e-01],\n",
       "        [7.15150137e-03, 1.90707363e-01, 2.12561960e-04, ...,\n",
       "         2.52443386e-06, 2.56000000e+03, 8.16716273e-01],\n",
       "        [1.00483545e+01, 9.06058731e+00, 1.89299765e+01, ...,\n",
       "         5.74612541e-01, 2.86000000e+03, 5.10309137e+00]]),\n",
       " 'MI_Prob_12': array([[8.79444386e+00, 5.12014160e+00, 1.00595423e+01, ...,\n",
       "         9.76554371e+00, 6.41000000e+02, 4.08490738e+00],\n",
       "        [1.49000211e+01, 3.52730135e+00, 2.46758408e+01, ...,\n",
       "         4.13327454e-03, 6.41000000e+02, 3.13351434e+00],\n",
       "        [1.14823909e-03, 2.89671572e+00, 1.86213396e-03, ...,\n",
       "         7.41102985e+01, 6.42000000e+02, 3.17207025e+00],\n",
       "        ...,\n",
       "        [3.61179660e+02, 2.70064733e+02, 2.46050892e+00, ...,\n",
       "         4.90064065e-13, 6.41000000e+02, 7.78594682e-01],\n",
       "        [4.81405173e+02, 1.45823103e+02, 4.74818517e+00, ...,\n",
       "         1.63177264e-16, 6.40000000e+02, 6.61297516e-01],\n",
       "        [2.80284885e+01, 7.50632903e+00, 2.63574236e+01, ...,\n",
       "         2.60539386e+00, 9.40000000e+02, 3.93012069e+00]]),\n",
       " 'MI_Prob_23': array([[8.68792730e+01, 4.23385225e-01, 2.97725480e-01, 1.59804426e-03,\n",
       "         3.94404648e-01, 2.11697058e-03, 1.48865867e-03, 7.99038910e-06,\n",
       "         8.80000000e+01, 8.24556295e-02],\n",
       "        [8.18598098e+01, 1.99875776e-02, 9.81182680e-02, 2.45573649e-05,\n",
       "         2.20272264e-02, 5.51304714e-06, 2.70633415e-05, 6.77350270e-09,\n",
       "         8.20000000e+01, 1.40060928e-02],\n",
       "        [1.36204374e-01, 4.25886774e-05, 6.52796958e-06, 2.04117960e-09,\n",
       "         8.68349560e+01, 2.49629662e-02, 3.82631003e-03, 1.19641887e-06,\n",
       "         8.70000000e+01, 1.47981304e-02],\n",
       "        [1.13552549e-01, 2.31787570e-05, 1.29049490e-05, 2.63420486e-09,\n",
       "         8.58610320e+01, 1.63015227e-02, 9.07599655e-03, 1.85262524e-06,\n",
       "         8.60000000e+01, 1.29662626e-02],\n",
       "        [5.89260377e-02, 9.09055244e+01, 2.33276985e-05, 3.16330785e-02,\n",
       "         2.86773992e-06, 3.88874377e-03, 1.13528373e-09, 1.53947976e-06,\n",
       "         9.10000000e+01, 8.99531728e-03],\n",
       "        [8.04723491e-02, 8.38963107e+01, 2.12144343e-05, 2.10625768e-02,\n",
       "         2.14579371e-06, 2.13043366e-03, 5.65682501e-10, 5.61633224e-07,\n",
       "         8.40000000e+01, 1.02441383e-02],\n",
       "        [6.22315640e-05, 4.01004965e-02, 4.78034475e-09, 3.08033714e-06,\n",
       "         1.23887086e-01, 8.28298054e+01, 9.51644054e-06, 6.13216134e-03,\n",
       "         8.30000000e+01, 1.61659623e-02],\n",
       "        [1.82192195e-05, 2.73259240e-02, 3.73235262e-09, 5.59793378e-06,\n",
       "         5.32743275e-02, 8.29029962e+01, 1.09136715e-05, 1.63687670e-02,\n",
       "         8.30000000e+01, 1.02154762e-02],\n",
       "        [8.28717649e+01, 6.97227493e-02, 3.26875533e-02, 2.85340644e-05,\n",
       "         2.57632003e-02, 2.24895638e-05, 1.05436005e-05, 9.20386344e-09,\n",
       "         8.30000000e+01, 1.30989538e-02],\n",
       "        [8.29669961e+01, 8.48294658e-03, 1.74750472e-02, 1.85376341e-06,\n",
       "         7.04177520e-03, 7.46995708e-07, 1.53882676e-06, 1.63239660e-10,\n",
       "         8.30000000e+01, 3.91533179e-03],\n",
       "        [1.50235966e-01, 2.44270280e-05, 4.39954103e-06, 7.15326131e-10,\n",
       "         8.28344166e+01, 1.29803641e-02, 2.33788754e-03, 3.80119661e-07,\n",
       "         8.30000000e+01, 1.50934777e-02],\n",
       "        [1.22173601e-01, 2.00383762e-05, 8.48155231e-06, 1.39110687e-09,\n",
       "         8.08591548e+01, 1.30981470e-02, 5.54399307e-03, 9.09301334e-07,\n",
       "         8.10000000e+01, 1.36101310e-02],\n",
       "        [6.19174434e-02, 8.69181879e+01, 1.46008927e-05, 1.88456890e-02,\n",
       "         8.00574589e-07, 1.03331900e-03, 1.88785308e-10, 2.43669295e-07,\n",
       "         8.70000000e+01, 8.06341431e-03],\n",
       "        [6.54528469e-02, 8.19162645e+01, 1.37097983e-05, 1.67393157e-02,\n",
       "         1.25151064e-06, 1.52806272e-03, 2.62142278e-10, 3.20069067e-07,\n",
       "         8.20000000e+01, 8.65476249e-03],\n",
       "        [4.75706567e-05, 3.83189456e-02, 4.28080244e-09, 3.44825671e-06,\n",
       "         9.91355315e-02, 8.18552995e+01, 8.92103778e-06, 7.18604251e-03,\n",
       "         8.20000000e+01, 1.42979647e-02],\n",
       "        [1.47538227e-05, 2.74194439e-02, 3.49308286e-09, 6.49176770e-06,\n",
       "         4.29982202e-02, 8.19106314e+01, 1.01801648e-05, 1.89194667e-02,\n",
       "         8.20000000e+01, 9.66435495e-03],\n",
       "        [8.69363575e+01, 1.30047138e-02, 1.05384733e-02, 1.71448680e-06,\n",
       "         4.00857583e-02, 6.52148572e-06, 5.28473785e-06, 8.59765262e-10,\n",
       "         8.70000000e+01, 6.68277774e-03],\n",
       "        [9.09772507e+01, 2.09845992e-03, 7.17429049e-03, 1.88240543e-07,\n",
       "         1.34747831e-02, 3.53554194e-07, 1.20874383e-06, 3.17152749e-11,\n",
       "         9.10000000e+01, 2.54710394e-03],\n",
       "        [1.64448873e-01, 1.49491192e-05, 3.54624798e-06, 3.22369394e-10,\n",
       "         8.58265545e+01, 7.25658166e-03, 1.72141500e-03, 1.56484125e-07,\n",
       "         8.60000000e+01, 1.49956615e-02],\n",
       "        [1.41527611e-01, 1.25436198e-05, 6.54743403e-06, 5.80300356e-10,\n",
       "         8.68476821e+01, 7.07691564e-03, 3.69396068e-03, 3.27396456e-07,\n",
       "         8.70000000e+01, 1.33919257e-02],\n",
       "        [1.89703224e-01, 8.88031145e+01, 1.28100255e-05, 5.38883795e-03,\n",
       "         4.22251451e-06, 1.77629985e-03, 2.85132311e-10, 1.19947600e-07,\n",
       "         8.90000000e+01, 1.61275736e-02],\n",
       "        [1.97119923e-01, 9.17937261e+01, 1.20624795e-05, 4.88286610e-03,\n",
       "         1.04947844e-05, 4.24826645e-03, 6.42213737e-10, 2.59966757e-07,\n",
       "         9.20000000e+01, 1.63948831e-02],\n",
       "        [9.40547182e-05, 4.15255748e-02, 4.34938642e-09, 1.92027338e-06,\n",
       "         1.80686736e-01, 8.87739944e+01, 8.35552377e-06, 3.68900077e-03,\n",
       "         8.90000000e+01, 1.91359844e-02],\n",
       "        [3.30454933e-05, 3.28004871e-02, 3.92987627e-09, 3.90073934e-06,\n",
       "         8.04738298e-02, 8.58771799e+01, 9.57020648e-06, 9.49925095e-03,\n",
       "         8.60000000e+01, 1.19699868e-02],\n",
       "        [8.09866004e+01, 4.33592471e-03, 2.05146918e-03, 1.11206326e-07,\n",
       "         7.01157023e-03, 3.80084171e-07, 1.79830374e-07, 9.74826982e-12,\n",
       "         8.10000000e+01, 1.76986938e-03],\n",
       "        [8.39886413e+01, 1.56832682e-03, 1.98588221e-03, 3.89369324e-08,\n",
       "         7.80412032e-03, 1.53014365e-07, 1.93753306e-07, 3.79889570e-12,\n",
       "         8.40000000e+01, 1.45297619e-03],\n",
       "        [1.73937229e-01, 1.16121868e-05, 2.74296939e-06, 1.83122802e-10,\n",
       "         8.08194608e+01, 5.32881023e-03, 1.25874339e-03, 8.40347026e-08,\n",
       "         8.10000000e+01, 1.62272015e-02],\n",
       "        [1.51702390e-01, 1.30604613e-05, 5.13522088e-06, 4.42104794e-10,\n",
       "         8.08387031e+01, 6.87352583e-03, 2.70259009e-03, 2.32673153e-07,\n",
       "         8.10000000e+01, 1.48945506e-02],\n",
       "        [1.67771741e-01, 8.28266485e+01, 9.65457860e-06, 4.59369765e-03,\n",
       "         2.04772197e-06, 9.74316540e-04, 1.17838038e-10, 5.60679382e-08,\n",
       "         8.30000000e+01, 1.53048685e-02],\n",
       "        [1.55404786e-01, 8.38361020e+01, 9.18807426e-06, 4.72018947e-03,\n",
       "         7.31185286e-06, 3.75631823e-03, 4.32302305e-10, 2.22086667e-07,\n",
       "         8.40000000e+01, 1.45918383e-02],\n",
       "        [7.56090247e-05, 4.04788499e-02, 4.12714247e-09, 2.20955079e-06,\n",
       "         1.49066852e-01, 8.28060121e+01, 8.13686117e-06, 4.35623635e-03,\n",
       "         8.30000000e+01, 1.79400980e-02],\n",
       "        [3.11983406e-05, 3.64671571e-02, 4.25363465e-09, 4.97199402e-06,\n",
       "         6.83424852e-02, 8.18842533e+01, 9.31793030e-06, 1.08915545e-02,\n",
       "         8.20000000e+01, 1.19461133e-02],\n",
       "        [1.98869641e-02, 1.81434634e-05, 9.39062995e+01, 7.29008716e-02,\n",
       "         2.22377435e-07, 2.02881487e-10, 8.93517875e-04, 8.15182687e-07,\n",
       "         9.40000000e+01, 8.45384972e-03],\n",
       "        [3.65180058e-02, 1.51030071e-05, 8.79300405e+01, 3.30572259e-02,\n",
       "         1.68498533e-07, 6.96871169e-11, 3.68806957e-04, 1.52530072e-07,\n",
       "         8.80000000e+01, 7.04356575e-03],\n",
       "        [3.11942881e-06, 2.92238388e-09, 2.00784451e-02, 1.88101501e-05,\n",
       "         1.24122844e-02, 1.16282378e-05, 8.28926297e+01, 7.48460526e-02,\n",
       "         8.30000000e+01, 1.09530345e-02],\n",
       "        [2.15201142e-06, 1.65586241e-09, 1.22217303e-02, 9.40399463e-06,\n",
       "         1.40709994e-02, 1.08269123e-05, 8.79121965e+01, 6.14883365e-02,\n",
       "         8.80000000e+01, 8.70972806e-03],\n",
       "        [1.00893720e-05, 1.74796168e-02, 4.61394639e-02, 8.39356138e+01,\n",
       "         9.54796353e-11, 1.65416383e-07, 4.36635616e-07, 7.56461672e-04,\n",
       "         8.40000000e+01, 6.76036773e-03],\n",
       "        [1.49184735e-05, 2.13000877e-02, 5.59770602e-02, 8.59221376e+01,\n",
       "         1.06350536e-10, 1.51843669e-07, 3.99048227e-07, 5.69747450e-04,\n",
       "         8.60000000e+01, 7.81906697e-03],\n",
       "        [1.81612461e-10, 1.28685977e-06, 3.62650986e-06, 2.56965278e-02,\n",
       "         5.65117303e-07, 4.00427767e-03, 1.12844871e-02, 8.69590092e+01,\n",
       "         8.70000000e+01, 4.49318282e-03],\n",
       "        [2.56200969e-10, 9.95986703e-07, 2.74650465e-06, 1.06770951e-02,\n",
       "         1.91870164e-06, 7.45899333e-03, 2.05687082e-02, 8.19612895e+01,\n",
       "         8.20000000e+01, 4.56411156e-03],\n",
       "        [2.56962064e-02, 1.44409697e-05, 8.09289894e+01, 4.49191644e-02,\n",
       "         1.22318857e-07, 6.87417783e-11, 3.80477278e-04, 2.13823815e-07,\n",
       "         8.10000000e+01, 7.65009655e-03],\n",
       "        [3.80859709e-02, 1.31198867e-05, 8.39340640e+01, 2.75357523e-02,\n",
       "         1.43393875e-07, 4.93964406e-11, 3.00952160e-04, 1.03672248e-07,\n",
       "         8.40000000e+01, 6.95273500e-03],\n",
       "        [3.12906856e-06, 2.79018605e-09, 2.08118560e-02, 1.85579028e-05,\n",
       "         1.20123717e-02, 1.07114150e-05, 8.08959003e+01, 7.12430622e-02,\n",
       "         8.10000000e+01, 1.09106507e-02],\n",
       "        [2.49912391e-06, 1.83324673e-09, 1.38888321e-02, 1.01882328e-05,\n",
       "         1.43793738e-02, 1.05480725e-05, 7.99130879e+01, 5.86207058e-02,\n",
       "         8.00000000e+01, 9.43335824e-03],\n",
       "        [1.25254748e-05, 1.99805843e-02, 5.01062877e-02, 8.39293381e+01,\n",
       "         8.80738757e-11, 1.40495074e-07, 3.52326362e-07, 5.62029522e-04,\n",
       "         8.40000000e+01, 7.33642606e-03],\n",
       "        [1.65476247e-05, 2.25032419e-02, 5.87673575e-02, 7.99181809e+01,\n",
       "         1.10040475e-10, 1.49644887e-07, 3.90798562e-07, 5.31449966e-04,\n",
       "         8.00000000e+01, 8.70555458e-03],\n",
       "        [2.53293758e-10, 1.77812981e-06, 4.16239098e-06, 2.92201101e-02,\n",
       "         6.93083644e-07, 4.86546807e-03, 1.13894836e-02, 8.19545183e+01,\n",
       "         8.20000000e+01, 5.19623097e-03],\n",
       "        [6.28677339e-10, 2.04571918e-06, 5.01538321e-06, 1.63200818e-02,\n",
       "         3.07977464e-06, 1.00216019e-02, 2.45694397e-02, 8.19490787e+01,\n",
       "         8.20000000e+01, 5.85099210e-03],\n",
       "        [3.53588316e-02, 1.46432404e-05, 8.79310878e+01, 3.31020591e-02,\n",
       "         1.93013835e-07, 7.99332972e-11, 4.36321143e-04, 1.80694754e-07,\n",
       "         8.80000000e+01, 6.95416831e-03],\n",
       "        [5.70710497e-02, 1.38775205e-05, 9.39229473e+01, 1.94342376e-02,\n",
       "         3.80600845e-07, 9.25477288e-11, 5.32997753e-04, 1.29604892e-07,\n",
       "         9.40000000e+01, 7.14127273e-03],\n",
       "        [3.15819101e-06, 1.95669754e-09, 1.38861322e-02, 8.60333039e-06,\n",
       "         1.81762329e-02, 1.12613170e-05, 8.49184001e+01, 4.95144645e-02,\n",
       "         8.50000000e+01, 8.53408504e-03],\n",
       "        [2.77364680e-06, 1.62975290e-09, 1.20108041e-02, 7.05736679e-06,\n",
       "         1.84564601e-02, 1.08447368e-05, 8.59225508e+01, 4.69612818e-02,\n",
       "         8.60000000e+01, 8.05891631e-03],\n",
       "        [1.55175240e-05, 2.21703435e-02, 5.59387096e-02, 8.89212818e+01,\n",
       "         1.15146347e-10, 1.64512977e-07, 4.15088004e-07, 5.93048456e-04,\n",
       "         8.90000000e+01, 7.66689628e-03],\n",
       "        [2.23071866e-05, 2.61091617e-02, 6.82692983e-02, 8.39049283e+01,\n",
       "         1.87094890e-10, 2.18982825e-07, 5.72588426e-07, 6.70178809e-04,\n",
       "         8.40000000e+01, 9.52198348e-03],\n",
       "        [3.01796379e-10, 1.23992772e-06, 2.84977671e-06, 1.17082821e-02,\n",
       "         2.06108339e-06, 8.46794263e-03, 1.94622198e-02, 8.49603554e+01,\n",
       "         8.50000000e+01, 4.52925242e-03],\n",
       "        [9.19296905e-10, 1.98538993e-06, 5.03230621e-06, 1.08681863e-02,\n",
       "         6.76159715e-06, 1.46029067e-02, 3.70135341e-02, 8.29375016e+01,\n",
       "         8.30000000e+01, 6.88785685e-03],\n",
       "        [3.86413112e-02, 1.29838406e-05, 8.49341852e+01, 2.68586311e-02,\n",
       "         1.45804947e-07, 4.89918211e-11, 3.01615016e-04, 1.01345456e-07,\n",
       "         8.50000000e+01, 6.86641866e-03],\n",
       "        [5.69872641e-02, 1.29337739e-05, 8.79243749e+01, 1.81395582e-02,\n",
       "         3.45710341e-07, 7.84620821e-11, 4.84857158e-04, 1.10042708e-07,\n",
       "         8.80000000e+01, 7.43229618e-03],\n",
       "        [3.30543137e-06, 2.05069424e-09, 1.47520507e-02, 9.15219291e-06,\n",
       "         1.79068387e-02, 1.11094278e-05, 8.39177365e+01, 4.95810755e-02,\n",
       "         8.40000000e+01, 8.69253640e-03],\n",
       "        [4.12956986e-06, 2.27774598e-09, 1.63378916e-02, 9.01148749e-06,\n",
       "         2.02004378e-02, 1.11419513e-05, 8.19193563e+01, 4.40811027e-02,\n",
       "         8.20000000e+01, 8.77941364e-03],\n",
       "        [1.80512109e-05, 2.44431041e-02, 5.90178946e-02, 8.19159983e+01,\n",
       "         1.17932483e-10, 1.59692110e-07, 3.85576728e-07, 5.22108580e-04,\n",
       "         8.20000000e+01, 8.73189954e-03],\n",
       "        [2.58244205e-05, 2.78315682e-02, 7.41353542e-02, 8.48973656e+01,\n",
       "         2.07124248e-10, 2.23222536e-07, 5.94601126e-07, 6.40815224e-04,\n",
       "         8.50000000e+01, 1.00718683e-02],\n",
       "        [5.96234484e-10, 2.23520751e-06, 4.01699012e-06, 1.50591868e-02,\n",
       "         3.16550843e-06, 1.18670899e-02, 2.13268713e-02, 7.99517374e+01,\n",
       "         8.00000000e+01, 5.72149462e-03],\n",
       "        [6.32590975e-09, 8.89184161e-06, 1.97413577e-05, 2.77488983e-02,\n",
       "         1.82124516e-05, 2.55998332e-02, 5.68358601e-02, 8.08897686e+01,\n",
       "         8.10000000e+01, 1.17451375e-02],\n",
       "        [5.13012763e+01, 6.52254896e+01, 7.93340555e+01, 4.20177241e+01,\n",
       "         4.13301051e+01, 3.72860430e+01, 3.66225939e+01, 2.68827125e+01,\n",
       "         3.80000000e+02, 2.02531205e+00]])}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "fc531c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_M(MI_mtx['MI_Prob_01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "ba1edfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.51928191, 1.51791469, 1.52087758, 1.51788483, 1.51931307,\n",
       "        1.5173035 , 1.5218682 , 1.5185824 , 1.51703268, 1.51690761,\n",
       "        1.51702476, 1.51760939, 1.51630805, 1.5175223 , 1.51774905,\n",
       "        1.51755216, 1.52187543, 1.517512  , 1.5237119 , 1.5193406 ,\n",
       "        1.52191155, 1.51772629, 1.52381179, 1.51964044, 1.51750756,\n",
       "        1.51687402, 1.51783402, 1.51825551, 1.517324  , 1.51696747,\n",
       "        1.51764924, 1.51730915, 1.52034379, 1.51743946, 1.52305258,\n",
       "        1.51864306, 1.52176939, 1.51800592, 1.52312497, 1.51930765,\n",
       "        1.51748027, 1.51662977, 1.51803927, 1.51806317, 1.51701136,\n",
       "        1.51824596, 1.51805379, 1.51793209, 1.52352101, 1.51844293,\n",
       "        1.52588795, 1.52107487, 1.52389362, 1.51992025, 1.52635514,\n",
       "        1.52184144, 1.51666818, 1.51610409, 1.51700916, 1.51716333,\n",
       "        1.51786071, 1.51661067, 1.51928016, 1.51834302]),\n",
       " array([[2.41037698, 2.41309517, 2.40884545, 2.41247717, 2.41017823,\n",
       "         2.41303098, 2.40862578, 2.41236363, 2.41367725, 2.41367725,\n",
       "         2.41367725, 2.41367725, 2.41367725, 2.41367725, 2.41367725,\n",
       "         2.41367725, 2.40783442, 2.41190051, 2.40616426, 2.41066922,\n",
       "         2.4076086 , 2.41175314, 2.40593397, 2.41047717, 2.41367725,\n",
       "         2.41367725, 2.41367725, 2.41367725, 2.41367725, 2.41367725,\n",
       "         2.41367725, 2.41367725, 2.40884976, 2.41247933, 2.40721018,\n",
       "         2.41147749, 2.40863013, 2.41236594, 2.40698212, 2.41131118,\n",
       "         2.41367725, 2.41367725, 2.41367725, 2.41367725, 2.41367725,\n",
       "         2.41367725, 2.41367725, 2.41367725, 2.40616879, 2.41067295,\n",
       "         2.40447693, 2.40917822, 2.40593851, 2.410481  , 2.40422558,\n",
       "         2.40896158, 2.41367725, 2.41367725, 2.41367725, 2.41367725,\n",
       "         2.41367725, 2.41367725, 2.41367725, 2.41367725],\n",
       "        [3.92965888, 3.93100986, 3.92972304, 3.93036199, 3.9294913 ,\n",
       "         3.93033449, 3.93049399, 3.93094603, 3.93070994, 3.93058486,\n",
       "         3.93070201, 3.93128664, 3.92998531, 3.93119955, 3.9314263 ,\n",
       "         3.93122942, 3.92970985, 3.92941252, 3.92987616, 3.93000982,\n",
       "         3.92952015, 3.92947943, 3.92974576, 3.93011762, 3.93118482,\n",
       "         3.93055128, 3.93151127, 3.93193277, 3.93100126, 3.93064472,\n",
       "         3.93132649, 3.9309864 , 3.92919355, 3.92991879, 3.93026276,\n",
       "         3.93012055, 3.93039951, 3.93037186, 3.93010709, 3.93061883,\n",
       "         3.93115752, 3.93030702, 3.93171652, 3.93174042, 3.93068862,\n",
       "         3.93192321, 3.93173104, 3.93160934, 3.9296898 , 3.92911589,\n",
       "         3.93036488, 3.93025309, 3.92983213, 3.93040124, 3.93058073,\n",
       "         3.93080302, 3.93034543, 3.92978134, 3.93068641, 3.93084058,\n",
       "         3.93153796, 3.93028792, 3.93295741, 3.93202027]]))"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info(MI_mtx['MI_Prob_12'][[5,-1],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "fe694d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79244\\AppData\\Local\\Temp\\ipykernel_26484\\1478077684.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  y = 1/(1+np.exp(-x))\n",
      "C:\\Users\\79244\\AppData\\Local\\Temp\\ipykernel_26484\\2618074585.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  prob_mtx[:-1,-1] = -np.sum(prob_mtx[:-1,:-1]*np.log(prob_mtx[:-1,:-1]),axis=1)\n",
      "C:\\Users\\79244\\AppData\\Local\\Temp\\ipykernel_26484\\2618074585.py:14: RuntimeWarning: invalid value encountered in multiply\n",
      "  prob_mtx[:-1,-1] = -np.sum(prob_mtx[:-1,:-1]*np.log(prob_mtx[:-1,:-1]),axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Prob_01': array([[6.35244012e-04, 5.50594295e-04, 2.49406431e-03, ...,\n",
       "         3.09573665e-03, 2.68321291e-03, 4.58702400e+00],\n",
       "        [2.82385562e-07, 7.55652993e-09, 2.46209119e-05, ...,\n",
       "         1.86688735e-07, 4.99571936e-09, 8.96751821e-01],\n",
       "        [2.22044879e-06, 3.30047665e-08, 1.26878214e-08, ...,\n",
       "         4.27608779e-07, 6.35597990e-09, 1.19423354e+00],\n",
       "        ...,\n",
       "        [1.29639079e-10, 1.14646164e-08, 5.00129310e-12, ...,\n",
       "         1.12945628e-10, 9.98833308e-09, 4.56914310e-01],\n",
       "        [3.16827737e-06, 8.79664766e-05, 7.78738185e-08, ...,\n",
       "         2.24349635e-10, 6.22901490e-09, 1.06082207e+00],\n",
       "        [3.47935969e-03, 3.14641201e-03, 6.57073289e-03, ...,\n",
       "         3.30967099e-03, 3.06572545e-04, 2.68085631e-01]]),\n",
       " 'Prob_12': array([[1.37413185e-02, 8.00022125e-03, 1.57180349e-02, ...,\n",
       "         2.62085421e-02, 1.52586620e-02, 4.08391377e+00],\n",
       "        [1.65799611e-02, 4.64596277e-03, 3.04813937e-02, ...,\n",
       "         3.39080829e-05, 9.50157179e-06, 3.15225119e+00],\n",
       "        [1.89173598e-06, 5.11736232e-03, 3.29247297e-06, ...,\n",
       "         4.03460979e-05, 1.09140812e-01, 3.20608242e+00],\n",
       "        ...,\n",
       "        [4.53735481e-01, 5.29684710e-01, 4.14785855e-03, ...,\n",
       "         5.08038242e-16, 5.93077025e-16, 7.93351283e-01],\n",
       "        [6.24649453e-01, 3.50969751e-01, 9.17780936e-03, ...,\n",
       "         7.49955164e-19, 4.21374862e-19, 7.94585977e-01],\n",
       "        [2.95471748e-02, 8.66536383e-03, 2.89104286e-02, ...,\n",
       "         1.13536763e-03, 1.96442901e-03, 6.92440450e-01]]),\n",
       " 'Prob_23': array([[9.85945676e-01, 5.33659562e-03, 3.71518935e-03, 2.01090828e-05,\n",
       "         4.93700370e-03, 2.67223570e-05, 1.86033613e-05, 1.00693800e-07,\n",
       "         8.95936050e-02],\n",
       "        [9.98228342e-01, 2.56234130e-04, 1.23999431e-03, 3.18292769e-07,\n",
       "         2.74699320e-04, 7.05122648e-08, 3.41230137e-07, 8.75899867e-11,\n",
       "         1.44513358e-02],\n",
       "        [1.68542800e-03, 5.36027744e-07, 8.15553041e-08, 2.59375694e-11,\n",
       "         9.97948266e-01, 3.17384046e-04, 4.82892028e-05, 1.53577325e-08,\n",
       "         1.58582411e-02],\n",
       "        [1.39949825e-03, 2.94478085e-07, 1.62706626e-07, 3.42362242e-11,\n",
       "         9.98273906e-01, 2.10053703e-04, 1.16060009e-04, 2.44209876e-08,\n",
       "         1.37594366e-02],\n",
       "        [6.92257879e-04, 9.98843731e-01, 2.87379021e-07, 4.14652895e-04,\n",
       "         3.39710239e-08, 4.90160464e-05, 1.41024897e-11, 2.03481735e-08,\n",
       "         9.91318087e-03],\n",
       "        [9.34026977e-04, 9.98759918e-01, 2.61108215e-07, 2.79204376e-04,\n",
       "         2.48363561e-08, 2.65576451e-05, 6.94302924e-12, 7.42421738e-09,\n",
       "         1.03243263e-02],\n",
       "        [7.07061392e-07, 4.89647398e-04, 5.75422574e-11, 3.98486142e-08,\n",
       "         1.44111495e-03, 9.97987155e-01, 1.17281199e-07, 8.12184549e-05,\n",
       "         1.59485969e-02],\n",
       "        [2.02455389e-07, 3.31754555e-04, 4.47687039e-11, 7.33604647e-08,\n",
       "         6.09547073e-04, 9.98837416e-01, 1.34788373e-07, 2.20871653e-04,\n",
       "         1.01977242e-02],\n",
       "        [9.98382123e-01, 8.83797538e-04, 4.11197296e-04, 3.64004072e-07,\n",
       "         3.22099943e-04, 2.85132446e-07, 1.32661255e-07, 1.17435687e-10,\n",
       "         1.36383611e-02],\n",
       "        [9.99579665e-01, 1.09497207e-04, 2.22973274e-04, 2.44252175e-08,\n",
       "         8.78110657e-05, 9.61910968e-09, 1.95877542e-08, 2.14570629e-12,\n",
       "         4.11481748e-03],\n",
       "        [1.85580153e-03, 3.08310379e-07, 5.52089814e-08, 9.17204869e-12,\n",
       "         9.97948349e-01, 1.65792424e-04, 2.96883643e-05, 4.93222507e-09,\n",
       "         1.54798135e-02],\n",
       "        [1.50193361e-03, 2.55696692e-07, 1.07652113e-07, 1.83272344e-11,\n",
       "         9.98256192e-01, 1.69948129e-04, 7.15506919e-05, 1.21811478e-08,\n",
       "         1.36703024e-02],\n",
       "        [7.22308982e-04, 9.99015440e-01, 1.80068482e-07, 2.49050197e-04,\n",
       "         9.40487822e-09, 1.30077554e-05, 2.34459516e-12, 3.24277674e-09,\n",
       "         8.42453046e-03],\n",
       "        [7.55974872e-04, 9.99001376e-01, 1.69080078e-07, 2.23434980e-04,\n",
       "         1.43976551e-08, 1.90261314e-05, 3.22015551e-12, 4.25535279e-09,\n",
       "         8.51976457e-03],\n",
       "        [5.34339285e-07, 4.66655542e-04, 5.14647717e-11, 4.49458268e-08,\n",
       "         1.14308594e-03, 9.98293419e-01, 1.10096073e-07, 9.61504129e-05,\n",
       "         1.39272210e-02],\n",
       "        [1.62327536e-07, 3.32068366e-04, 4.18563494e-11, 8.56242256e-08,\n",
       "         4.88310572e-04, 9.98921674e-01, 1.25911465e-07, 2.57573149e-04,\n",
       "         9.59539146e-03],\n",
       "        [9.99197736e-01, 1.68804406e-04, 1.35622631e-04, 2.29120792e-08,\n",
       "         4.97662229e-04, 8.40750272e-08, 6.75484526e-08, 1.14116315e-11,\n",
       "         7.26401517e-03],\n",
       "        [9.99710384e-01, 2.77604274e-05, 9.36684897e-05, 2.60103061e-09,\n",
       "         1.68163690e-04, 4.66964833e-09, 1.57562021e-08, 4.37525620e-13,\n",
       "         2.91154152e-03],\n",
       "        [2.02523052e-03, 1.90041877e-07, 4.48151744e-08, 4.20532861e-12,\n",
       "         9.97858815e-01, 9.36362355e-05, 2.20810502e-05, 2.07202300e-09,\n",
       "         1.58085615e-02],\n",
       "        [1.73271589e-03, 1.61503948e-07, 8.38282363e-08, 7.81350893e-12,\n",
       "         9.98125711e-01, 9.30338572e-05, 4.82890001e-05, 4.50094802e-09,\n",
       "         1.42367321e-02],\n",
       "        [2.14850707e-03, 9.97755481e-01, 1.58469060e-07, 7.35922055e-05,\n",
       "         4.78293857e-08, 2.22117174e-05, 3.52778817e-12, 1.63828643e-09,\n",
       "         1.63819302e-02],\n",
       "        [2.20594883e-03, 9.97673601e-01, 1.49218212e-07, 6.74861849e-05,\n",
       "         1.16511978e-07, 5.26942979e-05, 7.88128392e-12, 3.56442942e-09,\n",
       "         1.69880680e-02],\n",
       "        [1.03559121e-06, 5.03233491e-04, 5.21090322e-11, 2.53217774e-08,\n",
       "         2.05250912e-03, 9.97392906e-01, 1.03278458e-07, 5.01869642e-05,\n",
       "         1.96410396e-02],\n",
       "        [3.53484294e-07, 3.94406289e-04, 4.68570695e-11, 5.22815955e-08,\n",
       "         8.94969361e-04, 9.98577731e-01, 1.18635091e-07, 1.32369180e-04,\n",
       "         1.19842714e-02],\n",
       "        [9.99829465e-01, 5.67236699e-05, 2.66803426e-05, 1.51366508e-09,\n",
       "         8.71217608e-05, 4.94270890e-09, 2.32483489e-09, 1.31895660e-13,\n",
       "         1.82071516e-03],\n",
       "        [9.99855320e-01, 2.09165588e-05, 2.62334075e-05, 5.48792010e-10,\n",
       "         9.75248589e-05, 2.04017962e-09, 2.55877956e-09, 5.35286078e-14,\n",
       "         1.54755183e-03],\n",
       "        [2.13743994e-03, 1.48216506e-07, 3.48595425e-08, 2.41726540e-12,\n",
       "         9.97776914e-01, 6.91888486e-05, 1.62727598e-05, 1.12840205e-09,\n",
       "         1.62070201e-02],\n",
       "        [1.85195460e-03, 1.69057368e-07, 6.62669826e-08, 6.04924205e-12,\n",
       "         9.98020990e-01, 9.11052578e-05, 3.57113720e-05, 3.25994522e-09,\n",
       "         1.48457265e-02],\n",
       "        [1.88566309e-03, 9.98038686e-01, 1.19644596e-07, 6.33251698e-05,\n",
       "         2.30172094e-08, 1.21824867e-05, 1.46043306e-12, 7.72974082e-10,\n",
       "         1.45414091e-02],\n",
       "        [1.73064292e-03, 9.98156917e-01, 1.13908473e-07, 6.56972786e-05,\n",
       "         8.07014924e-08, 4.65449872e-05, 5.31165825e-12, 3.06352532e-09,\n",
       "         1.39471801e-02],\n",
       "        [8.21854133e-07, 4.89023718e-04, 4.93732516e-11, 2.93783167e-08,\n",
       "         1.67685946e-03, 9.97773223e-01, 1.00738076e-07, 5.99416689e-05,\n",
       "         1.72651108e-02],\n",
       "        [3.30020084e-07, 4.37261896e-04, 5.06572874e-11, 6.71186471e-08,\n",
       "         7.53727407e-04, 9.98655207e-01, 1.15695340e-07, 1.53291167e-04,\n",
       "         1.15000272e-02],\n",
       "        [2.47662782e-04, 2.26222363e-07, 9.98828605e-01, 9.12358996e-04,\n",
       "         2.76078829e-09, 2.52178404e-12, 1.11343105e-05, 1.01704019e-08,\n",
       "         9.74389377e-03],\n",
       "        [4.53491209e-04, 1.88284394e-07, 9.99126910e-01, 4.14826133e-04,\n",
       "         2.07902795e-09, 8.63188765e-13, 4.58049181e-06, 1.90176812e-09,\n",
       "         7.65375445e-03],\n",
       "        [3.85470158e-08, 3.62533697e-11, 2.49604578e-04, 2.34752467e-07,\n",
       "         1.54224850e-04, 1.45048076e-07, 9.98656518e-01, 9.39233899e-04,\n",
       "         1.13202590e-02],\n",
       "        [2.65892387e-08, 2.05723770e-11, 1.52170672e-04, 1.17736069e-07,\n",
       "         1.74540824e-04, 1.35044094e-07, 9.98900149e-01, 7.72859679e-04,\n",
       "         9.48960383e-03],\n",
       "        [1.23428013e-07, 2.17359598e-04, 5.67400622e-04, 9.99205677e-01,\n",
       "         1.16513413e-12, 2.05182827e-09, 5.35614090e-09, 9.43228855e-06,\n",
       "         6.97946514e-03],\n",
       "        [1.82330574e-07, 2.64754302e-04, 6.88017258e-04, 9.99039957e-01,\n",
       "         1.29250883e-12, 1.87679590e-09, 4.87723131e-09, 7.08201560e-06,\n",
       "         8.23715719e-03],\n",
       "        [2.20240497e-12, 1.59557713e-08, 4.45262633e-08, 3.22579581e-04,\n",
       "         6.82399562e-09, 4.94378257e-05, 1.37961469e-04, 9.99489954e-01,\n",
       "         4.82079510e-03],\n",
       "        [3.11308010e-12, 1.23720604e-08, 3.37461883e-08, 1.34114724e-04,\n",
       "         2.32009752e-08, 9.22057436e-05, 2.51501552e-04, 9.99522109e-01,\n",
       "         4.61605598e-03],\n",
       "        [3.19662877e-04, 1.80002865e-07, 9.99112812e-01, 5.62602609e-04,\n",
       "         1.51586888e-09, 8.53589080e-13, 4.73787896e-06, 2.66791000e-09,\n",
       "         7.73040348e-03],\n",
       "        [4.72549314e-04, 1.63524316e-07, 9.99177787e-01, 3.45762567e-04,\n",
       "         1.76627289e-09, 6.11213595e-13, 3.73468035e-06, 1.29237527e-09,\n",
       "         7.24529612e-03],\n",
       "        [3.86156499e-08, 3.46218750e-11, 2.58890206e-04, 2.32114813e-07,\n",
       "         1.48963951e-04, 1.33557542e-07, 9.98696334e-01, 8.95407424e-04,\n",
       "         1.10441556e-02],\n",
       "        [3.08599500e-08, 2.27821865e-11, 1.72991309e-04, 1.27709872e-07,\n",
       "         1.78195923e-04, 1.31552149e-07, 9.98911082e-01, 7.37440553e-04,\n",
       "         9.44842779e-03],\n",
       "        [1.53233955e-07, 2.48482848e-04, 6.16140587e-04, 9.99128214e-01,\n",
       "         1.07400273e-12, 1.74159348e-09, 4.31847284e-09, 7.00279797e-06,\n",
       "         7.57402044e-03],\n",
       "        [2.02253286e-07, 2.79729697e-04, 7.22301732e-04, 9.98991157e-01,\n",
       "         1.33686148e-12, 1.84896800e-09, 4.77429750e-09, 6.60316979e-06,\n",
       "         8.60346096e-03],\n",
       "        [3.07236209e-12, 2.20722836e-08, 5.11582436e-08, 3.67528054e-04,\n",
       "         8.35479306e-09, 6.00220145e-05, 1.39116590e-04, 9.99433252e-01,\n",
       "         5.29350904e-03],\n",
       "        [7.64015437e-12, 2.54310121e-08, 6.16445527e-08, 2.05190012e-04,\n",
       "         3.72110976e-08, 1.23860832e-04, 3.00237581e-04, 9.99370587e-01,\n",
       "         5.92320926e-03],\n",
       "        [4.37330916e-04, 1.82414722e-07, 9.99140319e-01, 4.16750559e-04,\n",
       "         2.36891781e-09, 9.88097267e-13, 5.41210605e-06, 2.25743890e-09,\n",
       "         7.55412060e-03],\n",
       "        [7.02639535e-04, 1.72793199e-07, 9.99044939e-01, 2.45685251e-04,\n",
       "         4.61175418e-09, 1.13412314e-12, 6.55720245e-06, 1.61254801e-09,\n",
       "         8.17931078e-03],\n",
       "        [3.86049991e-08, 2.42648491e-11, 1.72792598e-04, 1.08607341e-07,\n",
       "         2.23189313e-04, 1.40283774e-07, 9.98975833e-01, 6.27897898e-04,\n",
       "         9.03126873e-03],\n",
       "        [3.39962021e-08, 2.02200261e-11, 1.49434648e-04, 8.88797070e-08,\n",
       "         2.27277854e-04, 1.35178751e-07, 9.99028834e-01, 5.94195463e-04,\n",
       "         8.61175234e-03],\n",
       "        [1.88650488e-07, 2.74803130e-04, 6.85828562e-04, 9.99031797e-01,\n",
       "         1.39266591e-12, 2.02866664e-09, 5.06296098e-09, 7.37510697e-06,\n",
       "         8.30736338e-03],\n",
       "        [2.70386483e-07, 3.23095540e-04, 8.35885244e-04, 9.98832454e-01,\n",
       "         2.24275853e-12, 2.67996118e-09, 6.93336716e-09, 8.28495560e-06,\n",
       "         9.78891450e-03],\n",
       "        [3.61859180e-12, 1.53214696e-08, 3.48729176e-08, 1.47655324e-04,\n",
       "         2.44950714e-08, 1.03714514e-04, 2.36062715e-04, 9.99512493e-01,\n",
       "         4.71402580e-03],\n",
       "        [1.10896467e-11, 2.45782935e-08, 6.15856861e-08, 1.36494075e-04,\n",
       "         8.11840082e-08, 1.79930383e-04, 4.50850506e-04, 9.99232558e-01,\n",
       "         7.00966896e-03],\n",
       "        [4.77426986e-04, 1.61719704e-07, 9.99180214e-01, 3.38454117e-04,\n",
       "         1.78699494e-09, 6.05312022e-13, 3.73990169e-06, 1.26682365e-09,\n",
       "         7.22433573e-03],\n",
       "        [7.00993761e-04, 1.61020521e-07, 9.99063393e-01, 2.29488073e-04,\n",
       "         4.18080341e-09, 9.60343984e-13, 5.95852327e-06, 1.36869195e-09,\n",
       "         8.02483828e-03],\n",
       "        [4.03826250e-08, 2.54416795e-11, 1.83669404e-04, 1.15714570e-07,\n",
       "         2.19638711e-04, 1.38375791e-07, 9.98967033e-01, 6.29364709e-04,\n",
       "         9.10620182e-03],\n",
       "        [5.06330052e-08, 2.82807133e-11, 2.03388687e-04, 1.13601338e-07,\n",
       "         2.48695481e-04, 1.38907133e-07, 9.98989634e-01, 5.57978719e-04,\n",
       "         8.98751827e-03],\n",
       "        [2.19522999e-07, 3.03054807e-04, 7.23618769e-04, 9.98966610e-01,\n",
       "         1.42625480e-12, 1.96896623e-09, 4.70139687e-09, 6.49034919e-06,\n",
       "         8.80177293e-03],\n",
       "        [3.13177950e-07, 3.44513961e-04, 9.07896856e-04, 9.98739348e-01,\n",
       "         2.48278449e-12, 2.73120735e-09, 7.19754451e-09, 7.91771759e-06,\n",
       "         1.04639343e-02],\n",
       "        [7.16267301e-12, 2.76809998e-08, 4.92293031e-08, 1.90252484e-04,\n",
       "         3.76258723e-08, 1.45409648e-04, 2.58603941e-04, 9.99405619e-01,\n",
       "         5.64703743e-03],\n",
       "        [7.64727169e-11, 1.10295484e-07, 2.41845052e-07, 3.48809590e-04,\n",
       "         2.18941517e-07, 3.15776155e-04, 6.92402788e-04, 9.98642440e-01,\n",
       "         1.17250901e-02],\n",
       "        [1.47541647e-01, 1.67750791e-01, 2.11846782e-01, 1.04710221e-01,\n",
       "         1.14106029e-01, 9.40796101e-02, 9.22152795e-02, 6.77496407e-02,\n",
       "         1.24885767e-02]])}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.prob_matrix(data_dist)\n",
    "HM.Prob_mtx_wake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "afcdf12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob_01 0.268085631308785 [5.10781212]\n",
      "Prob_12 0.6924404502938856 [3.91717823]\n",
      "Prob_23 0.012488576723520507 [2.01910608]\n"
     ]
    }
   ],
   "source": [
    "for key in HM.Prob_mtx_wake:\n",
    "    print(key,HM.Prob_mtx_wake[key][-1,-1],entropy(HM.Prob_mtx_wake[key][-1:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "177e197a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07789531, 0.08751907, 0.09788371, 0.1       ],\n",
       "       [0.05305889, 0.07391439, 0.07321018, 0.08883479]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "84136e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sl_1': {'l_1': array([[ 0.        ,  0.        , -0.1411917 ],\n",
       "         [ 0.        ,  0.        , -0.05167739],\n",
       "         [ 0.        ,  0.        , -0.13098464],\n",
       "         [ 0.        ,  0.        , -0.17981982],\n",
       "         [ 0.        ,  0.        , -0.40439025],\n",
       "         [ 0.        ,  0.        ,  0.14740791],\n",
       "         [ 0.        ,  0.        , -0.51521349],\n",
       "         [ 0.        ,  0.        , -0.393252  ],\n",
       "         [ 0.        ,  0.        , -0.17180692],\n",
       "         [ 0.        ,  0.        ,  0.        ]]),\n",
       "  'l_2': array([[ 0.        ,  0.        , -0.44067472],\n",
       "         [ 0.        ,  0.        ,  0.0217773 ],\n",
       "         [ 0.        ,  0.        ,  0.15156644],\n",
       "         [ 0.        ,  0.        , -0.29405343],\n",
       "         [ 0.        ,  0.        ,  0.37962508],\n",
       "         [ 0.        ,  0.        ,  0.12516417],\n",
       "         [ 0.        ,  0.        ,  0.0753112 ],\n",
       "         [ 0.        ,  0.        ,  0.15786384],\n",
       "         [ 0.        ,  0.        ,  0.        ]])},\n",
       " 'sl_2': {'l_1': array([[ 0.        ,  0.        ,  0.30175887],\n",
       "         [ 0.        ,  0.        ,  0.17828998],\n",
       "         [ 0.        ,  0.        ,  0.31504953],\n",
       "         [ 0.        ,  0.        , -0.28382175],\n",
       "         [ 0.        ,  0.        ,  0.13440153],\n",
       "         [ 0.        ,  0.        , -0.54093805],\n",
       "         [ 0.        ,  0.        ,  0.        ]])},\n",
       " 'sl_3': {'l_1': array([[ 0.        ,  0.        , -0.12103377],\n",
       "         [ 0.        ,  0.        ,  0.59716995],\n",
       "         [ 0.        ,  0.        ,  0.2672846 ],\n",
       "         [ 0.        ,  0.        , -0.48203702],\n",
       "         [ 0.        ,  0.        , -0.58614781],\n",
       "         [ 0.        ,  0.        ,  0.        ]]),\n",
       "  'l_2': array([[ 0.        ,  0.        ,  0.18688496],\n",
       "         [ 0.        ,  0.        ,  0.06481529],\n",
       "         [ 0.        ,  0.        , -0.29677912],\n",
       "         [ 0.        ,  0.        ,  0.        ]])},\n",
       " 'sl_4': {'l_1': array([[0., 0., 0.],\n",
       "         [0., 0., 0.]])}}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Norm_wake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f2979289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sl_0': {'l_1': array([[ 0.        ,  0.        , -0.08358382],\n",
       "         [ 0.        ,  0.        , -0.10210273],\n",
       "         [ 0.        ,  0.        , -2.70417184],\n",
       "         [ 0.        ,  0.        , -0.10661096],\n",
       "         [ 0.        ,  0.        , -0.46691547],\n",
       "         [ 0.        ,  0.        , -0.26548855],\n",
       "         [ 0.        ,  0.        ,  0.03510919],\n",
       "         [ 0.        ,  0.        , -0.18552643],\n",
       "         [ 0.        ,  0.        , -0.59209361],\n",
       "         [ 0.        ,  0.        ,  0.        ]]),\n",
       "  'l_0': array([[ 0.00000000e+00,  0.00000000e+00, -4.88010616e-02],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  1.52716122e+01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.57957399e+01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  6.76118976e-03],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.60606512e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  3.09083544e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  8.69239509e-02],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -3.64272734e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  4.69651797e-02],\n",
       "         [ 0.00000000e+00,  0.00000000e+00, -1.81549801e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])},\n",
       " 'sl_1': {'l_0': array([[ 0.        ,  0.        , -0.5175914 ],\n",
       "         [ 0.        ,  0.        , -1.94154039],\n",
       "         [ 0.        ,  0.        ,  0.37725617],\n",
       "         [ 0.        ,  0.        , -0.37132355],\n",
       "         [ 0.        ,  0.        ,  0.54930491],\n",
       "         [ 0.        ,  0.        ,  0.28173933],\n",
       "         [ 0.        ,  0.        ,  0.92840443],\n",
       "         [ 0.        ,  0.        ,  0.0992284 ],\n",
       "         [ 0.        ,  0.        ,  0.        ]])},\n",
       " 'sl_2': {'l_1': array([[ 0.        ,  0.        , -0.6229706 ],\n",
       "         [ 0.        ,  0.        , -0.930528  ],\n",
       "         [ 0.        ,  0.        ,  0.51873547],\n",
       "         [ 0.        ,  0.        , -0.58309667],\n",
       "         [ 0.        ,  0.        ,  1.5218072 ],\n",
       "         [ 0.        ,  0.        ,  0.        ]]),\n",
       "  'l_0': array([[  0.        ,   0.        ,   0.0490641 ],\n",
       "         [  0.        ,   0.        ,   1.52501647],\n",
       "         [  0.        ,   0.        , -24.90005711],\n",
       "         [  0.        ,   0.        ,  -0.24250634],\n",
       "         [  0.        ,   0.        ,  -0.34633422],\n",
       "         [  0.        ,   0.        ,  -3.78100518],\n",
       "         [  0.        ,   0.        ,   0.        ]])},\n",
       " 'sl_3': {'l_0': array([[    0.        ,     0.        ,  1781.58596855],\n",
       "         [    0.        ,     0.        ,    62.89695624],\n",
       "         [    0.        ,     0.        , -1170.98483542],\n",
       "         [    0.        ,     0.        ,     0.        ]])}}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Norm_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "6e6d7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fz_binary = [True,False,False,False,True]\n",
    "# Fz_scale = [False,False,False,False]\n",
    "# HM.set_control(Fz_binary,Fz_scale,fz_W=False,layer_norm=False,layer_norm_rate=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e1b27563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "624c7443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Phi_01': {'Phi_01': array([[ -0.53324827,  -1.09180099,  -1.40767198,   0.36357302,\n",
       "            0.2988655 ,  -1.11258203,  11.27637847,  -2.34318828,\n",
       "           -1.16905775,  -0.92323546,   0.        ,   1.        ],\n",
       "         [ -1.3929769 ,  -1.50506143,  -0.91378995,  -1.47183145,\n",
       "           -0.46923902,  14.65625801,  -1.44940545,  -1.33690895,\n",
       "           -1.72471612,  -1.68263377,   0.        ,   1.        ],\n",
       "         [ 11.95275329,  -0.02876025,  -0.52663499,  -0.53857618,\n",
       "           -0.79357699,   0.08994943,  -0.56914728,  -0.67371699,\n",
       "           -1.91667585,  -1.01755067,   0.        ,   1.        ],\n",
       "         [ -2.22251212,  -0.55513427,  -1.60820148,  12.74831419,\n",
       "           -1.14767453,  -0.97164625,  -0.94379903,  -1.42480229,\n",
       "           -1.22450248,  -0.9583076 ,   0.        ,   1.        ],\n",
       "         [ -1.53662656,  -0.47918945,  -1.85661568,  -0.62226513,\n",
       "           -0.91979306,  -0.98098899,  -0.79859322,  -0.86405181,\n",
       "           -1.32505571,  11.96823119,   0.        ,   1.        ],\n",
       "         [-15.76842491,   1.5695158 ,   1.50716577,   1.12394019,\n",
       "            1.54831381,   1.96062174,   1.70302363,   1.56368748,\n",
       "            1.39224563,   1.14076846,   0.        ,   1.        ],\n",
       "         [ -1.33343649,  -0.58723094,  -0.99085533,  -1.00150011,\n",
       "           -0.96270235,  -0.64648016,  -0.80954277,  11.78789638,\n",
       "           -1.31566937,  -1.12207883,   0.        ,   1.        ],\n",
       "         [ -1.81079369,  -0.69012469,  -1.04499747,  -0.71368086,\n",
       "           13.46826798,  -1.21440085,  -0.83361928,  -1.2789931 ,\n",
       "           -1.42062271,  -1.36441501,   0.        ,   1.        ],\n",
       "         [  1.06705281,   0.23091342,  -1.04954727,  -0.97188483,\n",
       "           -2.19242376,   0.16929836,   0.13847336,  -1.31661735,\n",
       "           10.26464606,  -2.14614666,   0.        ,   1.        ]]),\n",
       "  'Phi_12': array([[  0.07205269,   0.18875366,  -5.0607986 ,   0.08154321,\n",
       "           -0.07371318,  -5.4637939 ,  11.87711179,   0.02252707,\n",
       "           -0.68720819,   0.        ,   1.        ],\n",
       "         [ -0.32364643,  -0.03322146,  -5.6810834 ,   0.25079303,\n",
       "           -0.26378022,   8.84874744,  -0.10818897,  -0.61028586,\n",
       "           -0.569788  ,   0.        ,   1.        ],\n",
       "         [  0.01381002,   0.40573967,   5.96678551, -11.21570146,\n",
       "           -0.05051334,   5.5498376 ,   0.46004396,   0.45745454,\n",
       "           -0.86759573,   0.        ,   1.        ],\n",
       "         [ -0.33507023,   0.26381671,  -5.0367235 ,   0.20229959,\n",
       "           -0.29092987,  -6.02752899,   0.35188375,  11.52908184,\n",
       "           -0.43051932,   0.        ,   1.        ],\n",
       "         [  0.36720934,   0.38323179,   4.55653931,   0.42994702,\n",
       "          -10.98033763,   3.49218891,   0.60271824,   0.57256733,\n",
       "           -0.51175407,   0.        ,   1.        ],\n",
       "         [-10.68817507,   1.71945153,   4.13426538,  -1.40737645,\n",
       "            0.76647362,   3.53420264,   0.7934129 ,  -0.74227137,\n",
       "            0.91476659,   0.        ,   1.        ],\n",
       "         [ -0.43536046,   0.55373456,   7.28669138,   0.6459744 ,\n",
       "           -1.08848251,   5.9763809 ,  -0.30264119,  -0.43877401,\n",
       "          -10.84211517,   0.        ,   1.        ],\n",
       "         [ -0.96204873,  11.37059842,  -4.66360895,  -0.18773798,\n",
       "           -0.30635533,  -4.73359824,   0.45679444,  -0.04172794,\n",
       "           -1.35160649,   0.        ,   1.        ]])},\n",
       " 'Phi_12': {'Phi_01': array([[  1.1864842 ,   1.64330396, -13.64220118,   1.91354268,\n",
       "            0.10104951,   0.46500502,   0.51935463,   0.52510356,\n",
       "            0.        ,   1.        ],\n",
       "         [  1.64885457,   1.81027902,   0.69632426,   1.36323513,\n",
       "          -13.48868016,   1.46036263,   0.85223697,   0.27951962,\n",
       "            0.        ,   1.        ],\n",
       "         [  0.91411541,   0.33314277,   0.34851269,   0.36653839,\n",
       "            0.12905618, -13.45946558,   0.15834232,  -8.35658548,\n",
       "            0.        ,   1.        ],\n",
       "         [  1.12879403,  -9.95359774,   0.1529172 ,   1.58408132,\n",
       "            0.28967303,   0.77200506,   0.46464519,   0.23946737,\n",
       "            0.        ,   1.        ],\n",
       "         [-10.49125786,   1.64402736,   0.58172301,   1.85397091,\n",
       "            0.43331542,   0.72915493,   0.4197425 ,   0.47452013,\n",
       "            0.        ,   1.        ],\n",
       "         [ -1.03485144,  -0.32089901,  -1.84906506,  -1.41480365,\n",
       "           -1.67458218,  -1.45393061,   8.44383755,  -0.73125813,\n",
       "            0.        ,   1.        ]])},\n",
       " 'Phi_23': {'Phi_01': array([[-2.76672473,  0.60640162, -0.02053699, -1.13055055,  4.78101299,\n",
       "           0.786563  ,  0.        ,  1.        ],\n",
       "         [-4.10164946,  0.16944369, -0.11531839, -0.88672045,  0.7740732 ,\n",
       "          -0.48074932,  0.        ,  1.        ],\n",
       "         [ 1.79050919,  0.96641626,  0.73396742,  1.1695657 , -8.59355324,\n",
       "           1.67344804,  0.        ,  1.        ],\n",
       "         [ 8.06229154, -0.42574586, -0.57253596, -2.27204974, -1.50199748,\n",
       "           0.08301383,  0.        ,  1.        ],\n",
       "         [-2.46897737, -0.69253001, -0.56973   ,  9.0593612 , -1.05263362,\n",
       "          -0.64419427,  0.        ,  1.        ]]),\n",
       "  'Phi_12': array([[  8.05682534,  -2.68282508, -14.96217167,   1.27339596,\n",
       "            1.26039143,   0.        ,   1.        ],\n",
       "         [ -4.44358165,  -9.06796613,  -4.9955567 ,  13.16096728,\n",
       "            0.26729514,   0.        ,   1.        ],\n",
       "         [ -3.48911792,  -7.97931261,  -4.02782566,  -3.39973032,\n",
       "           15.22432368,   0.        ,   1.        ]])},\n",
       " 'Phi_34': {'Phi_01': array([[-0.49240823,  1.32402199, -0.08582301,  0.        ,  1.        ]])}}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "49567947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': {'Theta_21': array([[ -1.70974166,  -3.02973252,  -1.29390612,  -1.4894521 ,\n",
       "           14.11553895,  -1.40434415,  -2.7222107 ,  -0.82089229,\n",
       "            0.        ,   1.        ],\n",
       "         [ -1.9792689 ,  -3.17638044,  15.8914161 ,  -1.53335101,\n",
       "           -1.58381091,  -1.88073122,  -2.680289  ,  -1.62275541,\n",
       "            0.        ,   1.        ],\n",
       "         [  3.74508019,   9.77535758,   9.5723317 ,   8.31156668,\n",
       "            5.75627927,   4.04244396,   5.62078541,   7.73748181,\n",
       "            0.        ,   1.        ],\n",
       "         [ -1.88014653,  -2.902558  ,  -1.558326  ,  -1.42416958,\n",
       "           -1.09494764,  -1.46835796,  -3.11708898,  15.78132032,\n",
       "            0.        ,   1.        ],\n",
       "         [ -1.55550915,  -2.78077948,  -1.630239  ,  15.24269783,\n",
       "           -0.81280611,  -1.34820686,  -3.20860258,  -0.71190457,\n",
       "            0.        ,   1.        ],\n",
       "         [ -3.24364804,  -3.58712284,  -1.33000544,  -0.17786029,\n",
       "           -0.18817699,  12.71112856,  -0.29964629,  -0.75179633,\n",
       "            0.        ,   1.        ],\n",
       "         [  1.82438981, -15.35810887,   3.80386835,   1.0928101 ,\n",
       "            1.67966787,   1.63852576,   3.20190331,   2.80309105,\n",
       "            0.        ,   1.        ],\n",
       "         [ -1.06512235,  -3.43236834,   0.28501231,  -2.12542895,\n",
       "            1.0630469 ,  -0.31221816,  10.44953042,   0.64385097,\n",
       "            0.        ,   1.        ],\n",
       "         [ 13.01702961,  -2.15507906,  -1.43784011,  -1.2687812 ,\n",
       "           -0.35421804,  -1.21778994,  -2.60640023,  -0.81804028,\n",
       "            0.        ,   1.        ]]),\n",
       "  'Theta_10': array([[ -0.15963482,  -0.62899004,  -5.35150698,  -0.89247173,\n",
       "           -0.63356011,  -0.06231394,  14.31526489,  -0.60273495,\n",
       "           -0.54136941,   0.        ,   1.        ],\n",
       "         [ -0.09639249,  -0.23488587, -15.66254061,  -0.08209942,\n",
       "            0.31634781,   0.35304025,  -0.14453523,   0.02758809,\n",
       "            0.22080727,   0.        ,   1.        ],\n",
       "         [  0.24775635,   0.03429059,  15.84674766,   0.02872647,\n",
       "            0.11402335,   0.1469351 ,  -0.41419564,   0.30758485,\n",
       "           -0.18886888,   0.        ,   1.        ],\n",
       "         [  0.93635038, -11.97337614,   2.39243718,   1.12075344,\n",
       "            1.01986707,   0.07657658,   1.04961963,   1.73334425,\n",
       "            1.0322403 ,   0.        ,   1.        ],\n",
       "         [ -0.77860522,  -0.85744938,  -2.28126942,  -0.97581583,\n",
       "           12.32651655,  -0.61213839,  -1.28077827,  -1.36544978,\n",
       "           -1.22980381,   0.        ,   1.        ],\n",
       "         [ -0.90136346,  -0.84818355,  -3.04980343,  11.99123762,\n",
       "           -0.80894184,  -0.64176596,  -1.17841232,  -1.28044592,\n",
       "           -1.08267505,   0.        ,   1.        ],\n",
       "         [ -0.47014355,   1.25378359,   3.55624371,   1.69616023,\n",
       "           -1.14675416, -11.26990906,   2.00454273,  -0.82566789,\n",
       "            1.07375316,   0.        ,   1.        ],\n",
       "         [ -0.754795  ,  -0.63266335,  -2.61252283,  -0.94486036,\n",
       "           -0.75644115,  -0.689628  ,  -1.55954812,  -1.05283425,\n",
       "           12.38047574,   0.        ,   1.        ],\n",
       "         [  0.97370378,  -0.9693586 ,   4.74216762,   0.5039338 ,\n",
       "           -0.06802854,   0.86952732,   3.42410555, -12.3022502 ,\n",
       "           -0.16972528,   0.        ,   1.        ],\n",
       "         [-11.86100733,   0.35750847,   2.76768238,   0.63315317,\n",
       "            0.80673773,   0.55766211,   1.8811646 ,   1.35586901,\n",
       "            0.83256456,   0.        ,   1.        ]])},\n",
       " 'Theta_21': {'Theta_10': array([[ 2.11700992e+00,  1.93686214e+00,  1.31471367e+00,\n",
       "           1.73998255e+00, -1.18498966e+01,  1.45696756e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 2.69672804e+00,  1.85533519e+00,  1.38617360e+00,\n",
       "          -1.15449201e+01,  1.35455949e+00,  1.62026001e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-1.38879191e+01,  1.63323466e+00,  1.11745507e+00,\n",
       "           1.29604234e+00,  1.36048251e+00,  8.21938713e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 6.08329478e-01,  2.53476179e-01,  9.89423664e-03,\n",
       "           7.02547551e-01,  5.03767381e-01, -4.89369385e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.74662065e+00, -1.33390530e+01,  1.32603889e+00,\n",
       "           1.54923388e+00,  1.35150528e+00,  9.28291829e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-3.69510053e-01,  6.78180151e-01, -1.34332462e+01,\n",
       "           5.43755173e-01,  1.61316432e-01, -3.32766079e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-3.37279122e+00, -2.65750430e+00, -2.00935344e+00,\n",
       "          -2.17150465e+00, -2.68500072e+00,  1.23274913e+01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 3.81981490e-01, -1.43693687e-01, -7.19614943e+00,\n",
       "          -3.49968024e-01, -1.55889703e-02, -2.63061154e-02,\n",
       "           0.00000000e+00,  1.00000000e+00]])},\n",
       " 'Theta_32': {'Theta_21': array([[ 14.30675837,  -5.11023578,  -5.03811963,   0.        ,\n",
       "            1.        ],\n",
       "         [ -0.26933298,  -0.89317   ,   8.59728318,   0.        ,\n",
       "            1.        ],\n",
       "         [-14.95775248,   3.10878485,   3.31565988,   0.        ,\n",
       "            1.        ],\n",
       "         [ -2.11198577,   9.16673741,  -0.87620748,   0.        ,\n",
       "            1.        ],\n",
       "         [  3.26123906,  -8.66150772,   3.29698972,   0.        ,\n",
       "            1.        ]]),\n",
       "  'Theta_10': array([[-7.60091549e-01,  2.41376735e+00, -2.63135144e+00,\n",
       "           9.15603357e+00, -1.05168377e+01,  0.00000000e+00,\n",
       "           1.00000000e+00],\n",
       "         [ 1.79215060e-02,  1.13016038e-01,  2.20092028e-01,\n",
       "          -1.22320135e+00, -1.18359934e+00,  0.00000000e+00,\n",
       "           1.00000000e+00],\n",
       "         [ 2.29199316e+01,  2.58444309e-02,  2.36763455e+01,\n",
       "           1.01078500e+00,  9.70202160e-01,  0.00000000e+00,\n",
       "           1.00000000e+00],\n",
       "         [-3.65637402e+00,  1.86797333e+01, -3.97418854e+00,\n",
       "          -6.28083457e+00, -6.69958185e+00,  0.00000000e+00,\n",
       "           1.00000000e+00],\n",
       "         [ 9.71710873e+00,  2.31432737e+00, -1.06556031e+01,\n",
       "          -8.09757746e-01, -3.52851537e+00,  0.00000000e+00,\n",
       "           1.00000000e+00],\n",
       "         [ 3.16002091e+00, -4.22856663e-01,  3.25920768e+00,\n",
       "           8.17133120e-01,  4.66294976e-01,  0.00000000e+00,\n",
       "           1.00000000e+00]])},\n",
       " 'Theta_43': {'Theta_10': array([[-1.78206936e+03,  0.00000000e+00,  1.00000000e+00],\n",
       "         [-6.29539983e+01,  0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.17095887e+03,  0.00000000e+00,  1.00000000e+00]])}}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fcf86",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "526b91c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 1],\n",
       "       [1, 1, 0, ..., 1, 1, 1],\n",
       "       [1, 1, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 1, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 10000\n",
    "Alpha_P_bnr = HM.sleep_sample_batch(n_sample)\n",
    "generation = Alpha_P_bnr['z0']\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "4fada9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([460, 440, 251, 223, 354, 235, 198, 140, 207, 177,  69,  85, 157,\n",
       "       114,  44,  42, 427, 334, 216, 192, 246, 169, 141, 107, 177, 166,\n",
       "        65,  48, 114,  61,  23,  27, 258, 271, 149, 213, 187, 158, 169,\n",
       "       172, 212, 193,  60, 105, 148, 115,  25,  25, 233, 244, 101, 131,\n",
       "       147, 118,  50,  56, 194, 212,  52,  76, 114,  77,  11,  15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(Alpha_P_bnr['z2'],axis=1, return_counts=True) #[10,8,6,3,1]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "79d3b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha_Q_bnr = HM.wake_sample_batch(np.repeat(dataset,30,axis=1),n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "90182fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([268,  74, 265, 119, 305, 188, 159,  88,  94, 112,  84,  65, 184,\n",
       "        48,  44,  48, 255, 341, 232, 187, 327, 313, 241, 195,  77, 104,\n",
       "        45,  23,  61,  87,  30,  67, 396, 208, 159, 146, 156, 127, 203,\n",
       "       143,  71, 148,  34,  47,  78,  32,  22,  28, 250, 480, 143, 153,\n",
       "       215, 147,  92,  94, 132, 214,  67,  80,  35, 141,  10,  19],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(Alpha_Q_bnr['z2'],axis=1, return_counts=True) #[10,8,6,3,1]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "ef02b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dcm = decimalize(dataset)\n",
    "gen_dcm = decimalize(generation)\n",
    "values_d, counts_d = np.unique(data_dcm, return_counts=True)\n",
    "values_g, counts_g = np.unique(gen_dcm, return_counts=True)\n",
    "values_dg = np.unique(np.append(values_d,values_g))\n",
    "dist_d = counts_d\n",
    "dist_g = counts_g/n_sample*n_data\n",
    "dist = np.zeros((2,values_dg.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "f7d6675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy, d_ind, dg_ind_d = np.intersect1d(values_d, values_dg, return_indices=True)\n",
    "xy, g_ind, dg_ind_g = np.intersect1d(values_g, values_dg, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "65896d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist[0,dg_ind_d] = dist_d\n",
    "dist[1,dg_ind_g] = dist_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "63376188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319.34400000000005"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = np.sum((dist[0,:] - dist[1,:])**2)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4a399104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3132"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistics\n",
    "percent = np.in1d(gen_dcm,values_d).sum()/gen_dcm.size\n",
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7841985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "711"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_n = np.in1d(values_g,values_d,invert=True).sum()\n",
    "outlier_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "517043fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6731"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_index = np.where(np.in1d(values_g,values_d,invert=True))\n",
    "outlier = np.array([values_g[outlier_index],counts_g[outlier_index]])\n",
    "counts_g[outlier_index].sum()/n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "36c886dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_n = np.in1d(values_d,values_g,invert=True).sum()\n",
    "fp_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "6a79d932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_index = np.where(np.in1d(values_d,values_g,invert=True))\n",
    "fp = np.array([values_d[fp_index],counts_d[fp_index]])\n",
    "counts_d[fp_index].sum()/n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "1061516b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1de769e1510>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization (original order)\n",
    "x_lim = 2**HM.n_d\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d, dist_d,label = \"Ground Truth\")\n",
    "ax.bar(values_g,dist_g,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "78c001e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 650,  651,  652, ..., 1008, 1009, 1017], dtype=int64)"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set = np.append(values_d,np.where(np.in1d(np.arange(0,x_lim),values_d,invert=True)))\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "f6093a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy, g_ind, re_ind = np.intersect1d(values_g, reordered_set, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "d7367dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21a5a41a1d0>"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization (re-order)\n",
    "x_lim = 2**HM.n_d\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - counts_d.size))] = counts_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(np.arange(counts_d.size),dist_d,label = \"Ground Truth\")\n",
    "ax.bar(np.sort(re_ind),dist_g[np.argsort(re_ind)],label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482919c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cda822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
