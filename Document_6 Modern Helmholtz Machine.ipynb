{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Document 6</span> \n",
    "\n",
    "# Modern Helmholtz Machine - Modifications and Implementations\n",
    "\n",
    "This notebook presents an analysis on the structures of the Helmholtz Machine, along with experiments of certain modifications to the vanilla model. The goal (not yet achieved and not sure if it's possible) is to build a generative model that not only generates data from the dataset but recovers the data distribution.\n",
    "\n",
    "The modifications include:\n",
    "- Adding bias in linear combinations\n",
    "- Changing assignment values\n",
    "- Inserting MLP to compute better correlation between adjacent layers, enabling backpropagation of multiple layers as DNN\n",
    "- Modifying updating rules\n",
    "- Conditional distribution that takes care of explain-away and independent sampling (not sure if it's necessary.. which would add much computational load)\n",
    "\n",
    "*Updated:* December 21, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: It does! The (vanilla) Helmholtz machine approximates a full variational Bayes approach and reconstructs the data distribution within its ability. The result shows that it tries to approximate the evidence (given data distribution) with decreasing distribution difference (over the entire dataset). In the mean time, due to the limitation of the machine, it cannot reconstruct the distribution perfectly. The model presents a distribution difference $ \\approx 0.076$  and accuracy $ \\approx 0.94$) after training, which is not bad honestly. Therefore, the model is generating a few false instances in order to minimize the global distribution difference between the evidence and generation. Then we can perform **active sampling**, which is the core idea of the work and thought I am proposing all the time, to achieve a perfect accuracy with skewed distribution (the niche). Then all the theory of free energy principle, niche co-construction, etc. follows naturally. The ability of performing active sampling during training is the core reason for me choose the Helmholtz machine. This trait empowers the neural network to train and perform more human-likely, which paves the way to study cognition, humanity, culture, creativity and musicology quantitively by neural networks. It contributes to music research and explaining musical phenomena naturally because music is a human thing, it's created by human, defined by human, evaluated by human and updated by human. It's subjective, which cannot simply be learned by data fitting and generalized by interpolation. All these thoughts are embryonic and needs to be illustrated, realized and tested by huge amount of work. \n",
    "\n",
    "<img src=\"result.png\" style=\"width:800px\">\n",
    "<caption><center> Vanilla Training Result    </center></caption>\n",
    "\n",
    "<img src=\"{1,-1} Training Result.png\" style=\"width:800px\">\n",
    "<caption><center> {1,-1} Training Result    </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified Helmholtz machine structure is given below:\n",
    "\n",
    "<img src=\"Modern.jpg\" style=\"width:800px\">\n",
    "<caption><center> **Figure 1**: Modern Helmholtz Machine. Blue neurons represent instantiation layers, where the neurons take binary values. Orange neurons indicate the inserted activations, where the neurons take value in real numbers in given range.    </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation on data:\n",
    "- Dataset $D$: random variable, the goal is to (implicitly) maximize evidence $P(D)$\n",
    "- Phenotype layer $d_0$: single data input, observable state\n",
    "- $d$: single data point, observable variable\n",
    "\n",
    "Notation on hidden layers:\n",
    "- $z_i$: $i$-th instantiated hidden layer, random variable.\n",
    "- $s_i$: value assignment of $i$-th instantiated hidden layer, hidden state\n",
    "- $q(z_i[k])$: probability of $k$-th neuron in $z_i$ in recognition process\n",
    "- $p(z_i[k])$: probability of $k$-th neuron in $z_i$ in generative process\n",
    "- $q(z_i)$: probability of $z_i$ in recognition process, since the sampling of neurons are independent, $q(z_i) = \\prod_k q(z_i[k])$\n",
    "- $p(z_i)$: probability of $z_i$ in generative process, since the sampling of neurons are independent, $p(z_i) = \\prod_k p(z_i[k])$\n",
    "- $l^{i,i+1}_j$: $j$-th inserted hidden layers between $z_i$ and $z_{i+1}$, multilayer perceptron\n",
    "\n",
    "Parameters (amortized):\n",
    "- $\\Phi$: recognition weights, compute neuron values bottom-up in wake phase.\n",
    "- $\\Theta$: generative weights, compute neuron values top-down in sleep phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model structure\n",
    "\n",
    "n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "n is the maximum number of inserted layers between adjacent instantiation layers.\n",
    "\n",
    "A neural network in Figure 1 has structure ndarray:\n",
    "$$\n",
    "n_{dz} = \n",
    "\\begin{pmatrix}\n",
    "10 & 8 & 6 & 3 & 1 \\\\\n",
    "9 & 0 & 5 & 0 & 0 \\\\\n",
    "0 & 0 & 4 & 0 & 0  \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The first row gives the number of neurons in each instantiation layer. The second row represents the first inserted layer, and there is a 9-neuron layer inserted between $d_0$ and $z_1$, a 5-neuron layer inserted between $z_2$ and $z_3$. The third row represents the second inserted layer, and there is only one 4-neuron layer inserted between $z_2$ and $z_3$ above the first 5-neuron inserted layer. Obviously, $n_{dz}(i,k) \\ne 0$ if and only if $n_{dz}(i,j) \\ne 0, \\forall j < k$; the last column of  $n_{dz}$ is all zero except for the value in the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  6,  3,  1],\n",
       "       [ 9,  0,  5,  0,  0],\n",
       "       [ 0,  0,  4,  0,  0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[10,8,6,3,1],\n",
    "             [9, 0,5,0,0],\n",
    "             [0, 0,4,0,0]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter initialization\n",
    "\n",
    "When deciding parameter sets, the question is on whether to add bias and where to add it. In vanilla Helmholtz machine, the activations are calculated without bias term:\n",
    "\n",
    "$$p_n^m(\\theta, s^{m+1}) = \\sigma(\\sum_k \\theta_{k,n}^{m+1,m} \\centerdot s_k^{m+1})$$\n",
    "$$q_n^m(\\phi, s^{m-1}) = \\sigma(\\sum_k \\phi_{k,n}^{m-1,m} \\centerdot s_k^{m-1})$$\n",
    "\n",
    "We'd love to add the option of neuron bias to see how the model behaves, thus the computation becomes\n",
    "\n",
    "$$p_n^m(\\theta, s^{m+1}) = \\sigma(\\sum_k \\theta_{k,n}^{m+1,m} \\centerdot s_k^{m+1}+ b_{n}^{m+1,m})$$\n",
    "$$q_n^m(\\phi, s^{m-1}) = \\sigma(\\sum_k \\phi_{k,n}^{m-1,m} \\centerdot s_k^{m-1}+ b_{n}^{m-1,m})$$\n",
    "\n",
    "This minor modification seems trivial, but it may influence the precision of certain neuron predictions significantly. To observe its effects, we further divide the bias trial into 3 situations:\n",
    "\n",
    "1. Bias on data (bottom) layer\n",
    "2. Bias on hidden instantiation layers\n",
    "3. Bias on hidden inserted layers\n",
    "\n",
    "The experiment will be performed as thus but for now for parameter defining, we initialize the parameter matrix with weights and bias, and choose to use or leave out the bias term afterwards.\n",
    "\n",
    "There are two types of initialization, random or zero. By neutralizaing all weights and bias to zero, we erase the prior of distributions of hidden variables. By setting the parameters to random numbers, we impose a random prior on hidden $z$'s, which may or may not (needs to be validated) influence the generative distribution on fixed dataset fitting, but it will play a significant role when we perform more subjective tasks such as active sampling and predictive processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(init_type,n_dz):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \"\"\"\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    m = n_dz.shape[1]\n",
    "    \n",
    "    if init_type == \"zero\":\n",
    "        for i in range(m-1):\n",
    "            n = np.where(n_dz[1:,i] != 0)[0].size  # number of inserted layers between i and i+1\n",
    "            if n == 0:\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((n_dz[0,i+1],n_dz[0,i]+1))\n",
    "                Theta[\"Theta_\" + str(i+1) + str(i)] = np.zeros((n_dz[0,i],n_dz[0,i+1]+1))\n",
    "            else:\n",
    "                for j in range(1,n+1):\n",
    "                    Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j)] = np.zeros((n_dz[j,i],n_dz[j-1,i]+1))\n",
    "                    Theta[\"Theta_\" + str(i+1) + str(i) + \"_\" + str(j)] = np.zeros((n_dz[j-1,i],n_dz[j,i]+1))\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j+1)] = np.zeros((n_dz[0,i+1],n_dz[j,i]+1))\n",
    "                Theta[\"Theta_\" + str(i+1) + str(i) + \"_\" + str(j+1)] = np.zeros((n_dz[j,i],n_dz[0,i+1]+1))\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(m-1):\n",
    "            n = np.where(n_dz[1:,i] != 0)[0].size\n",
    "            if n == 0:\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(n_dz[0,i+1],n_dz[0,i]+1)\n",
    "                Theta[\"Theta_\" + str(i+1) + str(i)] = np.random.randn(n_dz[0,i],n_dz[0,i+1]+1)\n",
    "            else:\n",
    "                for j in range(1,n+1):\n",
    "                    Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j)] = np.random.randn(n_dz[j,i],n_dz[j-1,i]+1)\n",
    "                    Theta[\"Theta_\" + str(i+1) + str(i) + \"_\" + str(j)] = np.random.randn(n_dz[j-1,i],n_dz[j,i]+1)\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j+1)] = np.random.randn(n_dz[0,i+1],n_dz[j,i]+1)\n",
    "                Theta[\"Theta_\" + str(i+1) + str(i) + \"_\" + str(j+1)] = np.random.randn(n_dz[j,i],n_dz[0,i+1]+1)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "        \n",
    "    return Phi, Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, Theta = parameter_initialization(\"random\",n_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Free Energy\n",
    "\n",
    "The minimization goal of the Helmholtz machine is the variational free energy, also negative ELBO of the log evidence $\\log(P(D))$:\n",
    "\n",
    "$$\n",
    "F_{\\phi,\\theta}(D) = \\mathbb{E}_Q[\\log Q_\\phi(\\alpha|D)] - \\mathbb{E}_Q[\\log P_\\theta(\\alpha,D)] \n",
    "$$\n",
    "\n",
    "where $D$ is the given dataset and $\\alpha$ are the latent variable $z_i$'s. Instead of updating parameters on the whole dataset, we perform a stochastic optimization where single data point gets inputted and $\\Theta$ and $\\Phi$ are updated accordingly. The point-wise objective is\n",
    "\n",
    "$$\n",
    "F_{\\phi,\\theta}(d) = \\mathbb{E}_Q[\\log Q_\\phi(\\alpha|d)] - \\mathbb{E}_Q[\\log P_\\theta(\\alpha,d)] \n",
    "$$\n",
    "\n",
    "$\\Theta$ and $\\Phi$ are amortized to all data points and latent assignments, but basically different data points correspond to different latent distributions. It's under question that if the stochastic optimization really approxiamates the integral optimization, but let's proceed here first and see if we can find the answer somewhere somhow..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter updating is performed in an expectation-maximization manner, except that we have two sets of parameters $\\Theta$ and $\\Phi$. Namely, we use $\\Phi$ to calculate the latent assignment $\\alpha$ (or in a better way, the distribution of latent variables $z$) and use the computed terms to update $\\Theta$; conversely, we can compute the latent terms by $\\Theta$ and update $\\Phi$ accordingly. Since the latent variables are shared in the feedback neural network, this alternative parameter updating is feasible, which is known as *the wake-sleep algorithm*.\n",
    "\n",
    "Now let's derive the parameter updation rules. Let's say we fix parameter $\\Phi$ and want to update $\\Theta$. The first term $\\mathbb{E}_Q[\\log Q_\\phi(\\alpha|d)]$ is constant with fixed $\\Phi$, thus the minimization goal becomes\n",
    "\n",
    "$$\n",
    "L = - \\mathbb{E}_Q[\\log P_\\theta(\\alpha,d)] = -\\sum_\\alpha Q_\\phi(\\alpha|d)\\log P_\\theta(\\alpha,d)\n",
    "$$\n",
    "\n",
    "The term that involves parameter $\\Theta$ is $P_\\theta(\\alpha,d)$, now we try to decompose this term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Helmholtz machine, each neuron takes binary value, with probability $p$. Thus every neuron variale $z_n^m$ encodes a Bernoulli distribution, where the probability mass function\n",
    "\n",
    "$$\n",
    "p(z_n^m) = \n",
    "\\left \\{\n",
    "\\begin{array}{ll}\n",
    "p & \\text{if } s_n^m = a \\\\\n",
    "1-p & \\text{if } s_n^m = b\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "when the neurons take value in $\\{a,b\\}$. $a$ is the positive outcome with probability $p$, while $b$ is the negative outcome with probability $1-p$. Then the distribution can be written as\n",
    "\n",
    "$$p(z_n^m) = p^{\\frac{s_n^m-b}{a-b}}(1-p)^{\\frac{a-s_n^m}{a-b}}$$\n",
    "\n",
    "Since all neurons are independent random variables,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_\\theta(\\alpha,d) &= \\prod_{n,m} [p_n^m(\\theta, s^{m+1})]^{\\frac{s_n^m-b}{a-b}}[1-p_n^m(\\theta, s^{m+1})]^{\\frac{a-s_n^m}{a-b}} \\\\\n",
    "\\log P_\\theta(\\alpha,d) &= \\sum_{n,m} \\frac{s_n^m-b}{a-b} \\log [p_n^m] + \\frac{a-s_n^m}{a-b} \\log [1-p_n^m] \\\\\n",
    "\\frac{\\partial \\log P_\\theta(\\alpha,d)}{\\partial p_n^m} &= \\frac{s_n^m-b}{a-b} \\centerdot \\frac{1}{p_n^m} - \\frac{a-s_n^m}{a-b} \\centerdot \\frac{1}{1-p_n^m}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "From $p_n^m(\\theta, s^{m+1}) = \\sigma(\\sum_k \\theta_{k,n}^{m+1,m} \\centerdot s_k^{m+1})$ (the bias is integrated into the weights by setting its multiplicative value $s$ to $1$ always) and $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$ we derive,\n",
    "\n",
    "$$\\frac{\\partial p_n^m}{\\partial \\theta_{k,n}^{m+1,m}} = p_n^m(1-p_n^m)s_k^{m+1}$$\n",
    "\n",
    "Multiplying the two terms we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log P_\\theta(\\alpha,d)}{\\partial \\theta_{k,n}^{m+1,m}} &= \\frac{\\partial \\log P_\\theta(\\alpha,d)}{\\partial p_n^m} \\bullet \\frac{\\partial p_n^m}{\\partial \\theta_{k,n}^{m+1,m}} \\\\\n",
    "&= s_k^{m+1}(\\frac{s_n^m-b}{a-b} - p_n^m) \\\\\n",
    "&=  \\left \\{\n",
    "\\begin{array}{ll}\n",
    "s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m = a \\\\\n",
    "-s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m = b\n",
    "\\end{array} \\right.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since $L = -\\sum_\\alpha Q_\\phi(\\alpha|d)\\log P_\\theta(\\alpha,d)$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_{k,n}^{m+1,m}} = -\\sum_\\alpha Q_\\phi(\\alpha|d)\\frac{\\partial \\log P_\\theta(\\alpha,d)}{\\partial \\theta_{k,n}^{m+1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-\\sum_\\alpha Q_\\phi(\\alpha|d)s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m = a \\\\\n",
    "\\sum_\\alpha Q_\\phi(\\alpha|d)s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m = b\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "By gradient descent,\n",
    "\n",
    "$$\\theta_{k,n}^{m+1,m} = \\theta_{k,n}^{m+1,m} - lr \\centerdot \\frac{\\partial L}{\\partial \\theta_{k,n}^{m+1,m}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the formula above, we have an important observation: when updating parameter $\\Theta$, more specifically $\\theta_{k,n}^{m+1,m}$, let alone the expectation over $Q$, it multiplicatively depends on the *possibility* of current neuron $z_n^m$ and the *value assignments* of its connected previous-layer neuron $s_k^{m+1}$. Therefore, one conclusion is that, the binary outcomes of these Bernoulli neuron matter. The larger the outcome value is, the further parameter $\\theta_{k,n}^{m+1,m}$ is updated. In the case of $s_k^{m+1} = 0$, no updation is performed for $\\theta_{k,n}^{m+1,m}$.\n",
    "\n",
    "In vanilla Helmholtz machine, binary outcomes $\\{0,1\\}$ are taken, which actually halfly paralyzed the model because the parameters are not updated when the previous neuron takes value $0$. In our experiment, we changed the values to $\\{-1,1\\}$, which achieved much better results. What's more, we can experiment with bipolar values such as $\\{10,1\\}$, $\\{100,1\\}$, and see how the model behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_sample(n_dz,d0,value_set,Phi,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias], taking binary value in {True, False}. For example, [False,True] means \n",
    "    no instantiation bias but has MLP bias\n",
    "    \n",
    "    Returns:\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m-1 with each key-value pair being \n",
    "    a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "    \"\"\"\n",
    "    \n",
    "    m = n_dz.shape[1]\n",
    "    S = d0  # assignment of each layer\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    inst_bias = bias[0]\n",
    "    mlp_bias = bias[1]\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    \n",
    "    for i in range(m-2):\n",
    "        n = np.where(n_dz[1:,i] != 0)[0].size  # number of inserted layers between i and i+1\n",
    "        if n == 0:\n",
    "            phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "            if inst_bias == True:\n",
    "                q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0)))\n",
    "            else:\n",
    "                q = sigmoid(np.matmul(phi[:,:-1],S))\n",
    "            S = ((q > np.random.rand(len(q),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "            Alpha_Q[\"z\"+str(i+1)] = S\n",
    "        else:\n",
    "            g = S\n",
    "            for j in range(1,n+1):\n",
    "                phi = Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j)]\n",
    "                if activation_type == \"sigmoid\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))*(a-b)+b  # scale to [b,a]\n",
    "                    else:\n",
    "                        g = sigmoid(np.matmul(phi[:,:-1],g))*(a-b)+b\n",
    "                elif activation_type == \"tanh\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = np.tanh(np.matmul(phi,np.append(g,[[1]], axis=0)))*(a-b)/2+(a+b)/2 # scale to [b,a]\n",
    "                    else:\n",
    "                        g = np.tanh(np.matmul(phi[:,:-1],g))*(a-b)/2+(a+b)/2\n",
    "                    \n",
    "            phi = Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j+1)]\n",
    "            if inst_bias == True:\n",
    "                q = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))\n",
    "            else:\n",
    "                q = sigmoid(np.matmul(phi[:,:-1],g))\n",
    "            S = ((q > np.random.rand(len(q),1)).astype(int))*(a-b)+b\n",
    "            Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    Alpha_Q[\"z\"+str(m-1)] = [[1]]\n",
    "        \n",
    "    return Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 1],\n",
       "       [-1],\n",
       "       [ 1],\n",
       "       [-1],\n",
       "       [-1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [-1],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = -1\n",
    "d0 = np.array([[a],[a],[b],[a],[b],[b],[a],[a],[b],[a]])\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_type = \"tanh\"\n",
    "bias = [False,True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha_Q = wake_sample(n_dz,d0,[a,b],Phi,activation_type,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_probability(n_dz,Alpha_Q,value_set,Phi,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    Alpha_Q -- given assignment of each neuron (binary value), Python dictionary of length m-1 with each key-value pair being \n",
    "    a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias], taking binary value in {True, False}. For example, [False,True] means \n",
    "    no instantiation bias but has MLP bias\n",
    "    \n",
    "    Returns:\n",
    "    Q -- Probability of given recognition assignment Alpha_Q, number in (0,1)\n",
    "    Prob (optional) -- probability of given assignment for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    m = n_dz.shape[1]\n",
    "    S = Alpha_Q['z0']\n",
    "    inst_bias = bias[0]\n",
    "    mlp_bias = bias[1]\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    Q = 1\n",
    "    Prob = {}\n",
    "    \n",
    "    for i in range(m-2):\n",
    "        n = np.where(n_dz[1:,i] != 0)[0].size  # number of inserted layers between i and i+1\n",
    "        if n == 0:\n",
    "            phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "            if inst_bias == True:\n",
    "                q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0)))\n",
    "            else:\n",
    "                q = sigmoid(np.matmul(phi[:,:-1],S))\n",
    "        else:\n",
    "            g = S\n",
    "            for j in range(1,n+1):\n",
    "                phi = Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j)]\n",
    "                if activation_type == \"sigmoid\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))*(a-b)+b  # scale to [b,a]\n",
    "                    else:\n",
    "                        g = sigmoid(np.matmul(phi[:,:-1],g))*(a-b)+b\n",
    "                elif activation_type == \"tanh\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = np.tanh(np.matmul(phi,np.append(g,[[1]], axis=0)))*(a-b)/2+(a+b)/2 # scale to [b,a]\n",
    "                    else:\n",
    "                        g = np.tanh(np.matmul(phi[:,:-1],g))*(a-b)/2+(a+b)/2\n",
    "                    \n",
    "            phi = Phi[\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j+1)]\n",
    "            if inst_bias == True:\n",
    "                q = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))\n",
    "            else:\n",
    "                q = sigmoid(np.matmul(phi[:,:-1],g))\n",
    "        S = Alpha_Q['z'+ str(i+1)]\n",
    "        prob = ((2*q-1)*S + a-(a+b)*q)/(a-b)  # s=a: multiply q, s=b: multiply (1-q)\n",
    "        Prob['z'+ str(i+1)] = prob\n",
    "        Q = Q * np.cumprod(prob)[-1] * 2**q.size\n",
    "                    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = wake_probability(n_dz,Alpha_Q,[a,b],Phi,activation_type,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_sample(n_dz,value_set,Theta,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias,data bias], taking binary value in {True, False}. For example, \n",
    "    [False,True,True] means no instantiation bias but has MLP bias and data bias\n",
    "    \n",
    "    Returns:\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m-1 with each key-value pair being \n",
    "    a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "    \"\"\"\n",
    "    m = n_dz.shape[1]\n",
    "    inst_bias = bias[0]\n",
    "    mlp_bias = bias[1]\n",
    "    data_bias = bias[2]\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    S = [[1]]\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        n = np.where(n_dz[1:,i-1] != 0)[0].size  # number of inserted layers between i and i-1\n",
    "        if n == 0:\n",
    "            theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "            if i > 1:\n",
    "                if inst_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0))) #\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],S))\n",
    "                S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "                Alpha_P[\"z\"+str(i-1)] = S\n",
    "            else:\n",
    "                if data_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],S))\n",
    "                S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "                Alpha_P[\"z\"+str(i-1)] = S\n",
    "        else:\n",
    "            g = S\n",
    "            for j in range(n+1,1,-1):\n",
    "                theta = Theta[\"Theta_\" + str(i) + str(i-1) + \"_\" + str(j)]\n",
    "                if activation_type == \"sigmoid\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = sigmoid(np.matmul(theta,np.append(g,[[1]], axis=0)))*(a-b)+b  # scale to [b,a]\n",
    "                    else:\n",
    "                        g = sigmoid(np.matmul(theta[:,:-1],g))*(a-b)+b\n",
    "                elif activation_type == \"tanh\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = np.tanh(np.matmul(theta,np.append(g,[[1]], axis=0)))*(a-b)/2+(a+b)/2 # scale to [b,a]\n",
    "                    else:\n",
    "                        g = np.tanh(np.matmul(theta[:,:-1],g))*(a-b)/2+(a+b)/2\n",
    "                    \n",
    "            theta = Theta[\"Theta_\" + str(i) + str(i-1) + \"_\" + str(j-1)]\n",
    "            \n",
    "            if i > 1:\n",
    "                if inst_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(g,[[1]], axis=0)))\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],g))\n",
    "                S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "                Alpha_P[\"z\"+str(i-1)] = S\n",
    "            else:\n",
    "                if data_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(g,[[1]], axis=0)))\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],g))\n",
    "                S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "                Alpha_P[\"z\"+str(i-1)] = S\n",
    "            \n",
    "    return Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = [False,True,True]\n",
    "Alpha_P = sleep_sample(n_dz,[a,b],Theta,activation_type,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_probability(n_dz,Alpha_P,value_set,Theta,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m-1 with each key-value pair being \n",
    "    a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias,data bias], taking binary value in {True, False}. For example, \n",
    "    [False,True,True] means no instantiation bias but has MLP bias and data bias\n",
    "    \n",
    "    Returns:\n",
    "    P -- Probability of given generative assignment Alpha_P, number in (0,1)\n",
    "    Prob (optional) -- probability of given assignment for each layer\n",
    "    \"\"\"\n",
    "    m = n_dz.shape[1]\n",
    "    inst_bias = bias[0]\n",
    "    mlp_bias = bias[1]\n",
    "    data_bias = bias[2]\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    keys = [*Alpha_P]\n",
    "    P = 1\n",
    "    Prob = {}\n",
    "    \n",
    "    for i in range(len(keys)-1):  # parallelable\n",
    "        S = Alpha_P[keys[i]]\n",
    "        l = int(keys[i][1])\n",
    "        n = np.where(n_dz[1:,l-1] != 0)[0].size  # number of inserted layers between l and l-1\n",
    "        if n == 0:\n",
    "            theta = Theta[\"Theta_\" + str(l) + str(l-1)]\n",
    "            if i > 1:\n",
    "                if inst_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0))) #\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],S))\n",
    "            else:\n",
    "                if data_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],S))\n",
    "        else:\n",
    "            g = S\n",
    "            for j in range(n+1,1,-1):\n",
    "                theta = Theta[\"Theta_\" + str(l) + str(l-1) + \"_\" + str(j)]\n",
    "                if activation_type == \"sigmoid\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = sigmoid(np.matmul(theta,np.append(g,[[1]], axis=0)))*(a-b)+b  # scale to [b,a]\n",
    "                    else:\n",
    "                        g = sigmoid(np.matmul(theta[:,:-1],g))*(a-b)+b\n",
    "                elif activation_type == \"tanh\":\n",
    "                    if mlp_bias == True:\n",
    "                        g = np.tanh(np.matmul(theta,np.append(g,[[1]], axis=0)))*(a-b)/2+(a+b)/2 # scale to [b,a]\n",
    "                    else:\n",
    "                        g = np.tanh(np.matmul(theta[:,:-1],g))*(a-b)/2+(a+b)/2\n",
    "                    \n",
    "            theta = Theta[\"Theta_\" + str(l) + str(l-1) + \"_\" + str(j-1)]\n",
    "            if i > 1:\n",
    "                if inst_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(g,[[1]], axis=0)))\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],g))\n",
    "            else:\n",
    "                if data_bias == True:\n",
    "                    p = sigmoid(np.matmul(theta,np.append(g,[[1]], axis=0)))\n",
    "                else:\n",
    "                    p = sigmoid(np.matmul(theta[:,:-1],g))\n",
    "                    \n",
    "        S = Alpha_P[keys[i+1]]\n",
    "        prob = ((2*p-1)*S + a-(a+b)*p)/(a-b)  # s=a: multiply q, s=b: multiply (1-q)\n",
    "        Prob['z'+ str(l-1)] = prob\n",
    "        P = P * np.cumprod(prob)[-1] * 2**p.size\n",
    "            \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = sleep_probability(n_dz,Alpha_P,[a,b],Theta,activation_type,bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've computed the forward functions, let's look at the parameter updating. For vanilla Helmholtz machine, where there are only instantiation layers, the parameter updating is one-step, which is named as local delta rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (-\\log P_\\theta(\\alpha,d))}{\\partial \\theta_{k,n}^{m+1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m \\text{ is positive outcome }a \\\\\n",
    "s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m \\text{ is negative outcome }b\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Comparing to the formula we got previously, the vanilla Helmholtz machine simplified the objective further by taking out the variational expectation over $Q(\\alpha|d)$, which surrogates the sampling frequency of each instance $\\{\\alpha,d\\}$ as the distribution weights.\n",
    "\n",
    "The objective function $-\\log P_\\theta(\\alpha,d)$ is exactly the cross-entropy of an indepent multivariate Bernoulli distribution, when we write it out as\n",
    "\n",
    "$$\n",
    "-\\log P_\\theta(\\alpha,d) = -\\sum_{n,m} \\frac{s_n^m-b}{a-b} \\log [p_n^m] - \\frac{a-s_n^m}{a-b} \\log [1-p_n^m]\n",
    "$$\n",
    "\n",
    "Since non-adjacent instantiation layers are independent, it doesn't hurt if we consider each adjacent pairs separately instead of the entire model. Therefore, it's legitimate to study a submodel in Figure 2 to derive the parameter updating rules for (modern) Helmholtz machine.\n",
    "\n",
    "<img src=\"muti-bernoulli.jpg\" style=\"width:800px\">\n",
    "<caption><center> **Figure 2**: Multivariate Bernoulli Distribution. In wake phase, we go from input $\\mathbf{x}$ to output $\\mathbf{y}$ by weight $\\Phi$. Blue neurons represent instantiation layers, where each neuron takes binary value and is computed as an independent Bernoulli variable. Orange neurons are inserted activations, which transform a shallow neural network with one-step prameter updating to deep neural network with backpropogation.\n",
    "</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition and effects of adding MLP will be explored in future notebooks (Helmholtz machine on XOR), so let's move ahead to derivative computing directly. Let's say we are in wake phase and the input $\\mathbf{x}$ and the target value $\\mathbf{y}$ are given.\n",
    "\n",
    "Notations: \n",
    "- Bold lower-case math symbols represent column vectors\n",
    "- Bold upper-case math symbols represent matrices\n",
    "- “$\\centerdot$” represents element-wise multiplication\n",
    "- \"$\\times$\" or \"\" represents matrix multiplication\n",
    "- $\\otimes$ represents outer product\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log Q &= \\sum_{n} \\frac{y_n-b}{a-b} \\log [q_n] + \\frac{a-y_n}{a-b} \\log [1-q_n] \\\\\n",
    "\\frac{\\partial \\log Q}{\\partial \\mathbf{q}} &= \\frac{\\mathbf{y}-b}{a-b} \\centerdot \\frac{1}{\\mathbf{q}} - \\frac{a-\\mathbf{y}}{a-b} \\centerdot \\frac{1}{1-\\mathbf{q}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "\\mathbf{q} = \\sigma(\\Phi^{l3,y}\\times\\mathbf{z^3})\n",
    "$$\n",
    "\n",
    "where $\\Phi^{l3,y}$ is a matrix and $\\sigma'(\\centerdot) = \\sigma(\\centerdot)(1-\\sigma(\\centerdot))$，\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{q}}{\\partial \\sigma(\\mathbf{\\centerdot})} = \\mathbf{q}\\centerdot(1-\\mathbf{q})\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log Q}{\\partial \\sigma(\\mathbf{\\centerdot})} = \\frac{\\partial \\log Q}{\\partial \\mathbf{q}} \\centerdot \\frac{\\partial \\mathbf{q}}{\\partial \\sigma(\\mathbf{\\centerdot})} = \\frac{\\mathbf{y}-b}{a-b} - \\mathbf{q}\n",
    "$$\n",
    "\n",
    "The objective cross entropy is $L = -\\log Q$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\sigma(\\mathbf{\\centerdot})} = \\mathbf{q} - \\frac{\\mathbf{y}-b}{a-b}\n",
    "$$\n",
    "\n",
    "Let's denote $\\mathbf{q} - \\frac{\\mathbf{y}-b}{a-b}$ as $\\mathbf{u}$, which in more general represents the **element-wise** product of all previous derivative computations $\\frac{\\partial L}{\\partial \\mathbf{q}}$ and the derivative of the activation function $\\frac{\\partial \\mathbf{q}}{\\partial g(\\mathbf{\\centerdot})}$ (times $c$) if the activation layer is scaled by $c$. The activation function can be replaced by $tanh(\\centerdot)$, whose derivative is $tanh'(\\centerdot) = 1-tanh^2(\\centerdot)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\Phi} &= \\mathbf{u} \\otimes \\mathbf{z} \\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{z}} &= \\Phi^T \\mathbf{u}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We know there are automatic differentiation packages utilizable for derivative computing (not-manually). We are not using it here because first of all, the derivative formulas are not complicated in our case. By writing them out, we can get more intuition on the parameter updating rules. More importantly, the gradient descent is problematic sometimes, especially for deep neural networks, so by doing it manually, I want to modify some terms inside the computation of gradient in the future and see if it will help with the neural network performance.\n",
    "\n",
    "One thing worth noting is that, there is no rule saying that the parameters have to walk along its directional derivative direction. The iterative method to a local minimum along the directional derivative is known as steepest descent, which gives the quickest route for minimization. Sometimes this \"fastest\" method (within 1-st order family) is where the problem comes from, that's why people use Adam to equalibriate the uneven updating rates. However, Adam is a \"posterior\" method, which somehow remedies the problem of gradient descent afterwards but not solving it from the root. It's a misunderstanding (at least for me) that the exact derivatives have to be used to update parameters. Actually to decrease the objective function, the condition is much loose. From the first-order Taylor expansion (deductions to be added), we can tell that as long as the parameters are updated in a an acute angle with the directional derivative, or simply speaking, each parameter modifying term keeps the same sign as its partial derivative, the function value is decreased (with good learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_Bernoulli_update(x,y,parameter_set,lr,value_set,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input instantiation layer, numpy array of shape (n,1)\n",
    "    y -- target instantiation layer, numpy array of shape (m,1)\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l+1, l is the number of inserted layers. \n",
    "    The keys are ordered sequentially from layer x to y.\n",
    "    lr -- learning rate, decimals\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation (data) bias, MLP bias], taking binary value in {True, False}. For example, \n",
    "    [False,True] means no instantiation bias but has MLP (data) bias\n",
    "    \n",
    "    Returns:\n",
    "    parameter_set -- updated parameters\n",
    "    loss -- value of loss function before updating, a number\n",
    "    \"\"\"\n",
    "    \n",
    "    inst_bias = bias[0]\n",
    "    mlp_bias = bias[1]\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    l = len(parameter_set)\n",
    "    keys = [*parameter_set]\n",
    "    G = {'z0': x}\n",
    "    g = x\n",
    "    \n",
    "    for i in range(l-1):\n",
    "        phi = parameter_set[keys[i]]\n",
    "        if activation_type == \"sigmoid\":\n",
    "            if mlp_bias == True:\n",
    "                g = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))*(a-b)+b  # scale to [b,a]\n",
    "            else:\n",
    "                g = sigmoid(np.matmul(phi[:,:-1],g))*(a-b)+b\n",
    "        elif activation_type == \"tanh\":\n",
    "            if mlp_bias == True:\n",
    "                g = np.tanh(np.matmul(phi,np.append(g,[[1]], axis=0)))*(a-b)/2+(a+b)/2 # scale to [b,a]\n",
    "            else:\n",
    "                g = np.tanh(np.matmul(phi[:,:-1],g))*(a-b)/2+(a+b)/2\n",
    "        G['z'+str(i+1)] = g\n",
    "\n",
    "    phi = parameter_set[keys[l-1]]\n",
    "    if inst_bias == True:\n",
    "        q = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))\n",
    "    else:\n",
    "        q = sigmoid(np.matmul(phi[:,:-1],g))\n",
    "        \n",
    "    # derivatives\n",
    "    u = q - (y-b)/(a-b)\n",
    "    loss = np.sum(np.abs(u))  # for visulization\n",
    "    for i in range(l-1,0,-1):\n",
    "        phi = parameter_set[keys[i]][:,:-1]\n",
    "        dz = np.matmul(phi.T,u)\n",
    "        z = G['z'+str(i)]\n",
    "        parameter_set[keys[i]] -= lr * np.outer(u,np.append(z,[[1]], axis=0))\n",
    "        if activation_type == \"sigmoid\":\n",
    "            u = dz * z * (1-z) * (a-b)\n",
    "        elif activation_type == \"tanh\":\n",
    "            u = dz * (1-z**2) * (a-b)/2\n",
    "            \n",
    "    parameter_set[keys[0]] -= lr * np.outer(u,np.append(x,[[1]], axis=0))\n",
    "    \n",
    "    return parameter_set,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d0\n",
    "y = d0[:6]\n",
    "parameter_set = {k: Phi[k] for k in [*Phi][:3]}\n",
    "bias = [False,False]\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_set,loss = multi_Bernoulli_update(x,y,parameter_set,lr,[a,b],activation_type,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias], taking binary value in {True, False}. For example, [False,True] means \n",
    "    no instantiation bias but has MLP bias\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "    \"\"\"\n",
    "    m = n_dz.shape[1]\n",
    "    Loss = np.zeros(m)\n",
    "    for i in range(m-2):\n",
    "        n = np.where(n_dz[1:,i] != 0)[0].size  # number of inserted layers between i and i+1\n",
    "        if n == 0:\n",
    "            parameter_set = {\"Phi_\" + str(i) + str(i+1): Phi[\"Phi_\" + str(i) + str(i+1)]}\n",
    "        else:\n",
    "            parameter_set = {k: Phi[k] for k in [\"Phi_\" + str(i) + str(i+1) + \"_\" + str(j) for j in range(1,n+2)]}\n",
    "            \n",
    "        x = Alpha_P['z'+str(i)]\n",
    "        y = Alpha_P['z'+str(i+1)]\n",
    "        parameter_set,loss = multi_Bernoulli_update(x,y,parameter_set,lr,value_set,activation_type,bias)\n",
    "        Loss[i] = loss\n",
    "        Loss[-1] += loss\n",
    "        for k in [*parameter_set]:\n",
    "            Phi[k] = parameter_set[k]\n",
    "        \n",
    "    return Phi,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.94891024, 3.00888968, 1.46373679, 0.        , 8.42153671])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi,Loss = wake_update_delta(Phi,Alpha_P,lr,n_dz,[a,b],activation_type,bias)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias], taking binary value in {True, False}. For example, [False,True] means \n",
    "    no instantiation bias but has MLP bias\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Loss -- numpy array of length m; the first m-1 values are layer loss, the last term is the total loss\n",
    "    \"\"\"\n",
    "    \n",
    "    m = n_dz.shape[1]\n",
    "    inst_bias = bias[0]\n",
    "    mlp_bias = bias[1]\n",
    "    data_bias = bias[2]\n",
    "    \n",
    "    Loss = np.zeros(m)\n",
    "    bias = [inst_bias,mlp_bias]\n",
    "    for i in range(m-1,1,-1):\n",
    "        n = np.where(n_dz[1:,i-1] != 0)[0].size  # number of inserted layers between i and i+1\n",
    "        if n == 0:\n",
    "            parameter_set = {\"Theta_\" + str(i) + str(i-1): Theta[\"Theta_\" + str(i) + str(i-1)]}\n",
    "        else:\n",
    "            parameter_set = {k: Theta[k] for k in [\"Theta_\" + str(i) + str(i-1) + \"_\" + str(j) for j in range(n+1,0,-1)]}\n",
    "            \n",
    "        x = Alpha_Q['z'+str(i)]\n",
    "        y = Alpha_Q['z'+str(i-1)]\n",
    "        parameter_set,loss = multi_Bernoulli_update(x,y,parameter_set,lr,value_set,activation_type,bias)\n",
    "        Loss[i-1] = loss\n",
    "        Loss[-1] += loss\n",
    "        for k in [*parameter_set]:\n",
    "            Theta[k] = parameter_set[k]\n",
    "            \n",
    "    bias = [data_bias,mlp_bias]     \n",
    "    n = np.where(n_dz[1:,0] != 0)[0].size  # number of inserted layers between 0 and 1\n",
    "    if n == 0:\n",
    "        parameter_set = {\"Theta_10\": Theta[\"Theta_10\"]}\n",
    "    else:\n",
    "        parameter_set = {k: Theta[k] for k in [\"Theta_10_\"+ str(j) for j in range(n+1,0,-1)]}\n",
    "\n",
    "    x = Alpha_Q['z1']\n",
    "    y = Alpha_Q['z0']\n",
    "    parameter_set,loss = multi_Bernoulli_update(x,y,parameter_set,lr,value_set,activation_type,bias)\n",
    "    Loss[0] = loss\n",
    "    Loss[-1] += loss\n",
    "    for k in [*parameter_set]:\n",
    "        Theta[k] = parameter_set[k]\n",
    "\n",
    "    return Theta,Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = [False,True,True]\n",
    "Theta,Loss = sleep_update_delta(Theta,Alpha_Q,lr,n_dz,[a,b],activation_type,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.9634216 ,  3.9697228 ,  2.64197557,  1.4464744 , 13.02159436])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've finished the draft version of functions, which could be decomposed and investigated further in the future, but let's run a set of control experiments first and see how the model behaves comparing to the vanilla Helmholtz machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_formed_generate(n,value_set):\n",
    "    \"\"\"\n",
    "    Well-formedness rules:\n",
    "        1. Start with 1\n",
    "        2. Forbid 00100 (no 100, 001 on the boundary)\n",
    "        3. Forbid 0000\n",
    "        \n",
    "    Arguments:\n",
    "    n -- length of input layer (single data point)\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    \n",
    "    Returns:\n",
    "    well_formed_set -- a dataset obeys the well-formedness rules, numpy array of shape (n,n_data), n_data is the number of datapoints \n",
    "    in the generated dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    well_formed_set = np.zeros([1,n])\n",
    "    well_formed_set[0,0] = 1\n",
    "\n",
    "    for i in range(1,n):\n",
    "        for j in range(np.shape(well_formed_set)[0]):\n",
    "            if i == 2 and np.array_equal(well_formed_set[j,i-2:i], [1,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            elif i > 3 and np.array_equal(well_formed_set[j,i-3:i], [0,0,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            elif i > 3 and np.array_equal(well_formed_set[j,i-4:i], [0,0,1,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            else:\n",
    "                well_formed_set = np.append(well_formed_set, well_formed_set[j:j+1,:], axis=0)\n",
    "                well_formed_set[j,i] = 1\n",
    "\n",
    "    ind = np.array([], dtype=np.int8)\n",
    "    for i in range(well_formed_set.shape[0]):\n",
    "        if np.array_equal(well_formed_set[i,-3:], [0,0,1]):\n",
    "            ind = np.append(ind,i)\n",
    "\n",
    "    well_formed_set = np.delete(well_formed_set,ind,0)\n",
    "    well_formed_set = np.transpose(well_formed_set)\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    well_formed_set = well_formed_set*(a-b)+b\n",
    "    \n",
    "    return well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "value_set = [0,1]\n",
    "well_formed_set = well_formed_generate(n,value_set)\n",
    "well_formed_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generate(k,n,n_data,value_set):\n",
    "    \"\"\"\n",
    "    The dataset is generated in a favor of Bayesian mixure of Gaussians. Given k mixture Gaussian components, we sample their \n",
    "    means u_1...u_k uniformly from [0,1]. Then we randomly assign each neuron to one of the components, and sample from its \n",
    "    Gaussian distribution (u_k, sigma). sigma is a hyperparameter, we default it to 1.\n",
    "    \n",
    "    The \"Bayesian mixure of Gaussians\" generation is just a way to generate dataset with non-singular distributions. The \n",
    "    generated data distribution is not identified with the mixure of Gaussian distributions that generated it. In other words, \n",
    "    the data is treated as sole evidence without any prior on how it's been generated thus its reconstruction is not convolved \n",
    "    with it's generative distributions, which is a major difference from varietional inference.\n",
    "        \n",
    "    Arguments:\n",
    "    k -- number of Gaussian components\n",
    "    n -- length of input layer (single data point)\n",
    "    n_data -- number of datapoints to generate\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    \n",
    "    Returns:\n",
    "    random_set -- generated dataset, numpy array of shape (n,n_data), n_data is the number of datapoints in the generated dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    u = np.random.rand(k,)\n",
    "    c = np.random.randint(k, size=(n,1))\n",
    "    mean = u[c]\n",
    "    prob = np.random.randn(n,n_data) + mean\n",
    "    random_set = (prob>0.5).astype(int)\n",
    "    \n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    random_set = random_set *(a-b)+b\n",
    "    \n",
    "    return random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 500)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "n_data = 500\n",
    "random_set = random_generate(k,n,n_data,value_set)\n",
    "random_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_comb(n, value_set):\n",
    "    \"\"\"\n",
    "    All combinations of possible datapoints. 2^n\n",
    "        \n",
    "    Arguments:\n",
    "    n -- length of input layer (single data point)\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    \n",
    "    Returns:\n",
    "    entire_set -- a set containing all possible datapoints the input could be, numpy array of shape (n,2^n), \n",
    "    2^n is the number of all possible combinations of n binary neurons\n",
    "    \"\"\"\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]\n",
    "    \n",
    "    entire_set = np.zeros((2,n))\n",
    "    entire_set[0,0] = 1\n",
    "    for i in range(1,n):\n",
    "        for j in range(entire_set.shape[0]):\n",
    "            entire_set = np.append(entire_set, entire_set[j:j+1,:], axis=0)\n",
    "            entire_set[j,i] = 1\n",
    "    entire_set = entire_set*(a-b)+b\n",
    "    entire_set = np.transpose(entire_set)\n",
    "    \n",
    "    return entire_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_set = all_comb(n, value_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_all_comb(entire_set,dataset):\n",
    "    \"\"\"\n",
    "    This function reorders the entire set with respect to the generated (or given) dataset. Since we are dealing with \n",
    "    categorical distributions, to visualize the result better, we put the datapoints contained in the dataset \n",
    "    (let's say k datapoints) to the first k columns of the entire_set, which are followed by false instances in subsequent\n",
    "    columns.\n",
    "        \n",
    "    Arguments:\n",
    "    entire_set -- a set containing all possible datapoints the input could be, numpy array of shape (n,2^n), \n",
    "    2^n is the number of all possible combinations of n binary neurons\n",
    "    dataset -- generated (or given) dataset, numpy array of shape (n,n_data), n_data is the number of datapoints.\n",
    "    \n",
    "    Returns:\n",
    "    reordered_set -- entire_set reordered as columns 0-k represents valid instances contained in the dataset, \n",
    "    columns k-2^n represents false instances not in the dataset. numpy array of shape (n,2^n)\n",
    "    k -- number of distinct datapoints in dataset, integer\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = np.unique(dataset, axis=1)\n",
    "    reordered_set = np.zeros(entire_set.shape)\n",
    "    \n",
    "    k = dataset.shape[1]\n",
    "    reordered_set[:,:k] = dataset\n",
    "    r = k\n",
    "    for i in range(entire_set.shape[1]):\n",
    "        flag = 0\n",
    "        for j in range(k):\n",
    "            if np.array_equal(entire_set[:,i], dataset[:,j]):\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            reordered_set[:,r] = entire_set[:,i]\n",
    "            r += 1\n",
    "    return reordered_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = random_set\n",
    "reordered_set = reorder_all_comb(entire_set,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n_sample,n_dz,value_set,Theta,activation_type,bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_sample -- number of samples\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias,data bias], taking binary value in {True, False}. For example, \n",
    "    [False,True,True] means no instantiation bias but has MLP bias and data bias\n",
    "    \n",
    "    Returns:\n",
    "    generation -- generated instances after training, numpy array of shape (n,n_data), n is the length of input layer, \n",
    "    n_data is the number of datapoints generated\n",
    "    \"\"\"\n",
    "    generation = np.zeros((n_dz[0,0],n_sample))\n",
    "    for i in range(n_sample):\n",
    "        Alpha_P = sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        generation[:,i:i+1] = Alpha_P['z0']\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 5000\n",
    "generation = generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "generation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(generation,reordered_set,dataset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    generation -- generated instances after training, numpy array of shape (n,n_sample), n is the length of input layer, \n",
    "    n_sample is the number of datapoints generated\n",
    "    reordered_set -- entire_set reordered as columns 0-k represents valid instances contained in the dataset, \n",
    "    columns k-2^n represents false instances not in the dataset. numpy array of shape (n,2^n)\n",
    "    dataset -- numpy array of shape (n,n_data), n_data is the number of datapoints.\n",
    "    \n",
    "    Returns:\n",
    "    distribution -- assigned category for generated samples based on reordered set, numpy array of shape (n_sample, )\n",
    "    data_dist -- assigned category for dataset  based on reordered set, numpy array of shape (n_data, )\n",
    "    statistics -- python dictionary with keys:\n",
    "        percent -- percentage of positive instances\n",
    "        n_fn -- number of false negative samples, missing evidence\n",
    "        FN -- position of false negative samples, numpy array of shape (k-n_fn, )\n",
    "        n_fp -- number of false positive samples, outliers\n",
    "        FP -- position and counts of false positive samples, numpy array of shape (2,n_fp)\n",
    "    MSE -- mean squared error between the generation Q and the data evidence P on the support of P (on positive instances only).\n",
    "    \"\"\"\n",
    "    n_sample = generation.shape[1]\n",
    "    n_data = dataset.shape[1]\n",
    "    distribution = np.zeros((n_sample, ),dtype = int)\n",
    "    for i in range(n_sample):\n",
    "        for j in range(reordered_set.shape[1]):\n",
    "            if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "                distribution[i] = j\n",
    "                break\n",
    "    values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "    \n",
    "#     data_dist = np.zeros((n_data, ),dtype = int)\n",
    "#     for i in range(n_data):\n",
    "#         for j in range(reordered_set.shape[1]):\n",
    "#             if np.array_equal(dataset[:,i], reordered_set[:,j]):\n",
    "#                 data_dist[i] = j\n",
    "#                 break\n",
    "#     values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "    values_d, counts_d  = np.unique(dataset, axis = 1, return_counts=True)\n",
    "    k = counts_d.size\n",
    "    \n",
    "    \n",
    "    # statistics\n",
    "    percent = np.sum(counts_t[values_t < k])/n_sample\n",
    "    n_fn = k-values_t[values_t < k].size\n",
    "    FN = np.zeros((n_fn,),dtype = int)\n",
    "    dist_positive = np.array([values_t[values_t < k], counts_t[values_t < k]])\n",
    "    s = 0\n",
    "    values_t[values_t < k]\n",
    "    dist_values = np.append(np.append(-1,values_t[values_t < k]),k)   # append 0 and k in the range\n",
    "    \n",
    "    for i in range(dist_values.size-1):\n",
    "        diff = dist_values[i+1] - dist_values[i]\n",
    "        for j in range(1,diff):\n",
    "            FN[s] = dist_values[i]+j\n",
    "            dist_positive = np.append(dist_positive, np.array([[dist_values[i]+j],[0]]),axis = 1)\n",
    "            s += 1\n",
    "    dist_positive = np.unique(dist_positive,axis = 1)\n",
    "    n_fp = values_t[values_t >= k].size\n",
    "    FP = np.array([values_t[values_t >= k], counts_t[values_t >= k]])\n",
    "    statistics = {'percent': percent, 'FN': FN, 'n_fn':n_fn, 'FP': FP, 'n_fp':n_fp}\n",
    "    \n",
    "    # metric 2: distribution difference. Since our ditributions are discrete, we calculate a mean squared error (MSE) between \n",
    "    #           the generation Q and the data evidence P on the support of P (on positive instances only).\n",
    "    \n",
    "    counts_t = counts_t/n_sample*n_data  # distribution in the same scale as dataset\n",
    "    MSE = np.sum((dist_positive[1,:]/n_sample*n_data - counts_d)**2)/k\n",
    "    ABS_Error = np.abs(dist_positive[1,:]/n_sample*n_data - counts_d).sum()/k\n",
    "    \n",
    "    return distribution,statistics, MSE,ABS_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution,statistics, MSE,ABS_Error = metrics(generation,reordered_set,dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmholtz machine Control Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  9,  8,  7,  6,  5,  3,  2,  1]])"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[10,9,8,7,6,5,3,2,1]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9)"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla Helmholtz machine only contains instantiation layers thus $n_{dz}[i,\\centerdot] = 0$ for $i \\ge 1$ (counts from $0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, Theta = parameter_initialization(\"zero\",n_dz)  # \"zero\" or \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_set = [1,0] # vanilla Helmholtz machine takes value {0,1}\n",
    "activation_type = \"sigmoid\" # doesn't matter since vanilla Helmholtz machine doesn't have hidden MLP\n",
    "bias = [False,False,False] # vanilla Helmholtz machine has no bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 0., ..., 1., 1., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = n_dz[0,0]\n",
    "well_formed_set = well_formed_generate(n,value_set)\n",
    "well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       [1, 0, 1, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "n_data = 500\n",
    "random_set = random_generate(k,n,n_data,value_set)\n",
    "random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = well_formed_set # well_formed_set or random_set\n",
    "entire_set = all_comb(n, value_set)\n",
    "reordered_set = reorder_all_comb(entire_set,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.006\n",
    "epoch = 5000\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.76 1.06 0.52 0.45 0.06 0.16 0.03 0.   3.05] Loss_P: [2.3  1.8  1.61 0.51 0.43 0.05 0.18 0.02 6.89]\n",
      "Loss_Q: [0.84 1.03 0.52 0.51 0.04 0.17 0.03 0.   3.13] Loss_P: [2.34 1.75 1.6  0.64 0.48 0.05 0.18 0.03 7.07]\n",
      "Loss_Q: [0.8  1.09 0.5  0.51 0.06 0.15 0.02 0.   3.12] Loss_P: [2.35 1.73 1.56 0.58 0.52 0.05 0.16 0.02 6.95]\n",
      "Loss_Q: [0.85 1.06 0.48 0.49 0.07 0.17 0.02 0.   3.14] Loss_P: [2.35 1.79 1.58 0.57 0.52 0.05 0.16 0.02 7.05]\n",
      "Loss_Q: [0.83 1.09 0.56 0.47 0.05 0.15 0.03 0.   3.18] Loss_P: [2.31 1.75 1.56 0.58 0.51 0.06 0.16 0.02 6.95]\n",
      "Loss_Q: [0.8  1.05 0.55 0.49 0.05 0.14 0.03 0.   3.11] Loss_P: [2.34 1.68 1.54 0.59 0.5  0.06 0.15 0.04 6.9 ]\n",
      "Loss_Q: [0.79 1.04 0.58 0.47 0.04 0.19 0.02 0.   3.13] Loss_P: [2.25 1.83 1.54 0.61 0.51 0.06 0.18 0.02 7.  ]\n",
      "Loss_Q: [0.85 1.1  0.57 0.49 0.05 0.16 0.03 0.   3.25] Loss_P: [2.35 1.75 1.58 0.66 0.56 0.05 0.16 0.02 7.14]\n",
      "Loss_Q: [0.84 1.13 0.59 0.52 0.06 0.15 0.02 0.   3.32] Loss_P: [2.33 1.78 1.57 0.58 0.49 0.05 0.18 0.02 6.99]\n",
      "Loss_Q: [0.83 1.07 0.53 0.49 0.04 0.15 0.03 0.   3.15] Loss_P: [2.29 1.84 1.56 0.59 0.49 0.05 0.15 0.02 6.99]\n",
      "Loss_Q: [0.8  1.08 0.53 0.46 0.07 0.18 0.03 0.   3.14] Loss_P: [2.34 1.78 1.55 0.58 0.51 0.08 0.15 0.02 6.99]\n",
      "Loss_Q: [0.79 1.11 0.57 0.5  0.04 0.14 0.01 0.   3.16] Loss_P: [2.32 1.79 1.6  0.56 0.52 0.07 0.19 0.02 7.06]\n",
      "Loss_Q: [0.74 1.1  0.53 0.44 0.07 0.15 0.02 0.   3.06] Loss_P: [2.36 1.75 1.6  0.59 0.53 0.05 0.18 0.02 7.09]\n",
      "Loss_Q: [0.82 1.11 0.54 0.47 0.07 0.2  0.03 0.   3.24] Loss_P: [2.29 1.75 1.6  0.56 0.47 0.06 0.16 0.03 6.93]\n",
      "Loss_Q: [0.89 1.06 0.6  0.55 0.08 0.15 0.01 0.   3.34] Loss_P: [2.28 1.78 1.64 0.61 0.52 0.06 0.16 0.03 7.08]\n",
      "Loss_Q: [0.93 1.1  0.58 0.45 0.07 0.18 0.03 0.   3.34] Loss_P: [2.35 1.77 1.61 0.58 0.48 0.07 0.16 0.01 7.04]\n",
      "Loss_Q: [0.88 1.11 0.55 0.47 0.05 0.17 0.03 0.   3.28] Loss_P: [2.32 1.78 1.66 0.59 0.47 0.06 0.18 0.03 7.1 ]\n",
      "Loss_Q: [0.82 1.07 0.57 0.46 0.04 0.15 0.02 0.   3.12] Loss_P: [2.33 1.77 1.6  0.61 0.5  0.04 0.17 0.02 7.04]\n",
      "Loss_Q: [0.85 1.11 0.54 0.47 0.05 0.16 0.02 0.   3.2 ] Loss_P: [2.33 1.78 1.63 0.6  0.46 0.06 0.18 0.02 7.05]\n",
      "Loss_Q: [0.82 1.08 0.54 0.43 0.06 0.17 0.03 0.   3.12] Loss_P: [2.36 1.7  1.58 0.62 0.44 0.07 0.18 0.02 6.97]\n",
      "Loss_Q: [0.89 1.08 0.55 0.47 0.07 0.17 0.02 0.   3.25] Loss_P: [2.3  1.71 1.58 0.62 0.45 0.05 0.17 0.02 6.89]\n",
      "Loss_Q: [0.86 1.09 0.56 0.49 0.07 0.15 0.02 0.   3.24] Loss_P: [2.36 1.72 1.58 0.66 0.49 0.06 0.15 0.03 7.06]\n",
      "Loss_Q: [0.8  1.04 0.54 0.47 0.05 0.2  0.02 0.   3.12] Loss_P: [2.38 1.66 1.63 0.57 0.46 0.05 0.17 0.02 6.93]\n",
      "Loss_Q: [0.82 1.04 0.49 0.42 0.07 0.16 0.02 0.   3.02] Loss_P: [2.33 1.76 1.62 0.58 0.46 0.04 0.15 0.02 6.95]\n",
      "Loss_Q: [0.78 1.05 0.52 0.44 0.07 0.14 0.02 0.   3.03] Loss_P: [2.38 1.72 1.6  0.54 0.43 0.05 0.17 0.03 6.92]\n",
      "Loss_Q: [0.85 0.99 0.57 0.42 0.06 0.18 0.02 0.   3.1 ] Loss_P: [2.36 1.71 1.61 0.61 0.47 0.06 0.14 0.02 6.97]\n",
      "Loss_Q: [0.79 1.09 0.55 0.41 0.05 0.18 0.03 0.   3.1 ] Loss_P: [2.36 1.66 1.54 0.6  0.42 0.06 0.16 0.03 6.83]\n",
      "Loss_Q: [0.74 1.   0.54 0.48 0.05 0.17 0.03 0.   3.02] Loss_P: [2.31 1.68 1.56 0.57 0.46 0.05 0.17 0.03 6.82]\n",
      "Loss_Q: [0.79 1.03 0.55 0.45 0.06 0.17 0.02 0.   3.08] Loss_P: [2.31 1.67 1.58 0.53 0.47 0.06 0.18 0.03 6.84]\n",
      "Loss_Q: [0.86 1.06 0.5  0.46 0.06 0.17 0.02 0.   3.12] Loss_P: [2.33 1.72 1.66 0.54 0.44 0.05 0.16 0.04 6.93]\n",
      "Loss_Q: [0.87 1.05 0.55 0.49 0.06 0.16 0.01 0.   3.2 ] Loss_P: [2.34 1.71 1.64 0.57 0.51 0.07 0.18 0.02 7.04]\n",
      "Loss_Q: [0.88 1.09 0.54 0.54 0.07 0.16 0.03 0.   3.31] Loss_P: [2.34 1.67 1.67 0.54 0.49 0.06 0.13 0.02 6.92]\n",
      "Loss_Q: [0.8  1.05 0.51 0.49 0.04 0.16 0.02 0.   3.07] Loss_P: [2.32 1.72 1.65 0.58 0.5  0.07 0.15 0.02 7.02]\n",
      "Loss_Q: [0.83 1.11 0.52 0.51 0.05 0.15 0.02 0.   3.19] Loss_P: [2.3  1.7  1.67 0.56 0.42 0.05 0.17 0.02 6.9 ]\n",
      "Loss_Q: [0.75 1.1  0.5  0.46 0.07 0.16 0.02 0.   3.06] Loss_P: [2.31 1.69 1.63 0.54 0.47 0.07 0.16 0.02 6.89]\n",
      "Loss_Q: [0.87 1.08 0.5  0.46 0.06 0.16 0.02 0.   3.16] Loss_P: [2.38 1.7  1.66 0.52 0.42 0.05 0.16 0.02 6.91]\n",
      "Loss_Q: [0.85 1.07 0.46 0.43 0.04 0.15 0.02 0.   3.03] Loss_P: [2.31 1.78 1.65 0.49 0.49 0.06 0.15 0.02 6.95]\n",
      "Loss_Q: [0.84 1.08 0.46 0.44 0.07 0.19 0.02 0.   3.1 ] Loss_P: [2.34 1.81 1.62 0.52 0.52 0.05 0.14 0.02 7.02]\n",
      "Loss_Q: [0.85 1.09 0.43 0.46 0.05 0.18 0.02 0.   3.08] Loss_P: [2.34 1.72 1.61 0.57 0.52 0.06 0.16 0.02 6.99]\n",
      "Loss_Q: [0.78 1.06 0.52 0.51 0.07 0.15 0.02 0.   3.12] Loss_P: [2.33 1.69 1.68 0.49 0.52 0.05 0.15 0.02 6.93]\n",
      "Loss_Q: [0.83 1.05 0.54 0.51 0.06 0.16 0.02 0.   3.17] Loss_P: [2.31 1.72 1.64 0.54 0.54 0.07 0.15 0.03 7.  ]\n",
      "Loss_Q: [0.84 1.06 0.48 0.59 0.07 0.17 0.01 0.   3.22] Loss_P: [2.3  1.73 1.65 0.47 0.51 0.07 0.16 0.03 6.91]\n",
      "Loss_Q: [0.85 1.1  0.5  0.55 0.07 0.14 0.03 0.   3.24] Loss_P: [2.33 1.77 1.7  0.54 0.55 0.09 0.16 0.02 7.17]\n",
      "Loss_Q: [0.86 1.1  0.47 0.56 0.07 0.17 0.03 0.   3.25] Loss_P: [2.29 1.77 1.68 0.49 0.54 0.06 0.13 0.01 6.98]\n",
      "Loss_Q: [0.87 1.12 0.46 0.55 0.06 0.18 0.02 0.   3.25] Loss_P: [2.36 1.82 1.68 0.53 0.62 0.07 0.18 0.03 7.31]\n",
      "Loss_Q: [0.85 1.09 0.43 0.58 0.07 0.19 0.02 0.   3.23] Loss_P: [2.3  1.77 1.67 0.47 0.54 0.07 0.17 0.01 7.  ]\n",
      "Loss_Q: [0.82 1.1  0.45 0.62 0.06 0.17 0.02 0.   3.24] Loss_P: [2.33 1.78 1.73 0.5  0.59 0.06 0.16 0.02 7.17]\n",
      "Loss_Q: [0.89 1.09 0.51 0.59 0.05 0.17 0.03 0.   3.33] Loss_P: [2.32 1.77 1.68 0.45 0.57 0.07 0.18 0.01 7.05]\n",
      "Loss_Q: [0.84 1.13 0.46 0.62 0.06 0.2  0.02 0.   3.33] Loss_P: [2.3  1.79 1.64 0.49 0.59 0.06 0.16 0.04 7.08]\n",
      "Loss_Q: [0.87 1.08 0.48 0.58 0.06 0.2  0.03 0.   3.3 ] Loss_P: [2.29 1.79 1.66 0.45 0.62 0.09 0.2  0.03 7.13]\n",
      "Loss_Q: [0.76 1.08 0.51 0.6  0.06 0.17 0.02 0.   3.18] Loss_P: [2.3  1.79 1.66 0.46 0.61 0.08 0.19 0.02 7.13]\n",
      "Loss_Q: [0.89 1.1  0.46 0.57 0.08 0.17 0.03 0.   3.32] Loss_P: [2.28 1.82 1.63 0.45 0.63 0.06 0.17 0.03 7.08]\n",
      "Loss_Q: [0.82 1.06 0.43 0.6  0.06 0.17 0.02 0.   3.16] Loss_P: [2.29 1.87 1.63 0.49 0.64 0.07 0.17 0.03 7.18]\n",
      "Loss_Q: [0.83 1.1  0.46 0.64 0.07 0.18 0.03 0.   3.32] Loss_P: [2.35 1.81 1.65 0.48 0.6  0.06 0.16 0.02 7.13]\n",
      "Loss_Q: [0.84 1.08 0.41 0.54 0.05 0.2  0.03 0.   3.16] Loss_P: [2.23 1.87 1.63 0.43 0.52 0.07 0.17 0.02 6.95]\n",
      "Loss_Q: [0.84 1.08 0.43 0.56 0.06 0.23 0.02 0.   3.22] Loss_P: [2.35 1.82 1.62 0.44 0.62 0.07 0.21 0.02 7.15]\n",
      "Loss_Q: [0.84 1.07 0.46 0.56 0.06 0.2  0.02 0.   3.21] Loss_P: [2.26 1.83 1.61 0.45 0.63 0.07 0.17 0.02 7.03]\n",
      "Loss_Q: [0.86 1.08 0.45 0.51 0.06 0.17 0.04 0.   3.17] Loss_P: [2.33 1.81 1.63 0.45 0.58 0.08 0.21 0.02 7.11]\n",
      "Loss_Q: [0.84 1.12 0.45 0.59 0.09 0.21 0.03 0.   3.33] Loss_P: [2.37 1.8  1.69 0.46 0.58 0.06 0.19 0.02 7.17]\n",
      "Loss_Q: [0.83 1.01 0.38 0.55 0.09 0.24 0.04 0.   3.13] Loss_P: [2.3  1.76 1.59 0.53 0.57 0.08 0.2  0.02 7.05]\n",
      "Loss_Q: [0.82 1.01 0.45 0.56 0.06 0.25 0.03 0.   3.18] Loss_P: [2.33 1.75 1.61 0.43 0.6  0.07 0.22 0.04 7.05]\n",
      "Loss_Q: [0.82 1.07 0.38 0.54 0.08 0.22 0.02 0.   3.14] Loss_P: [2.3  1.75 1.66 0.44 0.6  0.06 0.24 0.04 7.09]\n",
      "Loss_Q: [0.83 1.06 0.38 0.51 0.07 0.21 0.02 0.   3.09] Loss_P: [2.37 1.78 1.58 0.43 0.57 0.07 0.21 0.03 7.05]\n",
      "Loss_Q: [0.83 1.   0.36 0.55 0.08 0.2  0.02 0.   3.06] Loss_P: [2.28 1.77 1.6  0.38 0.56 0.06 0.26 0.04 6.95]\n",
      "Loss_Q: [0.79 1.   0.37 0.58 0.06 0.25 0.03 0.   3.08] Loss_P: [2.28 1.76 1.54 0.4  0.58 0.06 0.23 0.02 6.88]\n",
      "Loss_Q: [0.77 1.05 0.34 0.55 0.07 0.22 0.02 0.   3.01] Loss_P: [2.36 1.76 1.51 0.39 0.54 0.07 0.23 0.02 6.89]\n",
      "Loss_Q: [0.83 1.07 0.39 0.52 0.09 0.24 0.04 0.   3.17] Loss_P: [2.3  1.79 1.59 0.42 0.57 0.07 0.24 0.02 7.  ]\n",
      "Loss_Q: [0.84 1.07 0.41 0.51 0.07 0.21 0.03 0.   3.13] Loss_P: [2.24 1.86 1.56 0.39 0.57 0.06 0.24 0.02 6.95]\n",
      "Loss_Q: [0.8  1.03 0.38 0.53 0.07 0.27 0.03 0.   3.11] Loss_P: [2.27 1.88 1.61 0.48 0.58 0.06 0.21 0.03 7.11]\n",
      "Loss_Q: [0.81 1.11 0.33 0.54 0.04 0.24 0.04 0.   3.12] Loss_P: [2.27 1.88 1.67 0.45 0.57 0.07 0.26 0.04 7.21]\n",
      "Loss_Q: [0.83 1.08 0.36 0.54 0.07 0.25 0.03 0.   3.17] Loss_P: [2.29 1.78 1.68 0.47 0.61 0.08 0.27 0.03 7.21]\n",
      "Loss_Q: [0.85 1.03 0.4  0.57 0.07 0.26 0.03 0.   3.2 ] Loss_P: [2.26 1.82 1.62 0.38 0.56 0.07 0.22 0.02 6.96]\n",
      "Loss_Q: [0.81 1.05 0.43 0.59 0.06 0.23 0.02 0.   3.18] Loss_P: [2.29 1.79 1.58 0.42 0.66 0.08 0.24 0.01 7.07]\n",
      "Loss_Q: [0.85 1.   0.45 0.6  0.08 0.25 0.04 0.   3.27] Loss_P: [2.29 1.81 1.59 0.42 0.58 0.07 0.22 0.02 7.  ]\n",
      "Loss_Q: [0.85 1.05 0.41 0.55 0.08 0.23 0.01 0.   3.17] Loss_P: [2.3  1.75 1.56 0.45 0.55 0.07 0.24 0.03 6.96]\n",
      "Loss_Q: [0.75 1.09 0.4  0.58 0.07 0.26 0.03 0.   3.16] Loss_P: [2.26 1.88 1.63 0.4  0.58 0.08 0.26 0.04 7.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.78 1.05 0.36 0.59 0.07 0.29 0.02 0.   3.14] Loss_P: [2.35 1.72 1.61 0.41 0.59 0.07 0.26 0.03 7.04]\n",
      "Loss_Q: [0.79 1.06 0.37 0.56 0.06 0.29 0.04 0.   3.17] Loss_P: [2.34 1.7  1.59 0.41 0.59 0.07 0.29 0.04 7.02]\n",
      "Loss_Q: [0.85 1.07 0.39 0.53 0.06 0.25 0.03 0.   3.17] Loss_P: [2.32 1.76 1.57 0.43 0.62 0.08 0.29 0.03 7.09]\n",
      "Loss_Q: [0.79 1.08 0.38 0.59 0.06 0.29 0.05 0.   3.23] Loss_P: [2.32 1.79 1.63 0.43 0.58 0.07 0.27 0.02 7.1 ]\n",
      "Loss_Q: [0.73 1.05 0.39 0.67 0.06 0.26 0.03 0.   3.19] Loss_P: [2.35 1.72 1.57 0.41 0.63 0.07 0.26 0.02 7.04]\n",
      "Loss_Q: [0.81 1.1  0.37 0.66 0.08 0.25 0.02 0.   3.29] Loss_P: [2.29 1.78 1.57 0.36 0.59 0.06 0.25 0.02 6.93]\n",
      "Loss_Q: [0.84 1.03 0.37 0.66 0.06 0.24 0.03 0.   3.23] Loss_P: [2.25 1.82 1.62 0.38 0.69 0.06 0.25 0.03 7.11]\n",
      "Loss_Q: [0.76 1.08 0.37 0.65 0.06 0.24 0.02 0.   3.17] Loss_P: [2.35 1.7  1.59 0.35 0.66 0.07 0.24 0.02 6.98]\n",
      "Loss_Q: [0.8  1.08 0.37 0.69 0.07 0.22 0.03 0.   3.27] Loss_P: [2.31 1.76 1.54 0.39 0.72 0.08 0.24 0.03 7.06]\n",
      "Loss_Q: [0.83 0.93 0.37 0.67 0.07 0.24 0.02 0.   3.12] Loss_P: [2.34 1.66 1.58 0.37 0.65 0.07 0.25 0.04 6.97]\n",
      "Loss_Q: [0.8  1.06 0.33 0.63 0.07 0.24 0.03 0.   3.18] Loss_P: [2.33 1.72 1.54 0.38 0.61 0.05 0.25 0.01 6.9 ]\n",
      "Loss_Q: [0.78 0.95 0.36 0.7  0.07 0.25 0.02 0.   3.14] Loss_P: [2.32 1.76 1.57 0.35 0.66 0.06 0.27 0.03 7.03]\n",
      "Loss_Q: [0.75 0.95 0.38 0.68 0.08 0.29 0.03 0.   3.15] Loss_P: [2.33 1.72 1.47 0.41 0.71 0.07 0.28 0.02 7.01]\n",
      "Loss_Q: [0.84 0.97 0.33 0.65 0.07 0.27 0.02 0.   3.15] Loss_P: [2.31 1.72 1.49 0.38 0.67 0.07 0.24 0.03 6.93]\n",
      "Loss_Q: [0.83 1.01 0.37 0.67 0.05 0.26 0.02 0.   3.21] Loss_P: [2.36 1.67 1.56 0.43 0.67 0.07 0.27 0.02 7.04]\n",
      "Loss_Q: [0.73 1.01 0.37 0.65 0.08 0.26 0.02 0.   3.12] Loss_P: [2.36 1.68 1.47 0.4  0.65 0.09 0.26 0.03 6.94]\n",
      "Loss_Q: [0.75 0.94 0.31 0.63 0.07 0.24 0.02 0.   2.95] Loss_P: [2.29 1.74 1.5  0.39 0.63 0.05 0.24 0.03 6.88]\n",
      "Loss_Q: [0.83 0.94 0.37 0.7  0.07 0.27 0.02 0.   3.2 ] Loss_P: [2.34 1.73 1.53 0.34 0.6  0.06 0.27 0.02 6.89]\n",
      "Loss_Q: [0.87 0.96 0.35 0.62 0.06 0.26 0.02 0.   3.14] Loss_P: [2.28 1.75 1.47 0.35 0.66 0.06 0.25 0.02 6.84]\n",
      "Loss_Q: [0.82 0.96 0.32 0.62 0.09 0.28 0.02 0.   3.11] Loss_P: [2.3  1.75 1.51 0.37 0.63 0.08 0.27 0.03 6.94]\n",
      "Loss_Q: [0.87 0.96 0.32 0.58 0.05 0.28 0.03 0.   3.09] Loss_P: [2.32 1.74 1.53 0.33 0.63 0.07 0.27 0.03 6.92]\n",
      "Loss_Q: [0.75 0.9  0.29 0.57 0.06 0.26 0.03 0.   2.87] Loss_P: [2.3  1.75 1.46 0.32 0.61 0.07 0.29 0.03 6.82]\n",
      "Loss_Q: [0.79 0.97 0.32 0.59 0.07 0.29 0.02 0.   3.06] Loss_P: [2.35 1.69 1.49 0.38 0.58 0.06 0.3  0.02 6.86]\n",
      "Loss_Q: [0.88 0.91 0.35 0.58 0.06 0.31 0.02 0.   3.1 ] Loss_P: [2.32 1.74 1.49 0.36 0.62 0.07 0.3  0.03 6.92]\n",
      "Loss_Q: [0.75 0.94 0.3  0.53 0.06 0.29 0.04 0.   2.91] Loss_P: [2.38 1.71 1.5  0.36 0.58 0.07 0.26 0.04 6.91]\n",
      "Loss_Q: [0.83 0.91 0.32 0.57 0.06 0.3  0.03 0.   3.01] Loss_P: [2.44 1.67 1.48 0.3  0.54 0.07 0.27 0.03 6.79]\n",
      "Loss_Q: [0.82 0.95 0.33 0.57 0.08 0.3  0.02 0.   3.07] Loss_P: [2.33 1.7  1.47 0.32 0.58 0.07 0.31 0.04 6.82]\n",
      "Loss_Q: [0.81 1.   0.33 0.6  0.08 0.32 0.02 0.   3.17] Loss_P: [2.35 1.71 1.5  0.32 0.56 0.07 0.31 0.04 6.86]\n",
      "Loss_Q: [0.82 0.94 0.29 0.59 0.06 0.3  0.03 0.   3.03] Loss_P: [2.33 1.75 1.49 0.36 0.62 0.06 0.31 0.04 6.96]\n",
      "Loss_Q: [0.78 0.96 0.31 0.58 0.07 0.29 0.02 0.   3.01] Loss_P: [2.37 1.75 1.56 0.31 0.55 0.06 0.29 0.02 6.9 ]\n",
      "Loss_Q: [0.79 0.94 0.31 0.55 0.07 0.34 0.02 0.   3.03] Loss_P: [2.34 1.72 1.56 0.34 0.55 0.07 0.33 0.03 6.93]\n",
      "Loss_Q: [0.84 0.98 0.32 0.53 0.08 0.34 0.02 0.   3.11] Loss_P: [2.27 1.77 1.58 0.36 0.54 0.06 0.32 0.03 6.91]\n",
      "Loss_Q: [0.86 1.02 0.3  0.56 0.07 0.32 0.02 0.   3.15] Loss_P: [2.34 1.74 1.58 0.32 0.56 0.07 0.33 0.02 6.96]\n",
      "Loss_Q: [0.8  1.04 0.34 0.59 0.07 0.3  0.02 0.   3.15] Loss_P: [2.38 1.77 1.58 0.32 0.55 0.06 0.32 0.03 7.  ]\n",
      "Loss_Q: [0.93 1.05 0.33 0.53 0.05 0.29 0.02 0.   3.18] Loss_P: [2.32 1.8  1.52 0.35 0.52 0.07 0.31 0.02 6.91]\n",
      "Loss_Q: [0.95 1.   0.32 0.47 0.06 0.28 0.02 0.   3.11] Loss_P: [2.32 1.79 1.59 0.33 0.58 0.08 0.3  0.04 7.02]\n",
      "Loss_Q: [0.88 1.02 0.32 0.55 0.08 0.31 0.03 0.   3.19] Loss_P: [2.33 1.75 1.62 0.37 0.6  0.07 0.32 0.01 7.07]\n",
      "Loss_Q: [0.9  1.06 0.32 0.56 0.07 0.31 0.02 0.   3.23] Loss_P: [2.33 1.81 1.64 0.32 0.54 0.06 0.28 0.02 7.01]\n",
      "Loss_Q: [0.83 1.05 0.31 0.51 0.07 0.26 0.02 0.   3.05] Loss_P: [2.23 1.87 1.61 0.33 0.53 0.06 0.28 0.02 6.93]\n",
      "Loss_Q: [0.92 1.06 0.33 0.55 0.06 0.26 0.02 0.   3.19] Loss_P: [2.26 1.83 1.58 0.34 0.58 0.07 0.28 0.03 6.96]\n",
      "Loss_Q: [0.89 1.   0.34 0.55 0.07 0.27 0.03 0.   3.14] Loss_P: [2.29 1.81 1.62 0.35 0.59 0.08 0.27 0.01 7.01]\n",
      "Loss_Q: [0.9  1.   0.29 0.48 0.07 0.28 0.03 0.   3.05] Loss_P: [2.32 1.84 1.62 0.31 0.54 0.06 0.29 0.02 7.01]\n",
      "Loss_Q: [0.92 1.09 0.35 0.55 0.07 0.23 0.02 0.   3.23] Loss_P: [2.32 1.79 1.63 0.33 0.53 0.08 0.27 0.03 6.98]\n",
      "Loss_Q: [0.84 1.04 0.31 0.48 0.07 0.26 0.03 0.   3.04] Loss_P: [2.3  1.81 1.64 0.39 0.53 0.06 0.27 0.02 7.03]\n",
      "Loss_Q: [0.93 1.09 0.36 0.56 0.05 0.26 0.03 0.   3.28] Loss_P: [2.3  1.81 1.62 0.38 0.54 0.05 0.26 0.02 6.97]\n",
      "Loss_Q: [0.9  1.08 0.31 0.5  0.04 0.28 0.03 0.   3.14] Loss_P: [2.26 1.8  1.63 0.39 0.5  0.06 0.24 0.02 6.91]\n",
      "Loss_Q: [0.89 1.01 0.35 0.48 0.08 0.29 0.03 0.   3.13] Loss_P: [2.27 1.8  1.66 0.4  0.48 0.06 0.27 0.03 6.97]\n",
      "Loss_Q: [0.83 0.99 0.38 0.5  0.06 0.25 0.04 0.   3.05] Loss_P: [2.31 1.71 1.64 0.36 0.49 0.07 0.26 0.01 6.86]\n",
      "Loss_Q: [0.91 1.   0.34 0.47 0.05 0.29 0.03 0.   3.1 ] Loss_P: [2.28 1.78 1.63 0.38 0.46 0.06 0.26 0.02 6.89]\n",
      "Loss_Q: [0.9  1.09 0.38 0.5  0.08 0.26 0.04 0.   3.26] Loss_P: [2.31 1.71 1.53 0.37 0.48 0.06 0.31 0.02 6.79]\n",
      "Loss_Q: [0.85 0.94 0.36 0.45 0.06 0.29 0.03 0.   2.96] Loss_P: [2.34 1.7  1.62 0.39 0.46 0.05 0.28 0.04 6.88]\n",
      "Loss_Q: [0.91 1.05 0.34 0.44 0.07 0.3  0.03 0.   3.16] Loss_P: [2.36 1.75 1.62 0.36 0.45 0.05 0.31 0.03 6.93]\n",
      "Loss_Q: [0.8  1.01 0.39 0.41 0.06 0.26 0.04 0.   2.97] Loss_P: [2.32 1.68 1.61 0.34 0.41 0.07 0.27 0.02 6.72]\n",
      "Loss_Q: [0.9  1.03 0.33 0.38 0.07 0.28 0.02 0.   3.  ] Loss_P: [2.3  1.77 1.54 0.34 0.42 0.04 0.27 0.02 6.71]\n",
      "Loss_Q: [0.78 0.96 0.31 0.44 0.07 0.28 0.04 0.   2.89] Loss_P: [2.37 1.7  1.52 0.35 0.43 0.05 0.3  0.02 6.76]\n",
      "Loss_Q: [0.85 0.98 0.31 0.41 0.06 0.3  0.02 0.   2.94] Loss_P: [2.31 1.79 1.57 0.34 0.37 0.06 0.29 0.02 6.75]\n",
      "Loss_Q: [0.82 1.   0.34 0.39 0.08 0.29 0.01 0.   2.93] Loss_P: [2.31 1.72 1.59 0.36 0.43 0.05 0.3  0.03 6.79]\n",
      "Loss_Q: [0.88 1.05 0.33 0.41 0.06 0.32 0.04 0.   3.08] Loss_P: [2.32 1.78 1.63 0.36 0.42 0.05 0.3  0.04 6.9 ]\n",
      "Loss_Q: [0.87 1.13 0.32 0.41 0.06 0.31 0.02 0.   3.12] Loss_P: [2.36 1.73 1.68 0.37 0.45 0.05 0.27 0.04 6.95]\n",
      "Loss_Q: [0.85 1.05 0.34 0.44 0.08 0.29 0.03 0.   3.08] Loss_P: [2.3  1.75 1.66 0.33 0.44 0.08 0.27 0.03 6.87]\n",
      "Loss_Q: [0.85 1.05 0.35 0.44 0.06 0.28 0.02 0.   3.05] Loss_P: [2.3  1.75 1.66 0.4  0.44 0.08 0.3  0.02 6.95]\n",
      "Loss_Q: [0.83 1.08 0.27 0.36 0.06 0.26 0.02 0.   2.89] Loss_P: [2.34 1.71 1.69 0.34 0.45 0.07 0.33 0.02 6.95]\n",
      "Loss_Q: [0.87 1.06 0.29 0.37 0.06 0.28 0.02 0.   2.95] Loss_P: [2.37 1.7  1.67 0.36 0.4  0.07 0.29 0.02 6.88]\n",
      "Loss_Q: [0.84 1.1  0.33 0.34 0.07 0.28 0.02 0.   2.98] Loss_P: [2.31 1.73 1.65 0.38 0.43 0.05 0.3  0.02 6.87]\n",
      "Loss_Q: [0.78 1.07 0.32 0.38 0.06 0.29 0.03 0.   2.93] Loss_P: [2.33 1.71 1.72 0.31 0.39 0.03 0.27 0.02 6.79]\n",
      "Loss_Q: [0.91 1.06 0.31 0.38 0.07 0.27 0.03 0.   3.04] Loss_P: [2.31 1.7  1.68 0.33 0.39 0.04 0.29 0.03 6.77]\n",
      "Loss_Q: [0.85 1.1  0.37 0.36 0.08 0.32 0.03 0.   3.11] Loss_P: [2.31 1.73 1.69 0.34 0.39 0.06 0.26 0.02 6.82]\n",
      "Loss_Q: [0.8  1.09 0.34 0.4  0.09 0.31 0.03 0.   3.06] Loss_P: [2.3  1.75 1.72 0.37 0.42 0.08 0.26 0.03 6.93]\n",
      "Loss_Q: [0.84 1.02 0.37 0.43 0.05 0.3  0.02 0.   3.04] Loss_P: [2.29 1.81 1.66 0.35 0.36 0.08 0.29 0.03 6.87]\n",
      "Loss_Q: [0.81 1.1  0.36 0.4  0.05 0.28 0.03 0.   3.02] Loss_P: [2.32 1.81 1.67 0.42 0.4  0.07 0.3  0.03 7.03]\n",
      "Loss_Q: [0.87 1.09 0.36 0.36 0.07 0.26 0.03 0.   3.02] Loss_P: [2.32 1.74 1.66 0.39 0.4  0.05 0.28 0.02 6.86]\n",
      "Loss_Q: [0.91 1.1  0.4  0.39 0.08 0.29 0.03 0.   3.19] Loss_P: [2.3  1.73 1.69 0.36 0.37 0.06 0.26 0.04 6.81]\n",
      "Loss_Q: [0.93 1.05 0.37 0.35 0.07 0.28 0.02 0.   3.07] Loss_P: [2.3  1.81 1.71 0.38 0.38 0.07 0.28 0.04 6.98]\n",
      "Loss_Q: [0.78 1.13 0.33 0.37 0.08 0.27 0.02 0.   2.97] Loss_P: [2.35 1.73 1.74 0.43 0.39 0.05 0.27 0.02 6.98]\n",
      "Loss_Q: [0.89 1.11 0.36 0.4  0.06 0.3  0.02 0.   3.14] Loss_P: [2.37 1.67 1.65 0.37 0.4  0.06 0.3  0.02 6.85]\n",
      "Loss_Q: [0.89 1.05 0.38 0.36 0.07 0.27 0.02 0.   3.05] Loss_P: [2.35 1.71 1.66 0.37 0.34 0.06 0.28 0.04 6.8 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.83 1.   0.36 0.39 0.05 0.28 0.03 0.   2.93] Loss_P: [2.26 1.78 1.68 0.38 0.4  0.06 0.27 0.02 6.85]\n",
      "Loss_Q: [0.82 1.06 0.34 0.39 0.07 0.27 0.03 0.   2.99] Loss_P: [2.3  1.71 1.64 0.44 0.37 0.05 0.25 0.02 6.77]\n",
      "Loss_Q: [0.88 1.05 0.34 0.35 0.05 0.28 0.02 0.   2.97] Loss_P: [2.29 1.7  1.63 0.37 0.38 0.06 0.26 0.01 6.71]\n",
      "Loss_Q: [0.83 1.05 0.35 0.36 0.06 0.26 0.03 0.   2.93] Loss_P: [2.28 1.87 1.65 0.34 0.39 0.05 0.27 0.03 6.88]\n",
      "Loss_Q: [0.82 1.1  0.34 0.4  0.07 0.27 0.02 0.   3.03] Loss_P: [2.27 1.81 1.59 0.45 0.42 0.07 0.3  0.04 6.93]\n",
      "Loss_Q: [0.9  1.07 0.4  0.38 0.05 0.27 0.02 0.   3.09] Loss_P: [2.32 1.77 1.67 0.41 0.39 0.06 0.27 0.02 6.91]\n",
      "Loss_Q: [0.81 1.08 0.33 0.42 0.07 0.27 0.02 0.   3.01] Loss_P: [2.25 1.76 1.63 0.4  0.44 0.08 0.28 0.02 6.86]\n",
      "Loss_Q: [0.86 1.05 0.38 0.42 0.07 0.26 0.02 0.   3.05] Loss_P: [2.25 1.77 1.59 0.4  0.42 0.06 0.27 0.02 6.79]\n",
      "Loss_Q: [0.99 1.1  0.41 0.44 0.07 0.24 0.02 0.   3.27] Loss_P: [2.26 1.83 1.69 0.35 0.4  0.06 0.27 0.02 6.89]\n",
      "Loss_Q: [0.85 1.05 0.35 0.4  0.06 0.24 0.02 0.   2.97] Loss_P: [2.27 1.78 1.64 0.45 0.41 0.05 0.25 0.02 6.87]\n",
      "Loss_Q: [0.89 1.13 0.39 0.46 0.06 0.26 0.02 0.   3.21] Loss_P: [2.24 1.78 1.75 0.4  0.41 0.06 0.28 0.03 6.95]\n",
      "Loss_Q: [0.86 1.07 0.34 0.41 0.05 0.25 0.03 0.   3.01] Loss_P: [2.27 1.74 1.66 0.46 0.42 0.06 0.26 0.02 6.9 ]\n",
      "Loss_Q: [0.81 1.11 0.37 0.36 0.04 0.3  0.03 0.   3.02] Loss_P: [2.29 1.87 1.74 0.39 0.44 0.06 0.31 0.02 7.12]\n",
      "Loss_Q: [0.85 1.08 0.37 0.45 0.06 0.27 0.02 0.   3.1 ] Loss_P: [2.3  1.72 1.73 0.38 0.38 0.05 0.26 0.02 6.84]\n",
      "Loss_Q: [0.9  1.07 0.35 0.42 0.07 0.26 0.01 0.   3.07] Loss_P: [2.32 1.82 1.7  0.38 0.43 0.06 0.29 0.02 7.02]\n",
      "Loss_Q: [1.02 1.14 0.43 0.44 0.04 0.25 0.02 0.   3.34] Loss_P: [2.32 1.8  1.76 0.43 0.47 0.07 0.27 0.02 7.14]\n",
      "Loss_Q: [0.87 1.12 0.36 0.4  0.05 0.27 0.03 0.   3.11] Loss_P: [2.27 1.82 1.79 0.42 0.44 0.06 0.29 0.03 7.12]\n",
      "Loss_Q: [0.93 1.06 0.39 0.43 0.05 0.28 0.03 0.   3.16] Loss_P: [2.34 1.81 1.73 0.39 0.41 0.03 0.25 0.02 6.99]\n",
      "Loss_Q: [0.88 1.08 0.38 0.47 0.08 0.27 0.03 0.   3.18] Loss_P: [2.37 1.73 1.68 0.36 0.45 0.06 0.27 0.03 6.96]\n",
      "Loss_Q: [0.82 1.11 0.35 0.37 0.04 0.24 0.03 0.   2.95] Loss_P: [2.36 1.69 1.63 0.39 0.44 0.06 0.28 0.02 6.88]\n",
      "Loss_Q: [0.85 1.06 0.38 0.44 0.05 0.3  0.03 0.   3.11] Loss_P: [2.32 1.77 1.69 0.42 0.42 0.05 0.3  0.03 7.01]\n",
      "Loss_Q: [0.83 1.06 0.36 0.42 0.04 0.25 0.03 0.   3.  ] Loss_P: [2.36 1.65 1.64 0.36 0.45 0.06 0.26 0.03 6.82]\n",
      "Loss_Q: [0.85 1.04 0.39 0.41 0.03 0.28 0.02 0.   3.02] Loss_P: [2.35 1.7  1.62 0.38 0.45 0.05 0.26 0.02 6.83]\n",
      "Loss_Q: [0.89 1.05 0.38 0.43 0.06 0.28 0.03 0.   3.12] Loss_P: [2.34 1.73 1.59 0.46 0.49 0.06 0.28 0.02 6.97]\n",
      "Loss_Q: [0.97 1.06 0.39 0.46 0.05 0.24 0.03 0.   3.18] Loss_P: [2.29 1.73 1.68 0.42 0.44 0.06 0.28 0.02 6.93]\n",
      "Loss_Q: [0.88 1.1  0.4  0.44 0.05 0.28 0.02 0.   3.16] Loss_P: [2.34 1.71 1.72 0.41 0.5  0.06 0.25 0.03 7.02]\n",
      "Loss_Q: [0.88 1.1  0.39 0.46 0.06 0.26 0.03 0.   3.18] Loss_P: [2.33 1.73 1.72 0.51 0.46 0.07 0.26 0.03 7.12]\n",
      "Loss_Q: [0.81 1.06 0.4  0.48 0.04 0.29 0.04 0.   3.12] Loss_P: [2.34 1.77 1.7  0.45 0.45 0.06 0.25 0.02 7.05]\n",
      "Loss_Q: [0.91 1.07 0.38 0.4  0.07 0.26 0.02 0.   3.11] Loss_P: [2.31 1.71 1.74 0.42 0.47 0.05 0.27 0.05 7.01]\n",
      "Loss_Q: [0.95 1.07 0.39 0.47 0.07 0.25 0.03 0.   3.23] Loss_P: [2.35 1.7  1.72 0.4  0.49 0.05 0.25 0.02 6.99]\n",
      "Loss_Q: [0.85 1.07 0.36 0.42 0.06 0.25 0.03 0.   3.04] Loss_P: [2.28 1.75 1.65 0.42 0.44 0.06 0.26 0.03 6.88]\n",
      "Loss_Q: [0.94 1.11 0.35 0.42 0.07 0.25 0.02 0.   3.15] Loss_P: [2.32 1.7  1.73 0.43 0.44 0.08 0.26 0.02 6.98]\n",
      "Loss_Q: [0.84 1.09 0.35 0.44 0.05 0.25 0.02 0.   3.05] Loss_P: [2.32 1.75 1.63 0.38 0.44 0.08 0.26 0.02 6.87]\n",
      "Loss_Q: [0.88 1.07 0.35 0.43 0.06 0.27 0.02 0.   3.08] Loss_P: [2.31 1.7  1.66 0.41 0.4  0.04 0.28 0.03 6.81]\n",
      "Loss_Q: [0.95 1.02 0.3  0.5  0.06 0.24 0.03 0.   3.11] Loss_P: [2.33 1.8  1.64 0.43 0.49 0.06 0.26 0.03 7.04]\n",
      "Loss_Q: [0.99 1.14 0.38 0.53 0.05 0.27 0.03 0.   3.38] Loss_P: [2.34 1.78 1.71 0.44 0.54 0.07 0.24 0.02 7.14]\n",
      "Loss_Q: [0.87 1.07 0.37 0.51 0.05 0.22 0.02 0.   3.1 ] Loss_P: [2.32 1.78 1.73 0.42 0.52 0.05 0.22 0.02 7.06]\n",
      "Loss_Q: [0.9  1.08 0.35 0.53 0.05 0.22 0.01 0.   3.14] Loss_P: [2.4  1.67 1.67 0.42 0.51 0.04 0.22 0.02 6.96]\n",
      "Loss_Q: [0.83 1.1  0.36 0.52 0.06 0.21 0.03 0.   3.1 ] Loss_P: [2.36 1.68 1.62 0.39 0.52 0.08 0.23 0.02 6.91]\n",
      "Loss_Q: [0.9  1.02 0.39 0.46 0.08 0.24 0.02 0.   3.12] Loss_P: [2.36 1.73 1.68 0.45 0.54 0.05 0.21 0.01 7.03]\n",
      "Loss_Q: [0.85 1.07 0.4  0.55 0.05 0.21 0.02 0.   3.16] Loss_P: [2.31 1.75 1.69 0.44 0.48 0.05 0.19 0.01 6.91]\n",
      "Loss_Q: [0.84 1.07 0.4  0.53 0.07 0.19 0.01 0.   3.12] Loss_P: [2.36 1.7  1.69 0.41 0.54 0.07 0.26 0.03 7.06]\n",
      "Loss_Q: [0.9  1.08 0.43 0.51 0.05 0.24 0.02 0.   3.23] Loss_P: [2.39 1.73 1.72 0.44 0.48 0.04 0.23 0.02 7.05]\n",
      "Loss_Q: [0.85 1.08 0.41 0.51 0.06 0.27 0.03 0.   3.2 ] Loss_P: [2.32 1.73 1.68 0.43 0.51 0.06 0.22 0.02 6.97]\n",
      "Loss_Q: [0.91 1.02 0.39 0.49 0.07 0.26 0.02 0.   3.16] Loss_P: [2.35 1.73 1.64 0.48 0.52 0.06 0.22 0.02 7.01]\n",
      "Loss_Q: [0.88 1.06 0.36 0.46 0.04 0.23 0.02 0.   3.04] Loss_P: [2.41 1.73 1.68 0.42 0.5  0.06 0.21 0.03 7.05]\n",
      "Loss_Q: [0.89 1.12 0.38 0.5  0.05 0.22 0.02 0.   3.18] Loss_P: [2.41 1.71 1.69 0.44 0.52 0.06 0.22 0.02 7.07]\n",
      "Loss_Q: [0.92 1.07 0.4  0.46 0.05 0.24 0.02 0.   3.16] Loss_P: [2.35 1.65 1.65 0.44 0.5  0.07 0.25 0.03 6.93]\n",
      "Loss_Q: [0.89 1.11 0.34 0.48 0.08 0.21 0.03 0.   3.14] Loss_P: [2.34 1.66 1.69 0.46 0.49 0.05 0.23 0.03 6.94]\n",
      "Loss_Q: [0.87 1.13 0.37 0.51 0.05 0.26 0.02 0.   3.21] Loss_P: [2.38 1.7  1.68 0.43 0.5  0.06 0.22 0.02 7.  ]\n",
      "Loss_Q: [0.94 1.17 0.39 0.46 0.06 0.22 0.03 0.   3.27] Loss_P: [2.36 1.74 1.75 0.44 0.49 0.06 0.23 0.03 7.1 ]\n",
      "Loss_Q: [0.87 1.06 0.4  0.45 0.05 0.25 0.02 0.   3.1 ] Loss_P: [2.39 1.74 1.7  0.44 0.48 0.08 0.23 0.02 7.07]\n",
      "Loss_Q: [0.86 1.1  0.39 0.48 0.07 0.22 0.02 0.   3.13] Loss_P: [2.32 1.8  1.7  0.45 0.47 0.05 0.22 0.02 7.02]\n",
      "Loss_Q: [0.86 1.1  0.4  0.44 0.08 0.21 0.03 0.   3.12] Loss_P: [2.33 1.69 1.74 0.44 0.5  0.06 0.21 0.02 7.01]\n",
      "Loss_Q: [0.88 1.16 0.38 0.46 0.06 0.21 0.02 0.   3.17] Loss_P: [2.34 1.76 1.73 0.48 0.47 0.08 0.22 0.02 7.09]\n",
      "Loss_Q: [0.95 1.14 0.4  0.42 0.05 0.19 0.03 0.   3.18] Loss_P: [2.36 1.75 1.73 0.47 0.47 0.06 0.21 0.02 7.06]\n",
      "Loss_Q: [0.96 1.16 0.45 0.44 0.07 0.21 0.03 0.   3.33] Loss_P: [2.34 1.8  1.73 0.45 0.45 0.07 0.18 0.02 7.04]\n",
      "Loss_Q: [0.89 1.14 0.44 0.46 0.06 0.2  0.03 0.   3.21] Loss_P: [2.3  1.71 1.75 0.47 0.44 0.05 0.2  0.03 6.95]\n",
      "Loss_Q: [0.99 1.09 0.45 0.48 0.07 0.19 0.03 0.   3.29] Loss_P: [2.38 1.8  1.71 0.49 0.5  0.05 0.18 0.03 7.13]\n",
      "Loss_Q: [0.87 1.16 0.43 0.45 0.07 0.2  0.03 0.   3.2 ] Loss_P: [2.3  1.75 1.72 0.49 0.47 0.05 0.17 0.02 6.97]\n",
      "Loss_Q: [0.97 1.08 0.43 0.45 0.06 0.19 0.01 0.   3.2 ] Loss_P: [2.34 1.82 1.74 0.47 0.47 0.05 0.16 0.02 7.06]\n",
      "Loss_Q: [0.94 1.11 0.47 0.41 0.06 0.16 0.02 0.   3.18] Loss_P: [2.33 1.75 1.71 0.52 0.43 0.05 0.18 0.02 6.99]\n",
      "Loss_Q: [0.87 1.05 0.44 0.4  0.05 0.19 0.03 0.   3.02] Loss_P: [2.28 1.77 1.67 0.5  0.42 0.07 0.2  0.02 6.93]\n",
      "Loss_Q: [0.88 1.07 0.45 0.42 0.06 0.18 0.02 0.   3.07] Loss_P: [2.32 1.79 1.74 0.48 0.45 0.07 0.19 0.02 7.05]\n",
      "Loss_Q: [0.84 1.17 0.41 0.43 0.04 0.18 0.02 0.   3.09] Loss_P: [2.38 1.72 1.68 0.46 0.38 0.07 0.2  0.02 6.91]\n",
      "Loss_Q: [0.94 1.14 0.46 0.4  0.04 0.18 0.02 0.   3.18] Loss_P: [2.39 1.76 1.75 0.44 0.39 0.05 0.18 0.02 6.98]\n",
      "Loss_Q: [0.91 1.09 0.41 0.44 0.07 0.18 0.03 0.   3.14] Loss_P: [2.33 1.77 1.72 0.47 0.46 0.06 0.19 0.02 7.  ]\n",
      "Loss_Q: [0.95 1.11 0.43 0.49 0.08 0.19 0.03 0.   3.29] Loss_P: [2.37 1.75 1.69 0.43 0.44 0.04 0.18 0.04 6.92]\n",
      "Loss_Q: [0.86 1.12 0.39 0.44 0.06 0.17 0.01 0.   3.06] Loss_P: [2.35 1.7  1.72 0.47 0.5  0.05 0.19 0.02 7.  ]\n",
      "Loss_Q: [0.83 1.14 0.43 0.42 0.06 0.17 0.03 0.   3.07] Loss_P: [2.35 1.75 1.68 0.5  0.46 0.06 0.19 0.02 7.01]\n",
      "Loss_Q: [0.89 1.15 0.44 0.44 0.07 0.2  0.03 0.   3.2 ] Loss_P: [2.38 1.73 1.72 0.45 0.42 0.05 0.17 0.01 6.93]\n",
      "Loss_Q: [0.92 1.1  0.43 0.46 0.05 0.18 0.03 0.   3.17] Loss_P: [2.36 1.77 1.68 0.52 0.5  0.07 0.18 0.02 7.11]\n",
      "Loss_Q: [0.87 1.15 0.39 0.46 0.04 0.2  0.03 0.   3.13] Loss_P: [2.37 1.75 1.68 0.48 0.46 0.05 0.2  0.03 7.02]\n",
      "Loss_Q: [0.96 1.06 0.4  0.43 0.05 0.21 0.03 0.   3.14] Loss_P: [2.34 1.7  1.62 0.48 0.47 0.06 0.23 0.04 6.94]\n",
      "Loss_Q: [0.89 1.07 0.44 0.41 0.04 0.23 0.02 0.   3.11] Loss_P: [2.37 1.72 1.67 0.43 0.46 0.05 0.22 0.02 6.94]\n",
      "Loss_Q: [0.91 1.07 0.4  0.45 0.05 0.21 0.02 0.   3.11] Loss_P: [2.38 1.7  1.61 0.48 0.43 0.05 0.21 0.03 6.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.91 1.1  0.41 0.42 0.06 0.2  0.02 0.   3.12] Loss_P: [2.39 1.69 1.6  0.48 0.42 0.07 0.2  0.05 6.9 ]\n",
      "Loss_Q: [0.87 1.09 0.42 0.43 0.06 0.19 0.02 0.   3.08] Loss_P: [2.36 1.78 1.64 0.46 0.43 0.05 0.19 0.01 6.93]\n",
      "Loss_Q: [0.93 1.1  0.41 0.47 0.06 0.16 0.02 0.   3.16] Loss_P: [2.38 1.73 1.61 0.45 0.52 0.06 0.19 0.02 6.96]\n",
      "Loss_Q: [0.9  1.06 0.42 0.43 0.05 0.19 0.03 0.   3.08] Loss_P: [2.37 1.77 1.63 0.46 0.45 0.06 0.2  0.02 6.98]\n",
      "Loss_Q: [0.87 1.04 0.45 0.4  0.07 0.18 0.03 0.   3.04] Loss_P: [2.4  1.69 1.62 0.47 0.48 0.06 0.18 0.03 6.93]\n",
      "Loss_Q: [0.83 1.08 0.47 0.5  0.08 0.21 0.02 0.   3.19] Loss_P: [2.35 1.72 1.73 0.45 0.45 0.06 0.17 0.01 6.95]\n",
      "Loss_Q: [0.92 1.11 0.43 0.44 0.08 0.18 0.02 0.   3.18] Loss_P: [2.34 1.74 1.66 0.44 0.51 0.07 0.21 0.02 6.98]\n",
      "Loss_Q: [0.86 1.08 0.44 0.46 0.08 0.2  0.02 0.   3.15] Loss_P: [2.3  1.81 1.68 0.49 0.42 0.05 0.19 0.01 6.95]\n",
      "Loss_Q: [0.9  1.1  0.41 0.46 0.05 0.18 0.02 0.   3.12] Loss_P: [2.3  1.72 1.73 0.46 0.42 0.07 0.16 0.02 6.87]\n",
      "Loss_Q: [0.95 1.14 0.41 0.45 0.05 0.21 0.02 0.   3.23] Loss_P: [2.33 1.77 1.66 0.45 0.49 0.07 0.18 0.02 6.96]\n",
      "Loss_Q: [0.97 1.11 0.48 0.42 0.06 0.18 0.02 0.   3.25] Loss_P: [2.34 1.85 1.7  0.55 0.43 0.05 0.18 0.02 7.12]\n",
      "Loss_Q: [0.83 1.03 0.45 0.42 0.06 0.19 0.03 0.   3.01] Loss_P: [2.33 1.83 1.67 0.45 0.4  0.05 0.15 0.03 6.91]\n",
      "Loss_Q: [0.94 1.12 0.42 0.49 0.04 0.16 0.04 0.   3.22] Loss_P: [2.37 1.81 1.7  0.45 0.48 0.07 0.19 0.02 7.08]\n",
      "Loss_Q: [0.88 1.11 0.44 0.48 0.09 0.17 0.02 0.   3.19] Loss_P: [2.35 1.74 1.71 0.51 0.48 0.05 0.17 0.03 7.03]\n",
      "Loss_Q: [0.88 1.09 0.39 0.42 0.06 0.17 0.02 0.   3.02] Loss_P: [2.35 1.78 1.68 0.46 0.51 0.07 0.16 0.03 7.04]\n",
      "Loss_Q: [0.92 1.08 0.43 0.49 0.07 0.2  0.02 0.   3.22] Loss_P: [2.31 1.72 1.61 0.42 0.46 0.05 0.17 0.02 6.76]\n",
      "Loss_Q: [0.87 1.04 0.36 0.44 0.07 0.17 0.03 0.   2.98] Loss_P: [2.33 1.88 1.6  0.41 0.47 0.07 0.19 0.03 6.97]\n",
      "Loss_Q: [0.94 1.06 0.4  0.45 0.06 0.15 0.02 0.   3.1 ] Loss_P: [2.34 1.76 1.65 0.44 0.47 0.06 0.18 0.02 6.91]\n",
      "Loss_Q: [0.93 1.04 0.36 0.44 0.07 0.16 0.03 0.   3.04] Loss_P: [2.34 1.79 1.63 0.41 0.5  0.07 0.19 0.04 6.98]\n",
      "Loss_Q: [0.9  1.07 0.37 0.48 0.04 0.17 0.03 0.   3.07] Loss_P: [2.43 1.78 1.61 0.39 0.52 0.07 0.16 0.04 6.99]\n",
      "Loss_Q: [0.94 1.06 0.36 0.43 0.06 0.18 0.03 0.   3.06] Loss_P: [2.32 1.83 1.72 0.37 0.47 0.05 0.15 0.01 6.92]\n",
      "Loss_Q: [0.96 1.16 0.35 0.46 0.06 0.16 0.03 0.   3.18] Loss_P: [2.39 1.79 1.72 0.42 0.47 0.06 0.16 0.02 7.03]\n",
      "Loss_Q: [0.92 1.14 0.35 0.5  0.08 0.18 0.02 0.   3.18] Loss_P: [2.34 1.83 1.74 0.42 0.47 0.07 0.16 0.02 7.06]\n",
      "Loss_Q: [0.98 1.04 0.32 0.48 0.07 0.18 0.02 0.   3.09] Loss_P: [2.29 1.87 1.74 0.47 0.53 0.05 0.18 0.02 7.15]\n",
      "Loss_Q: [0.85 1.1  0.36 0.46 0.07 0.19 0.02 0.   3.05] Loss_P: [2.37 1.77 1.73 0.4  0.47 0.05 0.17 0.03 7.  ]\n",
      "Loss_Q: [1.02 1.1  0.35 0.47 0.09 0.17 0.03 0.   3.24] Loss_P: [2.34 1.83 1.64 0.41 0.43 0.05 0.18 0.04 6.92]\n",
      "Loss_Q: [0.92 1.1  0.34 0.44 0.07 0.17 0.03 0.   3.08] Loss_P: [2.31 1.85 1.66 0.37 0.44 0.04 0.17 0.04 6.88]\n",
      "Loss_Q: [0.87 1.14 0.35 0.43 0.05 0.19 0.02 0.   3.05] Loss_P: [2.36 1.84 1.67 0.37 0.42 0.05 0.18 0.03 6.92]\n",
      "Loss_Q: [0.97 1.05 0.34 0.42 0.05 0.17 0.02 0.   3.02] Loss_P: [2.31 1.8  1.68 0.38 0.45 0.05 0.15 0.02 6.84]\n",
      "Loss_Q: [0.98 1.12 0.42 0.48 0.06 0.19 0.02 0.   3.28] Loss_P: [2.29 1.89 1.62 0.41 0.48 0.07 0.18 0.03 6.97]\n",
      "Loss_Q: [0.93 1.05 0.37 0.41 0.06 0.18 0.03 0.   3.03] Loss_P: [2.3  1.87 1.66 0.42 0.47 0.06 0.21 0.04 7.02]\n",
      "Loss_Q: [0.91 1.08 0.41 0.43 0.04 0.18 0.02 0.   3.08] Loss_P: [2.39 1.8  1.65 0.44 0.49 0.07 0.18 0.02 7.03]\n",
      "Loss_Q: [0.85 1.09 0.38 0.46 0.06 0.24 0.02 0.   3.1 ] Loss_P: [2.37 1.84 1.61 0.4  0.45 0.07 0.2  0.02 6.96]\n",
      "Loss_Q: [0.95 1.11 0.46 0.47 0.05 0.17 0.01 0.   3.21] Loss_P: [2.33 1.82 1.57 0.37 0.45 0.08 0.18 0.02 6.81]\n",
      "Loss_Q: [0.88 1.1  0.37 0.49 0.06 0.2  0.02 0.   3.11] Loss_P: [2.36 1.84 1.53 0.42 0.45 0.08 0.2  0.04 6.91]\n",
      "Loss_Q: [0.86 1.07 0.34 0.43 0.07 0.19 0.02 0.   2.98] Loss_P: [2.38 1.84 1.58 0.38 0.46 0.07 0.19 0.02 6.92]\n",
      "Loss_Q: [0.87 1.08 0.39 0.49 0.06 0.21 0.02 0.   3.12] Loss_P: [2.31 1.85 1.63 0.39 0.48 0.04 0.21 0.03 6.95]\n",
      "Loss_Q: [0.87 1.07 0.38 0.45 0.07 0.2  0.03 0.   3.07] Loss_P: [2.35 1.78 1.55 0.4  0.49 0.06 0.17 0.02 6.82]\n",
      "Loss_Q: [0.89 1.06 0.38 0.5  0.05 0.18 0.03 0.   3.09] Loss_P: [2.38 1.79 1.52 0.36 0.47 0.07 0.19 0.02 6.8 ]\n",
      "Loss_Q: [0.84 1.   0.36 0.44 0.07 0.17 0.03 0.   2.91] Loss_P: [2.36 1.8  1.5  0.39 0.41 0.05 0.15 0.01 6.68]\n",
      "Loss_Q: [0.87 1.02 0.36 0.43 0.06 0.21 0.03 0.   2.98] Loss_P: [2.37 1.81 1.43 0.41 0.45 0.05 0.18 0.01 6.72]\n",
      "Loss_Q: [0.85 1.   0.38 0.45 0.06 0.2  0.02 0.   2.97] Loss_P: [2.37 1.83 1.51 0.42 0.41 0.07 0.23 0.02 6.86]\n",
      "Loss_Q: [0.82 0.99 0.39 0.45 0.08 0.21 0.02 0.   2.95] Loss_P: [2.42 1.79 1.44 0.38 0.47 0.08 0.18 0.02 6.78]\n",
      "Loss_Q: [0.83 0.91 0.35 0.39 0.04 0.22 0.02 0.   2.76] Loss_P: [2.38 1.81 1.48 0.39 0.4  0.06 0.2  0.02 6.76]\n",
      "Loss_Q: [0.84 1.08 0.35 0.4  0.05 0.19 0.03 0.   2.93] Loss_P: [2.39 1.78 1.5  0.34 0.42 0.07 0.22 0.04 6.77]\n",
      "Loss_Q: [0.85 0.98 0.34 0.42 0.07 0.16 0.01 0.   2.84] Loss_P: [2.39 1.83 1.52 0.42 0.42 0.06 0.18 0.02 6.82]\n",
      "Loss_Q: [0.84 1.02 0.36 0.41 0.06 0.19 0.02 0.   2.9 ] Loss_P: [2.41 1.73 1.47 0.36 0.42 0.07 0.19 0.02 6.67]\n",
      "Loss_Q: [0.9  0.96 0.35 0.42 0.06 0.2  0.02 0.   2.9 ] Loss_P: [2.43 1.78 1.45 0.37 0.41 0.07 0.19 0.02 6.72]\n",
      "Loss_Q: [0.86 1.   0.36 0.47 0.07 0.17 0.02 0.   2.95] Loss_P: [2.35 1.8  1.51 0.39 0.43 0.07 0.2  0.02 6.78]\n",
      "Loss_Q: [0.87 1.07 0.4  0.47 0.06 0.19 0.03 0.   3.08] Loss_P: [2.4  1.75 1.48 0.38 0.45 0.07 0.19 0.03 6.77]\n",
      "Loss_Q: [0.83 0.97 0.41 0.46 0.07 0.15 0.02 0.   2.91] Loss_P: [2.36 1.8  1.44 0.42 0.42 0.06 0.22 0.02 6.74]\n",
      "Loss_Q: [0.85 0.99 0.36 0.41 0.06 0.19 0.03 0.   2.89] Loss_P: [2.38 1.74 1.45 0.38 0.46 0.04 0.19 0.02 6.66]\n",
      "Loss_Q: [0.83 0.97 0.41 0.43 0.06 0.22 0.03 0.   2.95] Loss_P: [2.43 1.75 1.5  0.45 0.48 0.08 0.19 0.03 6.91]\n",
      "Loss_Q: [0.81 0.98 0.35 0.39 0.05 0.18 0.02 0.   2.78] Loss_P: [2.46 1.74 1.49 0.43 0.5  0.07 0.23 0.03 6.95]\n",
      "Loss_Q: [0.81 1.   0.43 0.41 0.07 0.2  0.02 0.   2.95] Loss_P: [2.39 1.75 1.49 0.41 0.41 0.08 0.22 0.01 6.75]\n",
      "Loss_Q: [0.88 1.01 0.38 0.41 0.05 0.19 0.02 0.   2.94] Loss_P: [2.31 1.79 1.51 0.39 0.43 0.07 0.19 0.02 6.73]\n",
      "Loss_Q: [0.9  1.01 0.36 0.46 0.07 0.18 0.02 0.   2.99] Loss_P: [2.39 1.71 1.51 0.44 0.45 0.05 0.21 0.02 6.78]\n",
      "Loss_Q: [0.91 1.01 0.43 0.42 0.08 0.2  0.03 0.   3.07] Loss_P: [2.31 1.81 1.52 0.43 0.44 0.05 0.21 0.03 6.79]\n",
      "Loss_Q: [0.83 1.04 0.41 0.46 0.06 0.18 0.01 0.   3.  ] Loss_P: [2.36 1.82 1.54 0.44 0.4  0.05 0.19 0.02 6.82]\n",
      "Loss_Q: [0.86 1.03 0.44 0.48 0.06 0.13 0.02 0.   3.03] Loss_P: [2.35 1.81 1.49 0.42 0.39 0.07 0.19 0.02 6.73]\n",
      "Loss_Q: [0.86 0.94 0.42 0.45 0.06 0.19 0.02 0.   2.94] Loss_P: [2.41 1.77 1.54 0.48 0.4  0.07 0.17 0.01 6.84]\n",
      "Loss_Q: [0.89 1.04 0.48 0.46 0.07 0.2  0.03 0.   3.18] Loss_P: [2.41 1.69 1.51 0.48 0.44 0.07 0.19 0.02 6.82]\n",
      "Loss_Q: [0.84 0.94 0.37 0.38 0.08 0.16 0.02 0.   2.79] Loss_P: [2.39 1.71 1.52 0.45 0.44 0.08 0.2  0.03 6.83]\n",
      "Loss_Q: [0.8  1.   0.37 0.43 0.08 0.2  0.04 0.   2.91] Loss_P: [2.39 1.71 1.53 0.44 0.48 0.06 0.19 0.03 6.82]\n",
      "Loss_Q: [0.85 1.05 0.41 0.45 0.07 0.18 0.03 0.   3.02] Loss_P: [2.36 1.74 1.54 0.45 0.51 0.06 0.16 0.02 6.84]\n",
      "Loss_Q: [0.82 0.99 0.4  0.48 0.06 0.17 0.03 0.   2.94] Loss_P: [2.38 1.74 1.48 0.44 0.5  0.08 0.15 0.03 6.78]\n",
      "Loss_Q: [0.89 0.97 0.41 0.48 0.05 0.17 0.02 0.   2.98] Loss_P: [2.31 1.71 1.58 0.49 0.49 0.06 0.15 0.01 6.8 ]\n",
      "Loss_Q: [0.77 1.02 0.45 0.51 0.06 0.16 0.02 0.   2.98] Loss_P: [2.4  1.77 1.52 0.49 0.47 0.05 0.15 0.02 6.87]\n",
      "Loss_Q: [0.87 1.04 0.47 0.48 0.08 0.17 0.03 0.   3.14] Loss_P: [2.42 1.73 1.54 0.51 0.51 0.05 0.15 0.02 6.94]\n",
      "Loss_Q: [0.84 0.96 0.36 0.5  0.05 0.16 0.02 0.   2.91] Loss_P: [2.38 1.77 1.51 0.5  0.55 0.06 0.16 0.03 6.95]\n",
      "Loss_Q: [0.9  1.03 0.44 0.52 0.08 0.19 0.02 0.   3.19] Loss_P: [2.39 1.7  1.49 0.47 0.5  0.07 0.17 0.02 6.81]\n",
      "Loss_Q: [0.83 1.   0.42 0.57 0.07 0.15 0.04 0.   3.08] Loss_P: [2.42 1.74 1.51 0.5  0.52 0.06 0.18 0.04 6.97]\n",
      "Loss_Q: [0.88 0.98 0.45 0.58 0.05 0.17 0.02 0.   3.13] Loss_P: [2.36 1.75 1.57 0.47 0.55 0.08 0.18 0.02 6.98]\n",
      "Loss_Q: [0.82 0.96 0.43 0.52 0.06 0.18 0.02 0.   2.99] Loss_P: [2.44 1.72 1.56 0.47 0.51 0.06 0.19 0.02 6.98]\n",
      "Loss_Q: [0.84 0.99 0.42 0.52 0.08 0.21 0.02 0.   3.08] Loss_P: [2.42 1.69 1.56 0.5  0.54 0.08 0.19 0.03 7.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.82 1.   0.41 0.48 0.05 0.21 0.03 0.   3.  ] Loss_P: [2.41 1.7  1.6  0.52 0.52 0.06 0.17 0.03 7.  ]\n",
      "Loss_Q: [0.88 1.04 0.44 0.58 0.08 0.19 0.02 0.   3.22] Loss_P: [2.36 1.67 1.57 0.5  0.55 0.09 0.2  0.03 6.98]\n",
      "Loss_Q: [0.77 1.04 0.42 0.5  0.05 0.2  0.04 0.   3.02] Loss_P: [2.37 1.72 1.64 0.49 0.55 0.09 0.22 0.02 7.11]\n",
      "Loss_Q: [0.8  1.07 0.39 0.6  0.06 0.24 0.02 0.   3.19] Loss_P: [2.39 1.72 1.57 0.47 0.6  0.06 0.19 0.03 7.02]\n",
      "Loss_Q: [0.81 1.09 0.45 0.59 0.07 0.23 0.03 0.   3.27] Loss_P: [2.39 1.7  1.62 0.51 0.59 0.06 0.2  0.02 7.09]\n",
      "Loss_Q: [0.84 1.12 0.4  0.58 0.08 0.22 0.04 0.   3.27] Loss_P: [2.34 1.73 1.56 0.43 0.6  0.08 0.22 0.03 7.  ]\n",
      "Loss_Q: [0.78 1.11 0.45 0.58 0.06 0.22 0.02 0.   3.22] Loss_P: [2.42 1.66 1.62 0.42 0.58 0.06 0.21 0.02 6.99]\n",
      "Loss_Q: [0.81 1.1  0.39 0.54 0.05 0.24 0.04 0.   3.18] Loss_P: [2.39 1.71 1.62 0.48 0.53 0.06 0.23 0.02 7.05]\n",
      "Loss_Q: [0.88 1.06 0.44 0.49 0.06 0.19 0.02 0.   3.15] Loss_P: [2.37 1.79 1.59 0.49 0.57 0.06 0.21 0.02 7.11]\n",
      "Loss_Q: [0.85 1.07 0.42 0.56 0.06 0.2  0.02 0.   3.17] Loss_P: [2.41 1.67 1.61 0.51 0.57 0.04 0.22 0.01 7.05]\n",
      "Loss_Q: [0.86 1.11 0.42 0.58 0.04 0.22 0.02 0.   3.25] Loss_P: [2.39 1.73 1.63 0.5  0.54 0.07 0.24 0.02 7.11]\n",
      "Loss_Q: [0.84 1.13 0.45 0.55 0.07 0.26 0.04 0.   3.34] Loss_P: [2.43 1.68 1.64 0.52 0.5  0.07 0.24 0.02 7.09]\n",
      "Loss_Q: [0.86 1.08 0.43 0.58 0.07 0.21 0.03 0.   3.25] Loss_P: [2.44 1.7  1.59 0.49 0.51 0.05 0.23 0.03 7.04]\n",
      "Loss_Q: [0.83 1.   0.39 0.45 0.06 0.24 0.03 0.   3.  ] Loss_P: [2.39 1.69 1.56 0.52 0.51 0.08 0.23 0.03 7.02]\n",
      "Loss_Q: [0.82 1.04 0.4  0.53 0.07 0.26 0.04 0.   3.16] Loss_P: [2.38 1.71 1.64 0.48 0.5  0.09 0.26 0.03 7.08]\n",
      "Loss_Q: [0.83 1.09 0.42 0.48 0.07 0.27 0.04 0.   3.19] Loss_P: [2.38 1.72 1.61 0.47 0.48 0.07 0.24 0.02 6.99]\n",
      "Loss_Q: [0.86 1.1  0.41 0.46 0.06 0.24 0.02 0.   3.14] Loss_P: [2.38 1.74 1.56 0.48 0.52 0.08 0.25 0.03 7.05]\n",
      "Loss_Q: [0.82 1.06 0.4  0.45 0.07 0.23 0.02 0.   3.04] Loss_P: [2.37 1.75 1.61 0.45 0.44 0.06 0.24 0.03 6.94]\n",
      "Loss_Q: [0.85 1.06 0.4  0.46 0.06 0.25 0.05 0.   3.12] Loss_P: [2.41 1.7  1.62 0.45 0.49 0.06 0.23 0.02 6.98]\n",
      "Loss_Q: [0.87 1.07 0.38 0.43 0.05 0.24 0.02 0.   3.05] Loss_P: [2.34 1.7  1.62 0.42 0.41 0.05 0.23 0.03 6.81]\n",
      "Loss_Q: [0.9  1.03 0.38 0.43 0.05 0.23 0.03 0.   3.05] Loss_P: [2.4  1.73 1.62 0.38 0.51 0.06 0.26 0.02 6.98]\n",
      "Loss_Q: [0.82 1.07 0.41 0.44 0.05 0.26 0.03 0.   3.07] Loss_P: [2.39 1.66 1.69 0.46 0.39 0.06 0.25 0.03 6.94]\n",
      "Loss_Q: [0.89 1.03 0.37 0.44 0.07 0.23 0.01 0.   3.05] Loss_P: [2.43 1.67 1.59 0.47 0.4  0.04 0.22 0.02 6.85]\n",
      "Loss_Q: [0.86 0.94 0.39 0.51 0.05 0.24 0.02 0.   3.  ] Loss_P: [2.41 1.67 1.62 0.42 0.47 0.07 0.22 0.03 6.9 ]\n",
      "Loss_Q: [0.9  1.05 0.43 0.47 0.06 0.24 0.02 0.   3.17] Loss_P: [2.4  1.7  1.57 0.45 0.47 0.06 0.25 0.03 6.94]\n",
      "Loss_Q: [0.86 0.99 0.39 0.46 0.05 0.25 0.02 0.   3.02] Loss_P: [2.34 1.73 1.6  0.44 0.49 0.06 0.24 0.02 6.92]\n",
      "Loss_Q: [0.84 0.98 0.42 0.49 0.06 0.24 0.04 0.   3.08] Loss_P: [2.35 1.75 1.57 0.44 0.44 0.05 0.26 0.03 6.89]\n",
      "Loss_Q: [0.88 1.03 0.43 0.48 0.04 0.3  0.03 0.   3.19] Loss_P: [2.39 1.72 1.62 0.54 0.48 0.05 0.24 0.02 7.06]\n",
      "Loss_Q: [0.81 0.99 0.47 0.51 0.05 0.27 0.02 0.   3.12] Loss_P: [2.35 1.74 1.61 0.49 0.52 0.06 0.26 0.03 7.07]\n",
      "Loss_Q: [0.86 1.04 0.42 0.54 0.07 0.28 0.03 0.   3.23] Loss_P: [2.35 1.74 1.65 0.43 0.57 0.08 0.27 0.02 7.11]\n",
      "Loss_Q: [0.82 1.02 0.39 0.6  0.06 0.26 0.02 0.   3.19] Loss_P: [2.38 1.71 1.65 0.46 0.57 0.05 0.26 0.02 7.11]\n",
      "Loss_Q: [0.83 1.04 0.41 0.56 0.06 0.25 0.02 0.   3.17] Loss_P: [2.4  1.71 1.65 0.44 0.61 0.04 0.28 0.03 7.19]\n",
      "Loss_Q: [0.94 1.09 0.39 0.55 0.05 0.26 0.02 0.   3.29] Loss_P: [2.42 1.71 1.65 0.45 0.58 0.07 0.28 0.03 7.19]\n",
      "Loss_Q: [0.83 1.1  0.39 0.59 0.06 0.27 0.02 0.   3.26] Loss_P: [2.37 1.65 1.7  0.44 0.59 0.06 0.28 0.02 7.11]\n",
      "Loss_Q: [0.87 1.05 0.45 0.56 0.06 0.29 0.02 0.   3.3 ] Loss_P: [2.36 1.69 1.61 0.46 0.57 0.06 0.29 0.04 7.09]\n",
      "Loss_Q: [0.81 1.07 0.35 0.52 0.04 0.28 0.03 0.   3.11] Loss_P: [2.36 1.72 1.59 0.49 0.6  0.08 0.29 0.02 7.16]\n",
      "Loss_Q: [0.85 1.06 0.39 0.56 0.05 0.3  0.03 0.   3.25] Loss_P: [2.36 1.68 1.63 0.45 0.53 0.04 0.25 0.02 6.96]\n",
      "Loss_Q: [0.88 1.05 0.34 0.52 0.05 0.29 0.03 0.   3.16] Loss_P: [2.38 1.69 1.57 0.42 0.57 0.08 0.29 0.03 7.03]\n",
      "Loss_Q: [0.82 1.   0.35 0.56 0.07 0.28 0.02 0.   3.12] Loss_P: [2.34 1.66 1.62 0.52 0.53 0.06 0.29 0.04 7.06]\n",
      "Loss_Q: [0.92 1.   0.37 0.52 0.05 0.25 0.02 0.   3.14] Loss_P: [2.35 1.68 1.57 0.48 0.56 0.06 0.3  0.02 7.04]\n",
      "Loss_Q: [0.81 1.04 0.36 0.52 0.05 0.26 0.02 0.   3.06] Loss_P: [2.36 1.7  1.61 0.45 0.54 0.06 0.28 0.02 7.02]\n",
      "Loss_Q: [0.88 1.05 0.36 0.58 0.05 0.25 0.02 0.   3.2 ] Loss_P: [2.35 1.8  1.68 0.44 0.6  0.07 0.25 0.03 7.23]\n",
      "Loss_Q: [0.82 1.06 0.4  0.58 0.03 0.24 0.03 0.   3.16] Loss_P: [2.38 1.73 1.68 0.42 0.53 0.06 0.25 0.03 7.08]\n",
      "Loss_Q: [0.88 1.04 0.43 0.54 0.06 0.25 0.02 0.   3.22] Loss_P: [2.4  1.73 1.65 0.42 0.59 0.05 0.24 0.02 7.12]\n",
      "Loss_Q: [0.86 1.03 0.42 0.53 0.06 0.27 0.01 0.   3.19] Loss_P: [2.36 1.72 1.65 0.41 0.55 0.06 0.23 0.02 7.  ]\n",
      "Loss_Q: [0.87 1.02 0.38 0.56 0.06 0.24 0.02 0.   3.15] Loss_P: [2.38 1.68 1.65 0.41 0.54 0.07 0.25 0.02 6.99]\n",
      "Loss_Q: [0.86 1.11 0.41 0.5  0.06 0.25 0.02 0.   3.21] Loss_P: [2.42 1.72 1.66 0.42 0.53 0.06 0.24 0.02 7.06]\n",
      "Loss_Q: [0.88 1.04 0.4  0.54 0.05 0.25 0.03 0.   3.19] Loss_P: [2.4  1.69 1.59 0.5  0.48 0.08 0.24 0.03 7.02]\n",
      "Loss_Q: [0.88 1.01 0.4  0.53 0.06 0.22 0.03 0.   3.13] Loss_P: [2.33 1.7  1.62 0.4  0.5  0.06 0.22 0.02 6.85]\n",
      "Loss_Q: [0.88 1.04 0.37 0.49 0.08 0.25 0.02 0.   3.12] Loss_P: [2.36 1.72 1.56 0.45 0.51 0.05 0.25 0.01 6.91]\n",
      "Loss_Q: [0.89 1.01 0.4  0.52 0.05 0.23 0.03 0.   3.12] Loss_P: [2.39 1.65 1.59 0.5  0.5  0.06 0.26 0.02 6.98]\n",
      "Loss_Q: [0.8  1.03 0.37 0.52 0.06 0.28 0.01 0.   3.07] Loss_P: [2.41 1.69 1.61 0.46 0.53 0.04 0.26 0.02 7.03]\n",
      "Loss_Q: [0.9  1.04 0.42 0.51 0.04 0.26 0.02 0.   3.2 ] Loss_P: [2.36 1.66 1.6  0.42 0.51 0.05 0.28 0.02 6.9 ]\n",
      "Loss_Q: [0.88 1.05 0.4  0.52 0.06 0.31 0.03 0.   3.25] Loss_P: [2.41 1.72 1.59 0.43 0.56 0.06 0.29 0.02 7.08]\n",
      "Loss_Q: [0.87 0.99 0.36 0.53 0.04 0.29 0.02 0.   3.1 ] Loss_P: [2.41 1.71 1.58 0.43 0.55 0.06 0.29 0.02 7.04]\n",
      "Loss_Q: [0.87 1.01 0.4  0.57 0.04 0.25 0.03 0.   3.17] Loss_P: [2.39 1.7  1.52 0.44 0.52 0.06 0.28 0.02 6.93]\n",
      "Loss_Q: [0.93 1.01 0.42 0.56 0.05 0.26 0.02 0.   3.26] Loss_P: [2.4  1.65 1.59 0.45 0.5  0.06 0.28 0.02 6.94]\n",
      "Loss_Q: [0.88 0.99 0.37 0.55 0.06 0.27 0.03 0.   3.15] Loss_P: [2.34 1.69 1.55 0.47 0.54 0.08 0.25 0.02 6.94]\n",
      "Loss_Q: [0.89 1.06 0.39 0.58 0.05 0.24 0.03 0.   3.24] Loss_P: [2.41 1.63 1.62 0.44 0.57 0.06 0.23 0.02 6.99]\n",
      "Loss_Q: [0.84 1.02 0.35 0.56 0.06 0.28 0.02 0.   3.12] Loss_P: [2.41 1.65 1.62 0.41 0.56 0.06 0.23 0.02 6.96]\n",
      "Loss_Q: [0.87 1.   0.43 0.58 0.06 0.24 0.02 0.   3.21] Loss_P: [2.36 1.71 1.59 0.49 0.61 0.06 0.23 0.02 7.06]\n",
      "Loss_Q: [0.84 1.08 0.4  0.52 0.05 0.26 0.02 0.   3.17] Loss_P: [2.39 1.67 1.66 0.46 0.58 0.07 0.22 0.03 7.08]\n",
      "Loss_Q: [0.94 1.08 0.39 0.52 0.06 0.22 0.01 0.   3.22] Loss_P: [2.4  1.63 1.62 0.5  0.54 0.06 0.21 0.02 6.99]\n",
      "Loss_Q: [0.86 1.07 0.37 0.52 0.06 0.23 0.02 0.   3.14] Loss_P: [2.39 1.69 1.65 0.47 0.53 0.06 0.25 0.03 7.06]\n",
      "Loss_Q: [0.84 1.1  0.41 0.57 0.06 0.22 0.02 0.   3.22] Loss_P: [2.38 1.71 1.64 0.44 0.52 0.05 0.21 0.02 6.97]\n",
      "Loss_Q: [0.88 1.04 0.32 0.51 0.03 0.25 0.02 0.   3.05] Loss_P: [2.37 1.73 1.62 0.38 0.54 0.05 0.21 0.02 6.93]\n",
      "Loss_Q: [0.91 1.11 0.44 0.6  0.04 0.27 0.02 0.   3.39] Loss_P: [2.37 1.74 1.69 0.46 0.56 0.07 0.24 0.02 7.14]\n",
      "Loss_Q: [0.86 1.05 0.34 0.55 0.06 0.24 0.02 0.   3.12] Loss_P: [2.33 1.76 1.69 0.41 0.6  0.05 0.24 0.02 7.09]\n",
      "Loss_Q: [0.87 1.05 0.35 0.55 0.06 0.24 0.03 0.   3.14] Loss_P: [2.31 1.75 1.66 0.48 0.58 0.06 0.21 0.03 7.09]\n",
      "Loss_Q: [0.83 1.06 0.35 0.52 0.05 0.22 0.01 0.   3.04] Loss_P: [2.35 1.68 1.72 0.43 0.6  0.08 0.26 0.03 7.15]\n",
      "Loss_Q: [0.82 1.11 0.38 0.62 0.05 0.26 0.03 0.   3.27] Loss_P: [2.36 1.74 1.63 0.42 0.57 0.04 0.27 0.03 7.06]\n",
      "Loss_Q: [0.92 1.08 0.36 0.64 0.05 0.23 0.03 0.   3.31] Loss_P: [2.36 1.76 1.67 0.42 0.65 0.06 0.23 0.02 7.17]\n",
      "Loss_Q: [0.9  1.17 0.36 0.62 0.04 0.25 0.02 0.   3.36] Loss_P: [2.33 1.78 1.69 0.47 0.68 0.05 0.21 0.02 7.22]\n",
      "Loss_Q: [0.88 1.08 0.38 0.64 0.06 0.23 0.02 0.   3.29] Loss_P: [2.38 1.75 1.67 0.45 0.62 0.03 0.23 0.02 7.14]\n",
      "Loss_Q: [0.9  1.07 0.36 0.59 0.05 0.23 0.02 0.   3.22] Loss_P: [2.4  1.72 1.64 0.45 0.66 0.07 0.27 0.03 7.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.89 1.04 0.39 0.64 0.04 0.28 0.02 0.   3.29] Loss_P: [2.33 1.74 1.64 0.44 0.61 0.07 0.26 0.02 7.11]\n",
      "Loss_Q: [0.88 1.05 0.36 0.61 0.05 0.21 0.02 0.   3.17] Loss_P: [2.39 1.75 1.61 0.49 0.62 0.05 0.25 0.03 7.17]\n",
      "Loss_Q: [0.89 1.07 0.38 0.61 0.06 0.26 0.02 0.   3.28] Loss_P: [2.37 1.75 1.67 0.39 0.64 0.07 0.24 0.02 7.15]\n",
      "Loss_Q: [0.95 1.1  0.37 0.63 0.06 0.24 0.03 0.   3.37] Loss_P: [2.37 1.72 1.72 0.42 0.62 0.05 0.23 0.03 7.15]\n",
      "Loss_Q: [0.92 1.1  0.41 0.67 0.06 0.28 0.03 0.   3.45] Loss_P: [2.34 1.75 1.7  0.43 0.6  0.06 0.22 0.02 7.11]\n",
      "Loss_Q: [0.87 1.2  0.41 0.61 0.04 0.24 0.01 0.   3.38] Loss_P: [2.39 1.74 1.73 0.41 0.62 0.05 0.25 0.03 7.22]\n",
      "Loss_Q: [0.81 1.09 0.35 0.55 0.06 0.24 0.02 0.   3.1 ] Loss_P: [2.37 1.7  1.67 0.43 0.67 0.05 0.22 0.02 7.13]\n",
      "Loss_Q: [0.87 1.07 0.37 0.64 0.07 0.23 0.02 0.   3.28] Loss_P: [2.34 1.74 1.62 0.43 0.59 0.07 0.22 0.02 7.04]\n",
      "Loss_Q: [0.79 1.04 0.36 0.56 0.06 0.26 0.02 0.   3.09] Loss_P: [2.33 1.71 1.64 0.38 0.61 0.04 0.25 0.02 6.98]\n",
      "Loss_Q: [0.89 1.07 0.35 0.59 0.07 0.26 0.02 0.   3.26] Loss_P: [2.34 1.67 1.63 0.39 0.61 0.05 0.28 0.02 7.  ]\n",
      "Loss_Q: [0.86 1.03 0.36 0.62 0.06 0.24 0.01 0.   3.19] Loss_P: [2.33 1.71 1.66 0.41 0.58 0.06 0.26 0.03 7.05]\n",
      "Loss_Q: [0.84 1.01 0.3  0.58 0.05 0.24 0.02 0.   3.04] Loss_P: [2.34 1.76 1.7  0.37 0.62 0.06 0.26 0.04 7.15]\n",
      "Loss_Q: [0.85 1.05 0.37 0.65 0.03 0.25 0.02 0.   3.23] Loss_P: [2.42 1.69 1.73 0.34 0.6  0.06 0.24 0.02 7.11]\n",
      "Loss_Q: [0.88 1.05 0.4  0.58 0.06 0.26 0.02 0.   3.24] Loss_P: [2.41 1.69 1.69 0.4  0.65 0.07 0.26 0.02 7.19]\n",
      "Loss_Q: [0.84 1.06 0.34 0.6  0.05 0.3  0.01 0.   3.2 ] Loss_P: [2.34 1.72 1.64 0.39 0.59 0.05 0.24 0.03 7.01]\n",
      "Loss_Q: [0.84 1.09 0.36 0.56 0.05 0.26 0.03 0.   3.19] Loss_P: [2.36 1.73 1.6  0.41 0.55 0.05 0.26 0.02 6.98]\n",
      "Loss_Q: [0.87 1.12 0.38 0.54 0.06 0.27 0.02 0.   3.26] Loss_P: [2.34 1.73 1.71 0.41 0.56 0.07 0.26 0.02 7.11]\n",
      "Loss_Q: [0.82 1.1  0.36 0.59 0.05 0.28 0.02 0.   3.22] Loss_P: [2.36 1.73 1.68 0.46 0.56 0.05 0.27 0.04 7.15]\n",
      "Loss_Q: [0.77 1.08 0.33 0.57 0.05 0.27 0.03 0.   3.1 ] Loss_P: [2.36 1.74 1.68 0.44 0.6  0.06 0.26 0.02 7.15]\n",
      "Loss_Q: [0.87 1.06 0.38 0.6  0.08 0.3  0.03 0.   3.32] Loss_P: [2.35 1.75 1.67 0.45 0.6  0.06 0.26 0.03 7.16]\n",
      "Loss_Q: [0.84 1.06 0.41 0.63 0.06 0.27 0.03 0.   3.3 ] Loss_P: [2.35 1.74 1.65 0.41 0.61 0.07 0.31 0.04 7.17]\n",
      "Loss_Q: [0.79 1.09 0.37 0.64 0.05 0.31 0.03 0.   3.28] Loss_P: [2.35 1.71 1.63 0.41 0.62 0.05 0.32 0.04 7.13]\n",
      "Loss_Q: [0.91 1.07 0.36 0.61 0.04 0.31 0.02 0.   3.33] Loss_P: [2.42 1.74 1.62 0.44 0.62 0.05 0.35 0.04 7.28]\n",
      "Loss_Q: [0.85 1.06 0.38 0.64 0.04 0.33 0.03 0.   3.32] Loss_P: [2.37 1.79 1.64 0.4  0.69 0.07 0.31 0.03 7.3 ]\n",
      "Loss_Q: [0.83 1.06 0.39 0.62 0.05 0.33 0.02 0.   3.31] Loss_P: [2.37 1.75 1.6  0.39 0.62 0.05 0.32 0.01 7.12]\n",
      "Loss_Q: [0.84 1.06 0.34 0.65 0.04 0.34 0.03 0.   3.3 ] Loss_P: [2.32 1.78 1.7  0.43 0.62 0.07 0.35 0.03 7.29]\n",
      "Loss_Q: [0.88 1.02 0.37 0.64 0.03 0.34 0.03 0.   3.32] Loss_P: [2.35 1.76 1.59 0.4  0.61 0.05 0.32 0.03 7.11]\n",
      "Loss_Q: [0.91 1.02 0.36 0.59 0.05 0.31 0.02 0.   3.26] Loss_P: [2.34 1.74 1.67 0.41 0.62 0.05 0.32 0.02 7.18]\n",
      "Loss_Q: [0.94 1.1  0.4  0.59 0.04 0.33 0.02 0.   3.42] Loss_P: [2.37 1.75 1.72 0.43 0.59 0.05 0.34 0.02 7.28]\n",
      "Loss_Q: [0.85 1.09 0.37 0.59 0.05 0.31 0.02 0.   3.3 ] Loss_P: [2.34 1.75 1.67 0.5  0.62 0.04 0.36 0.03 7.32]\n",
      "Loss_Q: [0.92 1.07 0.41 0.65 0.05 0.33 0.04 0.   3.46] Loss_P: [2.36 1.69 1.64 0.43 0.67 0.06 0.34 0.04 7.23]\n",
      "Loss_Q: [0.92 1.14 0.41 0.65 0.06 0.37 0.03 0.   3.58] Loss_P: [2.36 1.72 1.68 0.43 0.65 0.05 0.35 0.04 7.28]\n",
      "Loss_Q: [0.9  1.09 0.37 0.66 0.05 0.35 0.03 0.   3.46] Loss_P: [2.34 1.69 1.67 0.47 0.67 0.05 0.37 0.02 7.27]\n",
      "Loss_Q: [0.88 1.09 0.37 0.68 0.04 0.38 0.03 0.   3.47] Loss_P: [2.35 1.73 1.69 0.43 0.66 0.04 0.33 0.03 7.27]\n",
      "Loss_Q: [0.88 1.09 0.41 0.62 0.05 0.37 0.03 0.   3.46] Loss_P: [2.34 1.79 1.69 0.41 0.63 0.05 0.36 0.03 7.31]\n",
      "Loss_Q: [0.84 1.09 0.35 0.65 0.03 0.33 0.02 0.   3.3 ] Loss_P: [2.42 1.68 1.7  0.41 0.63 0.04 0.34 0.03 7.25]\n",
      "Loss_Q: [0.8  1.03 0.37 0.65 0.03 0.37 0.04 0.   3.29] Loss_P: [2.33 1.72 1.61 0.44 0.68 0.05 0.35 0.03 7.2 ]\n",
      "Loss_Q: [0.83 1.   0.27 0.67 0.05 0.39 0.04 0.   3.25] Loss_P: [2.39 1.7  1.63 0.35 0.64 0.04 0.38 0.03 7.17]\n",
      "Loss_Q: [0.83 1.06 0.31 0.58 0.04 0.38 0.03 0.   3.23] Loss_P: [2.4  1.72 1.69 0.38 0.61 0.05 0.37 0.03 7.24]\n",
      "Loss_Q: [0.86 1.   0.36 0.64 0.05 0.36 0.01 0.   3.29] Loss_P: [2.35 1.83 1.67 0.38 0.6  0.04 0.36 0.04 7.27]\n",
      "Loss_Q: [0.8  1.07 0.35 0.61 0.04 0.39 0.02 0.   3.28] Loss_P: [2.43 1.69 1.6  0.35 0.62 0.06 0.4  0.03 7.19]\n",
      "Loss_Q: [0.89 1.04 0.34 0.63 0.05 0.38 0.02 0.   3.35] Loss_P: [2.39 1.75 1.63 0.38 0.64 0.06 0.39 0.04 7.28]\n",
      "Loss_Q: [0.85 0.98 0.34 0.68 0.05 0.36 0.01 0.   3.27] Loss_P: [2.4  1.73 1.65 0.42 0.67 0.06 0.39 0.03 7.36]\n",
      "Loss_Q: [0.86 0.99 0.36 0.7  0.05 0.36 0.02 0.   3.34] Loss_P: [2.44 1.74 1.59 0.34 0.64 0.03 0.37 0.04 7.19]\n",
      "Loss_Q: [0.88 1.01 0.36 0.66 0.05 0.39 0.03 0.   3.38] Loss_P: [2.36 1.78 1.67 0.38 0.67 0.05 0.38 0.02 7.32]\n",
      "Loss_Q: [0.89 1.07 0.34 0.68 0.06 0.37 0.02 0.   3.43] Loss_P: [2.38 1.73 1.69 0.35 0.64 0.04 0.38 0.03 7.24]\n",
      "Loss_Q: [0.96 1.11 0.34 0.64 0.05 0.36 0.03 0.   3.48] Loss_P: [2.34 1.8  1.63 0.36 0.62 0.05 0.39 0.04 7.24]\n",
      "Loss_Q: [0.93 0.98 0.33 0.63 0.05 0.4  0.03 0.   3.35] Loss_P: [2.33 1.84 1.62 0.36 0.58 0.03 0.38 0.01 7.15]\n",
      "Loss_Q: [0.84 1.02 0.33 0.62 0.04 0.34 0.03 0.   3.21] Loss_P: [2.34 1.78 1.55 0.38 0.63 0.05 0.36 0.02 7.1 ]\n",
      "Loss_Q: [0.93 1.04 0.34 0.61 0.05 0.35 0.03 0.   3.36] Loss_P: [2.37 1.82 1.57 0.4  0.65 0.05 0.35 0.03 7.23]\n",
      "Loss_Q: [0.82 1.01 0.34 0.65 0.07 0.35 0.03 0.   3.27] Loss_P: [2.32 1.8  1.62 0.36 0.69 0.05 0.38 0.03 7.24]\n",
      "Loss_Q: [0.93 0.99 0.37 0.65 0.05 0.37 0.02 0.   3.38] Loss_P: [2.4  1.84 1.64 0.38 0.67 0.05 0.37 0.03 7.37]\n",
      "Loss_Q: [0.91 1.15 0.34 0.65 0.04 0.4  0.05 0.   3.53] Loss_P: [2.33 1.82 1.62 0.37 0.65 0.03 0.41 0.02 7.24]\n",
      "Loss_Q: [0.85 1.06 0.29 0.71 0.04 0.4  0.02 0.   3.38] Loss_P: [2.35 1.77 1.6  0.37 0.71 0.05 0.36 0.02 7.25]\n",
      "Loss_Q: [0.92 1.01 0.35 0.67 0.06 0.36 0.02 0.   3.38] Loss_P: [2.32 1.76 1.58 0.38 0.69 0.06 0.39 0.04 7.22]\n",
      "Loss_Q: [0.92 1.06 0.32 0.64 0.07 0.35 0.03 0.   3.39] Loss_P: [2.33 1.8  1.59 0.41 0.71 0.06 0.37 0.02 7.29]\n",
      "Loss_Q: [0.85 1.04 0.37 0.64 0.06 0.37 0.02 0.   3.36] Loss_P: [2.4  1.81 1.65 0.33 0.62 0.05 0.38 0.02 7.26]\n",
      "Loss_Q: [0.87 0.98 0.3  0.64 0.05 0.4  0.04 0.   3.28] Loss_P: [2.35 1.83 1.6  0.35 0.66 0.07 0.41 0.02 7.28]\n",
      "Loss_Q: [0.92 0.99 0.36 0.66 0.05 0.4  0.04 0.   3.41] Loss_P: [2.37 1.8  1.53 0.37 0.64 0.05 0.4  0.02 7.19]\n",
      "Loss_Q: [0.9  1.06 0.33 0.64 0.05 0.38 0.03 0.   3.39] Loss_P: [2.42 1.78 1.59 0.29 0.63 0.04 0.41 0.02 7.17]\n",
      "Loss_Q: [0.91 1.11 0.29 0.63 0.04 0.39 0.02 0.   3.38] Loss_P: [2.37 1.79 1.58 0.35 0.64 0.04 0.41 0.03 7.21]\n",
      "Loss_Q: [0.87 1.06 0.34 0.67 0.04 0.41 0.03 0.   3.41] Loss_P: [2.42 1.72 1.63 0.29 0.66 0.07 0.39 0.02 7.22]\n",
      "Loss_Q: [0.81 1.02 0.33 0.68 0.05 0.39 0.03 0.   3.31] Loss_P: [2.4  1.78 1.58 0.33 0.65 0.04 0.4  0.03 7.2 ]\n",
      "Loss_Q: [0.86 1.03 0.3  0.64 0.04 0.37 0.02 0.   3.25] Loss_P: [2.39 1.78 1.52 0.36 0.67 0.04 0.38 0.02 7.16]\n",
      "Loss_Q: [0.82 1.   0.34 0.65 0.05 0.39 0.03 0.   3.29] Loss_P: [2.43 1.74 1.58 0.29 0.65 0.05 0.39 0.03 7.16]\n",
      "Loss_Q: [0.88 1.   0.32 0.67 0.06 0.4  0.02 0.   3.34] Loss_P: [2.42 1.72 1.55 0.32 0.64 0.05 0.41 0.03 7.14]\n",
      "Loss_Q: [0.9  1.01 0.33 0.63 0.05 0.38 0.01 0.   3.31] Loss_P: [2.35 1.8  1.59 0.36 0.71 0.04 0.38 0.03 7.25]\n",
      "Loss_Q: [0.9  1.05 0.29 0.6  0.03 0.35 0.02 0.   3.24] Loss_P: [2.41 1.73 1.59 0.41 0.63 0.04 0.38 0.02 7.21]\n",
      "Loss_Q: [0.86 1.04 0.31 0.61 0.05 0.38 0.03 0.   3.28] Loss_P: [2.4  1.81 1.58 0.36 0.66 0.05 0.37 0.05 7.28]\n",
      "Loss_Q: [0.99 1.01 0.37 0.62 0.06 0.36 0.02 0.   3.44] Loss_P: [2.37 1.79 1.64 0.34 0.65 0.04 0.38 0.03 7.25]\n",
      "Loss_Q: [0.91 0.99 0.36 0.63 0.05 0.37 0.02 0.   3.34] Loss_P: [2.36 1.77 1.61 0.37 0.67 0.05 0.37 0.03 7.22]\n",
      "Loss_Q: [0.95 1.03 0.37 0.62 0.05 0.38 0.03 0.   3.42] Loss_P: [2.4  1.8  1.62 0.33 0.6  0.03 0.38 0.03 7.19]\n",
      "Loss_Q: [0.95 1.01 0.33 0.61 0.05 0.4  0.03 0.   3.38] Loss_P: [2.4  1.72 1.61 0.41 0.65 0.04 0.38 0.03 7.23]\n",
      "Loss_Q: [0.85 0.98 0.31 0.65 0.06 0.41 0.03 0.   3.27] Loss_P: [2.38 1.73 1.57 0.37 0.67 0.05 0.38 0.03 7.18]\n",
      "Loss_Q: [0.84 1.   0.3  0.68 0.05 0.42 0.03 0.   3.31] Loss_P: [2.43 1.72 1.51 0.32 0.64 0.06 0.41 0.02 7.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.8  0.99 0.31 0.66 0.04 0.38 0.02 0.   3.2 ] Loss_P: [2.38 1.69 1.52 0.33 0.65 0.07 0.39 0.03 7.07]\n",
      "Loss_Q: [0.84 0.96 0.29 0.63 0.05 0.38 0.02 0.   3.18] Loss_P: [2.39 1.73 1.48 0.34 0.64 0.05 0.38 0.03 7.04]\n",
      "Loss_Q: [0.95 0.98 0.3  0.64 0.05 0.4  0.04 0.   3.35] Loss_P: [2.38 1.76 1.52 0.34 0.63 0.05 0.37 0.03 7.09]\n",
      "Loss_Q: [0.85 1.   0.3  0.67 0.06 0.39 0.03 0.   3.29] Loss_P: [2.35 1.76 1.48 0.3  0.68 0.08 0.4  0.03 7.07]\n",
      "Loss_Q: [0.89 0.93 0.33 0.72 0.05 0.38 0.03 0.   3.32] Loss_P: [2.37 1.76 1.48 0.34 0.67 0.05 0.38 0.03 7.08]\n",
      "Loss_Q: [0.93 1.02 0.34 0.69 0.05 0.37 0.02 0.   3.44] Loss_P: [2.43 1.79 1.5  0.37 0.69 0.04 0.36 0.02 7.21]\n",
      "Loss_Q: [0.87 0.96 0.34 0.67 0.05 0.35 0.02 0.   3.24] Loss_P: [2.44 1.66 1.57 0.36 0.69 0.04 0.39 0.03 7.17]\n",
      "Loss_Q: [0.86 0.99 0.33 0.7  0.07 0.4  0.03 0.   3.37] Loss_P: [2.42 1.79 1.56 0.38 0.67 0.05 0.38 0.03 7.29]\n",
      "Loss_Q: [0.85 1.04 0.32 0.66 0.06 0.38 0.02 0.   3.32] Loss_P: [2.36 1.86 1.53 0.36 0.64 0.05 0.4  0.04 7.22]\n",
      "Loss_Q: [0.86 1.02 0.33 0.67 0.03 0.4  0.02 0.   3.33] Loss_P: [2.38 1.77 1.61 0.38 0.69 0.05 0.39 0.03 7.3 ]\n",
      "Loss_Q: [0.91 1.02 0.33 0.65 0.05 0.43 0.03 0.   3.43] Loss_P: [2.38 1.75 1.55 0.37 0.61 0.07 0.4  0.02 7.16]\n",
      "Loss_Q: [0.89 1.01 0.32 0.66 0.04 0.4  0.03 0.   3.36] Loss_P: [2.4  1.71 1.55 0.35 0.67 0.04 0.38 0.04 7.13]\n",
      "Loss_Q: [0.8  1.07 0.3  0.68 0.05 0.38 0.02 0.   3.3 ] Loss_P: [2.34 1.76 1.62 0.36 0.66 0.06 0.39 0.03 7.21]\n",
      "Loss_Q: [0.82 0.99 0.35 0.66 0.05 0.42 0.04 0.   3.33] Loss_P: [2.4  1.73 1.57 0.38 0.69 0.06 0.4  0.03 7.25]\n",
      "Loss_Q: [0.94 1.05 0.33 0.66 0.05 0.42 0.03 0.   3.47] Loss_P: [2.39 1.74 1.51 0.37 0.7  0.05 0.4  0.03 7.19]\n",
      "Loss_Q: [0.84 1.1  0.38 0.68 0.07 0.41 0.02 0.   3.51] Loss_P: [2.44 1.65 1.56 0.36 0.69 0.04 0.39 0.01 7.14]\n",
      "Loss_Q: [0.81 1.02 0.34 0.71 0.03 0.41 0.03 0.   3.35] Loss_P: [2.41 1.64 1.6  0.41 0.65 0.05 0.41 0.03 7.19]\n",
      "Loss_Q: [0.87 1.06 0.35 0.65 0.04 0.42 0.03 0.   3.41] Loss_P: [2.31 1.75 1.61 0.4  0.65 0.05 0.41 0.03 7.21]\n",
      "Loss_Q: [0.85 1.02 0.34 0.69 0.05 0.42 0.01 0.   3.38] Loss_P: [2.34 1.76 1.62 0.33 0.63 0.05 0.41 0.03 7.17]\n",
      "Loss_Q: [0.84 1.09 0.37 0.67 0.04 0.44 0.03 0.   3.49] Loss_P: [2.4  1.71 1.6  0.37 0.67 0.07 0.45 0.03 7.3 ]\n",
      "Loss_Q: [0.83 1.05 0.35 0.64 0.04 0.42 0.02 0.   3.36] Loss_P: [2.38 1.69 1.67 0.35 0.67 0.05 0.43 0.03 7.26]\n",
      "Loss_Q: [0.88 1.04 0.3  0.65 0.03 0.42 0.02 0.   3.34] Loss_P: [2.4  1.65 1.64 0.41 0.69 0.05 0.45 0.03 7.3 ]\n",
      "Loss_Q: [0.84 1.08 0.34 0.66 0.04 0.45 0.02 0.   3.43] Loss_P: [2.35 1.74 1.67 0.4  0.68 0.06 0.44 0.02 7.36]\n",
      "Loss_Q: [0.92 1.06 0.38 0.65 0.04 0.46 0.03 0.   3.54] Loss_P: [2.39 1.7  1.7  0.33 0.69 0.04 0.45 0.01 7.31]\n",
      "Loss_Q: [0.89 1.09 0.33 0.65 0.04 0.45 0.03 0.   3.48] Loss_P: [2.33 1.79 1.68 0.43 0.65 0.05 0.44 0.03 7.4 ]\n",
      "Loss_Q: [0.89 1.13 0.39 0.67 0.05 0.46 0.01 0.   3.61] Loss_P: [2.36 1.7  1.63 0.4  0.68 0.04 0.48 0.03 7.32]\n",
      "Loss_Q: [0.82 1.11 0.39 0.68 0.06 0.45 0.03 0.   3.55] Loss_P: [2.37 1.62 1.69 0.38 0.64 0.03 0.47 0.02 7.23]\n",
      "Loss_Q: [0.87 1.1  0.35 0.57 0.06 0.47 0.03 0.   3.45] Loss_P: [2.37 1.7  1.72 0.4  0.63 0.05 0.48 0.04 7.4 ]\n",
      "Loss_Q: [0.82 1.11 0.36 0.63 0.04 0.47 0.03 0.   3.45] Loss_P: [2.4  1.68 1.66 0.4  0.63 0.06 0.48 0.04 7.37]\n",
      "Loss_Q: [0.83 1.03 0.33 0.62 0.03 0.47 0.02 0.   3.33] Loss_P: [2.39 1.7  1.63 0.34 0.6  0.04 0.48 0.04 7.22]\n",
      "Loss_Q: [0.83 1.02 0.34 0.66 0.05 0.49 0.04 0.   3.43] Loss_P: [2.43 1.72 1.66 0.37 0.6  0.04 0.48 0.03 7.32]\n",
      "Loss_Q: [0.92 1.05 0.34 0.64 0.03 0.46 0.01 0.   3.46] Loss_P: [2.36 1.73 1.68 0.35 0.64 0.06 0.46 0.03 7.3 ]\n",
      "Loss_Q: [0.81 1.07 0.33 0.66 0.06 0.48 0.02 0.   3.42] Loss_P: [2.37 1.64 1.65 0.39 0.61 0.06 0.47 0.03 7.22]\n",
      "Loss_Q: [0.81 1.11 0.37 0.61 0.05 0.48 0.03 0.   3.48] Loss_P: [2.39 1.67 1.63 0.38 0.62 0.04 0.47 0.05 7.25]\n",
      "Loss_Q: [0.89 1.05 0.33 0.62 0.05 0.46 0.02 0.   3.44] Loss_P: [2.38 1.67 1.66 0.35 0.65 0.04 0.45 0.03 7.24]\n",
      "Loss_Q: [0.89 1.03 0.38 0.63 0.05 0.47 0.04 0.   3.49] Loss_P: [2.38 1.7  1.63 0.4  0.61 0.07 0.45 0.02 7.26]\n",
      "Loss_Q: [0.87 0.95 0.38 0.61 0.05 0.45 0.03 0.   3.34] Loss_P: [2.3  1.7  1.59 0.38 0.6  0.05 0.42 0.02 7.07]\n",
      "Loss_Q: [0.85 1.03 0.35 0.61 0.05 0.46 0.03 0.   3.38] Loss_P: [2.39 1.7  1.59 0.37 0.61 0.05 0.46 0.03 7.2 ]\n",
      "Loss_Q: [0.89 1.04 0.35 0.66 0.05 0.46 0.03 0.   3.48] Loss_P: [2.42 1.65 1.57 0.38 0.66 0.05 0.47 0.02 7.21]\n",
      "Loss_Q: [0.84 0.95 0.35 0.61 0.04 0.43 0.04 0.   3.25] Loss_P: [2.36 1.71 1.59 0.36 0.68 0.04 0.43 0.02 7.18]\n",
      "Loss_Q: [0.82 1.02 0.34 0.61 0.06 0.42 0.02 0.   3.3 ] Loss_P: [2.38 1.73 1.56 0.4  0.64 0.04 0.4  0.02 7.16]\n",
      "Loss_Q: [0.8  1.   0.37 0.59 0.04 0.41 0.03 0.   3.24] Loss_P: [2.36 1.69 1.53 0.37 0.61 0.06 0.4  0.03 7.04]\n",
      "Loss_Q: [0.82 0.97 0.36 0.62 0.05 0.4  0.02 0.   3.24] Loss_P: [2.4  1.68 1.63 0.39 0.64 0.05 0.42 0.01 7.22]\n",
      "Loss_Q: [0.83 1.04 0.36 0.66 0.04 0.39 0.02 0.   3.35] Loss_P: [2.4  1.71 1.61 0.37 0.64 0.04 0.38 0.02 7.18]\n",
      "Loss_Q: [0.85 1.04 0.36 0.64 0.04 0.41 0.04 0.   3.38] Loss_P: [2.36 1.66 1.55 0.38 0.63 0.06 0.41 0.03 7.08]\n",
      "Loss_Q: [0.86 0.98 0.39 0.65 0.05 0.41 0.02 0.   3.36] Loss_P: [2.35 1.69 1.54 0.39 0.64 0.04 0.42 0.02 7.08]\n",
      "Loss_Q: [0.81 0.94 0.35 0.68 0.06 0.38 0.03 0.   3.25] Loss_P: [2.34 1.7  1.59 0.37 0.71 0.07 0.4  0.02 7.2 ]\n",
      "Loss_Q: [0.76 0.98 0.33 0.7  0.04 0.39 0.02 0.   3.24] Loss_P: [2.42 1.63 1.57 0.41 0.7  0.04 0.37 0.03 7.18]\n",
      "Loss_Q: [0.84 1.05 0.34 0.66 0.05 0.37 0.02 0.   3.33] Loss_P: [2.35 1.71 1.61 0.38 0.7  0.04 0.37 0.04 7.21]\n",
      "Loss_Q: [0.83 0.98 0.38 0.68 0.05 0.4  0.02 0.   3.35] Loss_P: [2.38 1.72 1.53 0.41 0.71 0.04 0.41 0.04 7.23]\n",
      "Loss_Q: [0.88 0.94 0.4  0.64 0.04 0.42 0.03 0.   3.34] Loss_P: [2.32 1.71 1.52 0.4  0.66 0.06 0.39 0.03 7.08]\n",
      "Loss_Q: [0.84 1.   0.38 0.64 0.03 0.42 0.02 0.   3.33] Loss_P: [2.41 1.68 1.57 0.39 0.65 0.05 0.41 0.03 7.19]\n",
      "Loss_Q: [0.85 1.02 0.36 0.61 0.06 0.42 0.03 0.   3.35] Loss_P: [2.31 1.75 1.57 0.37 0.65 0.06 0.41 0.01 7.14]\n",
      "Loss_Q: [0.89 1.01 0.36 0.64 0.04 0.42 0.02 0.   3.37] Loss_P: [2.4  1.71 1.6  0.44 0.71 0.05 0.42 0.04 7.36]\n",
      "Loss_Q: [0.78 0.97 0.35 0.64 0.05 0.42 0.02 0.   3.22] Loss_P: [2.39 1.76 1.57 0.41 0.67 0.05 0.39 0.03 7.28]\n",
      "Loss_Q: [0.88 0.94 0.36 0.62 0.06 0.39 0.03 0.   3.27] Loss_P: [2.32 1.74 1.58 0.39 0.62 0.04 0.39 0.02 7.11]\n",
      "Loss_Q: [0.84 0.92 0.34 0.63 0.04 0.42 0.02 0.   3.19] Loss_P: [2.42 1.74 1.51 0.35 0.63 0.04 0.41 0.03 7.12]\n",
      "Loss_Q: [0.86 1.01 0.35 0.63 0.05 0.4  0.03 0.   3.32] Loss_P: [2.34 1.71 1.51 0.36 0.65 0.05 0.4  0.02 7.05]\n",
      "Loss_Q: [0.94 0.99 0.3  0.62 0.06 0.41 0.02 0.   3.34] Loss_P: [2.36 1.69 1.54 0.37 0.63 0.06 0.39 0.02 7.04]\n",
      "Loss_Q: [0.82 0.96 0.31 0.6  0.05 0.42 0.03 0.   3.17] Loss_P: [2.35 1.77 1.51 0.38 0.65 0.06 0.41 0.02 7.16]\n",
      "Loss_Q: [0.87 0.97 0.36 0.64 0.03 0.38 0.03 0.   3.29] Loss_P: [2.3  1.77 1.52 0.37 0.63 0.06 0.37 0.01 7.03]\n",
      "Loss_Q: [0.85 0.93 0.36 0.63 0.04 0.38 0.01 0.   3.19] Loss_P: [2.39 1.72 1.5  0.39 0.65 0.04 0.38 0.03 7.09]\n",
      "Loss_Q: [0.84 0.89 0.35 0.67 0.05 0.4  0.02 0.   3.23] Loss_P: [2.38 1.75 1.49 0.38 0.64 0.06 0.39 0.02 7.11]\n",
      "Loss_Q: [0.93 0.94 0.35 0.68 0.03 0.44 0.02 0.   3.39] Loss_P: [2.35 1.73 1.48 0.37 0.72 0.04 0.44 0.02 7.15]\n",
      "Loss_Q: [0.8  0.89 0.29 0.7  0.05 0.45 0.02 0.   3.21] Loss_P: [2.34 1.77 1.46 0.37 0.69 0.05 0.42 0.04 7.14]\n",
      "Loss_Q: [0.88 0.94 0.33 0.65 0.05 0.45 0.02 0.   3.32] Loss_P: [2.37 1.73 1.52 0.37 0.72 0.07 0.44 0.03 7.24]\n",
      "Loss_Q: [0.9  0.94 0.36 0.66 0.06 0.43 0.02 0.   3.37] Loss_P: [2.32 1.74 1.53 0.4  0.67 0.06 0.43 0.02 7.17]\n",
      "Loss_Q: [0.91 0.9  0.35 0.67 0.05 0.46 0.04 0.   3.38] Loss_P: [2.33 1.82 1.54 0.35 0.69 0.05 0.42 0.02 7.23]\n",
      "Loss_Q: [0.92 0.94 0.34 0.63 0.04 0.42 0.02 0.   3.32] Loss_P: [2.34 1.77 1.53 0.37 0.69 0.05 0.4  0.02 7.19]\n",
      "Loss_Q: [0.85 0.94 0.34 0.65 0.02 0.4  0.02 0.   3.22] Loss_P: [2.36 1.75 1.51 0.36 0.63 0.04 0.41 0.02 7.09]\n",
      "Loss_Q: [0.87 0.9  0.36 0.64 0.05 0.39 0.03 0.   3.24] Loss_P: [2.38 1.77 1.53 0.38 0.63 0.03 0.41 0.04 7.18]\n",
      "Loss_Q: [0.9  1.   0.37 0.67 0.03 0.41 0.03 0.   3.4 ] Loss_P: [2.28 1.79 1.58 0.39 0.68 0.05 0.41 0.02 7.22]\n",
      "Loss_Q: [0.85 1.   0.42 0.65 0.03 0.41 0.03 0.   3.4 ] Loss_P: [2.32 1.78 1.56 0.36 0.65 0.04 0.41 0.04 7.18]\n",
      "Loss_Q: [1.02 1.01 0.39 0.64 0.03 0.41 0.02 0.   3.53] Loss_P: [2.34 1.74 1.57 0.42 0.64 0.06 0.4  0.04 7.2 ]\n",
      "Loss_Q: [0.9  0.93 0.38 0.67 0.03 0.43 0.03 0.   3.36] Loss_P: [2.33 1.79 1.59 0.39 0.6  0.05 0.38 0.02 7.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.87 1.03 0.37 0.61 0.05 0.4  0.03 0.   3.35] Loss_P: [2.42 1.71 1.5  0.4  0.63 0.07 0.41 0.02 7.16]\n",
      "Loss_Q: [0.84 0.93 0.35 0.65 0.05 0.42 0.02 0.   3.26] Loss_P: [2.42 1.68 1.55 0.38 0.69 0.03 0.42 0.03 7.21]\n",
      "Loss_Q: [0.86 0.97 0.37 0.69 0.04 0.4  0.02 0.   3.34] Loss_P: [2.38 1.77 1.54 0.39 0.69 0.04 0.43 0.02 7.27]\n",
      "Loss_Q: [0.96 0.98 0.37 0.66 0.07 0.41 0.03 0.   3.48] Loss_P: [2.35 1.78 1.54 0.39 0.7  0.04 0.44 0.02 7.26]\n",
      "Loss_Q: [0.91 1.   0.36 0.68 0.06 0.42 0.03 0.   3.46] Loss_P: [2.36 1.82 1.54 0.38 0.67 0.05 0.4  0.02 7.23]\n",
      "Loss_Q: [0.96 1.   0.36 0.64 0.03 0.41 0.03 0.   3.44] Loss_P: [2.36 1.76 1.59 0.4  0.67 0.05 0.42 0.03 7.28]\n",
      "Loss_Q: [0.91 0.92 0.38 0.66 0.05 0.4  0.03 0.   3.36] Loss_P: [2.35 1.76 1.51 0.42 0.64 0.04 0.43 0.02 7.17]\n",
      "Loss_Q: [0.98 1.03 0.39 0.63 0.04 0.42 0.02 0.   3.51] Loss_P: [2.41 1.71 1.55 0.39 0.67 0.04 0.41 0.05 7.25]\n",
      "Loss_Q: [0.87 0.95 0.33 0.65 0.04 0.45 0.03 0.   3.31] Loss_P: [2.29 1.84 1.57 0.43 0.66 0.04 0.43 0.02 7.27]\n",
      "Loss_Q: [0.94 0.96 0.38 0.65 0.05 0.44 0.02 0.   3.44] Loss_P: [2.37 1.78 1.55 0.41 0.63 0.05 0.46 0.02 7.27]\n",
      "Loss_Q: [0.81 1.   0.31 0.64 0.04 0.43 0.03 0.   3.25] Loss_P: [2.35 1.81 1.57 0.41 0.67 0.05 0.44 0.02 7.32]\n",
      "Loss_Q: [0.92 0.99 0.36 0.63 0.05 0.45 0.02 0.   3.42] Loss_P: [2.35 1.78 1.55 0.39 0.65 0.05 0.46 0.02 7.25]\n",
      "Loss_Q: [0.79 1.01 0.32 0.65 0.03 0.45 0.02 0.   3.27] Loss_P: [2.31 1.73 1.54 0.45 0.69 0.05 0.45 0.03 7.25]\n",
      "Loss_Q: [0.83 1.02 0.38 0.65 0.04 0.45 0.03 0.   3.4 ] Loss_P: [2.33 1.71 1.53 0.39 0.67 0.05 0.44 0.02 7.14]\n",
      "Loss_Q: [0.8  1.   0.36 0.67 0.04 0.45 0.02 0.   3.33] Loss_P: [2.35 1.74 1.53 0.4  0.68 0.05 0.44 0.02 7.21]\n",
      "Loss_Q: [0.81 1.1  0.37 0.69 0.03 0.45 0.03 0.   3.48] Loss_P: [2.33 1.81 1.55 0.36 0.68 0.04 0.44 0.03 7.25]\n",
      "Loss_Q: [0.83 1.02 0.38 0.68 0.05 0.43 0.01 0.   3.4 ] Loss_P: [2.36 1.73 1.58 0.42 0.7  0.03 0.43 0.02 7.28]\n",
      "Loss_Q: [0.81 0.97 0.39 0.64 0.03 0.43 0.02 0.   3.29] Loss_P: [2.39 1.79 1.54 0.39 0.66 0.04 0.42 0.02 7.24]\n",
      "Loss_Q: [0.84 1.1  0.41 0.68 0.04 0.44 0.02 0.   3.52] Loss_P: [2.41 1.72 1.56 0.43 0.68 0.05 0.41 0.02 7.29]\n",
      "Loss_Q: [0.9  1.05 0.36 0.68 0.04 0.43 0.02 0.   3.48] Loss_P: [2.4  1.68 1.58 0.4  0.64 0.03 0.45 0.01 7.2 ]\n",
      "Loss_Q: [0.87 1.06 0.36 0.64 0.04 0.42 0.02 0.   3.4 ] Loss_P: [2.37 1.72 1.59 0.41 0.65 0.04 0.43 0.02 7.21]\n",
      "Loss_Q: [0.81 1.02 0.39 0.67 0.04 0.44 0.03 0.   3.39] Loss_P: [2.45 1.78 1.55 0.47 0.69 0.04 0.43 0.02 7.43]\n",
      "Loss_Q: [0.83 1.09 0.36 0.64 0.04 0.42 0.01 0.   3.38] Loss_P: [2.36 1.71 1.6  0.43 0.67 0.05 0.42 0.02 7.27]\n",
      "Loss_Q: [0.8  0.99 0.38 0.68 0.06 0.44 0.03 0.   3.38] Loss_P: [2.37 1.73 1.56 0.38 0.63 0.06 0.42 0.02 7.17]\n",
      "Loss_Q: [0.83 0.96 0.37 0.68 0.04 0.44 0.02 0.   3.33] Loss_P: [2.46 1.72 1.53 0.33 0.66 0.05 0.38 0.02 7.16]\n",
      "Loss_Q: [0.75 0.97 0.36 0.68 0.03 0.43 0.02 0.   3.24] Loss_P: [2.38 1.69 1.56 0.38 0.67 0.05 0.43 0.02 7.17]\n",
      "Loss_Q: [0.83 0.97 0.32 0.73 0.04 0.42 0.02 0.   3.34] Loss_P: [2.39 1.75 1.53 0.37 0.68 0.03 0.39 0.02 7.16]\n",
      "Loss_Q: [0.87 0.98 0.36 0.67 0.03 0.42 0.03 0.   3.36] Loss_P: [2.35 1.74 1.47 0.38 0.68 0.03 0.41 0.02 7.09]\n",
      "Loss_Q: [0.83 1.02 0.38 0.64 0.07 0.43 0.02 0.   3.4 ] Loss_P: [2.35 1.73 1.49 0.4  0.67 0.04 0.42 0.02 7.12]\n",
      "Loss_Q: [0.81 1.   0.36 0.66 0.03 0.45 0.02 0.   3.33] Loss_P: [2.43 1.7  1.59 0.36 0.66 0.03 0.43 0.03 7.23]\n",
      "Loss_Q: [0.84 1.07 0.37 0.63 0.03 0.42 0.01 0.   3.38] Loss_P: [2.38 1.74 1.65 0.39 0.64 0.03 0.42 0.02 7.28]\n",
      "Loss_Q: [0.87 0.97 0.35 0.69 0.06 0.41 0.03 0.   3.38] Loss_P: [2.43 1.69 1.55 0.35 0.68 0.04 0.43 0.02 7.19]\n",
      "Loss_Q: [0.86 1.01 0.29 0.64 0.04 0.43 0.02 0.   3.28] Loss_P: [2.41 1.73 1.64 0.37 0.67 0.03 0.43 0.03 7.31]\n",
      "Loss_Q: [0.84 1.   0.37 0.68 0.07 0.42 0.02 0.   3.39] Loss_P: [2.43 1.67 1.6  0.39 0.71 0.04 0.42 0.01 7.26]\n",
      "Loss_Q: [0.93 0.96 0.34 0.64 0.05 0.42 0.02 0.   3.36] Loss_P: [2.39 1.72 1.64 0.4  0.68 0.04 0.4  0.02 7.28]\n",
      "Loss_Q: [0.87 1.01 0.37 0.67 0.05 0.44 0.03 0.   3.43] Loss_P: [2.31 1.8  1.64 0.43 0.69 0.03 0.4  0.03 7.32]\n",
      "Loss_Q: [0.91 0.96 0.36 0.69 0.04 0.4  0.02 0.   3.38] Loss_P: [2.36 1.82 1.63 0.4  0.69 0.05 0.39 0.04 7.39]\n",
      "Loss_Q: [0.9  1.01 0.33 0.69 0.06 0.4  0.02 0.   3.41] Loss_P: [2.35 1.77 1.65 0.38 0.66 0.04 0.39 0.02 7.27]\n",
      "Loss_Q: [0.86 1.04 0.36 0.7  0.04 0.38 0.02 0.   3.4 ] Loss_P: [2.33 1.72 1.68 0.39 0.72 0.05 0.37 0.03 7.3 ]\n",
      "Loss_Q: [0.98 1.04 0.38 0.68 0.03 0.38 0.03 0.   3.51] Loss_P: [2.35 1.77 1.69 0.37 0.7  0.05 0.39 0.02 7.35]\n",
      "Loss_Q: [0.89 1.02 0.36 0.69 0.03 0.4  0.02 0.   3.41] Loss_P: [2.38 1.77 1.64 0.43 0.71 0.05 0.37 0.02 7.36]\n",
      "Loss_Q: [0.88 1.09 0.33 0.69 0.05 0.35 0.02 0.   3.41] Loss_P: [2.34 1.82 1.67 0.4  0.69 0.04 0.36 0.02 7.35]\n",
      "Loss_Q: [0.87 1.04 0.31 0.68 0.04 0.39 0.03 0.   3.37] Loss_P: [2.35 1.72 1.68 0.36 0.7  0.05 0.4  0.04 7.3 ]\n",
      "Loss_Q: [0.9  1.02 0.33 0.7  0.04 0.39 0.03 0.   3.4 ] Loss_P: [2.42 1.74 1.63 0.37 0.7  0.04 0.37 0.03 7.29]\n",
      "Loss_Q: [0.95 1.02 0.32 0.67 0.04 0.38 0.02 0.   3.4 ] Loss_P: [2.37 1.78 1.57 0.35 0.69 0.05 0.35 0.03 7.17]\n",
      "Loss_Q: [0.89 1.03 0.33 0.69 0.05 0.39 0.02 0.   3.41] Loss_P: [2.42 1.77 1.57 0.44 0.72 0.04 0.35 0.02 7.34]\n",
      "Loss_Q: [0.96 0.99 0.34 0.66 0.04 0.38 0.03 0.   3.41] Loss_P: [2.37 1.79 1.62 0.37 0.68 0.03 0.37 0.02 7.25]\n",
      "Loss_Q: [0.95 0.98 0.31 0.68 0.04 0.36 0.01 0.   3.35] Loss_P: [2.37 1.81 1.55 0.34 0.68 0.04 0.42 0.04 7.24]\n",
      "Loss_Q: [0.92 0.98 0.29 0.66 0.06 0.4  0.02 0.   3.34] Loss_P: [2.34 1.79 1.57 0.36 0.68 0.04 0.4  0.04 7.22]\n",
      "Loss_Q: [0.92 0.95 0.3  0.7  0.05 0.36 0.03 0.   3.3 ] Loss_P: [2.31 1.84 1.57 0.31 0.66 0.04 0.38 0.02 7.13]\n",
      "Loss_Q: [0.96 1.03 0.32 0.65 0.04 0.38 0.03 0.   3.4 ] Loss_P: [2.34 1.93 1.59 0.33 0.7  0.04 0.36 0.02 7.32]\n",
      "Loss_Q: [0.91 1.03 0.29 0.68 0.03 0.38 0.03 0.   3.35] Loss_P: [2.41 1.8  1.52 0.32 0.66 0.03 0.39 0.01 7.14]\n",
      "Loss_Q: [0.98 1.01 0.3  0.64 0.04 0.39 0.02 0.   3.38] Loss_P: [2.33 1.83 1.57 0.37 0.65 0.04 0.36 0.02 7.17]\n",
      "Loss_Q: [1.04 1.06 0.33 0.65 0.04 0.39 0.02 0.   3.53] Loss_P: [2.39 1.83 1.57 0.38 0.66 0.06 0.4  0.02 7.32]\n",
      "Loss_Q: [0.93 0.99 0.33 0.64 0.04 0.37 0.02 0.   3.33] Loss_P: [2.39 1.77 1.52 0.34 0.66 0.04 0.39 0.02 7.14]\n",
      "Loss_Q: [0.94 0.98 0.3  0.65 0.05 0.38 0.02 0.   3.3 ] Loss_P: [2.36 1.88 1.55 0.36 0.66 0.06 0.4  0.03 7.29]\n",
      "Loss_Q: [1.05 0.94 0.34 0.67 0.06 0.4  0.03 0.   3.48] Loss_P: [2.32 1.86 1.56 0.29 0.67 0.04 0.38 0.02 7.14]\n",
      "Loss_Q: [0.97 1.09 0.34 0.69 0.03 0.38 0.02 0.   3.52] Loss_P: [2.37 1.87 1.56 0.31 0.68 0.04 0.37 0.02 7.22]\n",
      "Loss_Q: [0.98 1.04 0.3  0.68 0.04 0.39 0.02 0.   3.46] Loss_P: [2.4  1.89 1.61 0.3  0.65 0.05 0.39 0.02 7.29]\n",
      "Loss_Q: [0.99 1.01 0.27 0.66 0.04 0.39 0.01 0.   3.37] Loss_P: [2.39 1.94 1.59 0.36 0.66 0.04 0.38 0.01 7.38]\n",
      "Loss_Q: [0.93 1.07 0.32 0.67 0.05 0.37 0.01 0.   3.42] Loss_P: [2.37 1.84 1.57 0.33 0.67 0.04 0.36 0.03 7.2 ]\n",
      "Loss_Q: [0.93 1.07 0.3  0.67 0.06 0.41 0.02 0.   3.45] Loss_P: [2.36 1.82 1.6  0.33 0.63 0.05 0.37 0.02 7.18]\n",
      "Loss_Q: [0.96 1.06 0.3  0.75 0.04 0.4  0.02 0.   3.52] Loss_P: [2.39 1.83 1.62 0.32 0.68 0.05 0.37 0.02 7.28]\n",
      "Loss_Q: [1.03 1.09 0.29 0.65 0.04 0.38 0.02 0.   3.49] Loss_P: [2.39 1.83 1.59 0.32 0.69 0.05 0.36 0.03 7.26]\n",
      "Loss_Q: [0.95 1.02 0.3  0.67 0.04 0.38 0.02 0.   3.37] Loss_P: [2.46 1.75 1.63 0.25 0.67 0.04 0.35 0.02 7.17]\n",
      "Loss_Q: [0.94 1.1  0.31 0.68 0.04 0.38 0.02 0.   3.46] Loss_P: [2.42 1.86 1.64 0.27 0.7  0.05 0.39 0.03 7.36]\n",
      "Loss_Q: [0.96 1.1  0.29 0.67 0.03 0.41 0.03 0.   3.49] Loss_P: [2.42 1.78 1.6  0.28 0.67 0.03 0.37 0.02 7.16]\n",
      "Loss_Q: [0.97 1.08 0.28 0.63 0.03 0.37 0.02 0.   3.38] Loss_P: [2.44 1.77 1.59 0.31 0.72 0.04 0.39 0.02 7.27]\n",
      "Loss_Q: [0.9  1.01 0.27 0.66 0.05 0.4  0.03 0.   3.31] Loss_P: [2.46 1.75 1.59 0.32 0.69 0.04 0.39 0.02 7.26]\n",
      "Loss_Q: [0.85 1.12 0.3  0.68 0.03 0.4  0.03 0.   3.4 ] Loss_P: [2.45 1.76 1.62 0.3  0.67 0.04 0.38 0.02 7.24]\n",
      "Loss_Q: [0.86 1.06 0.25 0.66 0.04 0.38 0.02 0.   3.25] Loss_P: [2.4  1.78 1.58 0.37 0.7  0.05 0.4  0.02 7.31]\n",
      "Loss_Q: [0.97 1.05 0.31 0.69 0.05 0.37 0.01 0.   3.45] Loss_P: [2.47 1.75 1.6  0.34 0.71 0.05 0.4  0.02 7.34]\n",
      "Loss_Q: [0.93 1.01 0.31 0.69 0.05 0.4  0.03 0.   3.43] Loss_P: [2.38 1.8  1.56 0.26 0.71 0.03 0.39 0.02 7.15]\n",
      "Loss_Q: [0.91 1.07 0.31 0.69 0.05 0.41 0.02 0.   3.47] Loss_P: [2.34 1.78 1.62 0.3  0.71 0.05 0.4  0.02 7.22]\n",
      "Loss_Q: [0.88 1.08 0.28 0.7  0.05 0.44 0.03 0.   3.45] Loss_P: [2.35 1.86 1.55 0.3  0.69 0.03 0.43 0.02 7.23]\n",
      "Loss_Q: [0.83 1.04 0.31 0.71 0.05 0.44 0.02 0.   3.4 ] Loss_P: [2.38 1.88 1.59 0.29 0.66 0.04 0.43 0.03 7.3 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.94 1.04 0.29 0.68 0.06 0.42 0.03 0.   3.46] Loss_P: [2.41 1.8  1.6  0.29 0.7  0.06 0.41 0.02 7.3 ]\n",
      "Loss_Q: [0.89 1.07 0.29 0.66 0.04 0.41 0.01 0.   3.37] Loss_P: [2.39 1.78 1.61 0.27 0.69 0.03 0.43 0.02 7.23]\n",
      "Loss_Q: [0.9  1.09 0.28 0.68 0.04 0.43 0.03 0.   3.43] Loss_P: [2.44 1.79 1.54 0.3  0.65 0.03 0.43 0.03 7.22]\n",
      "Loss_Q: [0.89 1.04 0.25 0.64 0.05 0.42 0.03 0.   3.32] Loss_P: [2.37 1.86 1.65 0.32 0.66 0.05 0.43 0.02 7.37]\n",
      "Loss_Q: [0.94 1.08 0.27 0.68 0.05 0.41 0.04 0.   3.47] Loss_P: [2.35 1.84 1.62 0.27 0.67 0.06 0.44 0.02 7.28]\n",
      "Loss_Q: [0.95 1.06 0.3  0.68 0.04 0.43 0.03 0.   3.49] Loss_P: [2.39 1.85 1.61 0.31 0.69 0.04 0.42 0.02 7.32]\n",
      "Loss_Q: [0.89 1.01 0.29 0.67 0.04 0.44 0.02 0.   3.36] Loss_P: [2.38 1.82 1.63 0.29 0.69 0.05 0.45 0.02 7.32]\n",
      "Loss_Q: [0.85 1.12 0.3  0.69 0.06 0.45 0.02 0.   3.49] Loss_P: [2.38 1.79 1.62 0.3  0.7  0.05 0.45 0.01 7.3 ]\n",
      "Loss_Q: [0.91 1.08 0.28 0.7  0.07 0.45 0.03 0.   3.51] Loss_P: [2.38 1.85 1.6  0.31 0.66 0.05 0.47 0.02 7.33]\n",
      "Loss_Q: [0.94 1.05 0.28 0.69 0.04 0.45 0.03 0.   3.47] Loss_P: [2.38 1.73 1.62 0.28 0.69 0.07 0.45 0.03 7.25]\n",
      "Loss_Q: [0.99 1.08 0.26 0.65 0.05 0.43 0.02 0.   3.48] Loss_P: [2.37 1.81 1.61 0.27 0.67 0.04 0.42 0.03 7.23]\n",
      "Loss_Q: [0.9  1.01 0.25 0.65 0.04 0.43 0.02 0.   3.31] Loss_P: [2.4  1.8  1.62 0.27 0.65 0.04 0.45 0.02 7.26]\n",
      "Loss_Q: [0.96 1.04 0.28 0.68 0.05 0.42 0.01 0.   3.44] Loss_P: [2.35 1.79 1.59 0.22 0.7  0.07 0.45 0.02 7.19]\n",
      "Loss_Q: [0.95 1.04 0.24 0.7  0.05 0.46 0.03 0.   3.47] Loss_P: [2.41 1.82 1.66 0.26 0.73 0.04 0.45 0.02 7.39]\n",
      "Loss_Q: [0.88 1.05 0.29 0.71 0.05 0.46 0.02 0.   3.46] Loss_P: [2.38 1.79 1.6  0.24 0.71 0.05 0.45 0.03 7.25]\n",
      "Loss_Q: [0.91 1.07 0.25 0.74 0.04 0.47 0.02 0.   3.49] Loss_P: [2.37 1.8  1.63 0.25 0.69 0.04 0.46 0.02 7.27]\n",
      "Loss_Q: [0.89 1.06 0.25 0.66 0.04 0.46 0.02 0.   3.38] Loss_P: [2.34 1.87 1.6  0.23 0.71 0.04 0.49 0.02 7.28]\n",
      "Loss_Q: [0.95 1.1  0.25 0.67 0.04 0.47 0.02 0.   3.49] Loss_P: [2.36 1.84 1.64 0.26 0.7  0.03 0.47 0.02 7.33]\n",
      "Loss_Q: [0.94 1.01 0.25 0.67 0.05 0.48 0.03 0.   3.43] Loss_P: [2.38 1.88 1.64 0.29 0.71 0.03 0.46 0.02 7.4 ]\n",
      "Loss_Q: [0.96 1.04 0.28 0.72 0.05 0.46 0.02 0.   3.54] Loss_P: [2.34 1.83 1.64 0.3  0.72 0.03 0.47 0.03 7.36]\n",
      "Loss_Q: [0.99 1.06 0.31 0.73 0.05 0.48 0.03 0.   3.65] Loss_P: [2.34 1.82 1.66 0.26 0.67 0.04 0.48 0.03 7.3 ]\n",
      "Loss_Q: [0.9  1.07 0.31 0.71 0.04 0.48 0.02 0.   3.53] Loss_P: [2.35 1.8  1.64 0.32 0.71 0.05 0.47 0.02 7.34]\n",
      "Loss_Q: [0.96 1.   0.26 0.68 0.03 0.47 0.02 0.   3.42] Loss_P: [2.32 1.89 1.66 0.27 0.75 0.05 0.48 0.02 7.44]\n",
      "Loss_Q: [1.01 1.07 0.28 0.7  0.03 0.47 0.01 0.   3.57] Loss_P: [2.35 1.8  1.68 0.31 0.68 0.05 0.45 0.02 7.33]\n",
      "Loss_Q: [0.89 1.06 0.27 0.7  0.04 0.45 0.03 0.   3.44] Loss_P: [2.37 1.82 1.64 0.3  0.73 0.03 0.44 0.02 7.34]\n",
      "Loss_Q: [0.93 1.09 0.29 0.68 0.03 0.45 0.02 0.   3.49] Loss_P: [2.33 1.84 1.63 0.3  0.69 0.03 0.47 0.01 7.32]\n",
      "Loss_Q: [0.96 1.1  0.3  0.72 0.03 0.45 0.02 0.   3.57] Loss_P: [2.34 1.84 1.69 0.31 0.7  0.05 0.44 0.01 7.39]\n",
      "Loss_Q: [1.   1.05 0.31 0.65 0.03 0.44 0.02 0.   3.5 ] Loss_P: [2.31 1.83 1.69 0.32 0.73 0.04 0.44 0.02 7.38]\n",
      "Loss_Q: [1.03 1.05 0.26 0.69 0.04 0.47 0.03 0.   3.56] Loss_P: [2.33 1.9  1.7  0.28 0.7  0.04 0.46 0.03 7.44]\n",
      "Loss_Q: [1.03 1.1  0.28 0.68 0.03 0.47 0.02 0.   3.61] Loss_P: [2.38 1.85 1.67 0.28 0.69 0.04 0.46 0.02 7.39]\n",
      "Loss_Q: [1.05 1.05 0.28 0.69 0.04 0.46 0.02 0.   3.59] Loss_P: [2.34 1.81 1.67 0.3  0.68 0.04 0.48 0.02 7.35]\n",
      "Loss_Q: [0.94 1.06 0.28 0.69 0.04 0.49 0.03 0.   3.53] Loss_P: [2.35 1.81 1.66 0.28 0.66 0.04 0.46 0.03 7.29]\n",
      "Loss_Q: [0.87 1.06 0.27 0.71 0.05 0.45 0.02 0.   3.43] Loss_P: [2.39 1.8  1.63 0.27 0.72 0.04 0.45 0.03 7.35]\n",
      "Loss_Q: [0.88 1.01 0.25 0.68 0.05 0.44 0.02 0.   3.33] Loss_P: [2.39 1.75 1.62 0.27 0.69 0.04 0.45 0.02 7.24]\n",
      "Loss_Q: [0.9  1.04 0.26 0.71 0.03 0.44 0.01 0.   3.39] Loss_P: [2.44 1.78 1.59 0.25 0.72 0.04 0.44 0.02 7.27]\n",
      "Loss_Q: [0.93 1.08 0.27 0.69 0.03 0.45 0.02 0.   3.47] Loss_P: [2.38 1.85 1.7  0.27 0.67 0.04 0.45 0.02 7.39]\n",
      "Loss_Q: [0.95 1.11 0.31 0.7  0.05 0.42 0.01 0.   3.55] Loss_P: [2.34 1.87 1.62 0.28 0.72 0.04 0.43 0.02 7.31]\n",
      "Loss_Q: [1.06 1.09 0.27 0.68 0.04 0.43 0.01 0.   3.57] Loss_P: [2.41 1.84 1.68 0.28 0.7  0.05 0.45 0.02 7.44]\n",
      "Loss_Q: [0.94 1.14 0.29 0.62 0.03 0.43 0.02 0.   3.46] Loss_P: [2.4  1.89 1.66 0.32 0.67 0.04 0.44 0.02 7.45]\n",
      "Loss_Q: [0.87 1.08 0.27 0.65 0.04 0.44 0.03 0.   3.37] Loss_P: [2.33 1.85 1.64 0.31 0.68 0.03 0.43 0.03 7.29]\n",
      "Loss_Q: [0.96 1.05 0.3  0.63 0.05 0.46 0.02 0.   3.47] Loss_P: [2.37 1.83 1.65 0.27 0.69 0.05 0.43 0.01 7.31]\n",
      "Loss_Q: [0.88 1.06 0.26 0.61 0.06 0.44 0.01 0.   3.33] Loss_P: [2.43 1.79 1.56 0.27 0.58 0.05 0.44 0.02 7.13]\n",
      "Loss_Q: [0.85 1.07 0.3  0.63 0.02 0.44 0.02 0.   3.33] Loss_P: [2.38 1.75 1.67 0.29 0.61 0.05 0.44 0.02 7.21]\n",
      "Loss_Q: [0.82 1.07 0.26 0.64 0.05 0.43 0.02 0.   3.29] Loss_P: [2.38 1.73 1.59 0.34 0.62 0.06 0.44 0.02 7.19]\n",
      "Loss_Q: [0.89 1.03 0.29 0.57 0.04 0.44 0.03 0.   3.3 ] Loss_P: [2.39 1.85 1.64 0.32 0.58 0.03 0.43 0.02 7.25]\n",
      "Loss_Q: [0.97 1.04 0.31 0.57 0.05 0.42 0.03 0.   3.38] Loss_P: [2.38 1.75 1.54 0.3  0.61 0.05 0.41 0.01 7.05]\n",
      "Loss_Q: [0.89 1.07 0.3  0.61 0.04 0.45 0.02 0.   3.37] Loss_P: [2.38 1.79 1.51 0.3  0.54 0.05 0.43 0.03 7.03]\n",
      "Loss_Q: [0.89 1.08 0.32 0.6  0.03 0.44 0.02 0.   3.38] Loss_P: [2.41 1.78 1.53 0.27 0.57 0.05 0.46 0.03 7.1 ]\n",
      "Loss_Q: [0.88 1.11 0.26 0.58 0.06 0.4  0.03 0.   3.31] Loss_P: [2.36 1.77 1.56 0.31 0.58 0.07 0.45 0.02 7.12]\n",
      "Loss_Q: [0.86 0.98 0.26 0.6  0.05 0.45 0.03 0.   3.24] Loss_P: [2.41 1.8  1.58 0.31 0.62 0.03 0.44 0.01 7.2 ]\n",
      "Loss_Q: [0.94 1.05 0.29 0.66 0.05 0.47 0.02 0.   3.48] Loss_P: [2.38 1.8  1.52 0.35 0.63 0.07 0.46 0.02 7.24]\n",
      "Loss_Q: [0.96 1.03 0.28 0.6  0.04 0.46 0.02 0.   3.39] Loss_P: [2.33 1.83 1.56 0.34 0.64 0.05 0.44 0.01 7.2 ]\n",
      "Loss_Q: [0.89 0.95 0.28 0.66 0.03 0.46 0.04 0.   3.32] Loss_P: [2.37 1.81 1.56 0.32 0.61 0.04 0.46 0.02 7.19]\n",
      "Loss_Q: [0.86 1.01 0.27 0.69 0.04 0.46 0.02 0.   3.35] Loss_P: [2.4  1.87 1.51 0.33 0.7  0.06 0.46 0.01 7.34]\n",
      "Loss_Q: [0.88 0.97 0.26 0.69 0.04 0.45 0.03 0.   3.31] Loss_P: [2.36 1.9  1.54 0.32 0.62 0.05 0.46 0.03 7.28]\n",
      "Loss_Q: [0.87 0.98 0.29 0.66 0.06 0.48 0.04 0.   3.38] Loss_P: [2.36 1.89 1.46 0.27 0.67 0.05 0.47 0.02 7.2 ]\n",
      "Loss_Q: [0.91 1.08 0.29 0.64 0.06 0.46 0.02 0.   3.45] Loss_P: [2.39 1.81 1.55 0.35 0.65 0.04 0.46 0.02 7.28]\n",
      "Loss_Q: [0.97 1.   0.26 0.63 0.06 0.45 0.02 0.   3.4 ] Loss_P: [2.4  1.82 1.5  0.28 0.69 0.04 0.47 0.03 7.23]\n",
      "Loss_Q: [0.93 1.07 0.3  0.64 0.04 0.46 0.02 0.   3.45] Loss_P: [2.4  1.88 1.55 0.31 0.64 0.05 0.45 0.01 7.29]\n",
      "Loss_Q: [0.93 1.03 0.3  0.66 0.04 0.48 0.02 0.   3.46] Loss_P: [2.41 1.82 1.55 0.31 0.7  0.05 0.47 0.01 7.33]\n",
      "Loss_Q: [0.98 1.06 0.3  0.64 0.05 0.47 0.02 0.   3.53] Loss_P: [2.41 1.79 1.49 0.27 0.65 0.05 0.44 0.02 7.14]\n",
      "Loss_Q: [0.9  1.04 0.3  0.65 0.03 0.45 0.03 0.   3.4 ] Loss_P: [2.39 1.79 1.56 0.26 0.64 0.04 0.44 0.03 7.13]\n",
      "Loss_Q: [0.85 1.07 0.3  0.67 0.04 0.41 0.02 0.   3.35] Loss_P: [2.39 1.85 1.53 0.31 0.67 0.04 0.43 0.03 7.26]\n",
      "Loss_Q: [0.94 1.06 0.29 0.67 0.05 0.46 0.03 0.   3.49] Loss_P: [2.38 1.88 1.53 0.32 0.66 0.06 0.43 0.01 7.27]\n",
      "Loss_Q: [0.89 1.12 0.25 0.66 0.05 0.44 0.01 0.   3.43] Loss_P: [2.38 1.8  1.59 0.27 0.67 0.04 0.43 0.03 7.23]\n",
      "Loss_Q: [1.03 1.04 0.31 0.67 0.03 0.45 0.03 0.   3.57] Loss_P: [2.36 1.84 1.61 0.29 0.65 0.04 0.43 0.01 7.23]\n",
      "Loss_Q: [0.92 1.13 0.28 0.68 0.04 0.45 0.03 0.   3.53] Loss_P: [2.38 1.88 1.53 0.28 0.69 0.04 0.43 0.03 7.26]\n",
      "Loss_Q: [0.92 1.07 0.27 0.65 0.04 0.43 0.02 0.   3.39] Loss_P: [2.38 1.83 1.58 0.3  0.67 0.04 0.44 0.03 7.26]\n",
      "Loss_Q: [0.92 1.09 0.3  0.68 0.05 0.43 0.02 0.   3.5 ] Loss_P: [2.37 1.83 1.56 0.3  0.7  0.05 0.44 0.01 7.27]\n",
      "Loss_Q: [0.95 1.09 0.29 0.66 0.04 0.47 0.02 0.   3.51] Loss_P: [2.4  1.85 1.51 0.29 0.69 0.04 0.42 0.02 7.23]\n",
      "Loss_Q: [0.9  0.98 0.25 0.69 0.06 0.46 0.02 0.   3.35] Loss_P: [2.36 1.89 1.55 0.27 0.69 0.04 0.44 0.02 7.27]\n",
      "Loss_Q: [0.88 1.04 0.31 0.67 0.05 0.46 0.03 0.   3.44] Loss_P: [2.39 1.85 1.53 0.29 0.68 0.05 0.44 0.02 7.26]\n",
      "Loss_Q: [0.93 1.06 0.3  0.69 0.05 0.42 0.02 0.   3.47] Loss_P: [2.36 1.86 1.51 0.29 0.72 0.05 0.42 0.03 7.25]\n",
      "Loss_Q: [0.93 1.07 0.28 0.68 0.04 0.43 0.03 0.   3.47] Loss_P: [2.41 1.79 1.59 0.27 0.66 0.04 0.42 0.02 7.2 ]\n",
      "Loss_Q: [0.96 1.1  0.28 0.7  0.05 0.42 0.02 0.   3.52] Loss_P: [2.35 1.81 1.61 0.29 0.69 0.06 0.43 0.01 7.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.92 1.09 0.25 0.71 0.04 0.44 0.02 0.   3.46] Loss_P: [2.41 1.86 1.54 0.29 0.69 0.04 0.43 0.02 7.29]\n",
      "Loss_Q: [0.89 1.08 0.3  0.7  0.03 0.43 0.03 0.   3.46] Loss_P: [2.48 1.86 1.54 0.33 0.66 0.03 0.44 0.02 7.36]\n",
      "Loss_Q: [0.95 1.07 0.3  0.7  0.06 0.47 0.02 0.   3.57] Loss_P: [2.43 1.82 1.65 0.34 0.69 0.06 0.44 0.01 7.43]\n",
      "Loss_Q: [0.97 1.03 0.33 0.7  0.05 0.46 0.03 0.   3.57] Loss_P: [2.43 1.8  1.58 0.32 0.68 0.05 0.45 0.03 7.33]\n",
      "Loss_Q: [0.95 1.09 0.28 0.7  0.03 0.46 0.02 0.   3.52] Loss_P: [2.43 1.79 1.64 0.34 0.64 0.05 0.48 0.03 7.4 ]\n",
      "Loss_Q: [0.87 1.13 0.32 0.67 0.06 0.43 0.02 0.   3.49] Loss_P: [2.4  1.87 1.68 0.33 0.71 0.05 0.46 0.03 7.53]\n",
      "Loss_Q: [0.98 1.15 0.3  0.68 0.06 0.42 0.03 0.   3.62] Loss_P: [2.4  1.82 1.62 0.33 0.69 0.05 0.42 0.01 7.36]\n",
      "Loss_Q: [0.92 1.11 0.33 0.7  0.05 0.45 0.02 0.   3.59] Loss_P: [2.34 1.79 1.65 0.34 0.7  0.07 0.44 0.03 7.36]\n",
      "Loss_Q: [0.99 1.07 0.35 0.7  0.05 0.47 0.03 0.   3.65] Loss_P: [2.36 1.82 1.58 0.34 0.69 0.03 0.46 0.02 7.3 ]\n",
      "Loss_Q: [0.91 1.06 0.33 0.71 0.04 0.44 0.03 0.   3.53] Loss_P: [2.35 1.82 1.69 0.39 0.68 0.05 0.48 0.02 7.48]\n",
      "Loss_Q: [0.89 1.1  0.29 0.67 0.05 0.45 0.02 0.   3.47] Loss_P: [2.4  1.78 1.63 0.33 0.69 0.04 0.46 0.02 7.36]\n",
      "Loss_Q: [0.98 1.04 0.36 0.67 0.05 0.46 0.01 0.   3.57] Loss_P: [2.4  1.82 1.63 0.42 0.69 0.03 0.46 0.02 7.47]\n",
      "Loss_Q: [0.96 1.08 0.35 0.69 0.03 0.46 0.02 0.   3.6 ] Loss_P: [2.39 1.84 1.61 0.39 0.7  0.07 0.46 0.02 7.48]\n",
      "Loss_Q: [0.92 1.09 0.35 0.7  0.05 0.48 0.03 0.   3.61] Loss_P: [2.32 1.88 1.58 0.36 0.73 0.05 0.47 0.02 7.42]\n",
      "Loss_Q: [0.92 1.08 0.36 0.7  0.03 0.45 0.02 0.   3.58] Loss_P: [2.37 1.91 1.56 0.36 0.7  0.05 0.47 0.03 7.44]\n",
      "Loss_Q: [0.93 1.02 0.35 0.69 0.04 0.46 0.02 0.   3.5 ] Loss_P: [2.39 1.92 1.57 0.39 0.74 0.04 0.44 0.03 7.51]\n",
      "Loss_Q: [0.94 1.   0.35 0.68 0.05 0.46 0.01 0.   3.49] Loss_P: [2.32 1.91 1.59 0.37 0.72 0.04 0.46 0.01 7.43]\n",
      "Loss_Q: [0.99 1.08 0.37 0.68 0.05 0.46 0.03 0.   3.66] Loss_P: [2.38 1.86 1.58 0.35 0.71 0.04 0.46 0.02 7.42]\n",
      "Loss_Q: [0.99 0.99 0.3  0.71 0.05 0.49 0.04 0.   3.56] Loss_P: [2.37 1.87 1.57 0.35 0.7  0.05 0.47 0.02 7.4 ]\n",
      "Loss_Q: [0.9  1.08 0.36 0.68 0.06 0.49 0.03 0.   3.6 ] Loss_P: [2.46 1.78 1.5  0.4  0.73 0.05 0.49 0.01 7.42]\n",
      "Loss_Q: [0.87 1.07 0.35 0.71 0.04 0.48 0.02 0.   3.55] Loss_P: [2.37 1.82 1.56 0.38 0.74 0.06 0.48 0.03 7.43]\n",
      "Loss_Q: [0.91 1.09 0.4  0.74 0.05 0.47 0.03 0.   3.68] Loss_P: [2.34 1.85 1.63 0.37 0.74 0.05 0.48 0.02 7.47]\n",
      "Loss_Q: [0.9  1.02 0.36 0.69 0.05 0.47 0.04 0.   3.53] Loss_P: [2.42 1.77 1.58 0.35 0.7  0.06 0.48 0.02 7.38]\n",
      "Loss_Q: [0.92 1.12 0.3  0.69 0.05 0.47 0.02 0.   3.58] Loss_P: [2.37 1.84 1.62 0.39 0.73 0.05 0.45 0.03 7.48]\n",
      "Loss_Q: [0.96 1.06 0.35 0.67 0.06 0.48 0.02 0.   3.59] Loss_P: [2.43 1.77 1.57 0.38 0.71 0.05 0.45 0.03 7.39]\n",
      "Loss_Q: [0.94 1.01 0.37 0.69 0.05 0.44 0.01 0.   3.5 ] Loss_P: [2.44 1.71 1.58 0.38 0.72 0.04 0.47 0.02 7.35]\n",
      "Loss_Q: [0.91 1.01 0.38 0.72 0.07 0.46 0.02 0.   3.57] Loss_P: [2.38 1.82 1.61 0.42 0.71 0.05 0.48 0.02 7.49]\n",
      "Loss_Q: [0.96 1.06 0.43 0.69 0.04 0.45 0.03 0.   3.65] Loss_P: [2.4  1.77 1.69 0.39 0.74 0.04 0.44 0.02 7.48]\n",
      "Loss_Q: [0.88 1.08 0.37 0.71 0.04 0.47 0.02 0.   3.58] Loss_P: [2.39 1.76 1.67 0.39 0.74 0.03 0.44 0.02 7.45]\n",
      "Loss_Q: [0.91 1.09 0.39 0.71 0.05 0.47 0.02 0.   3.64] Loss_P: [2.44 1.74 1.63 0.36 0.72 0.04 0.43 0.02 7.39]\n",
      "Loss_Q: [0.96 1.12 0.34 0.69 0.05 0.46 0.03 0.   3.66] Loss_P: [2.44 1.75 1.67 0.36 0.67 0.03 0.44 0.03 7.39]\n",
      "Loss_Q: [0.89 1.07 0.32 0.73 0.08 0.45 0.02 0.   3.56] Loss_P: [2.38 1.81 1.71 0.45 0.73 0.05 0.44 0.02 7.59]\n",
      "Loss_Q: [0.92 1.06 0.35 0.68 0.04 0.45 0.03 0.   3.53] Loss_P: [2.42 1.73 1.63 0.4  0.72 0.07 0.43 0.02 7.41]\n",
      "Loss_Q: [0.91 1.17 0.38 0.71 0.05 0.47 0.02 0.   3.71] Loss_P: [2.39 1.74 1.77 0.41 0.69 0.04 0.46 0.02 7.51]\n",
      "Loss_Q: [0.92 1.09 0.37 0.71 0.04 0.45 0.03 0.   3.62] Loss_P: [2.42 1.72 1.7  0.39 0.72 0.06 0.47 0.03 7.52]\n",
      "Loss_Q: [0.89 1.11 0.33 0.74 0.05 0.47 0.01 0.   3.61] Loss_P: [2.41 1.74 1.7  0.36 0.72 0.06 0.46 0.01 7.48]\n",
      "Loss_Q: [0.9  1.12 0.38 0.68 0.06 0.46 0.01 0.   3.61] Loss_P: [2.44 1.77 1.69 0.41 0.73 0.1  0.44 0.02 7.6 ]\n",
      "Loss_Q: [0.93 1.09 0.39 0.73 0.06 0.45 0.02 0.   3.68] Loss_P: [2.41 1.7  1.76 0.41 0.72 0.05 0.45 0.01 7.52]\n",
      "Loss_Q: [0.82 1.06 0.37 0.72 0.04 0.47 0.03 0.   3.51] Loss_P: [2.44 1.68 1.66 0.42 0.73 0.05 0.46 0.02 7.47]\n",
      "Loss_Q: [0.86 1.08 0.4  0.72 0.06 0.45 0.01 0.   3.59] Loss_P: [2.43 1.75 1.67 0.47 0.7  0.04 0.45 0.03 7.54]\n",
      "Loss_Q: [0.89 1.04 0.4  0.72 0.04 0.43 0.02 0.   3.54] Loss_P: [2.43 1.73 1.66 0.41 0.72 0.04 0.44 0.01 7.43]\n",
      "Loss_Q: [0.94 1.06 0.41 0.76 0.05 0.43 0.02 0.   3.67] Loss_P: [2.45 1.69 1.65 0.4  0.73 0.04 0.43 0.01 7.41]\n",
      "Loss_Q: [0.92 1.08 0.44 0.71 0.04 0.43 0.03 0.   3.65] Loss_P: [2.45 1.7  1.63 0.43 0.72 0.04 0.44 0.04 7.43]\n",
      "Loss_Q: [1.   1.05 0.4  0.73 0.06 0.42 0.02 0.   3.67] Loss_P: [2.39 1.73 1.64 0.43 0.73 0.06 0.42 0.03 7.43]\n",
      "Loss_Q: [0.85 1.12 0.4  0.71 0.04 0.44 0.03 0.   3.59] Loss_P: [2.46 1.74 1.65 0.39 0.7  0.04 0.42 0.02 7.42]\n",
      "Loss_Q: [0.93 1.06 0.33 0.72 0.05 0.46 0.03 0.   3.57] Loss_P: [2.4  1.67 1.65 0.4  0.71 0.05 0.48 0.01 7.38]\n",
      "Loss_Q: [0.87 1.05 0.31 0.72 0.07 0.44 0.03 0.   3.5 ] Loss_P: [2.47 1.74 1.69 0.42 0.71 0.04 0.41 0.02 7.51]\n",
      "Loss_Q: [0.91 1.14 0.42 0.72 0.04 0.42 0.02 0.   3.69] Loss_P: [2.44 1.77 1.64 0.36 0.74 0.05 0.42 0.02 7.43]\n",
      "Loss_Q: [0.92 1.1  0.39 0.7  0.03 0.43 0.02 0.   3.58] Loss_P: [2.43 1.77 1.7  0.39 0.71 0.05 0.4  0.02 7.45]\n",
      "Loss_Q: [0.9  1.14 0.38 0.66 0.04 0.43 0.02 0.   3.57] Loss_P: [2.43 1.73 1.71 0.39 0.69 0.03 0.44 0.04 7.46]\n",
      "Loss_Q: [0.91 1.11 0.34 0.67 0.05 0.42 0.02 0.   3.53] Loss_P: [2.4  1.74 1.67 0.38 0.7  0.04 0.43 0.03 7.4 ]\n",
      "Loss_Q: [0.89 1.11 0.37 0.63 0.06 0.42 0.03 0.   3.51] Loss_P: [2.43 1.74 1.68 0.41 0.66 0.05 0.41 0.02 7.41]\n",
      "Loss_Q: [0.87 1.1  0.34 0.65 0.04 0.38 0.01 0.   3.4 ] Loss_P: [2.37 1.71 1.71 0.39 0.68 0.03 0.42 0.02 7.34]\n",
      "Loss_Q: [0.87 1.21 0.39 0.72 0.03 0.41 0.01 0.   3.64] Loss_P: [2.41 1.76 1.7  0.43 0.75 0.05 0.42 0.01 7.52]\n",
      "Loss_Q: [0.87 1.11 0.38 0.69 0.05 0.41 0.01 0.   3.51] Loss_P: [2.43 1.78 1.57 0.41 0.72 0.04 0.4  0.02 7.35]\n",
      "Loss_Q: [0.81 1.02 0.36 0.69 0.05 0.4  0.01 0.   3.34] Loss_P: [2.47 1.67 1.56 0.39 0.71 0.05 0.4  0.02 7.26]\n",
      "Loss_Q: [0.86 1.12 0.39 0.65 0.04 0.42 0.01 0.   3.5 ] Loss_P: [2.41 1.78 1.63 0.39 0.71 0.05 0.43 0.02 7.42]\n",
      "Loss_Q: [0.76 1.1  0.34 0.68 0.06 0.42 0.02 0.   3.38] Loss_P: [2.4  1.68 1.61 0.34 0.68 0.05 0.43 0.01 7.21]\n",
      "Loss_Q: [0.84 1.07 0.38 0.65 0.04 0.44 0.04 0.   3.45] Loss_P: [2.43 1.69 1.62 0.43 0.71 0.03 0.45 0.02 7.39]\n",
      "Loss_Q: [0.89 1.06 0.37 0.68 0.07 0.43 0.02 0.   3.52] Loss_P: [2.43 1.71 1.61 0.41 0.7  0.05 0.43 0.02 7.37]\n",
      "Loss_Q: [0.9  1.11 0.42 0.7  0.06 0.45 0.02 0.   3.67] Loss_P: [2.42 1.67 1.61 0.43 0.71 0.05 0.44 0.03 7.35]\n",
      "Loss_Q: [0.85 1.05 0.39 0.67 0.04 0.46 0.02 0.   3.48] Loss_P: [2.43 1.75 1.57 0.42 0.68 0.03 0.42 0.02 7.32]\n",
      "Loss_Q: [0.87 1.05 0.37 0.67 0.04 0.45 0.01 0.   3.46] Loss_P: [2.41 1.76 1.61 0.4  0.69 0.05 0.45 0.01 7.38]\n",
      "Loss_Q: [0.89 1.04 0.37 0.7  0.06 0.46 0.02 0.   3.54] Loss_P: [2.44 1.7  1.58 0.37 0.67 0.04 0.44 0.02 7.26]\n",
      "Loss_Q: [0.86 1.05 0.33 0.71 0.05 0.48 0.03 0.   3.5 ] Loss_P: [2.47 1.73 1.61 0.39 0.7  0.03 0.48 0.02 7.43]\n",
      "Loss_Q: [0.86 1.09 0.36 0.7  0.05 0.46 0.03 0.   3.54] Loss_P: [2.41 1.76 1.56 0.33 0.7  0.04 0.46 0.02 7.28]\n",
      "Loss_Q: [0.88 1.06 0.33 0.74 0.06 0.46 0.01 0.   3.55] Loss_P: [2.42 1.69 1.6  0.33 0.69 0.05 0.49 0.02 7.29]\n",
      "Loss_Q: [0.88 1.05 0.29 0.68 0.04 0.48 0.02 0.   3.43] Loss_P: [2.45 1.79 1.61 0.34 0.69 0.02 0.49 0.03 7.41]\n",
      "Loss_Q: [0.9  1.02 0.33 0.68 0.04 0.48 0.03 0.   3.49] Loss_P: [2.36 1.69 1.66 0.37 0.67 0.04 0.49 0.03 7.32]\n",
      "Loss_Q: [0.87 1.08 0.39 0.73 0.04 0.47 0.02 0.   3.6 ] Loss_P: [2.37 1.7  1.61 0.36 0.69 0.06 0.47 0.01 7.27]\n",
      "Loss_Q: [0.88 1.04 0.37 0.69 0.03 0.47 0.02 0.   3.5 ] Loss_P: [2.38 1.73 1.62 0.35 0.72 0.05 0.47 0.01 7.34]\n",
      "Loss_Q: [0.87 1.01 0.36 0.69 0.05 0.46 0.02 0.   3.46] Loss_P: [2.38 1.72 1.66 0.37 0.66 0.04 0.44 0.03 7.3 ]\n",
      "Loss_Q: [0.92 1.13 0.36 0.7  0.03 0.44 0.01 0.   3.61] Loss_P: [2.33 1.77 1.65 0.35 0.7  0.03 0.45 0.02 7.3 ]\n",
      "Loss_Q: [0.87 1.15 0.4  0.68 0.04 0.47 0.02 0.   3.62] Loss_P: [2.36 1.77 1.73 0.37 0.69 0.04 0.46 0.02 7.44]\n",
      "Loss_Q: [0.99 1.14 0.3  0.7  0.05 0.44 0.02 0.   3.63] Loss_P: [2.37 1.73 1.8  0.33 0.69 0.03 0.44 0.01 7.4 ]\n",
      "Loss_Q: [0.9  1.11 0.34 0.67 0.03 0.45 0.02 0.   3.51] Loss_P: [2.38 1.73 1.65 0.35 0.69 0.06 0.43 0.01 7.3 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.88 1.14 0.39 0.75 0.06 0.45 0.01 0.   3.67] Loss_P: [2.39 1.77 1.75 0.39 0.7  0.05 0.45 0.02 7.52]\n",
      "Loss_Q: [0.99 1.11 0.38 0.65 0.03 0.44 0.02 0.   3.62] Loss_P: [2.41 1.72 1.73 0.37 0.67 0.03 0.45 0.01 7.39]\n",
      "Loss_Q: [0.91 1.13 0.36 0.71 0.06 0.41 0.02 0.   3.61] Loss_P: [2.32 1.74 1.72 0.38 0.71 0.04 0.45 0.02 7.38]\n",
      "Loss_Q: [0.96 1.12 0.39 0.68 0.04 0.44 0.02 0.   3.65] Loss_P: [2.37 1.77 1.74 0.38 0.69 0.04 0.44 0.02 7.45]\n",
      "Loss_Q: [0.97 1.08 0.35 0.69 0.05 0.44 0.02 0.   3.61] Loss_P: [2.39 1.72 1.76 0.4  0.7  0.05 0.43 0.01 7.48]\n",
      "Loss_Q: [0.95 1.06 0.34 0.71 0.05 0.44 0.02 0.   3.57] Loss_P: [2.4  1.69 1.73 0.41 0.67 0.03 0.43 0.01 7.36]\n",
      "Loss_Q: [0.95 1.12 0.39 0.69 0.05 0.45 0.02 0.   3.66] Loss_P: [2.36 1.74 1.78 0.39 0.71 0.05 0.45 0.03 7.51]\n",
      "Loss_Q: [0.91 1.14 0.35 0.71 0.04 0.45 0.02 0.   3.63] Loss_P: [2.32 1.78 1.78 0.36 0.7  0.06 0.49 0.01 7.5 ]\n",
      "Loss_Q: [0.96 1.13 0.36 0.71 0.03 0.45 0.02 0.   3.67] Loss_P: [2.36 1.79 1.77 0.37 0.68 0.04 0.46 0.02 7.49]\n",
      "Loss_Q: [0.94 1.05 0.31 0.65 0.04 0.43 0.02 0.   3.44] Loss_P: [2.42 1.68 1.76 0.38 0.67 0.04 0.45 0.01 7.4 ]\n",
      "Loss_Q: [0.91 1.06 0.37 0.69 0.06 0.45 0.01 0.   3.55] Loss_P: [2.33 1.73 1.76 0.38 0.66 0.04 0.43 0.02 7.35]\n",
      "Loss_Q: [0.87 1.08 0.38 0.7  0.05 0.46 0.03 0.   3.56] Loss_P: [2.36 1.7  1.74 0.4  0.66 0.03 0.44 0.02 7.35]\n",
      "Loss_Q: [0.94 1.06 0.36 0.67 0.04 0.42 0.01 0.   3.49] Loss_P: [2.35 1.75 1.77 0.38 0.71 0.03 0.44 0.02 7.46]\n",
      "Loss_Q: [0.91 1.12 0.35 0.69 0.05 0.43 0.02 0.   3.57] Loss_P: [2.43 1.71 1.72 0.37 0.71 0.03 0.46 0.04 7.48]\n",
      "Loss_Q: [0.84 1.12 0.39 0.67 0.05 0.43 0.02 0.   3.52] Loss_P: [2.38 1.72 1.73 0.38 0.71 0.03 0.45 0.01 7.42]\n",
      "Loss_Q: [0.9  1.06 0.35 0.7  0.06 0.45 0.02 0.   3.55] Loss_P: [2.4  1.69 1.71 0.38 0.71 0.03 0.45 0.01 7.37]\n",
      "Loss_Q: [0.88 1.07 0.41 0.7  0.04 0.45 0.02 0.   3.57] Loss_P: [2.4  1.73 1.76 0.4  0.7  0.05 0.43 0.01 7.48]\n",
      "Loss_Q: [0.88 1.08 0.39 0.71 0.06 0.41 0.01 0.   3.54] Loss_P: [2.36 1.72 1.75 0.4  0.68 0.02 0.43 0.02 7.37]\n",
      "Loss_Q: [0.88 1.05 0.38 0.72 0.04 0.42 0.03 0.   3.51] Loss_P: [2.36 1.66 1.72 0.43 0.72 0.04 0.4  0.02 7.33]\n",
      "Loss_Q: [0.88 1.1  0.38 0.66 0.05 0.41 0.03 0.   3.51] Loss_P: [2.35 1.78 1.73 0.37 0.7  0.06 0.44 0.01 7.44]\n",
      "Loss_Q: [0.96 1.09 0.4  0.65 0.04 0.42 0.02 0.   3.58] Loss_P: [2.4  1.64 1.71 0.37 0.67 0.04 0.4  0.01 7.23]\n",
      "Loss_Q: [0.98 1.07 0.37 0.63 0.03 0.43 0.01 0.   3.53] Loss_P: [2.37 1.74 1.73 0.38 0.67 0.05 0.43 0.02 7.38]\n",
      "Loss_Q: [0.89 1.14 0.39 0.65 0.05 0.44 0.02 0.   3.58] Loss_P: [2.4  1.71 1.7  0.37 0.64 0.02 0.4  0.02 7.26]\n",
      "Loss_Q: [0.88 1.01 0.32 0.62 0.06 0.44 0.01 0.   3.34] Loss_P: [2.37 1.71 1.72 0.4  0.63 0.05 0.41 0.02 7.31]\n",
      "Loss_Q: [0.87 1.03 0.34 0.66 0.05 0.41 0.02 0.   3.37] Loss_P: [2.39 1.65 1.67 0.36 0.6  0.04 0.42 0.03 7.15]\n",
      "Loss_Q: [0.93 1.05 0.42 0.67 0.03 0.42 0.01 0.   3.54] Loss_P: [2.43 1.69 1.66 0.35 0.64 0.04 0.41 0.01 7.23]\n",
      "Loss_Q: [0.95 1.11 0.35 0.63 0.04 0.42 0.02 0.   3.52] Loss_P: [2.35 1.7  1.73 0.37 0.6  0.04 0.42 0.01 7.24]\n",
      "Loss_Q: [0.96 1.04 0.4  0.58 0.04 0.43 0.02 0.   3.48] Loss_P: [2.37 1.71 1.73 0.38 0.66 0.02 0.41 0.01 7.29]\n",
      "Loss_Q: [0.87 1.15 0.4  0.58 0.02 0.4  0.02 0.   3.44] Loss_P: [2.41 1.72 1.71 0.35 0.61 0.03 0.42 0.02 7.26]\n",
      "Loss_Q: [0.9  1.07 0.39 0.61 0.02 0.43 0.02 0.   3.45] Loss_P: [2.39 1.65 1.73 0.4  0.59 0.03 0.42 0.02 7.23]\n",
      "Loss_Q: [0.89 1.16 0.39 0.61 0.04 0.39 0.02 0.   3.51] Loss_P: [2.4  1.7  1.74 0.4  0.59 0.04 0.41 0.03 7.31]\n",
      "Loss_Q: [0.86 1.06 0.39 0.63 0.04 0.4  0.02 0.   3.4 ] Loss_P: [2.42 1.71 1.74 0.43 0.62 0.04 0.36 0.03 7.34]\n",
      "Loss_Q: [0.86 1.07 0.37 0.58 0.02 0.38 0.01 0.   3.3 ] Loss_P: [2.41 1.65 1.75 0.4  0.64 0.05 0.39 0.02 7.3 ]\n",
      "Loss_Q: [0.98 1.13 0.36 0.59 0.03 0.37 0.02 0.   3.49] Loss_P: [2.37 1.72 1.78 0.41 0.63 0.04 0.38 0.02 7.35]\n",
      "Loss_Q: [0.91 1.09 0.37 0.56 0.04 0.36 0.02 0.   3.35] Loss_P: [2.35 1.71 1.76 0.38 0.63 0.02 0.37 0.03 7.25]\n",
      "Loss_Q: [0.94 1.11 0.33 0.61 0.04 0.36 0.01 0.   3.41] Loss_P: [2.41 1.71 1.76 0.39 0.62 0.06 0.37 0.02 7.34]\n",
      "Loss_Q: [0.9  1.16 0.38 0.59 0.04 0.38 0.02 0.   3.47] Loss_P: [2.41 1.71 1.73 0.42 0.65 0.05 0.38 0.01 7.38]\n",
      "Loss_Q: [0.92 1.15 0.4  0.62 0.05 0.37 0.02 0.   3.53] Loss_P: [2.34 1.84 1.74 0.43 0.69 0.04 0.37 0.02 7.46]\n",
      "Loss_Q: [0.94 1.1  0.36 0.64 0.04 0.38 0.02 0.   3.47] Loss_P: [2.41 1.7  1.68 0.43 0.67 0.05 0.37 0.01 7.32]\n",
      "Loss_Q: [0.93 1.08 0.4  0.67 0.04 0.41 0.03 0.   3.56] Loss_P: [2.33 1.79 1.78 0.42 0.67 0.05 0.41 0.01 7.47]\n",
      "Loss_Q: [0.91 1.11 0.4  0.67 0.03 0.42 0.03 0.   3.55] Loss_P: [2.4  1.71 1.71 0.45 0.67 0.03 0.42 0.01 7.39]\n",
      "Loss_Q: [0.91 1.08 0.4  0.69 0.06 0.43 0.01 0.   3.59] Loss_P: [2.41 1.75 1.67 0.4  0.69 0.03 0.44 0.01 7.41]\n",
      "Loss_Q: [0.96 1.11 0.41 0.66 0.03 0.43 0.02 0.   3.61] Loss_P: [2.39 1.75 1.71 0.47 0.71 0.04 0.41 0.02 7.5 ]\n",
      "Loss_Q: [0.93 1.11 0.38 0.66 0.04 0.41 0.02 0.   3.55] Loss_P: [2.39 1.77 1.77 0.41 0.68 0.06 0.43 0.03 7.52]\n",
      "Loss_Q: [1.03 1.09 0.41 0.68 0.04 0.43 0.01 0.   3.7 ] Loss_P: [2.36 1.73 1.76 0.46 0.69 0.04 0.43 0.02 7.49]\n",
      "Loss_Q: [1.   1.08 0.38 0.69 0.04 0.4  0.02 0.   3.61] Loss_P: [2.37 1.83 1.76 0.41 0.68 0.03 0.4  0.02 7.48]\n",
      "Loss_Q: [0.95 1.09 0.41 0.69 0.05 0.4  0.01 0.   3.6 ] Loss_P: [2.39 1.73 1.77 0.39 0.68 0.06 0.41 0.01 7.44]\n",
      "Loss_Q: [0.95 1.1  0.39 0.66 0.04 0.36 0.02 0.   3.53] Loss_P: [2.33 1.78 1.75 0.45 0.72 0.06 0.39 0.02 7.5 ]\n",
      "Loss_Q: [0.96 1.11 0.48 0.69 0.06 0.36 0.02 0.   3.68] Loss_P: [2.32 1.86 1.75 0.48 0.73 0.05 0.41 0.02 7.62]\n",
      "Loss_Q: [0.98 1.1  0.41 0.66 0.05 0.38 0.02 0.   3.61] Loss_P: [2.36 1.8  1.76 0.46 0.7  0.03 0.4  0.02 7.52]\n",
      "Loss_Q: [0.91 1.16 0.43 0.67 0.04 0.41 0.02 0.   3.64] Loss_P: [2.36 1.79 1.77 0.47 0.69 0.04 0.37 0.01 7.52]\n",
      "Loss_Q: [0.95 1.17 0.39 0.67 0.05 0.38 0.01 0.   3.62] Loss_P: [2.39 1.78 1.76 0.45 0.68 0.06 0.39 0.01 7.51]\n",
      "Loss_Q: [0.93 1.13 0.42 0.69 0.04 0.39 0.02 0.   3.62] Loss_P: [2.37 1.74 1.71 0.5  0.72 0.05 0.36 0.03 7.47]\n",
      "Loss_Q: [1.   1.15 0.44 0.71 0.05 0.36 0.01 0.   3.72] Loss_P: [2.34 1.79 1.78 0.45 0.71 0.05 0.39 0.01 7.53]\n",
      "Loss_Q: [0.99 1.11 0.42 0.7  0.05 0.38 0.02 0.   3.66] Loss_P: [2.37 1.75 1.78 0.45 0.74 0.07 0.4  0.02 7.58]\n",
      "Loss_Q: [0.89 1.14 0.4  0.69 0.07 0.37 0.02 0.   3.58] Loss_P: [2.38 1.76 1.78 0.45 0.7  0.04 0.36 0.02 7.48]\n",
      "Loss_Q: [0.97 1.2  0.45 0.73 0.05 0.32 0.01 0.   3.72] Loss_P: [2.39 1.77 1.76 0.43 0.75 0.05 0.34 0.01 7.48]\n",
      "Loss_Q: [0.95 1.11 0.39 0.68 0.04 0.3  0.01 0.   3.48] Loss_P: [2.37 1.72 1.73 0.46 0.74 0.05 0.33 0.02 7.43]\n",
      "Loss_Q: [0.96 1.11 0.41 0.73 0.04 0.32 0.02 0.   3.59] Loss_P: [2.34 1.81 1.75 0.46 0.7  0.06 0.31 0.01 7.44]\n",
      "Loss_Q: [0.89 1.12 0.44 0.7  0.06 0.35 0.01 0.   3.56] Loss_P: [2.33 1.79 1.71 0.5  0.71 0.06 0.36 0.02 7.48]\n",
      "Loss_Q: [0.85 1.07 0.44 0.64 0.06 0.37 0.02 0.   3.47] Loss_P: [2.33 1.76 1.73 0.46 0.67 0.04 0.35 0.02 7.36]\n",
      "Loss_Q: [0.92 1.08 0.37 0.68 0.04 0.36 0.02 0.   3.47] Loss_P: [2.35 1.8  1.75 0.44 0.71 0.05 0.33 0.02 7.44]\n",
      "Loss_Q: [0.91 1.08 0.4  0.67 0.04 0.36 0.02 0.   3.49] Loss_P: [2.37 1.74 1.7  0.41 0.68 0.04 0.37 0.01 7.32]\n",
      "Loss_Q: [0.88 1.12 0.4  0.66 0.04 0.4  0.03 0.   3.51] Loss_P: [2.45 1.69 1.66 0.43 0.7  0.04 0.39 0.02 7.39]\n",
      "Loss_Q: [0.97 1.07 0.44 0.72 0.05 0.38 0.02 0.   3.64] Loss_P: [2.31 1.75 1.7  0.39 0.65 0.04 0.4  0.03 7.27]\n",
      "Loss_Q: [0.88 1.09 0.37 0.68 0.05 0.38 0.02 0.   3.47] Loss_P: [2.41 1.77 1.7  0.47 0.68 0.05 0.38 0.03 7.48]\n",
      "Loss_Q: [0.94 1.11 0.36 0.66 0.04 0.37 0.02 0.   3.49] Loss_P: [2.36 1.78 1.76 0.4  0.7  0.05 0.37 0.02 7.45]\n",
      "Loss_Q: [0.93 1.12 0.39 0.69 0.04 0.37 0.02 0.   3.56] Loss_P: [2.38 1.7  1.74 0.39 0.67 0.05 0.37 0.03 7.32]\n",
      "Loss_Q: [0.89 1.12 0.39 0.69 0.06 0.36 0.01 0.   3.52] Loss_P: [2.4  1.7  1.7  0.41 0.76 0.05 0.38 0.02 7.4 ]\n",
      "Loss_Q: [0.81 1.07 0.37 0.71 0.05 0.38 0.03 0.   3.42] Loss_P: [2.38 1.79 1.72 0.44 0.72 0.04 0.37 0.02 7.48]\n",
      "Loss_Q: [0.94 1.13 0.42 0.72 0.05 0.39 0.01 0.   3.66] Loss_P: [2.34 1.72 1.72 0.44 0.73 0.06 0.4  0.02 7.43]\n",
      "Loss_Q: [0.84 1.08 0.37 0.73 0.05 0.39 0.02 0.   3.48] Loss_P: [2.44 1.74 1.7  0.43 0.72 0.07 0.37 0.03 7.49]\n",
      "Loss_Q: [0.94 1.16 0.39 0.73 0.06 0.39 0.01 0.   3.67] Loss_P: [2.4  1.7  1.73 0.45 0.77 0.06 0.4  0.02 7.52]\n",
      "Loss_Q: [0.86 1.08 0.39 0.7  0.04 0.41 0.03 0.   3.51] Loss_P: [2.38 1.78 1.76 0.39 0.7  0.03 0.39 0.01 7.45]\n",
      "Loss_Q: [0.89 1.11 0.41 0.68 0.05 0.42 0.02 0.   3.59] Loss_P: [2.4  1.72 1.73 0.37 0.7  0.05 0.4  0.03 7.4 ]\n",
      "Loss_Q: [0.91 1.14 0.4  0.72 0.05 0.41 0.01 0.   3.63] Loss_P: [2.39 1.72 1.73 0.44 0.73 0.06 0.44 0.02 7.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.99 1.16 0.44 0.73 0.05 0.4  0.02 0.   3.79] Loss_P: [2.35 1.71 1.73 0.5  0.73 0.03 0.41 0.02 7.48]\n",
      "Loss_Q: [0.9  1.1  0.42 0.75 0.07 0.42 0.02 0.   3.68] Loss_P: [2.32 1.79 1.69 0.41 0.77 0.03 0.4  0.01 7.42]\n",
      "Loss_Q: [0.94 1.08 0.36 0.7  0.05 0.42 0.03 0.   3.58] Loss_P: [2.37 1.74 1.72 0.46 0.73 0.05 0.41 0.01 7.49]\n",
      "Loss_Q: [0.86 1.1  0.41 0.77 0.05 0.42 0.01 0.   3.61] Loss_P: [2.37 1.76 1.7  0.4  0.73 0.06 0.42 0.02 7.45]\n",
      "Loss_Q: [0.85 1.11 0.44 0.69 0.05 0.4  0.02 0.   3.55] Loss_P: [2.4  1.73 1.69 0.42 0.72 0.05 0.42 0.03 7.47]\n",
      "Loss_Q: [0.89 1.06 0.42 0.69 0.05 0.42 0.01 0.   3.54] Loss_P: [2.35 1.73 1.72 0.42 0.7  0.07 0.42 0.03 7.42]\n",
      "Loss_Q: [0.81 1.11 0.4  0.73 0.05 0.39 0.03 0.   3.53] Loss_P: [2.39 1.65 1.69 0.45 0.71 0.04 0.38 0.01 7.34]\n",
      "Loss_Q: [0.89 1.06 0.37 0.69 0.05 0.4  0.02 0.   3.46] Loss_P: [2.42 1.65 1.7  0.4  0.7  0.04 0.41 0.01 7.32]\n",
      "Loss_Q: [0.85 1.03 0.37 0.71 0.05 0.42 0.01 0.   3.45] Loss_P: [2.42 1.69 1.69 0.4  0.7  0.04 0.4  0.01 7.36]\n",
      "Loss_Q: [0.92 1.08 0.35 0.73 0.05 0.4  0.02 0.   3.54] Loss_P: [2.45 1.74 1.67 0.42 0.74 0.06 0.39 0.03 7.48]\n",
      "Loss_Q: [0.91 1.09 0.38 0.72 0.04 0.4  0.02 0.   3.56] Loss_P: [2.34 1.73 1.7  0.41 0.73 0.05 0.42 0.03 7.39]\n",
      "Loss_Q: [0.97 1.09 0.4  0.72 0.05 0.4  0.02 0.   3.64] Loss_P: [2.37 1.79 1.67 0.4  0.69 0.04 0.4  0.01 7.37]\n",
      "Loss_Q: [0.88 1.05 0.38 0.72 0.05 0.42 0.02 0.   3.52] Loss_P: [2.34 1.76 1.71 0.49 0.73 0.04 0.43 0.01 7.51]\n",
      "Loss_Q: [0.87 1.08 0.43 0.72 0.04 0.43 0.01 0.   3.59] Loss_P: [2.37 1.76 1.76 0.39 0.74 0.03 0.44 0.01 7.51]\n",
      "Loss_Q: [0.88 1.13 0.36 0.72 0.05 0.45 0.01 0.   3.61] Loss_P: [2.36 1.74 1.72 0.43 0.72 0.04 0.44 0.02 7.48]\n",
      "Loss_Q: [0.86 1.08 0.39 0.74 0.05 0.44 0.01 0.   3.58] Loss_P: [2.39 1.72 1.7  0.43 0.74 0.06 0.45 0.01 7.48]\n",
      "Loss_Q: [0.83 1.1  0.4  0.68 0.04 0.44 0.02 0.   3.52] Loss_P: [2.39 1.69 1.7  0.41 0.73 0.04 0.44 0.02 7.42]\n",
      "Loss_Q: [0.94 1.06 0.36 0.67 0.04 0.43 0.02 0.   3.53] Loss_P: [2.47 1.69 1.7  0.39 0.68 0.06 0.44 0.03 7.46]\n",
      "Loss_Q: [0.93 1.09 0.41 0.67 0.04 0.41 0.02 0.   3.56] Loss_P: [2.35 1.7  1.7  0.42 0.71 0.05 0.45 0.03 7.41]\n",
      "Loss_Q: [0.87 1.09 0.42 0.69 0.03 0.41 0.02 0.   3.53] Loss_P: [2.38 1.74 1.69 0.5  0.72 0.05 0.42 0.02 7.51]\n",
      "Loss_Q: [0.84 1.03 0.47 0.73 0.05 0.4  0.01 0.   3.54] Loss_P: [2.37 1.74 1.68 0.48 0.74 0.04 0.42 0.02 7.49]\n",
      "Loss_Q: [0.85 1.   0.42 0.7  0.04 0.42 0.02 0.   3.46] Loss_P: [2.41 1.65 1.63 0.43 0.69 0.03 0.41 0.01 7.25]\n",
      "Loss_Q: [0.84 1.11 0.51 0.75 0.06 0.43 0.03 0.   3.71] Loss_P: [2.31 1.73 1.66 0.45 0.69 0.04 0.43 0.02 7.33]\n",
      "Loss_Q: [0.75 1.05 0.39 0.63 0.05 0.44 0.02 0.   3.33] Loss_P: [2.35 1.74 1.65 0.4  0.69 0.06 0.43 0.03 7.35]\n",
      "Loss_Q: [0.84 1.05 0.43 0.65 0.04 0.43 0.03 0.   3.47] Loss_P: [2.4  1.68 1.62 0.42 0.67 0.04 0.43 0.02 7.28]\n",
      "Loss_Q: [0.85 1.01 0.41 0.66 0.05 0.41 0.03 0.   3.42] Loss_P: [2.38 1.73 1.66 0.39 0.7  0.04 0.4  0.03 7.31]\n",
      "Loss_Q: [0.95 1.04 0.4  0.69 0.07 0.43 0.02 0.   3.6 ] Loss_P: [2.39 1.74 1.64 0.4  0.71 0.06 0.41 0.01 7.37]\n",
      "Loss_Q: [0.79 1.04 0.36 0.66 0.05 0.44 0.01 0.   3.35] Loss_P: [2.37 1.72 1.59 0.4  0.67 0.05 0.44 0.03 7.27]\n",
      "Loss_Q: [0.84 1.04 0.41 0.68 0.03 0.45 0.01 0.   3.45] Loss_P: [2.33 1.79 1.66 0.38 0.67 0.04 0.45 0.02 7.35]\n",
      "Loss_Q: [0.87 1.04 0.38 0.67 0.03 0.45 0.02 0.   3.46] Loss_P: [2.35 1.74 1.59 0.37 0.67 0.05 0.46 0.02 7.25]\n",
      "Loss_Q: [0.83 1.05 0.36 0.64 0.05 0.47 0.01 0.   3.4 ] Loss_P: [2.36 1.72 1.6  0.36 0.66 0.05 0.47 0.03 7.24]\n",
      "Loss_Q: [0.86 1.04 0.39 0.64 0.05 0.48 0.03 0.   3.48] Loss_P: [2.34 1.7  1.6  0.43 0.71 0.03 0.47 0.02 7.3 ]\n",
      "Loss_Q: [0.84 1.09 0.41 0.64 0.04 0.46 0.01 0.   3.49] Loss_P: [2.37 1.76 1.56 0.36 0.64 0.04 0.47 0.02 7.23]\n",
      "Loss_Q: [0.85 1.04 0.43 0.7  0.05 0.47 0.02 0.   3.55] Loss_P: [2.36 1.76 1.59 0.49 0.75 0.06 0.45 0.01 7.47]\n",
      "Loss_Q: [0.87 1.15 0.41 0.74 0.04 0.46 0.02 0.   3.69] Loss_P: [2.33 1.79 1.67 0.4  0.72 0.03 0.48 0.03 7.45]\n",
      "Loss_Q: [0.8  1.07 0.46 0.72 0.06 0.5  0.01 0.   3.62] Loss_P: [2.38 1.77 1.72 0.43 0.71 0.04 0.48 0.02 7.54]\n",
      "Loss_Q: [0.85 1.15 0.43 0.7  0.06 0.49 0.02 0.   3.69] Loss_P: [2.41 1.74 1.69 0.45 0.75 0.04 0.51 0.03 7.6 ]\n",
      "Loss_Q: [0.84 1.05 0.42 0.69 0.05 0.49 0.02 0.   3.56] Loss_P: [2.38 1.76 1.71 0.46 0.7  0.03 0.5  0.03 7.56]\n",
      "Loss_Q: [0.86 1.08 0.46 0.74 0.03 0.51 0.01 0.   3.68] Loss_P: [2.36 1.7  1.7  0.48 0.71 0.05 0.51 0.02 7.52]\n",
      "Loss_Q: [0.84 1.07 0.42 0.75 0.05 0.52 0.02 0.   3.67] Loss_P: [2.37 1.7  1.7  0.49 0.73 0.05 0.51 0.02 7.55]\n",
      "Loss_Q: [0.82 1.11 0.46 0.74 0.05 0.5  0.02 0.   3.71] Loss_P: [2.33 1.72 1.69 0.43 0.71 0.05 0.51 0.02 7.46]\n",
      "Loss_Q: [0.87 1.14 0.45 0.7  0.04 0.51 0.02 0.   3.73] Loss_P: [2.41 1.75 1.68 0.49 0.75 0.04 0.51 0.02 7.64]\n",
      "Loss_Q: [0.86 1.1  0.5  0.74 0.03 0.51 0.03 0.   3.77] Loss_P: [2.37 1.77 1.64 0.53 0.7  0.05 0.51 0.01 7.58]\n",
      "Loss_Q: [0.85 1.08 0.43 0.74 0.04 0.51 0.01 0.   3.67] Loss_P: [2.31 1.71 1.63 0.43 0.7  0.05 0.51 0.01 7.36]\n",
      "Loss_Q: [0.88 1.09 0.44 0.73 0.05 0.51 0.02 0.   3.72] Loss_P: [2.38 1.73 1.69 0.53 0.76 0.05 0.5  0.01 7.65]\n",
      "Loss_Q: [0.85 1.11 0.46 0.7  0.04 0.5  0.02 0.   3.68] Loss_P: [2.32 1.74 1.63 0.46 0.74 0.05 0.52 0.02 7.47]\n",
      "Loss_Q: [0.84 1.06 0.42 0.73 0.05 0.5  0.02 0.   3.63] Loss_P: [2.39 1.68 1.7  0.48 0.74 0.03 0.5  0.02 7.55]\n",
      "Loss_Q: [0.94 1.12 0.41 0.71 0.05 0.52 0.02 0.   3.78] Loss_P: [2.38 1.74 1.72 0.41 0.71 0.05 0.5  0.03 7.53]\n",
      "Loss_Q: [0.97 1.02 0.37 0.66 0.04 0.51 0.01 0.   3.58] Loss_P: [2.36 1.75 1.63 0.48 0.76 0.05 0.52 0.02 7.57]\n",
      "Loss_Q: [0.85 1.01 0.45 0.72 0.04 0.5  0.03 0.   3.59] Loss_P: [2.36 1.75 1.65 0.44 0.72 0.05 0.51 0.02 7.49]\n",
      "Loss_Q: [0.94 1.11 0.45 0.76 0.07 0.5  0.02 0.   3.84] Loss_P: [2.4  1.64 1.63 0.44 0.73 0.04 0.51 0.01 7.41]\n",
      "Loss_Q: [0.87 1.06 0.4  0.69 0.04 0.5  0.02 0.   3.58] Loss_P: [2.38 1.65 1.64 0.46 0.72 0.06 0.5  0.03 7.44]\n",
      "Loss_Q: [0.93 1.07 0.36 0.71 0.05 0.51 0.01 0.   3.64] Loss_P: [2.35 1.76 1.63 0.37 0.73 0.04 0.51 0.02 7.41]\n",
      "Loss_Q: [0.86 0.94 0.37 0.71 0.05 0.51 0.02 0.   3.45] Loss_P: [2.39 1.67 1.64 0.43 0.71 0.05 0.52 0.03 7.43]\n",
      "Loss_Q: [0.92 1.   0.38 0.71 0.04 0.5  0.02 0.   3.57] Loss_P: [2.38 1.71 1.61 0.4  0.74 0.05 0.5  0.02 7.42]\n",
      "Loss_Q: [0.92 1.03 0.38 0.68 0.06 0.5  0.02 0.   3.59] Loss_P: [2.36 1.74 1.67 0.37 0.7  0.03 0.5  0.02 7.41]\n",
      "Loss_Q: [0.9  1.01 0.4  0.75 0.05 0.49 0.02 0.   3.63] Loss_P: [2.43 1.68 1.63 0.43 0.72 0.04 0.49 0.02 7.44]\n",
      "Loss_Q: [0.9  1.01 0.38 0.7  0.04 0.49 0.02 0.   3.55] Loss_P: [2.36 1.67 1.69 0.38 0.71 0.04 0.5  0.03 7.38]\n",
      "Loss_Q: [0.94 1.   0.43 0.71 0.05 0.49 0.02 0.   3.63] Loss_P: [2.37 1.75 1.62 0.39 0.69 0.04 0.5  0.02 7.38]\n",
      "Loss_Q: [0.88 1.03 0.45 0.71 0.07 0.49 0.02 0.   3.65] Loss_P: [2.33 1.71 1.59 0.38 0.71 0.03 0.5  0.01 7.25]\n",
      "Loss_Q: [0.92 1.05 0.44 0.75 0.06 0.49 0.02 0.   3.73] Loss_P: [2.4  1.7  1.57 0.43 0.74 0.04 0.47 0.01 7.37]\n",
      "Loss_Q: [0.89 0.97 0.41 0.71 0.05 0.5  0.02 0.   3.56] Loss_P: [2.41 1.75 1.6  0.43 0.72 0.04 0.49 0.01 7.45]\n",
      "Loss_Q: [0.9  1.07 0.41 0.74 0.03 0.49 0.01 0.   3.67] Loss_P: [2.4  1.76 1.64 0.44 0.69 0.02 0.49 0.03 7.48]\n",
      "Loss_Q: [0.9  1.05 0.33 0.69 0.04 0.48 0.01 0.   3.48] Loss_P: [2.41 1.73 1.63 0.45 0.73 0.05 0.48 0.01 7.48]\n",
      "Loss_Q: [0.89 1.08 0.37 0.69 0.04 0.49 0.02 0.   3.57] Loss_P: [2.4  1.64 1.61 0.39 0.71 0.03 0.49 0.02 7.29]\n",
      "Loss_Q: [0.91 1.03 0.39 0.69 0.04 0.48 0.02 0.   3.56] Loss_P: [2.42 1.69 1.62 0.41 0.7  0.04 0.45 0.02 7.34]\n",
      "Loss_Q: [0.89 1.03 0.41 0.69 0.06 0.49 0.02 0.   3.58] Loss_P: [2.37 1.75 1.65 0.4  0.67 0.05 0.46 0.01 7.36]\n",
      "Loss_Q: [0.86 1.07 0.4  0.7  0.04 0.48 0.03 0.   3.58] Loss_P: [2.4  1.72 1.62 0.4  0.69 0.04 0.48 0.02 7.37]\n",
      "Loss_Q: [0.91 1.   0.4  0.7  0.05 0.46 0.01 0.   3.53] Loss_P: [2.4  1.72 1.66 0.46 0.69 0.02 0.46 0.02 7.43]\n",
      "Loss_Q: [0.87 1.   0.39 0.69 0.05 0.49 0.02 0.   3.49] Loss_P: [2.43 1.64 1.66 0.37 0.71 0.05 0.49 0.02 7.37]\n",
      "Loss_Q: [0.88 1.07 0.39 0.66 0.04 0.47 0.01 0.   3.52] Loss_P: [2.35 1.76 1.67 0.43 0.71 0.04 0.47 0.01 7.45]\n",
      "Loss_Q: [0.86 1.02 0.35 0.68 0.05 0.47 0.01 0.   3.44] Loss_P: [2.41 1.77 1.67 0.44 0.71 0.06 0.47 0.02 7.55]\n",
      "Loss_Q: [0.9  1.13 0.38 0.73 0.03 0.48 0.02 0.   3.68] Loss_P: [2.4  1.75 1.67 0.43 0.71 0.05 0.47 0.01 7.49]\n",
      "Loss_Q: [0.9  1.08 0.39 0.71 0.07 0.45 0.02 0.   3.63] Loss_P: [2.4  1.7  1.68 0.44 0.73 0.05 0.5  0.02 7.53]\n",
      "Loss_Q: [0.82 1.12 0.35 0.7  0.03 0.46 0.01 0.   3.5 ] Loss_P: [2.4  1.69 1.62 0.37 0.7  0.04 0.46 0.01 7.29]\n",
      "Loss_Q: [0.94 1.02 0.38 0.66 0.04 0.46 0.01 0.   3.49] Loss_P: [2.39 1.77 1.68 0.36 0.71 0.05 0.47 0.03 7.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.9  1.02 0.38 0.67 0.05 0.48 0.02 0.   3.53] Loss_P: [2.39 1.73 1.6  0.38 0.7  0.04 0.48 0.02 7.33]\n",
      "Loss_Q: [0.9  1.09 0.4  0.69 0.06 0.47 0.01 0.   3.61] Loss_P: [2.32 1.87 1.65 0.41 0.71 0.04 0.46 0.03 7.49]\n",
      "Loss_Q: [0.87 1.1  0.34 0.7  0.05 0.49 0.02 0.   3.57] Loss_P: [2.39 1.74 1.65 0.4  0.72 0.04 0.48 0.01 7.43]\n",
      "Loss_Q: [0.95 1.05 0.38 0.68 0.04 0.47 0.01 0.   3.58] Loss_P: [2.36 1.77 1.61 0.43 0.7  0.05 0.48 0.03 7.42]\n",
      "Loss_Q: [0.9  1.02 0.35 0.67 0.04 0.46 0.01 0.   3.45] Loss_P: [2.41 1.73 1.63 0.39 0.7  0.04 0.48 0.02 7.39]\n",
      "Loss_Q: [0.92 0.99 0.34 0.7  0.05 0.46 0.01 0.   3.48] Loss_P: [2.37 1.76 1.66 0.37 0.69 0.05 0.46 0.02 7.38]\n",
      "Loss_Q: [0.9  1.01 0.35 0.67 0.06 0.48 0.01 0.   3.49] Loss_P: [2.41 1.71 1.63 0.34 0.7  0.06 0.48 0.03 7.36]\n",
      "Loss_Q: [0.95 1.02 0.34 0.68 0.04 0.47 0.01 0.   3.52] Loss_P: [2.37 1.74 1.62 0.34 0.66 0.06 0.46 0.01 7.26]\n",
      "Loss_Q: [0.93 1.03 0.38 0.64 0.04 0.46 0.01 0.   3.49] Loss_P: [2.42 1.71 1.64 0.38 0.7  0.06 0.47 0.02 7.39]\n",
      "Loss_Q: [0.86 1.05 0.36 0.66 0.04 0.46 0.02 0.   3.45] Loss_P: [2.37 1.76 1.63 0.4  0.62 0.05 0.46 0.03 7.3 ]\n",
      "Loss_Q: [0.88 1.03 0.36 0.66 0.05 0.47 0.01 0.   3.47] Loss_P: [2.39 1.69 1.54 0.38 0.65 0.05 0.46 0.02 7.18]\n",
      "Loss_Q: [0.79 1.01 0.4  0.66 0.05 0.45 0.01 0.   3.37] Loss_P: [2.36 1.74 1.61 0.38 0.68 0.05 0.47 0.03 7.32]\n",
      "Loss_Q: [0.86 0.99 0.33 0.65 0.05 0.45 0.03 0.   3.36] Loss_P: [2.36 1.69 1.56 0.4  0.65 0.07 0.45 0.03 7.21]\n",
      "Loss_Q: [0.88 1.01 0.35 0.67 0.06 0.46 0.01 0.   3.45] Loss_P: [2.38 1.68 1.62 0.42 0.65 0.04 0.45 0.02 7.25]\n",
      "Loss_Q: [0.84 1.   0.32 0.64 0.06 0.43 0.01 0.   3.3 ] Loss_P: [2.41 1.73 1.63 0.4  0.68 0.04 0.44 0.02 7.37]\n",
      "Loss_Q: [0.86 1.   0.39 0.64 0.07 0.44 0.02 0.   3.42] Loss_P: [2.28 1.74 1.58 0.47 0.68 0.04 0.44 0.02 7.24]\n",
      "Loss_Q: [0.96 1.08 0.37 0.64 0.06 0.45 0.02 0.   3.58] Loss_P: [2.4  1.73 1.61 0.38 0.64 0.05 0.44 0.02 7.28]\n",
      "Loss_Q: [0.97 1.02 0.39 0.64 0.05 0.43 0.02 0.   3.52] Loss_P: [2.39 1.76 1.62 0.42 0.63 0.06 0.42 0.01 7.32]\n",
      "Loss_Q: [0.98 1.01 0.4  0.63 0.04 0.39 0.02 0.   3.46] Loss_P: [2.34 1.76 1.65 0.39 0.65 0.05 0.44 0.01 7.29]\n",
      "Loss_Q: [0.93 1.06 0.4  0.63 0.06 0.42 0.02 0.   3.53] Loss_P: [2.39 1.71 1.67 0.38 0.62 0.05 0.4  0.01 7.24]\n",
      "Loss_Q: [0.97 1.11 0.39 0.65 0.06 0.4  0.01 0.   3.59] Loss_P: [2.36 1.71 1.67 0.47 0.68 0.05 0.37 0.03 7.33]\n",
      "Loss_Q: [0.91 1.01 0.39 0.62 0.05 0.42 0.02 0.   3.41] Loss_P: [2.32 1.71 1.69 0.41 0.68 0.04 0.42 0.01 7.28]\n",
      "Loss_Q: [0.92 1.   0.36 0.61 0.07 0.39 0.01 0.   3.37] Loss_P: [2.36 1.68 1.68 0.4  0.65 0.05 0.42 0.02 7.26]\n",
      "Loss_Q: [0.94 1.01 0.39 0.62 0.05 0.39 0.02 0.   3.42] Loss_P: [2.4  1.75 1.61 0.38 0.65 0.04 0.41 0.04 7.28]\n",
      "Loss_Q: [0.92 1.05 0.35 0.66 0.04 0.37 0.02 0.   3.42] Loss_P: [2.34 1.79 1.62 0.4  0.64 0.05 0.42 0.02 7.27]\n",
      "Loss_Q: [0.86 1.06 0.37 0.62 0.05 0.41 0.02 0.   3.38] Loss_P: [2.37 1.67 1.68 0.43 0.65 0.04 0.41 0.03 7.28]\n",
      "Loss_Q: [0.94 1.04 0.39 0.63 0.06 0.43 0.01 0.   3.5 ] Loss_P: [2.31 1.74 1.63 0.4  0.68 0.06 0.44 0.02 7.26]\n",
      "Loss_Q: [0.95 1.06 0.38 0.64 0.05 0.41 0.02 0.   3.5 ] Loss_P: [2.34 1.75 1.71 0.43 0.62 0.04 0.41 0.01 7.33]\n",
      "Loss_Q: [0.91 1.04 0.39 0.67 0.04 0.43 0.02 0.   3.5 ] Loss_P: [2.41 1.68 1.71 0.37 0.67 0.04 0.44 0.01 7.33]\n",
      "Loss_Q: [0.88 0.96 0.38 0.64 0.04 0.42 0.03 0.   3.36] Loss_P: [2.36 1.77 1.74 0.42 0.68 0.05 0.46 0.01 7.49]\n",
      "Loss_Q: [0.9  1.   0.38 0.67 0.06 0.43 0.01 0.   3.46] Loss_P: [2.34 1.73 1.66 0.41 0.67 0.05 0.42 0.03 7.32]\n",
      "Loss_Q: [0.9  1.02 0.39 0.66 0.04 0.44 0.02 0.   3.46] Loss_P: [2.34 1.79 1.68 0.39 0.73 0.05 0.46 0.03 7.48]\n",
      "Loss_Q: [0.97 1.05 0.37 0.67 0.06 0.43 0.02 0.   3.57] Loss_P: [2.28 1.82 1.71 0.4  0.66 0.06 0.43 0.02 7.39]\n",
      "Loss_Q: [0.86 1.08 0.34 0.64 0.07 0.41 0.02 0.   3.42] Loss_P: [2.35 1.76 1.7  0.45 0.68 0.05 0.41 0.02 7.42]\n",
      "Loss_Q: [0.87 1.09 0.47 0.69 0.07 0.41 0.02 0.   3.62] Loss_P: [2.34 1.66 1.74 0.39 0.68 0.04 0.42 0.03 7.29]\n",
      "Loss_Q: [0.89 1.09 0.42 0.69 0.05 0.41 0.03 0.   3.57] Loss_P: [2.3  1.74 1.7  0.42 0.73 0.04 0.41 0.01 7.36]\n",
      "Loss_Q: [0.94 1.13 0.4  0.71 0.06 0.4  0.03 0.   3.67] Loss_P: [2.38 1.67 1.78 0.4  0.69 0.07 0.4  0.01 7.41]\n",
      "Loss_Q: [0.92 1.07 0.39 0.7  0.06 0.37 0.01 0.   3.52] Loss_P: [2.38 1.68 1.76 0.4  0.73 0.06 0.42 0.01 7.42]\n",
      "Loss_Q: [0.93 1.02 0.33 0.73 0.07 0.39 0.01 0.   3.48] Loss_P: [2.38 1.69 1.72 0.37 0.71 0.05 0.42 0.03 7.37]\n",
      "Loss_Q: [0.97 1.11 0.35 0.72 0.05 0.41 0.03 0.   3.64] Loss_P: [2.41 1.67 1.71 0.38 0.7  0.07 0.43 0.02 7.38]\n",
      "Loss_Q: [0.86 1.08 0.37 0.7  0.04 0.42 0.01 0.   3.48] Loss_P: [2.37 1.7  1.72 0.38 0.72 0.04 0.41 0.01 7.35]\n",
      "Loss_Q: [0.91 1.11 0.36 0.7  0.05 0.4  0.01 0.   3.53] Loss_P: [2.39 1.64 1.71 0.35 0.7  0.06 0.44 0.02 7.3 ]\n",
      "Loss_Q: [0.85 1.11 0.35 0.67 0.06 0.42 0.02 0.   3.48] Loss_P: [2.46 1.63 1.71 0.38 0.68 0.04 0.43 0.03 7.37]\n",
      "Loss_Q: [0.88 1.07 0.38 0.71 0.06 0.38 0.02 0.   3.51] Loss_P: [2.41 1.68 1.75 0.39 0.72 0.05 0.39 0.01 7.4 ]\n",
      "Loss_Q: [0.83 1.04 0.41 0.72 0.05 0.38 0.02 0.   3.47] Loss_P: [2.43 1.65 1.74 0.38 0.7  0.05 0.41 0.02 7.4 ]\n",
      "Loss_Q: [0.91 1.05 0.39 0.67 0.05 0.39 0.02 0.   3.47] Loss_P: [2.32 1.66 1.7  0.4  0.67 0.04 0.38 0.02 7.2 ]\n",
      "Loss_Q: [0.86 1.07 0.38 0.69 0.07 0.41 0.02 0.   3.5 ] Loss_P: [2.37 1.61 1.73 0.43 0.7  0.05 0.39 0.01 7.29]\n",
      "Loss_Q: [0.95 1.03 0.37 0.72 0.05 0.39 0.02 0.   3.52] Loss_P: [2.35 1.74 1.75 0.43 0.76 0.06 0.39 0.03 7.5 ]\n",
      "Loss_Q: [0.89 1.03 0.39 0.74 0.05 0.39 0.02 0.   3.51] Loss_P: [2.42 1.61 1.7  0.42 0.7  0.07 0.39 0.02 7.31]\n",
      "Loss_Q: [0.92 1.07 0.43 0.74 0.07 0.38 0.02 0.   3.64] Loss_P: [2.39 1.62 1.74 0.42 0.7  0.06 0.43 0.04 7.4 ]\n",
      "Loss_Q: [0.86 1.08 0.39 0.7  0.06 0.38 0.02 0.   3.48] Loss_P: [2.35 1.66 1.75 0.44 0.68 0.07 0.42 0.03 7.39]\n",
      "Loss_Q: [0.85 1.09 0.37 0.71 0.04 0.4  0.02 0.   3.48] Loss_P: [2.38 1.72 1.8  0.46 0.72 0.06 0.38 0.02 7.54]\n",
      "Loss_Q: [0.9  1.1  0.44 0.73 0.04 0.41 0.02 0.   3.64] Loss_P: [2.37 1.66 1.74 0.48 0.72 0.05 0.37 0.01 7.4 ]\n",
      "Loss_Q: [0.84 1.1  0.42 0.68 0.03 0.41 0.03 0.   3.52] Loss_P: [2.34 1.65 1.8  0.44 0.74 0.05 0.38 0.01 7.41]\n",
      "Loss_Q: [0.96 1.13 0.43 0.72 0.05 0.37 0.02 0.   3.68] Loss_P: [2.34 1.68 1.8  0.45 0.73 0.07 0.39 0.01 7.47]\n",
      "Loss_Q: [0.89 1.14 0.43 0.73 0.05 0.38 0.02 0.   3.64] Loss_P: [2.3  1.65 1.82 0.42 0.71 0.09 0.37 0.03 7.4 ]\n",
      "Loss_Q: [0.83 1.14 0.36 0.72 0.04 0.37 0.03 0.   3.5 ] Loss_P: [2.36 1.69 1.77 0.45 0.76 0.05 0.36 0.01 7.45]\n",
      "Loss_Q: [0.83 1.11 0.4  0.7  0.06 0.39 0.02 0.   3.52] Loss_P: [2.36 1.63 1.7  0.45 0.69 0.05 0.38 0.03 7.29]\n",
      "Loss_Q: [0.77 1.07 0.41 0.69 0.05 0.4  0.01 0.   3.4 ] Loss_P: [2.35 1.65 1.69 0.43 0.73 0.05 0.36 0.02 7.29]\n",
      "Loss_Q: [0.79 1.11 0.39 0.73 0.04 0.38 0.02 0.   3.45] Loss_P: [2.32 1.68 1.77 0.38 0.74 0.06 0.4  0.02 7.37]\n",
      "Loss_Q: [0.95 1.15 0.42 0.74 0.06 0.37 0.02 0.   3.71] Loss_P: [2.32 1.62 1.73 0.44 0.73 0.05 0.38 0.03 7.28]\n",
      "Loss_Q: [0.8  1.06 0.39 0.71 0.05 0.37 0.02 0.   3.41] Loss_P: [2.36 1.67 1.7  0.45 0.74 0.04 0.38 0.02 7.36]\n",
      "Loss_Q: [0.89 1.06 0.4  0.72 0.04 0.36 0.02 0.   3.49] Loss_P: [2.31 1.75 1.69 0.49 0.71 0.06 0.37 0.01 7.4 ]\n",
      "Loss_Q: [0.82 1.08 0.41 0.69 0.06 0.39 0.02 0.   3.47] Loss_P: [2.3  1.75 1.75 0.46 0.74 0.04 0.36 0.01 7.42]\n",
      "Loss_Q: [0.81 1.09 0.42 0.7  0.05 0.37 0.02 0.   3.45] Loss_P: [2.35 1.66 1.68 0.49 0.73 0.05 0.36 0.02 7.35]\n",
      "Loss_Q: [0.83 1.11 0.42 0.74 0.04 0.41 0.01 0.   3.57] Loss_P: [2.32 1.7  1.69 0.46 0.73 0.06 0.39 0.02 7.37]\n",
      "Loss_Q: [0.85 1.06 0.44 0.74 0.05 0.37 0.02 0.   3.53] Loss_P: [2.34 1.72 1.72 0.42 0.71 0.04 0.41 0.03 7.39]\n",
      "Loss_Q: [0.83 1.11 0.44 0.72 0.05 0.38 0.02 0.   3.55] Loss_P: [2.32 1.7  1.75 0.45 0.78 0.05 0.36 0.02 7.43]\n",
      "Loss_Q: [0.86 1.08 0.36 0.7  0.05 0.37 0.01 0.   3.45] Loss_P: [2.3  1.73 1.7  0.46 0.72 0.06 0.36 0.01 7.34]\n",
      "Loss_Q: [0.87 1.1  0.46 0.72 0.05 0.36 0.02 0.   3.58] Loss_P: [2.32 1.71 1.74 0.51 0.73 0.07 0.37 0.02 7.46]\n",
      "Loss_Q: [0.77 1.04 0.36 0.67 0.04 0.34 0.03 0.   3.26] Loss_P: [2.36 1.65 1.75 0.47 0.75 0.06 0.38 0.01 7.42]\n",
      "Loss_Q: [0.86 1.09 0.44 0.7  0.05 0.4  0.03 0.   3.57] Loss_P: [2.39 1.69 1.67 0.5  0.72 0.05 0.4  0.01 7.43]\n",
      "Loss_Q: [0.85 1.07 0.43 0.71 0.05 0.39 0.02 0.   3.52] Loss_P: [2.33 1.65 1.73 0.41 0.69 0.05 0.39 0.02 7.28]\n",
      "Loss_Q: [0.81 1.05 0.41 0.69 0.03 0.4  0.01 0.   3.4 ] Loss_P: [2.32 1.66 1.71 0.43 0.72 0.05 0.4  0.03 7.32]\n",
      "Loss_Q: [0.87 1.09 0.4  0.68 0.06 0.38 0.03 0.   3.5 ] Loss_P: [2.32 1.71 1.75 0.47 0.71 0.04 0.41 0.02 7.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.8  1.12 0.42 0.74 0.07 0.4  0.01 0.   3.57] Loss_P: [2.35 1.72 1.69 0.44 0.7  0.08 0.42 0.01 7.4 ]\n",
      "Loss_Q: [0.91 1.05 0.42 0.74 0.05 0.4  0.02 0.   3.57] Loss_P: [2.34 1.71 1.7  0.47 0.67 0.06 0.4  0.02 7.37]\n",
      "Loss_Q: [0.85 1.13 0.38 0.66 0.04 0.41 0.01 0.   3.48] Loss_P: [2.31 1.72 1.75 0.45 0.67 0.05 0.38 0.01 7.34]\n",
      "Loss_Q: [0.98 1.04 0.41 0.65 0.05 0.41 0.02 0.   3.57] Loss_P: [2.32 1.75 1.76 0.48 0.65 0.04 0.42 0.01 7.44]\n",
      "Loss_Q: [0.92 1.11 0.45 0.62 0.04 0.44 0.02 0.   3.6 ] Loss_P: [2.35 1.69 1.72 0.47 0.69 0.05 0.43 0.03 7.44]\n",
      "Loss_Q: [0.86 1.08 0.43 0.64 0.06 0.41 0.01 0.   3.49] Loss_P: [2.35 1.68 1.66 0.43 0.63 0.04 0.44 0.02 7.25]\n",
      "Loss_Q: [0.92 1.02 0.37 0.63 0.05 0.4  0.02 0.   3.41] Loss_P: [2.38 1.64 1.66 0.41 0.65 0.06 0.41 0.02 7.22]\n",
      "Loss_Q: [0.83 1.04 0.35 0.61 0.03 0.42 0.01 0.   3.29] Loss_P: [2.32 1.74 1.65 0.39 0.63 0.07 0.42 0.03 7.25]\n",
      "Loss_Q: [0.9  1.03 0.38 0.63 0.07 0.43 0.03 0.   3.46] Loss_P: [2.33 1.69 1.68 0.4  0.64 0.05 0.42 0.02 7.23]\n",
      "Loss_Q: [0.89 1.05 0.4  0.63 0.04 0.42 0.01 0.   3.45] Loss_P: [2.36 1.65 1.72 0.4  0.64 0.05 0.4  0.02 7.25]\n",
      "Loss_Q: [0.95 1.09 0.41 0.66 0.04 0.41 0.03 0.   3.6 ] Loss_P: [2.26 1.76 1.78 0.4  0.64 0.05 0.42 0.02 7.33]\n",
      "Loss_Q: [0.84 1.06 0.41 0.69 0.06 0.41 0.02 0.   3.5 ] Loss_P: [2.3  1.75 1.72 0.46 0.69 0.05 0.43 0.01 7.41]\n",
      "Loss_Q: [0.9  1.05 0.39 0.65 0.05 0.41 0.02 0.   3.48] Loss_P: [2.26 1.75 1.78 0.41 0.65 0.08 0.4  0.01 7.35]\n",
      "Loss_Q: [0.9  1.08 0.39 0.65 0.04 0.39 0.02 0.   3.47] Loss_P: [2.36 1.72 1.74 0.4  0.66 0.06 0.44 0.01 7.38]\n",
      "Loss_Q: [0.9  1.08 0.38 0.64 0.06 0.41 0.03 0.   3.49] Loss_P: [2.32 1.73 1.79 0.36 0.67 0.05 0.44 0.01 7.37]\n",
      "Loss_Q: [0.93 1.07 0.4  0.67 0.06 0.4  0.02 0.   3.56] Loss_P: [2.29 1.74 1.78 0.43 0.7  0.08 0.41 0.02 7.45]\n",
      "Loss_Q: [0.85 1.07 0.38 0.63 0.04 0.4  0.01 0.   3.38] Loss_P: [2.34 1.74 1.75 0.43 0.65 0.09 0.38 0.01 7.39]\n",
      "Loss_Q: [0.92 1.09 0.36 0.68 0.06 0.39 0.01 0.   3.51] Loss_P: [2.32 1.73 1.73 0.43 0.64 0.05 0.38 0.02 7.29]\n",
      "Loss_Q: [0.9  1.13 0.4  0.62 0.05 0.33 0.02 0.   3.45] Loss_P: [2.3  1.75 1.8  0.45 0.64 0.09 0.38 0.02 7.44]\n",
      "Loss_Q: [0.89 1.03 0.4  0.66 0.03 0.39 0.02 0.   3.41] Loss_P: [2.3  1.69 1.79 0.44 0.68 0.05 0.39 0.02 7.35]\n",
      "Loss_Q: [0.99 1.06 0.44 0.65 0.06 0.38 0.02 0.   3.6 ] Loss_P: [2.34 1.72 1.77 0.37 0.7  0.04 0.39 0.02 7.35]\n",
      "Loss_Q: [0.88 1.09 0.35 0.66 0.06 0.39 0.02 0.   3.46] Loss_P: [2.33 1.73 1.71 0.43 0.64 0.05 0.39 0.01 7.28]\n",
      "Loss_Q: [0.98 1.09 0.4  0.7  0.06 0.42 0.02 0.   3.68] Loss_P: [2.36 1.74 1.71 0.39 0.67 0.06 0.44 0.02 7.39]\n",
      "Loss_Q: [0.88 1.11 0.44 0.64 0.06 0.43 0.02 0.   3.57] Loss_P: [2.32 1.75 1.76 0.43 0.68 0.08 0.39 0.02 7.42]\n",
      "Loss_Q: [0.91 1.09 0.4  0.67 0.05 0.41 0.02 0.   3.56] Loss_P: [2.28 1.76 1.77 0.33 0.67 0.04 0.39 0.02 7.26]\n",
      "Loss_Q: [0.89 1.09 0.38 0.68 0.06 0.43 0.02 0.   3.56] Loss_P: [2.32 1.74 1.75 0.39 0.67 0.07 0.41 0.02 7.37]\n",
      "Loss_Q: [0.87 1.14 0.4  0.67 0.05 0.41 0.01 0.   3.54] Loss_P: [2.33 1.67 1.7  0.37 0.66 0.06 0.43 0.03 7.25]\n",
      "Loss_Q: [0.91 1.04 0.36 0.68 0.05 0.4  0.01 0.   3.46] Loss_P: [2.36 1.73 1.69 0.41 0.64 0.06 0.4  0.03 7.31]\n",
      "Loss_Q: [0.88 1.06 0.35 0.62 0.07 0.42 0.02 0.   3.43] Loss_P: [2.3  1.7  1.7  0.37 0.68 0.06 0.43 0.02 7.25]\n",
      "Loss_Q: [0.91 1.13 0.39 0.65 0.04 0.39 0.02 0.   3.53] Loss_P: [2.37 1.69 1.75 0.44 0.65 0.05 0.42 0.02 7.39]\n",
      "Loss_Q: [0.92 1.06 0.38 0.63 0.05 0.39 0.01 0.   3.43] Loss_P: [2.35 1.69 1.72 0.37 0.64 0.05 0.39 0.02 7.23]\n",
      "Loss_Q: [0.95 1.09 0.37 0.68 0.06 0.43 0.02 0.   3.6 ] Loss_P: [2.38 1.71 1.73 0.41 0.64 0.05 0.4  0.02 7.35]\n",
      "Loss_Q: [0.93 1.06 0.37 0.65 0.04 0.38 0.02 0.   3.44] Loss_P: [2.32 1.71 1.73 0.38 0.67 0.05 0.39 0.01 7.27]\n",
      "Loss_Q: [0.86 1.14 0.33 0.67 0.05 0.37 0.02 0.   3.43] Loss_P: [2.36 1.69 1.73 0.42 0.69 0.06 0.38 0.02 7.35]\n",
      "Loss_Q: [0.86 1.05 0.39 0.65 0.05 0.39 0.02 0.   3.4 ] Loss_P: [2.35 1.72 1.71 0.38 0.66 0.05 0.38 0.02 7.27]\n",
      "Loss_Q: [0.89 1.1  0.35 0.68 0.05 0.37 0.02 0.   3.48] Loss_P: [2.36 1.69 1.77 0.39 0.67 0.06 0.39 0.02 7.35]\n",
      "Loss_Q: [0.89 1.08 0.35 0.64 0.06 0.41 0.02 0.   3.45] Loss_P: [2.33 1.74 1.78 0.41 0.67 0.06 0.42 0.02 7.42]\n",
      "Loss_Q: [0.93 1.03 0.36 0.61 0.05 0.44 0.02 0.   3.44] Loss_P: [2.33 1.76 1.75 0.38 0.65 0.04 0.42 0.03 7.37]\n",
      "Loss_Q: [0.95 1.1  0.33 0.61 0.04 0.43 0.01 0.   3.47] Loss_P: [2.39 1.65 1.75 0.39 0.6  0.05 0.43 0.02 7.29]\n",
      "Loss_Q: [0.99 1.03 0.34 0.61 0.06 0.44 0.02 0.   3.49] Loss_P: [2.31 1.71 1.74 0.42 0.62 0.04 0.45 0.01 7.3 ]\n",
      "Loss_Q: [0.97 1.06 0.35 0.63 0.04 0.46 0.02 0.   3.53] Loss_P: [2.33 1.72 1.7  0.38 0.61 0.06 0.44 0.03 7.27]\n",
      "Loss_Q: [0.92 1.03 0.36 0.61 0.06 0.45 0.02 0.   3.44] Loss_P: [2.4  1.71 1.73 0.39 0.63 0.07 0.45 0.03 7.4 ]\n",
      "Loss_Q: [0.95 1.01 0.34 0.62 0.05 0.44 0.02 0.   3.43] Loss_P: [2.31 1.68 1.75 0.35 0.64 0.03 0.45 0.02 7.23]\n",
      "Loss_Q: [0.9  1.04 0.36 0.6  0.07 0.45 0.01 0.   3.43] Loss_P: [2.33 1.81 1.72 0.36 0.61 0.05 0.42 0.02 7.32]\n",
      "Loss_Q: [0.84 1.04 0.32 0.64 0.04 0.44 0.02 0.   3.34] Loss_P: [2.4  1.75 1.68 0.33 0.59 0.04 0.43 0.02 7.25]\n",
      "Loss_Q: [0.91 1.04 0.4  0.57 0.03 0.43 0.02 0.   3.4 ] Loss_P: [2.34 1.73 1.75 0.39 0.66 0.07 0.45 0.03 7.43]\n",
      "Loss_Q: [0.88 1.07 0.37 0.6  0.06 0.42 0.02 0.   3.42] Loss_P: [2.37 1.82 1.72 0.39 0.64 0.04 0.41 0.02 7.42]\n",
      "Loss_Q: [0.95 1.07 0.35 0.61 0.05 0.41 0.03 0.   3.48] Loss_P: [2.35 1.71 1.64 0.4  0.63 0.05 0.44 0.03 7.25]\n",
      "Loss_Q: [0.85 1.02 0.38 0.68 0.07 0.44 0.02 0.   3.45] Loss_P: [2.37 1.74 1.65 0.41 0.68 0.07 0.44 0.04 7.4 ]\n",
      "Loss_Q: [0.91 1.06 0.34 0.7  0.07 0.45 0.02 0.   3.55] Loss_P: [2.34 1.73 1.65 0.43 0.65 0.05 0.46 0.02 7.33]\n",
      "Loss_Q: [0.95 1.01 0.34 0.67 0.07 0.45 0.01 0.   3.5 ] Loss_P: [2.36 1.74 1.6  0.39 0.68 0.07 0.45 0.02 7.32]\n",
      "Loss_Q: [0.87 1.03 0.33 0.7  0.07 0.45 0.02 0.   3.47] Loss_P: [2.38 1.69 1.58 0.38 0.69 0.06 0.46 0.02 7.27]\n",
      "Loss_Q: [0.91 1.05 0.35 0.7  0.06 0.46 0.02 0.   3.56] Loss_P: [2.33 1.75 1.65 0.33 0.69 0.05 0.45 0.03 7.29]\n",
      "Loss_Q: [0.87 1.06 0.34 0.7  0.04 0.45 0.02 0.   3.5 ] Loss_P: [2.35 1.69 1.68 0.44 0.68 0.05 0.46 0.02 7.36]\n",
      "Loss_Q: [0.94 1.01 0.34 0.71 0.05 0.45 0.02 0.   3.51] Loss_P: [2.39 1.73 1.64 0.39 0.71 0.05 0.43 0.02 7.36]\n",
      "Loss_Q: [0.91 0.99 0.39 0.69 0.06 0.43 0.03 0.   3.51] Loss_P: [2.34 1.75 1.59 0.42 0.69 0.06 0.46 0.03 7.34]\n",
      "Loss_Q: [0.93 1.06 0.32 0.67 0.06 0.44 0.02 0.   3.51] Loss_P: [2.38 1.8  1.67 0.4  0.75 0.04 0.45 0.02 7.51]\n",
      "Loss_Q: [0.89 0.96 0.35 0.71 0.06 0.44 0.01 0.   3.43] Loss_P: [2.3  1.77 1.66 0.42 0.72 0.06 0.46 0.02 7.41]\n",
      "Loss_Q: [0.89 0.99 0.37 0.68 0.07 0.45 0.02 0.   3.47] Loss_P: [2.3  1.75 1.61 0.38 0.72 0.07 0.42 0.03 7.28]\n",
      "Loss_Q: [0.89 0.97 0.36 0.71 0.08 0.41 0.03 0.   3.46] Loss_P: [2.34 1.74 1.6  0.39 0.69 0.06 0.43 0.02 7.26]\n",
      "Loss_Q: [0.95 1.   0.35 0.63 0.06 0.42 0.01 0.   3.41] Loss_P: [2.32 1.77 1.61 0.39 0.72 0.06 0.42 0.03 7.31]\n",
      "Loss_Q: [0.86 1.01 0.35 0.69 0.04 0.45 0.02 0.   3.42] Loss_P: [2.35 1.73 1.62 0.38 0.67 0.06 0.42 0.02 7.25]\n",
      "Loss_Q: [0.97 1.07 0.38 0.69 0.06 0.45 0.01 0.   3.62] Loss_P: [2.31 1.72 1.65 0.41 0.68 0.05 0.44 0.02 7.3 ]\n",
      "Loss_Q: [0.91 0.96 0.37 0.67 0.05 0.42 0.02 0.   3.39] Loss_P: [2.3  1.75 1.61 0.36 0.66 0.05 0.45 0.01 7.21]\n",
      "Loss_Q: [0.95 1.05 0.34 0.66 0.05 0.43 0.01 0.   3.5 ] Loss_P: [2.26 1.81 1.66 0.36 0.66 0.05 0.46 0.01 7.28]\n",
      "Loss_Q: [0.97 1.02 0.35 0.68 0.07 0.43 0.02 0.   3.55] Loss_P: [2.3  1.82 1.68 0.42 0.7  0.06 0.45 0.02 7.45]\n",
      "Loss_Q: [0.95 1.02 0.36 0.71 0.06 0.45 0.01 0.   3.55] Loss_P: [2.34 1.77 1.66 0.41 0.7  0.06 0.48 0.02 7.44]\n",
      "Loss_Q: [0.86 1.01 0.39 0.65 0.06 0.44 0.01 0.   3.42] Loss_P: [2.35 1.81 1.65 0.38 0.69 0.07 0.46 0.01 7.41]\n",
      "Loss_Q: [0.94 1.04 0.32 0.66 0.04 0.46 0.04 0.   3.48] Loss_P: [2.37 1.81 1.69 0.37 0.68 0.06 0.44 0.03 7.44]\n",
      "Loss_Q: [0.9  1.06 0.36 0.68 0.05 0.46 0.02 0.   3.53] Loss_P: [2.4  1.73 1.68 0.4  0.68 0.05 0.46 0.02 7.41]\n",
      "Loss_Q: [0.95 1.04 0.35 0.69 0.05 0.46 0.03 0.   3.56] Loss_P: [2.33 1.77 1.63 0.39 0.71 0.06 0.48 0.03 7.4 ]\n",
      "Loss_Q: [0.86 0.99 0.32 0.68 0.05 0.47 0.02 0.   3.38] Loss_P: [2.34 1.78 1.62 0.35 0.68 0.05 0.44 0.03 7.29]\n",
      "Loss_Q: [0.89 1.03 0.36 0.71 0.04 0.47 0.02 0.   3.5 ] Loss_P: [2.38 1.71 1.59 0.37 0.67 0.07 0.47 0.02 7.27]\n",
      "Loss_Q: [0.9  1.03 0.35 0.7  0.04 0.48 0.02 0.   3.52] Loss_P: [2.33 1.73 1.59 0.34 0.69 0.05 0.46 0.02 7.21]\n",
      "Loss_Q: [0.88 0.94 0.37 0.71 0.05 0.45 0.02 0.   3.42] Loss_P: [2.29 1.76 1.6  0.38 0.67 0.05 0.45 0.02 7.23]\n",
      "Loss_Q: [0.91 0.96 0.32 0.69 0.06 0.43 0.02 0.   3.38] Loss_P: [2.33 1.72 1.65 0.37 0.7  0.06 0.44 0.02 7.29]\n",
      "Loss_Q: [0.91 1.02 0.35 0.69 0.05 0.46 0.03 0.   3.5 ] Loss_P: [2.36 1.68 1.67 0.32 0.68 0.03 0.46 0.02 7.23]\n",
      "Loss_Q: [0.88 1.02 0.31 0.66 0.05 0.46 0.02 0.   3.4 ] Loss_P: [2.31 1.76 1.69 0.32 0.67 0.06 0.46 0.02 7.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.95 1.06 0.36 0.67 0.04 0.44 0.02 0.   3.54] Loss_P: [2.28 1.84 1.67 0.38 0.71 0.04 0.46 0.01 7.4 ]\n",
      "Loss_Q: [0.97 1.07 0.31 0.68 0.06 0.46 0.01 0.   3.56] Loss_P: [2.34 1.73 1.65 0.32 0.7  0.05 0.46 0.01 7.26]\n",
      "Loss_Q: [0.9  0.98 0.34 0.68 0.04 0.47 0.02 0.   3.42] Loss_P: [2.32 1.75 1.69 0.34 0.68 0.05 0.46 0.01 7.31]\n",
      "Loss_Q: [0.9  1.03 0.33 0.7  0.05 0.47 0.02 0.   3.51] Loss_P: [2.29 1.7  1.63 0.37 0.7  0.06 0.46 0.01 7.2 ]\n",
      "Loss_Q: [0.86 0.99 0.32 0.76 0.05 0.48 0.02 0.   3.48] Loss_P: [2.26 1.75 1.62 0.39 0.73 0.06 0.47 0.03 7.32]\n",
      "Loss_Q: [0.89 1.01 0.3  0.67 0.04 0.46 0.03 0.   3.4 ] Loss_P: [2.33 1.73 1.66 0.32 0.7  0.05 0.47 0.02 7.27]\n",
      "Loss_Q: [0.94 1.1  0.33 0.67 0.05 0.44 0.02 0.   3.55] Loss_P: [2.25 1.82 1.64 0.35 0.65 0.04 0.47 0.01 7.24]\n",
      "Loss_Q: [0.95 1.02 0.33 0.72 0.05 0.47 0.01 0.   3.54] Loss_P: [2.31 1.72 1.69 0.39 0.71 0.05 0.45 0.02 7.33]\n",
      "Loss_Q: [0.91 1.03 0.33 0.72 0.04 0.46 0.02 0.   3.51] Loss_P: [2.3  1.85 1.71 0.32 0.7  0.04 0.46 0.01 7.4 ]\n",
      "Loss_Q: [0.85 1.05 0.35 0.69 0.04 0.45 0.02 0.   3.45] Loss_P: [2.32 1.78 1.67 0.36 0.69 0.05 0.46 0.04 7.36]\n",
      "Loss_Q: [0.83 1.06 0.34 0.71 0.05 0.48 0.02 0.   3.49] Loss_P: [2.3  1.8  1.74 0.41 0.7  0.04 0.47 0.02 7.49]\n",
      "Loss_Q: [0.88 1.   0.34 0.71 0.05 0.47 0.02 0.   3.47] Loss_P: [2.3  1.79 1.69 0.37 0.71 0.05 0.49 0.01 7.42]\n",
      "Loss_Q: [0.96 1.06 0.37 0.71 0.06 0.47 0.02 0.   3.64] Loss_P: [2.31 1.71 1.68 0.34 0.72 0.05 0.46 0.02 7.3 ]\n",
      "Loss_Q: [0.93 1.04 0.33 0.7  0.04 0.48 0.03 0.   3.56] Loss_P: [2.3  1.77 1.69 0.35 0.72 0.06 0.49 0.01 7.39]\n",
      "Loss_Q: [0.94 1.02 0.36 0.71 0.08 0.47 0.03 0.   3.6 ] Loss_P: [2.3  1.79 1.65 0.36 0.69 0.07 0.47 0.02 7.35]\n",
      "Loss_Q: [0.94 1.05 0.32 0.68 0.03 0.5  0.01 0.   3.53] Loss_P: [2.31 1.79 1.59 0.38 0.71 0.05 0.5  0.02 7.36]\n",
      "Loss_Q: [0.87 1.02 0.36 0.73 0.06 0.49 0.02 0.   3.55] Loss_P: [2.29 1.79 1.67 0.38 0.68 0.06 0.49 0.01 7.38]\n",
      "Loss_Q: [0.86 1.   0.37 0.7  0.06 0.49 0.02 0.   3.52] Loss_P: [2.29 1.79 1.66 0.35 0.68 0.05 0.49 0.02 7.33]\n",
      "Loss_Q: [0.83 1.02 0.35 0.68 0.05 0.49 0.02 0.   3.43] Loss_P: [2.33 1.73 1.71 0.35 0.7  0.04 0.51 0.02 7.38]\n",
      "Loss_Q: [0.9  1.02 0.36 0.71 0.06 0.5  0.02 0.   3.57] Loss_P: [2.29 1.85 1.65 0.37 0.68 0.07 0.49 0.03 7.43]\n",
      "Loss_Q: [0.93 1.05 0.34 0.69 0.04 0.5  0.02 0.   3.56] Loss_P: [2.34 1.7  1.64 0.35 0.7  0.06 0.51 0.03 7.33]\n",
      "Loss_Q: [0.87 0.98 0.32 0.62 0.04 0.52 0.02 0.   3.36] Loss_P: [2.33 1.7  1.59 0.4  0.67 0.06 0.51 0.02 7.29]\n",
      "Loss_Q: [0.88 0.98 0.32 0.64 0.07 0.49 0.01 0.   3.4 ] Loss_P: [2.3  1.71 1.65 0.31 0.64 0.08 0.51 0.02 7.21]\n",
      "Loss_Q: [0.89 1.02 0.37 0.63 0.06 0.52 0.02 0.   3.51] Loss_P: [2.34 1.69 1.68 0.37 0.7  0.06 0.51 0.01 7.36]\n",
      "Loss_Q: [0.86 1.   0.32 0.69 0.06 0.5  0.02 0.   3.44] Loss_P: [2.33 1.8  1.67 0.37 0.66 0.05 0.51 0.01 7.4 ]\n",
      "Loss_Q: [0.89 1.01 0.36 0.68 0.05 0.5  0.02 0.   3.51] Loss_P: [2.35 1.77 1.64 0.37 0.65 0.04 0.51 0.02 7.34]\n",
      "Loss_Q: [0.98 0.94 0.38 0.65 0.04 0.51 0.02 0.   3.52] Loss_P: [2.34 1.76 1.68 0.4  0.66 0.05 0.51 0.03 7.43]\n",
      "Loss_Q: [0.91 1.03 0.32 0.61 0.06 0.51 0.02 0.   3.45] Loss_P: [2.35 1.73 1.65 0.41 0.7  0.06 0.52 0.03 7.44]\n",
      "Loss_Q: [0.97 1.03 0.36 0.63 0.04 0.51 0.03 0.   3.57] Loss_P: [2.32 1.87 1.72 0.39 0.69 0.04 0.51 0.03 7.56]\n",
      "Loss_Q: [0.96 1.01 0.35 0.63 0.07 0.51 0.03 0.   3.56] Loss_P: [2.28 1.84 1.69 0.36 0.66 0.05 0.51 0.01 7.4 ]\n",
      "Loss_Q: [1.   1.06 0.41 0.61 0.06 0.51 0.03 0.   3.69] Loss_P: [2.29 1.8  1.71 0.38 0.58 0.04 0.51 0.01 7.31]\n",
      "Loss_Q: [0.99 1.01 0.37 0.61 0.05 0.51 0.02 0.   3.56] Loss_P: [2.3  1.81 1.73 0.38 0.61 0.05 0.51 0.01 7.4 ]\n",
      "Loss_Q: [1.01 1.02 0.35 0.64 0.05 0.51 0.02 0.   3.6 ] Loss_P: [2.31 1.82 1.78 0.41 0.66 0.07 0.52 0.02 7.58]\n",
      "Loss_Q: [0.96 1.08 0.37 0.64 0.04 0.51 0.02 0.   3.63] Loss_P: [2.33 1.77 1.73 0.39 0.61 0.07 0.52 0.01 7.43]\n",
      "Loss_Q: [0.96 1.03 0.3  0.59 0.05 0.51 0.02 0.   3.46] Loss_P: [2.33 1.76 1.66 0.35 0.63 0.05 0.52 0.02 7.31]\n",
      "Loss_Q: [0.94 0.99 0.33 0.6  0.08 0.5  0.02 0.   3.47] Loss_P: [2.36 1.82 1.68 0.36 0.63 0.06 0.52 0.03 7.47]\n",
      "Loss_Q: [0.95 0.99 0.3  0.59 0.07 0.51 0.02 0.   3.43] Loss_P: [2.33 1.75 1.7  0.36 0.59 0.05 0.52 0.02 7.32]\n",
      "Loss_Q: [0.93 1.05 0.38 0.57 0.05 0.51 0.03 0.   3.52] Loss_P: [2.36 1.73 1.68 0.32 0.58 0.06 0.51 0.02 7.26]\n",
      "Loss_Q: [0.94 1.01 0.37 0.56 0.05 0.51 0.02 0.   3.45] Loss_P: [2.36 1.81 1.59 0.36 0.59 0.06 0.51 0.02 7.3 ]\n",
      "Loss_Q: [0.95 1.02 0.35 0.65 0.05 0.52 0.02 0.   3.56] Loss_P: [2.32 1.75 1.69 0.36 0.67 0.06 0.51 0.03 7.38]\n",
      "Loss_Q: [0.86 1.02 0.28 0.67 0.03 0.52 0.02 0.   3.39] Loss_P: [2.36 1.75 1.68 0.4  0.67 0.05 0.53 0.01 7.45]\n",
      "Loss_Q: [0.91 1.05 0.37 0.67 0.06 0.52 0.02 0.   3.59] Loss_P: [2.29 1.78 1.63 0.39 0.66 0.05 0.51 0.03 7.35]\n",
      "Loss_Q: [0.98 1.   0.32 0.65 0.04 0.51 0.01 0.   3.51] Loss_P: [2.34 1.77 1.64 0.42 0.69 0.05 0.51 0.02 7.45]\n",
      "Loss_Q: [0.84 0.99 0.37 0.65 0.06 0.51 0.01 0.   3.43] Loss_P: [2.31 1.76 1.71 0.34 0.61 0.04 0.51 0.02 7.3 ]\n",
      "Loss_Q: [1.03 1.08 0.33 0.58 0.06 0.51 0.03 0.   3.62] Loss_P: [2.39 1.77 1.74 0.39 0.62 0.04 0.51 0.03 7.49]\n",
      "Loss_Q: [1.03 1.01 0.36 0.6  0.05 0.5  0.02 0.   3.57] Loss_P: [2.37 1.73 1.74 0.41 0.55 0.04 0.52 0.03 7.39]\n",
      "Loss_Q: [0.96 1.   0.36 0.66 0.05 0.52 0.04 0.   3.58] Loss_P: [2.33 1.7  1.71 0.4  0.66 0.06 0.51 0.01 7.38]\n",
      "Loss_Q: [1.01 1.03 0.33 0.65 0.04 0.51 0.02 0.   3.58] Loss_P: [2.34 1.77 1.64 0.41 0.65 0.04 0.52 0.04 7.4 ]\n",
      "Loss_Q: [1.   1.   0.4  0.67 0.06 0.5  0.03 0.   3.65] Loss_P: [2.37 1.75 1.62 0.35 0.64 0.08 0.5  0.03 7.35]\n",
      "Loss_Q: [0.94 1.03 0.37 0.67 0.07 0.5  0.01 0.   3.59] Loss_P: [2.35 1.75 1.64 0.39 0.66 0.08 0.51 0.01 7.39]\n",
      "Loss_Q: [0.9  1.05 0.36 0.66 0.05 0.5  0.01 0.   3.54] Loss_P: [2.33 1.77 1.69 0.34 0.63 0.05 0.51 0.03 7.36]\n",
      "Loss_Q: [0.87 1.   0.33 0.62 0.06 0.5  0.02 0.   3.4 ] Loss_P: [2.39 1.72 1.61 0.38 0.66 0.05 0.51 0.03 7.34]\n",
      "Loss_Q: [0.87 0.98 0.34 0.65 0.07 0.51 0.02 0.   3.44] Loss_P: [2.37 1.68 1.63 0.37 0.67 0.05 0.51 0.03 7.31]\n",
      "Loss_Q: [0.98 1.05 0.34 0.69 0.06 0.5  0.02 0.   3.64] Loss_P: [2.32 1.74 1.65 0.31 0.67 0.04 0.51 0.03 7.28]\n",
      "Loss_Q: [0.9  0.97 0.34 0.7  0.05 0.51 0.02 0.   3.47] Loss_P: [2.42 1.71 1.56 0.34 0.69 0.05 0.5  0.03 7.31]\n",
      "Loss_Q: [0.95 1.05 0.27 0.65 0.05 0.51 0.01 0.   3.5 ] Loss_P: [2.36 1.77 1.58 0.35 0.67 0.05 0.48 0.01 7.27]\n",
      "Loss_Q: [0.99 1.03 0.32 0.69 0.06 0.5  0.02 0.   3.61] Loss_P: [2.36 1.75 1.61 0.32 0.66 0.05 0.5  0.02 7.27]\n",
      "Loss_Q: [0.97 1.   0.34 0.64 0.06 0.51 0.03 0.   3.54] Loss_P: [2.39 1.7  1.57 0.34 0.67 0.05 0.51 0.02 7.25]\n",
      "Loss_Q: [0.9  1.03 0.31 0.67 0.04 0.5  0.02 0.   3.48] Loss_P: [2.31 1.86 1.6  0.38 0.71 0.05 0.5  0.02 7.43]\n",
      "Loss_Q: [0.91 0.99 0.35 0.69 0.04 0.51 0.02 0.   3.5 ] Loss_P: [2.35 1.81 1.63 0.4  0.69 0.04 0.51 0.02 7.45]\n",
      "Loss_Q: [0.91 1.04 0.35 0.65 0.04 0.5  0.02 0.   3.51] Loss_P: [2.35 1.79 1.64 0.36 0.66 0.06 0.51 0.02 7.38]\n",
      "Loss_Q: [0.9  1.04 0.36 0.68 0.06 0.5  0.02 0.   3.54] Loss_P: [2.34 1.78 1.59 0.36 0.71 0.04 0.51 0.03 7.36]\n",
      "Loss_Q: [0.92 1.   0.33 0.69 0.07 0.51 0.01 0.   3.52] Loss_P: [2.41 1.73 1.54 0.34 0.67 0.06 0.52 0.01 7.27]\n",
      "Loss_Q: [0.94 1.01 0.3  0.65 0.05 0.5  0.02 0.   3.45] Loss_P: [2.33 1.81 1.57 0.35 0.65 0.04 0.5  0.03 7.26]\n",
      "Loss_Q: [0.94 0.99 0.31 0.69 0.06 0.51 0.03 0.   3.53] Loss_P: [2.38 1.75 1.61 0.31 0.62 0.05 0.51 0.01 7.23]\n",
      "Loss_Q: [0.99 1.01 0.33 0.67 0.03 0.52 0.02 0.   3.57] Loss_P: [2.37 1.72 1.63 0.35 0.65 0.04 0.52 0.02 7.29]\n",
      "Loss_Q: [0.98 1.01 0.29 0.64 0.06 0.5  0.01 0.   3.49] Loss_P: [2.33 1.82 1.66 0.33 0.66 0.06 0.52 0.03 7.41]\n",
      "Loss_Q: [0.87 0.98 0.31 0.66 0.07 0.51 0.02 0.   3.42] Loss_P: [2.37 1.73 1.54 0.36 0.69 0.05 0.51 0.03 7.28]\n",
      "Loss_Q: [0.93 1.04 0.3  0.64 0.04 0.51 0.02 0.   3.48] Loss_P: [2.39 1.75 1.56 0.36 0.68 0.06 0.5  0.01 7.31]\n",
      "Loss_Q: [0.9  1.02 0.32 0.58 0.05 0.5  0.01 0.   3.37] Loss_P: [2.37 1.79 1.6  0.4  0.64 0.06 0.51 0.01 7.37]\n",
      "Loss_Q: [0.89 1.   0.29 0.59 0.05 0.52 0.02 0.   3.37] Loss_P: [2.36 1.81 1.56 0.33 0.62 0.05 0.51 0.02 7.25]\n",
      "Loss_Q: [0.98 0.94 0.32 0.59 0.04 0.51 0.01 0.   3.38] Loss_P: [2.35 1.86 1.54 0.3  0.58 0.05 0.51 0.03 7.23]\n",
      "Loss_Q: [0.91 1.   0.32 0.58 0.06 0.52 0.02 0.   3.42] Loss_P: [2.41 1.79 1.52 0.35 0.57 0.05 0.51 0.03 7.23]\n",
      "Loss_Q: [0.98 0.97 0.27 0.55 0.04 0.52 0.03 0.   3.35] Loss_P: [2.37 1.83 1.46 0.34 0.59 0.06 0.51 0.01 7.17]\n",
      "Loss_Q: [0.87 1.03 0.31 0.56 0.06 0.51 0.02 0.   3.37] Loss_P: [2.39 1.77 1.45 0.32 0.57 0.05 0.51 0.03 7.08]\n",
      "Loss_Q: [0.95 1.01 0.33 0.63 0.04 0.52 0.01 0.   3.49] Loss_P: [2.41 1.79 1.48 0.34 0.62 0.06 0.51 0.03 7.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.83 0.94 0.24 0.62 0.07 0.51 0.02 0.   3.23] Loss_P: [2.39 1.8  1.49 0.36 0.64 0.05 0.51 0.02 7.26]\n",
      "Loss_Q: [0.89 0.96 0.27 0.58 0.04 0.52 0.03 0.   3.3 ] Loss_P: [2.37 1.83 1.41 0.33 0.61 0.05 0.51 0.02 7.13]\n",
      "Loss_Q: [0.94 0.94 0.29 0.6  0.04 0.5  0.02 0.   3.34] Loss_P: [2.34 1.87 1.45 0.31 0.62 0.06 0.51 0.02 7.18]\n",
      "Loss_Q: [0.93 1.   0.26 0.61 0.05 0.5  0.02 0.   3.36] Loss_P: [2.34 1.83 1.52 0.31 0.58 0.05 0.5  0.03 7.16]\n",
      "Loss_Q: [0.96 0.97 0.31 0.63 0.06 0.5  0.01 0.   3.44] Loss_P: [2.37 1.82 1.48 0.3  0.59 0.07 0.51 0.02 7.16]\n",
      "Loss_Q: [0.93 0.97 0.27 0.65 0.05 0.5  0.04 0.   3.41] Loss_P: [2.37 1.92 1.48 0.31 0.62 0.05 0.51 0.01 7.27]\n",
      "Loss_Q: [0.93 0.97 0.27 0.56 0.05 0.51 0.02 0.   3.32] Loss_P: [2.4  1.91 1.47 0.31 0.59 0.06 0.5  0.01 7.25]\n",
      "Loss_Q: [0.97 1.05 0.26 0.56 0.04 0.51 0.02 0.   3.42] Loss_P: [2.35 1.75 1.47 0.34 0.58 0.04 0.51 0.03 7.08]\n",
      "Loss_Q: [0.85 1.   0.27 0.54 0.03 0.51 0.02 0.   3.22] Loss_P: [2.36 1.85 1.46 0.33 0.54 0.05 0.5  0.01 7.1 ]\n",
      "Loss_Q: [0.83 1.   0.28 0.61 0.06 0.51 0.03 0.   3.31] Loss_P: [2.37 1.86 1.51 0.34 0.57 0.04 0.51 0.01 7.2 ]\n",
      "Loss_Q: [0.89 1.03 0.28 0.57 0.05 0.51 0.04 0.   3.37] Loss_P: [2.41 1.81 1.46 0.35 0.59 0.06 0.51 0.02 7.21]\n",
      "Loss_Q: [0.87 0.98 0.29 0.55 0.05 0.51 0.01 0.   3.28] Loss_P: [2.36 1.84 1.4  0.29 0.55 0.05 0.51 0.01 7.  ]\n",
      "Loss_Q: [0.94 0.96 0.28 0.56 0.06 0.5  0.01 0.   3.32] Loss_P: [2.33 1.93 1.42 0.28 0.53 0.06 0.51 0.04 7.11]\n",
      "Loss_Q: [0.87 0.97 0.24 0.61 0.04 0.5  0.03 0.   3.27] Loss_P: [2.35 1.89 1.45 0.28 0.58 0.06 0.52 0.03 7.16]\n",
      "Loss_Q: [0.96 0.98 0.31 0.54 0.05 0.51 0.02 0.   3.36] Loss_P: [2.38 1.95 1.48 0.31 0.59 0.04 0.5  0.03 7.29]\n",
      "Loss_Q: [0.9  0.93 0.29 0.57 0.06 0.51 0.02 0.   3.28] Loss_P: [2.38 1.96 1.44 0.3  0.59 0.08 0.52 0.01 7.27]\n",
      "Loss_Q: [0.89 0.96 0.29 0.56 0.05 0.51 0.03 0.   3.29] Loss_P: [2.36 1.85 1.44 0.33 0.57 0.04 0.52 0.01 7.12]\n",
      "Loss_Q: [1.03 1.   0.29 0.57 0.06 0.51 0.02 0.   3.47] Loss_P: [2.32 1.89 1.47 0.32 0.58 0.05 0.51 0.03 7.17]\n",
      "Loss_Q: [0.93 1.03 0.31 0.57 0.04 0.51 0.02 0.   3.41] Loss_P: [2.37 1.95 1.44 0.28 0.6  0.06 0.53 0.03 7.26]\n",
      "Loss_Q: [0.95 0.95 0.32 0.58 0.05 0.51 0.02 0.   3.39] Loss_P: [2.35 1.92 1.45 0.3  0.56 0.06 0.51 0.01 7.17]\n",
      "Loss_Q: [0.92 1.   0.33 0.6  0.06 0.51 0.02 0.   3.43] Loss_P: [2.32 1.91 1.48 0.31 0.6  0.05 0.5  0.03 7.19]\n",
      "Loss_Q: [0.94 0.96 0.27 0.56 0.06 0.5  0.02 0.   3.3 ] Loss_P: [2.34 1.85 1.52 0.29 0.57 0.06 0.5  0.03 7.17]\n",
      "Loss_Q: [0.96 0.97 0.29 0.58 0.04 0.51 0.01 0.   3.37] Loss_P: [2.37 1.92 1.5  0.24 0.55 0.05 0.51 0.02 7.17]\n",
      "Loss_Q: [0.96 0.99 0.26 0.59 0.05 0.51 0.01 0.   3.36] Loss_P: [2.33 1.94 1.43 0.3  0.62 0.05 0.51 0.03 7.22]\n",
      "Loss_Q: [0.99 0.94 0.27 0.56 0.03 0.5  0.02 0.   3.3 ] Loss_P: [2.38 1.88 1.5  0.29 0.57 0.06 0.51 0.02 7.21]\n",
      "Loss_Q: [0.92 0.95 0.27 0.65 0.06 0.51 0.02 0.   3.36] Loss_P: [2.4  1.86 1.41 0.3  0.59 0.04 0.5  0.01 7.12]\n",
      "Loss_Q: [0.86 0.99 0.28 0.59 0.05 0.5  0.02 0.   3.28] Loss_P: [2.33 1.85 1.39 0.28 0.6  0.05 0.51 0.02 7.03]\n",
      "Loss_Q: [0.87 0.93 0.26 0.62 0.05 0.5  0.02 0.   3.26] Loss_P: [2.36 1.96 1.38 0.31 0.62 0.06 0.51 0.02 7.21]\n",
      "Loss_Q: [0.88 0.94 0.28 0.64 0.04 0.51 0.01 0.   3.31] Loss_P: [2.33 1.88 1.46 0.3  0.57 0.05 0.52 0.03 7.13]\n",
      "Loss_Q: [0.89 0.92 0.27 0.58 0.06 0.5  0.01 0.   3.23] Loss_P: [2.4  1.85 1.4  0.29 0.59 0.04 0.51 0.01 7.1 ]\n",
      "Loss_Q: [0.95 0.96 0.27 0.62 0.03 0.51 0.03 0.   3.37] Loss_P: [2.31 1.93 1.36 0.32 0.58 0.05 0.51 0.02 7.08]\n",
      "Loss_Q: [1.   0.94 0.29 0.61 0.04 0.51 0.01 0.   3.4 ] Loss_P: [2.3  1.89 1.39 0.34 0.62 0.03 0.51 0.01 7.08]\n",
      "Loss_Q: [0.94 0.92 0.28 0.62 0.03 0.51 0.02 0.   3.33] Loss_P: [2.35 1.88 1.39 0.26 0.61 0.03 0.5  0.02 7.06]\n",
      "Loss_Q: [0.99 0.97 0.32 0.6  0.05 0.5  0.02 0.   3.46] Loss_P: [2.35 1.91 1.47 0.33 0.63 0.05 0.5  0.02 7.27]\n",
      "Loss_Q: [0.97 1.01 0.3  0.6  0.05 0.5  0.01 0.   3.45] Loss_P: [2.31 1.93 1.46 0.35 0.63 0.05 0.52 0.02 7.27]\n",
      "Loss_Q: [0.99 1.   0.32 0.6  0.05 0.51 0.02 0.   3.5 ] Loss_P: [2.33 1.91 1.43 0.33 0.63 0.05 0.51 0.02 7.2 ]\n",
      "Loss_Q: [0.96 0.94 0.33 0.69 0.05 0.53 0.02 0.   3.53] Loss_P: [2.33 1.91 1.53 0.29 0.66 0.05 0.51 0.01 7.3 ]\n",
      "Loss_Q: [1.01 1.02 0.34 0.64 0.03 0.51 0.02 0.   3.57] Loss_P: [2.33 1.87 1.51 0.29 0.65 0.04 0.51 0.01 7.22]\n",
      "Loss_Q: [0.97 0.96 0.31 0.67 0.04 0.51 0.02 0.   3.47] Loss_P: [2.33 1.83 1.48 0.39 0.63 0.04 0.5  0.02 7.22]\n",
      "Loss_Q: [0.93 1.   0.3  0.63 0.04 0.51 0.01 0.   3.42] Loss_P: [2.37 1.92 1.48 0.36 0.63 0.06 0.51 0.01 7.36]\n",
      "Loss_Q: [0.93 0.98 0.28 0.64 0.03 0.5  0.02 0.   3.39] Loss_P: [2.36 1.88 1.48 0.35 0.62 0.03 0.5  0.01 7.23]\n",
      "Loss_Q: [1.   0.97 0.27 0.65 0.05 0.51 0.01 0.   3.46] Loss_P: [2.33 1.91 1.56 0.36 0.63 0.04 0.51 0.03 7.35]\n",
      "Loss_Q: [1.04 1.   0.33 0.64 0.04 0.5  0.02 0.   3.58] Loss_P: [2.35 1.86 1.58 0.35 0.67 0.06 0.51 0.02 7.39]\n",
      "Loss_Q: [0.94 1.   0.29 0.7  0.05 0.5  0.03 0.   3.52] Loss_P: [2.34 1.88 1.56 0.36 0.69 0.04 0.49 0.01 7.36]\n",
      "Loss_Q: [1.02 1.01 0.3  0.66 0.03 0.51 0.01 0.   3.54] Loss_P: [2.37 1.89 1.53 0.33 0.65 0.04 0.51 0.02 7.34]\n",
      "Loss_Q: [0.93 1.06 0.31 0.66 0.06 0.49 0.02 0.   3.55] Loss_P: [2.39 1.87 1.53 0.37 0.68 0.04 0.5  0.01 7.4 ]\n",
      "Loss_Q: [1.01 1.   0.32 0.68 0.05 0.51 0.02 0.   3.59] Loss_P: [2.33 1.81 1.53 0.29 0.7  0.06 0.5  0.02 7.25]\n",
      "Loss_Q: [0.94 1.05 0.28 0.7  0.05 0.5  0.03 0.   3.55] Loss_P: [2.31 1.87 1.55 0.34 0.69 0.04 0.52 0.03 7.35]\n",
      "Loss_Q: [0.97 0.99 0.31 0.68 0.05 0.5  0.02 0.   3.51] Loss_P: [2.36 1.9  1.49 0.32 0.64 0.05 0.5  0.04 7.3 ]\n",
      "Loss_Q: [0.93 0.99 0.3  0.68 0.04 0.5  0.01 0.   3.46] Loss_P: [2.34 1.89 1.52 0.35 0.66 0.04 0.51 0.01 7.3 ]\n",
      "Loss_Q: [0.96 1.01 0.35 0.65 0.05 0.52 0.03 0.   3.57] Loss_P: [2.31 1.87 1.52 0.32 0.7  0.04 0.5  0.02 7.28]\n",
      "Loss_Q: [0.97 1.   0.28 0.66 0.04 0.52 0.03 0.   3.5 ] Loss_P: [2.35 1.96 1.46 0.29 0.69 0.05 0.51 0.03 7.33]\n",
      "Loss_Q: [0.88 1.02 0.3  0.67 0.04 0.51 0.03 0.   3.44] Loss_P: [2.32 1.9  1.52 0.36 0.64 0.03 0.52 0.03 7.31]\n",
      "Loss_Q: [0.97 0.98 0.28 0.66 0.05 0.52 0.02 0.   3.47] Loss_P: [2.38 1.85 1.48 0.34 0.67 0.03 0.5  0.02 7.28]\n",
      "Loss_Q: [0.94 0.91 0.29 0.64 0.05 0.52 0.03 0.   3.39] Loss_P: [2.37 1.82 1.5  0.3  0.69 0.04 0.52 0.02 7.26]\n",
      "Loss_Q: [0.96 1.04 0.28 0.68 0.04 0.51 0.03 0.   3.55] Loss_P: [2.4  1.87 1.47 0.31 0.69 0.05 0.52 0.03 7.33]\n",
      "Loss_Q: [1.   0.95 0.31 0.68 0.05 0.51 0.01 0.   3.52] Loss_P: [2.36 1.84 1.48 0.31 0.66 0.04 0.51 0.02 7.22]\n",
      "Loss_Q: [0.94 0.96 0.29 0.67 0.04 0.51 0.01 0.   3.43] Loss_P: [2.38 1.8  1.52 0.28 0.65 0.03 0.5  0.03 7.2 ]\n",
      "Loss_Q: [0.91 0.97 0.28 0.68 0.05 0.51 0.02 0.   3.42] Loss_P: [2.35 1.83 1.49 0.32 0.71 0.04 0.5  0.03 7.27]\n",
      "Loss_Q: [0.91 0.95 0.29 0.65 0.06 0.51 0.03 0.   3.39] Loss_P: [2.35 1.88 1.5  0.37 0.67 0.04 0.51 0.03 7.34]\n",
      "Loss_Q: [0.93 0.98 0.27 0.67 0.06 0.5  0.03 0.   3.43] Loss_P: [2.39 1.85 1.47 0.35 0.67 0.03 0.5  0.03 7.29]\n",
      "Loss_Q: [0.86 1.02 0.29 0.67 0.05 0.5  0.02 0.   3.42] Loss_P: [2.36 1.82 1.48 0.31 0.68 0.04 0.5  0.02 7.21]\n",
      "Loss_Q: [0.91 1.   0.31 0.63 0.04 0.5  0.02 0.   3.42] Loss_P: [2.37 1.86 1.52 0.32 0.67 0.04 0.51 0.01 7.3 ]\n",
      "Loss_Q: [0.94 0.95 0.32 0.67 0.03 0.5  0.02 0.   3.44] Loss_P: [2.37 1.88 1.51 0.31 0.66 0.05 0.5  0.04 7.32]\n",
      "Loss_Q: [0.97 0.97 0.34 0.65 0.04 0.5  0.01 0.   3.5 ] Loss_P: [2.39 1.8  1.51 0.34 0.7  0.04 0.5  0.02 7.31]\n",
      "Loss_Q: [0.91 1.   0.32 0.7  0.06 0.5  0.02 0.   3.51] Loss_P: [2.34 1.88 1.53 0.31 0.66 0.04 0.5  0.02 7.28]\n",
      "Loss_Q: [0.99 1.03 0.31 0.65 0.04 0.5  0.03 0.   3.55] Loss_P: [2.35 1.9  1.58 0.3  0.65 0.03 0.5  0.01 7.32]\n",
      "Loss_Q: [0.94 1.07 0.33 0.67 0.04 0.51 0.02 0.   3.59] Loss_P: [2.36 1.81 1.58 0.38 0.69 0.06 0.51 0.01 7.4 ]\n",
      "Loss_Q: [1.05 1.08 0.3  0.63 0.05 0.51 0.02 0.   3.64] Loss_P: [2.37 1.84 1.56 0.33 0.65 0.02 0.51 0.02 7.3 ]\n",
      "Loss_Q: [0.98 1.04 0.34 0.67 0.05 0.51 0.01 0.   3.6 ] Loss_P: [2.34 1.84 1.56 0.36 0.66 0.05 0.51 0.02 7.34]\n",
      "Loss_Q: [0.95 0.97 0.3  0.64 0.04 0.51 0.03 0.   3.44] Loss_P: [2.29 1.88 1.53 0.36 0.68 0.05 0.51 0.01 7.31]\n",
      "Loss_Q: [0.97 1.06 0.34 0.67 0.03 0.51 0.01 0.   3.59] Loss_P: [2.3  1.93 1.59 0.37 0.68 0.05 0.52 0.01 7.46]\n",
      "Loss_Q: [1.04 1.02 0.32 0.68 0.05 0.51 0.02 0.   3.64] Loss_P: [2.36 1.84 1.56 0.38 0.67 0.04 0.51 0.02 7.38]\n",
      "Loss_Q: [0.92 0.98 0.31 0.64 0.05 0.52 0.02 0.   3.45] Loss_P: [2.28 1.86 1.58 0.39 0.65 0.05 0.52 0.02 7.36]\n",
      "Loss_Q: [0.97 1.03 0.33 0.69 0.04 0.5  0.02 0.   3.58] Loss_P: [2.32 1.9  1.59 0.37 0.68 0.04 0.51 0.01 7.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.03 1.   0.31 0.64 0.03 0.51 0.02 0.   3.54] Loss_P: [2.27 1.86 1.58 0.35 0.66 0.04 0.51 0.02 7.29]\n",
      "Loss_Q: [1.04 0.97 0.3  0.67 0.05 0.51 0.01 0.   3.56] Loss_P: [2.31 1.86 1.52 0.35 0.68 0.04 0.51 0.01 7.29]\n",
      "Loss_Q: [0.91 1.   0.31 0.67 0.04 0.51 0.01 0.   3.46] Loss_P: [2.31 1.83 1.57 0.37 0.69 0.04 0.51 0.03 7.35]\n",
      "Loss_Q: [0.9  0.97 0.32 0.66 0.05 0.52 0.02 0.   3.44] Loss_P: [2.35 1.84 1.53 0.37 0.7  0.05 0.5  0.01 7.36]\n",
      "Loss_Q: [0.88 0.97 0.27 0.68 0.05 0.51 0.02 0.   3.38] Loss_P: [2.36 1.82 1.53 0.32 0.7  0.05 0.51 0.02 7.31]\n",
      "Loss_Q: [0.95 0.95 0.3  0.68 0.04 0.51 0.03 0.   3.46] Loss_P: [2.39 1.76 1.49 0.32 0.65 0.04 0.51 0.02 7.19]\n",
      "Loss_Q: [0.9  0.96 0.33 0.68 0.05 0.51 0.03 0.   3.46] Loss_P: [2.36 1.75 1.51 0.31 0.72 0.04 0.51 0.02 7.22]\n",
      "Loss_Q: [0.85 0.97 0.27 0.68 0.04 0.52 0.03 0.   3.35] Loss_P: [2.4  1.76 1.51 0.32 0.7  0.05 0.51 0.03 7.28]\n",
      "Loss_Q: [0.91 0.99 0.33 0.65 0.04 0.5  0.02 0.   3.45] Loss_P: [2.36 1.73 1.56 0.31 0.65 0.05 0.51 0.01 7.19]\n",
      "Loss_Q: [0.87 1.02 0.3  0.66 0.06 0.52 0.01 0.   3.43] Loss_P: [2.31 1.85 1.52 0.32 0.68 0.03 0.51 0.02 7.26]\n",
      "Loss_Q: [0.95 0.98 0.29 0.64 0.03 0.52 0.02 0.   3.42] Loss_P: [2.41 1.76 1.52 0.33 0.71 0.05 0.52 0.02 7.32]\n",
      "Loss_Q: [0.84 0.99 0.3  0.65 0.06 0.52 0.01 0.   3.37] Loss_P: [2.35 1.79 1.54 0.34 0.65 0.06 0.51 0.01 7.24]\n",
      "Loss_Q: [0.86 1.01 0.3  0.69 0.05 0.51 0.01 0.   3.43] Loss_P: [2.31 1.78 1.59 0.3  0.66 0.04 0.52 0.03 7.22]\n",
      "Loss_Q: [0.83 0.98 0.3  0.67 0.03 0.51 0.02 0.   3.34] Loss_P: [2.36 1.78 1.61 0.32 0.71 0.05 0.52 0.02 7.36]\n",
      "Loss_Q: [0.91 1.01 0.31 0.69 0.03 0.52 0.02 0.   3.48] Loss_P: [2.37 1.76 1.56 0.31 0.7  0.04 0.51 0.01 7.27]\n",
      "Loss_Q: [0.86 1.01 0.27 0.64 0.03 0.53 0.02 0.   3.36] Loss_P: [2.35 1.84 1.57 0.29 0.67 0.05 0.51 0.01 7.3 ]\n",
      "Loss_Q: [0.94 1.01 0.31 0.69 0.05 0.52 0.03 0.   3.55] Loss_P: [2.33 1.76 1.62 0.34 0.67 0.04 0.52 0.02 7.31]\n",
      "Loss_Q: [0.96 1.04 0.29 0.71 0.03 0.52 0.01 0.   3.56] Loss_P: [2.38 1.76 1.6  0.33 0.67 0.04 0.51 0.02 7.31]\n",
      "Loss_Q: [0.91 1.04 0.33 0.65 0.03 0.51 0.03 0.   3.5 ] Loss_P: [2.36 1.76 1.55 0.32 0.69 0.05 0.5  0.03 7.27]\n",
      "Loss_Q: [0.97 1.06 0.31 0.69 0.04 0.51 0.02 0.   3.61] Loss_P: [2.38 1.81 1.51 0.32 0.69 0.04 0.51 0.01 7.28]\n",
      "Loss_Q: [0.87 0.99 0.28 0.65 0.05 0.51 0.02 0.   3.37] Loss_P: [2.32 1.74 1.54 0.34 0.7  0.04 0.52 0.03 7.23]\n",
      "Loss_Q: [0.83 0.96 0.33 0.7  0.04 0.51 0.02 0.   3.4 ] Loss_P: [2.35 1.79 1.6  0.31 0.67 0.04 0.52 0.01 7.29]\n",
      "Loss_Q: [0.9  1.04 0.24 0.65 0.03 0.51 0.02 0.   3.39] Loss_P: [2.34 1.76 1.6  0.27 0.7  0.05 0.5  0.04 7.26]\n",
      "Loss_Q: [0.92 0.97 0.29 0.69 0.04 0.51 0.02 0.   3.44] Loss_P: [2.38 1.81 1.58 0.28 0.66 0.04 0.52 0.01 7.28]\n",
      "Loss_Q: [0.92 1.   0.26 0.69 0.06 0.52 0.02 0.   3.46] Loss_P: [2.37 1.72 1.59 0.31 0.68 0.04 0.51 0.01 7.23]\n",
      "Loss_Q: [0.9  0.98 0.32 0.66 0.05 0.52 0.02 0.   3.44] Loss_P: [2.36 1.74 1.58 0.28 0.65 0.04 0.51 0.01 7.18]\n",
      "Loss_Q: [0.85 0.99 0.31 0.68 0.05 0.53 0.02 0.   3.43] Loss_P: [2.36 1.75 1.57 0.31 0.66 0.04 0.51 0.02 7.2 ]\n",
      "Loss_Q: [0.97 1.02 0.29 0.69 0.04 0.51 0.01 0.   3.52] Loss_P: [2.32 1.74 1.66 0.33 0.72 0.05 0.51 0.01 7.34]\n",
      "Loss_Q: [0.83 1.02 0.32 0.67 0.04 0.52 0.02 0.   3.42] Loss_P: [2.33 1.74 1.57 0.31 0.68 0.04 0.52 0.01 7.19]\n",
      "Loss_Q: [0.87 0.97 0.29 0.67 0.04 0.51 0.01 0.   3.37] Loss_P: [2.38 1.74 1.57 0.36 0.67 0.04 0.5  0.02 7.28]\n",
      "Loss_Q: [0.95 0.97 0.29 0.63 0.03 0.51 0.01 0.   3.4 ] Loss_P: [2.32 1.81 1.55 0.33 0.65 0.04 0.5  0.01 7.21]\n",
      "Loss_Q: [0.9  0.99 0.32 0.63 0.04 0.51 0.01 0.   3.4 ] Loss_P: [2.38 1.71 1.54 0.33 0.64 0.05 0.51 0.03 7.19]\n",
      "Loss_Q: [0.94 1.03 0.29 0.61 0.04 0.51 0.01 0.   3.44] Loss_P: [2.34 1.82 1.62 0.35 0.65 0.06 0.51 0.02 7.36]\n",
      "Loss_Q: [0.86 1.01 0.3  0.65 0.03 0.51 0.02 0.   3.37] Loss_P: [2.37 1.7  1.59 0.34 0.64 0.03 0.5  0.02 7.19]\n",
      "Loss_Q: [0.9  0.99 0.27 0.69 0.05 0.51 0.01 0.   3.42] Loss_P: [2.36 1.86 1.65 0.35 0.7  0.03 0.51 0.02 7.47]\n",
      "Loss_Q: [0.88 1.03 0.29 0.67 0.04 0.5  0.02 0.   3.44] Loss_P: [2.38 1.76 1.59 0.29 0.68 0.04 0.51 0.02 7.26]\n",
      "Loss_Q: [0.83 1.05 0.3  0.69 0.05 0.5  0.01 0.   3.44] Loss_P: [2.39 1.72 1.51 0.28 0.66 0.06 0.5  0.01 7.13]\n",
      "Loss_Q: [0.88 0.96 0.28 0.67 0.05 0.49 0.02 0.   3.35] Loss_P: [2.37 1.77 1.58 0.35 0.7  0.04 0.51 0.01 7.32]\n",
      "Loss_Q: [0.9  1.02 0.3  0.65 0.03 0.5  0.03 0.   3.43] Loss_P: [2.36 1.79 1.54 0.31 0.66 0.05 0.5  0.01 7.22]\n",
      "Loss_Q: [0.94 1.02 0.26 0.64 0.03 0.49 0.02 0.   3.41] Loss_P: [2.33 1.8  1.57 0.36 0.68 0.03 0.5  0.02 7.29]\n",
      "Loss_Q: [1.   0.98 0.29 0.66 0.04 0.51 0.01 0.   3.49] Loss_P: [2.31 1.84 1.6  0.31 0.64 0.04 0.5  0.02 7.27]\n",
      "Loss_Q: [0.93 1.   0.3  0.68 0.06 0.49 0.02 0.   3.49] Loss_P: [2.29 1.77 1.56 0.32 0.67 0.06 0.5  0.02 7.18]\n",
      "Loss_Q: [0.89 0.92 0.32 0.63 0.04 0.49 0.02 0.   3.32] Loss_P: [2.34 1.8  1.55 0.26 0.65 0.04 0.5  0.02 7.15]\n",
      "Loss_Q: [0.84 0.96 0.28 0.67 0.04 0.5  0.01 0.   3.3 ] Loss_P: [2.36 1.8  1.52 0.31 0.69 0.04 0.51 0.02 7.25]\n",
      "Loss_Q: [0.85 0.98 0.33 0.68 0.04 0.49 0.03 0.   3.4 ] Loss_P: [2.34 1.79 1.57 0.34 0.67 0.05 0.49 0.03 7.28]\n",
      "Loss_Q: [0.81 1.01 0.27 0.65 0.04 0.5  0.02 0.   3.3 ] Loss_P: [2.37 1.7  1.61 0.27 0.69 0.03 0.5  0.01 7.17]\n",
      "Loss_Q: [0.82 1.   0.27 0.62 0.04 0.5  0.01 0.   3.25] Loss_P: [2.38 1.71 1.54 0.3  0.69 0.05 0.48 0.03 7.17]\n",
      "Loss_Q: [0.84 0.95 0.28 0.7  0.05 0.52 0.02 0.   3.37] Loss_P: [2.39 1.74 1.5  0.33 0.66 0.03 0.5  0.02 7.19]\n",
      "Loss_Q: [0.87 0.99 0.34 0.69 0.04 0.5  0.02 0.   3.45] Loss_P: [2.33 1.74 1.56 0.3  0.65 0.03 0.51 0.02 7.14]\n",
      "Loss_Q: [0.9  0.98 0.29 0.66 0.04 0.51 0.02 0.   3.41] Loss_P: [2.39 1.74 1.6  0.32 0.68 0.04 0.5  0.02 7.29]\n",
      "Loss_Q: [0.96 1.   0.29 0.65 0.04 0.5  0.01 0.   3.45] Loss_P: [2.32 1.76 1.56 0.33 0.62 0.04 0.51 0.02 7.16]\n",
      "Loss_Q: [0.89 1.02 0.31 0.66 0.04 0.51 0.01 0.   3.43] Loss_P: [2.38 1.82 1.58 0.33 0.66 0.05 0.51 0.02 7.34]\n",
      "Loss_Q: [0.84 1.05 0.28 0.65 0.05 0.5  0.02 0.   3.39] Loss_P: [2.32 1.77 1.6  0.28 0.68 0.04 0.51 0.02 7.22]\n",
      "Loss_Q: [0.85 1.04 0.28 0.66 0.05 0.5  0.01 0.   3.4 ] Loss_P: [2.34 1.78 1.55 0.29 0.65 0.04 0.51 0.02 7.18]\n",
      "Loss_Q: [0.91 0.98 0.28 0.6  0.03 0.5  0.02 0.   3.33] Loss_P: [2.31 1.83 1.62 0.37 0.66 0.05 0.51 0.01 7.36]\n",
      "Loss_Q: [0.94 1.04 0.32 0.62 0.04 0.51 0.02 0.   3.48] Loss_P: [2.31 1.83 1.6  0.29 0.68 0.05 0.51 0.01 7.3 ]\n",
      "Loss_Q: [0.83 1.05 0.25 0.63 0.03 0.5  0.02 0.   3.31] Loss_P: [2.34 1.77 1.63 0.33 0.69 0.05 0.5  0.03 7.32]\n",
      "Loss_Q: [0.88 1.   0.3  0.67 0.03 0.49 0.03 0.   3.4 ] Loss_P: [2.26 1.79 1.62 0.31 0.6  0.03 0.5  0.03 7.14]\n",
      "Loss_Q: [0.89 0.99 0.28 0.67 0.05 0.47 0.01 0.   3.36] Loss_P: [2.32 1.74 1.6  0.3  0.66 0.04 0.49 0.02 7.17]\n",
      "Loss_Q: [0.86 1.01 0.28 0.59 0.04 0.47 0.02 0.   3.27] Loss_P: [2.28 1.79 1.64 0.31 0.64 0.05 0.48 0.02 7.21]\n",
      "Loss_Q: [1.05 1.02 0.31 0.64 0.05 0.47 0.02 0.   3.55] Loss_P: [2.31 1.77 1.63 0.31 0.59 0.02 0.47 0.02 7.11]\n",
      "Loss_Q: [1.02 1.05 0.34 0.66 0.05 0.49 0.02 0.   3.63] Loss_P: [2.31 1.8  1.69 0.33 0.68 0.05 0.47 0.01 7.35]\n",
      "Loss_Q: [0.92 1.08 0.29 0.62 0.04 0.46 0.03 0.   3.44] Loss_P: [2.27 1.83 1.71 0.32 0.65 0.03 0.45 0.02 7.27]\n",
      "Loss_Q: [0.92 1.03 0.31 0.69 0.04 0.44 0.03 0.   3.46] Loss_P: [2.31 1.81 1.71 0.33 0.64 0.05 0.46 0.01 7.31]\n",
      "Loss_Q: [0.98 1.03 0.29 0.66 0.05 0.46 0.01 0.   3.48] Loss_P: [2.33 1.76 1.72 0.26 0.65 0.04 0.44 0.01 7.22]\n",
      "Loss_Q: [0.92 1.09 0.33 0.65 0.03 0.47 0.01 0.   3.5 ] Loss_P: [2.29 1.81 1.72 0.33 0.69 0.04 0.46 0.03 7.37]\n",
      "Loss_Q: [1.   1.03 0.3  0.66 0.04 0.43 0.01 0.   3.48] Loss_P: [2.33 1.8  1.7  0.33 0.66 0.04 0.45 0.02 7.34]\n",
      "Loss_Q: [0.97 1.1  0.26 0.61 0.03 0.45 0.01 0.   3.44] Loss_P: [2.25 1.77 1.73 0.34 0.63 0.03 0.45 0.02 7.22]\n",
      "Loss_Q: [0.97 1.03 0.3  0.65 0.04 0.43 0.02 0.   3.43] Loss_P: [2.27 1.76 1.75 0.34 0.66 0.05 0.42 0.03 7.28]\n",
      "Loss_Q: [0.91 1.1  0.28 0.7  0.04 0.41 0.02 0.   3.47] Loss_P: [2.26 1.72 1.75 0.35 0.69 0.04 0.43 0.01 7.25]\n",
      "Loss_Q: [0.91 0.99 0.32 0.71 0.03 0.42 0.02 0.   3.38] Loss_P: [2.33 1.7  1.67 0.33 0.69 0.05 0.43 0.02 7.21]\n",
      "Loss_Q: [0.89 1.02 0.27 0.68 0.05 0.39 0.02 0.   3.32] Loss_P: [2.31 1.74 1.61 0.26 0.67 0.04 0.37 0.01 7.  ]\n",
      "Loss_Q: [0.83 1.03 0.28 0.62 0.05 0.38 0.02 0.   3.2 ] Loss_P: [2.34 1.69 1.61 0.31 0.69 0.05 0.37 0.01 7.07]\n",
      "Loss_Q: [0.9  1.01 0.3  0.67 0.04 0.35 0.01 0.   3.28] Loss_P: [2.33 1.71 1.67 0.28 0.68 0.05 0.35 0.01 7.08]\n",
      "Loss_Q: [0.95 1.07 0.32 0.68 0.06 0.35 0.02 0.   3.43] Loss_P: [2.3  1.78 1.68 0.28 0.65 0.03 0.36 0.02 7.1 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.87 1.   0.22 0.67 0.06 0.34 0.02 0.   3.18] Loss_P: [2.38 1.77 1.61 0.31 0.67 0.04 0.32 0.01 7.12]\n",
      "Loss_Q: [0.91 1.06 0.3  0.67 0.03 0.3  0.01 0.   3.3 ] Loss_P: [2.33 1.79 1.67 0.33 0.7  0.04 0.32 0.01 7.2 ]\n",
      "Loss_Q: [0.79 1.12 0.3  0.68 0.04 0.32 0.01 0.   3.25] Loss_P: [2.35 1.77 1.62 0.3  0.7  0.05 0.29 0.03 7.11]\n",
      "Loss_Q: [0.9  1.07 0.24 0.67 0.04 0.29 0.03 0.   3.23] Loss_P: [2.37 1.72 1.69 0.27 0.69 0.07 0.32 0.01 7.14]\n",
      "Loss_Q: [0.9  1.09 0.24 0.69 0.04 0.31 0.02 0.   3.29] Loss_P: [2.37 1.8  1.67 0.32 0.71 0.06 0.29 0.02 7.24]\n",
      "Loss_Q: [0.9  1.1  0.25 0.67 0.04 0.31 0.02 0.   3.28] Loss_P: [2.34 1.8  1.64 0.33 0.69 0.04 0.32 0.01 7.18]\n",
      "Loss_Q: [0.89 1.09 0.25 0.67 0.03 0.32 0.01 0.   3.26] Loss_P: [2.32 1.8  1.62 0.27 0.65 0.03 0.33 0.01 7.02]\n",
      "Loss_Q: [0.95 1.08 0.29 0.67 0.04 0.29 0.03 0.   3.35] Loss_P: [2.35 1.86 1.65 0.35 0.69 0.03 0.3  0.01 7.24]\n",
      "Loss_Q: [0.92 1.07 0.29 0.69 0.05 0.31 0.03 0.   3.36] Loss_P: [2.35 1.78 1.58 0.32 0.71 0.05 0.3  0.02 7.12]\n",
      "Loss_Q: [0.94 0.98 0.27 0.69 0.05 0.29 0.02 0.   3.24] Loss_P: [2.33 1.8  1.63 0.29 0.72 0.05 0.31 0.01 7.13]\n",
      "Loss_Q: [0.95 1.02 0.31 0.66 0.04 0.32 0.02 0.   3.32] Loss_P: [2.35 1.79 1.6  0.3  0.69 0.04 0.3  0.02 7.07]\n",
      "Loss_Q: [0.92 1.02 0.32 0.72 0.05 0.28 0.02 0.   3.34] Loss_P: [2.37 1.78 1.58 0.3  0.72 0.05 0.31 0.04 7.15]\n",
      "Loss_Q: [0.91 1.02 0.32 0.71 0.04 0.33 0.02 0.   3.33] Loss_P: [2.35 1.77 1.6  0.32 0.69 0.05 0.32 0.02 7.12]\n",
      "Loss_Q: [0.86 1.01 0.28 0.7  0.04 0.36 0.01 0.   3.27] Loss_P: [2.39 1.7  1.58 0.32 0.7  0.03 0.33 0.03 7.07]\n",
      "Loss_Q: [0.98 1.02 0.29 0.7  0.03 0.33 0.02 0.   3.36] Loss_P: [2.32 1.74 1.6  0.36 0.73 0.03 0.34 0.01 7.14]\n",
      "Loss_Q: [0.94 0.95 0.3  0.69 0.06 0.31 0.02 0.   3.27] Loss_P: [2.34 1.78 1.56 0.31 0.69 0.03 0.3  0.01 7.03]\n",
      "Loss_Q: [0.88 0.99 0.3  0.73 0.03 0.3  0.02 0.   3.24] Loss_P: [2.31 1.82 1.57 0.34 0.7  0.05 0.3  0.01 7.11]\n",
      "Loss_Q: [0.9  1.01 0.27 0.66 0.03 0.32 0.02 0.   3.21] Loss_P: [2.36 1.78 1.63 0.35 0.69 0.04 0.31 0.02 7.17]\n",
      "Loss_Q: [0.94 1.04 0.32 0.67 0.03 0.32 0.01 0.   3.33] Loss_P: [2.32 1.77 1.64 0.31 0.71 0.05 0.27 0.02 7.09]\n",
      "Loss_Q: [1.02 1.04 0.36 0.68 0.06 0.31 0.02 0.   3.49] Loss_P: [2.33 1.79 1.59 0.35 0.71 0.06 0.3  0.01 7.13]\n",
      "Loss_Q: [0.99 1.06 0.29 0.7  0.04 0.34 0.02 0.   3.45] Loss_P: [2.33 1.78 1.68 0.35 0.68 0.03 0.34 0.02 7.21]\n",
      "Loss_Q: [0.96 1.05 0.35 0.68 0.04 0.31 0.02 0.   3.4 ] Loss_P: [2.31 1.77 1.62 0.38 0.7  0.03 0.33 0.02 7.17]\n",
      "Loss_Q: [1.03 1.   0.38 0.67 0.07 0.32 0.01 0.   3.47] Loss_P: [2.3  1.8  1.67 0.43 0.71 0.05 0.34 0.01 7.32]\n",
      "Loss_Q: [1.03 1.02 0.29 0.67 0.05 0.36 0.02 0.   3.44] Loss_P: [2.32 1.82 1.67 0.38 0.68 0.04 0.32 0.01 7.25]\n",
      "Loss_Q: [0.98 0.99 0.36 0.71 0.03 0.37 0.02 0.   3.46] Loss_P: [2.38 1.79 1.62 0.39 0.69 0.06 0.33 0.03 7.29]\n",
      "Loss_Q: [1.04 0.99 0.3  0.68 0.05 0.31 0.01 0.   3.37] Loss_P: [2.28 1.77 1.68 0.36 0.68 0.03 0.31 0.02 7.13]\n",
      "Loss_Q: [0.97 1.03 0.35 0.71 0.03 0.3  0.02 0.   3.41] Loss_P: [2.33 1.81 1.63 0.36 0.67 0.04 0.32 0.02 7.19]\n",
      "Loss_Q: [0.99 0.99 0.3  0.67 0.04 0.32 0.02 0.   3.32] Loss_P: [2.32 1.78 1.59 0.37 0.69 0.03 0.32 0.01 7.11]\n",
      "Loss_Q: [0.88 1.04 0.3  0.69 0.03 0.34 0.02 0.   3.3 ] Loss_P: [2.3  1.84 1.56 0.36 0.68 0.04 0.34 0.02 7.14]\n",
      "Loss_Q: [0.96 0.99 0.29 0.68 0.04 0.31 0.02 0.   3.29] Loss_P: [2.33 1.79 1.62 0.35 0.7  0.04 0.35 0.02 7.19]\n",
      "Loss_Q: [0.97 1.02 0.3  0.71 0.03 0.36 0.02 0.   3.41] Loss_P: [2.35 1.74 1.61 0.33 0.74 0.05 0.33 0.02 7.17]\n",
      "Loss_Q: [0.94 1.01 0.24 0.69 0.03 0.39 0.03 0.   3.34] Loss_P: [2.3  1.8  1.64 0.37 0.72 0.03 0.37 0.02 7.25]\n",
      "Loss_Q: [0.92 0.92 0.27 0.71 0.05 0.34 0.02 0.   3.24] Loss_P: [2.35 1.73 1.56 0.32 0.69 0.04 0.33 0.01 7.02]\n",
      "Loss_Q: [1.05 0.94 0.27 0.67 0.03 0.36 0.03 0.   3.35] Loss_P: [2.29 1.85 1.6  0.34 0.71 0.04 0.36 0.01 7.2 ]\n",
      "Loss_Q: [0.96 0.99 0.3  0.66 0.04 0.33 0.02 0.   3.3 ] Loss_P: [2.41 1.67 1.53 0.3  0.68 0.04 0.32 0.02 6.97]\n",
      "Loss_Q: [0.95 0.96 0.26 0.67 0.04 0.31 0.01 0.   3.2 ] Loss_P: [2.32 1.79 1.53 0.31 0.71 0.04 0.32 0.01 7.02]\n",
      "Loss_Q: [0.88 0.95 0.27 0.72 0.03 0.34 0.02 0.   3.21] Loss_P: [2.38 1.79 1.53 0.38 0.69 0.03 0.31 0.02 7.14]\n",
      "Loss_Q: [0.93 0.92 0.26 0.74 0.04 0.34 0.02 0.   3.25] Loss_P: [2.34 1.8  1.57 0.35 0.68 0.04 0.34 0.02 7.14]\n",
      "Loss_Q: [0.92 0.95 0.31 0.71 0.03 0.32 0.02 0.   3.26] Loss_P: [2.3  1.8  1.58 0.34 0.71 0.05 0.3  0.02 7.11]\n",
      "Loss_Q: [0.92 0.96 0.32 0.68 0.04 0.36 0.01 0.   3.29] Loss_P: [2.36 1.75 1.63 0.34 0.7  0.05 0.33 0.02 7.18]\n",
      "Loss_Q: [1.02 0.95 0.29 0.66 0.04 0.36 0.02 0.   3.33] Loss_P: [2.33 1.78 1.52 0.36 0.7  0.05 0.35 0.03 7.12]\n",
      "Loss_Q: [0.97 0.92 0.3  0.7  0.05 0.37 0.02 0.   3.33] Loss_P: [2.36 1.76 1.56 0.34 0.69 0.04 0.35 0.02 7.11]\n",
      "Loss_Q: [1.06 0.95 0.3  0.68 0.03 0.32 0.02 0.   3.37] Loss_P: [2.32 1.79 1.57 0.31 0.68 0.04 0.35 0.02 7.08]\n",
      "Loss_Q: [0.86 0.93 0.26 0.67 0.04 0.34 0.02 0.   3.12] Loss_P: [2.29 1.73 1.59 0.3  0.67 0.05 0.35 0.01 6.98]\n",
      "Loss_Q: [0.96 0.97 0.32 0.73 0.05 0.33 0.01 0.   3.37] Loss_P: [2.37 1.75 1.53 0.34 0.75 0.05 0.31 0.02 7.1 ]\n",
      "Loss_Q: [0.91 0.96 0.25 0.72 0.04 0.35 0.01 0.   3.24] Loss_P: [2.33 1.77 1.61 0.28 0.7  0.04 0.32 0.01 7.07]\n",
      "Loss_Q: [0.91 0.93 0.25 0.71 0.03 0.37 0.02 0.   3.22] Loss_P: [2.33 1.8  1.52 0.29 0.71 0.03 0.34 0.01 7.03]\n",
      "Loss_Q: [1.01 0.95 0.26 0.67 0.05 0.34 0.03 0.   3.3 ] Loss_P: [2.37 1.69 1.54 0.28 0.68 0.05 0.34 0.02 6.97]\n",
      "Loss_Q: [0.89 0.98 0.25 0.69 0.03 0.32 0.01 0.   3.19] Loss_P: [2.35 1.79 1.52 0.3  0.69 0.05 0.31 0.01 7.02]\n",
      "Loss_Q: [0.94 0.99 0.27 0.66 0.03 0.33 0.01 0.   3.23] Loss_P: [2.33 1.81 1.56 0.29 0.7  0.04 0.31 0.02 7.06]\n",
      "Loss_Q: [0.87 0.92 0.28 0.67 0.05 0.3  0.01 0.   3.1 ] Loss_P: [2.33 1.72 1.53 0.33 0.69 0.04 0.31 0.02 6.96]\n",
      "Loss_Q: [0.94 0.95 0.25 0.69 0.03 0.31 0.01 0.   3.18] Loss_P: [2.38 1.76 1.49 0.27 0.7  0.05 0.3  0.01 6.94]\n",
      "Loss_Q: [0.87 0.9  0.27 0.71 0.05 0.33 0.03 0.   3.17] Loss_P: [2.32 1.81 1.51 0.29 0.69 0.03 0.29 0.02 6.95]\n",
      "Loss_Q: [0.92 0.93 0.3  0.68 0.05 0.31 0.02 0.   3.21] Loss_P: [2.39 1.78 1.5  0.33 0.68 0.03 0.3  0.01 7.03]\n",
      "Loss_Q: [0.9  0.98 0.3  0.7  0.04 0.32 0.02 0.   3.25] Loss_P: [2.32 1.77 1.52 0.28 0.7  0.04 0.31 0.02 6.97]\n",
      "Loss_Q: [0.92 0.93 0.27 0.7  0.05 0.33 0.03 0.   3.23] Loss_P: [2.33 1.79 1.49 0.34 0.69 0.05 0.29 0.02 6.99]\n",
      "Loss_Q: [0.9  0.92 0.31 0.7  0.04 0.35 0.02 0.   3.24] Loss_P: [2.32 1.82 1.52 0.29 0.7  0.04 0.32 0.02 7.04]\n",
      "Loss_Q: [0.94 0.96 0.28 0.7  0.04 0.32 0.02 0.   3.25] Loss_P: [2.34 1.88 1.52 0.34 0.67 0.03 0.31 0.02 7.12]\n",
      "Loss_Q: [1.01 0.92 0.31 0.72 0.08 0.31 0.02 0.   3.36] Loss_P: [2.31 1.85 1.5  0.3  0.66 0.04 0.31 0.02 6.99]\n",
      "Loss_Q: [0.98 0.87 0.3  0.71 0.05 0.35 0.02 0.   3.27] Loss_P: [2.31 1.81 1.49 0.32 0.73 0.04 0.35 0.03 7.07]\n",
      "Loss_Q: [1.01 0.93 0.29 0.65 0.04 0.34 0.02 0.   3.27] Loss_P: [2.3  1.84 1.54 0.32 0.66 0.05 0.33 0.01 7.06]\n",
      "Loss_Q: [1.01 0.92 0.3  0.69 0.04 0.35 0.02 0.   3.32] Loss_P: [2.31 1.8  1.56 0.35 0.7  0.05 0.34 0.02 7.12]\n",
      "Loss_Q: [1.09 0.94 0.29 0.67 0.05 0.34 0.02 0.   3.41] Loss_P: [2.32 1.77 1.54 0.3  0.74 0.04 0.34 0.01 7.06]\n",
      "Loss_Q: [0.94 0.94 0.27 0.68 0.05 0.32 0.01 0.   3.21] Loss_P: [2.3  1.87 1.56 0.31 0.67 0.03 0.33 0.02 7.09]\n",
      "Loss_Q: [0.95 0.96 0.27 0.69 0.04 0.33 0.02 0.   3.25] Loss_P: [2.33 1.74 1.47 0.33 0.7  0.06 0.34 0.01 6.98]\n",
      "Loss_Q: [0.86 0.88 0.26 0.71 0.03 0.3  0.02 0.   3.07] Loss_P: [2.32 1.78 1.49 0.3  0.7  0.05 0.29 0.02 6.95]\n",
      "Loss_Q: [0.97 0.97 0.29 0.7  0.04 0.32 0.03 0.   3.32] Loss_P: [2.31 1.77 1.5  0.32 0.74 0.04 0.32 0.01 7.  ]\n",
      "Loss_Q: [0.93 0.97 0.25 0.67 0.05 0.35 0.02 0.   3.23] Loss_P: [2.36 1.85 1.57 0.32 0.7  0.04 0.33 0.01 7.18]\n",
      "Loss_Q: [0.94 0.89 0.25 0.68 0.03 0.32 0.02 0.   3.13] Loss_P: [2.31 1.83 1.5  0.32 0.68 0.05 0.31 0.02 7.01]\n",
      "Loss_Q: [0.89 0.97 0.3  0.71 0.06 0.33 0.02 0.   3.28] Loss_P: [2.3  1.75 1.54 0.34 0.71 0.04 0.29 0.03 6.99]\n",
      "Loss_Q: [0.84 0.97 0.29 0.71 0.05 0.32 0.02 0.   3.19] Loss_P: [2.3  1.8  1.54 0.32 0.7  0.04 0.31 0.01 7.02]\n",
      "Loss_Q: [1.01 0.97 0.29 0.68 0.04 0.3  0.02 0.   3.31] Loss_P: [2.31 1.79 1.54 0.28 0.69 0.04 0.33 0.01 6.98]\n",
      "Loss_Q: [0.98 0.95 0.28 0.73 0.04 0.31 0.02 0.   3.32] Loss_P: [2.29 1.84 1.57 0.29 0.69 0.03 0.32 0.02 7.05]\n",
      "Loss_Q: [0.99 1.01 0.28 0.75 0.05 0.3  0.02 0.   3.4 ] Loss_P: [2.29 1.79 1.56 0.27 0.72 0.02 0.33 0.02 6.99]\n",
      "Loss_Q: [0.93 1.02 0.27 0.7  0.05 0.28 0.01 0.   3.25] Loss_P: [2.31 1.81 1.59 0.32 0.72 0.05 0.3  0.02 7.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.91 1.02 0.3  0.72 0.03 0.29 0.03 0.   3.29] Loss_P: [2.32 1.81 1.53 0.33 0.75 0.05 0.29 0.01 7.07]\n",
      "Loss_Q: [0.9  0.99 0.26 0.67 0.04 0.28 0.02 0.   3.16] Loss_P: [2.31 1.81 1.57 0.33 0.69 0.03 0.26 0.02 7.03]\n",
      "Loss_Q: [0.93 0.97 0.31 0.7  0.05 0.24 0.02 0.   3.23] Loss_P: [2.29 1.74 1.6  0.33 0.71 0.04 0.27 0.02 7.01]\n",
      "Loss_Q: [0.89 0.98 0.35 0.67 0.05 0.27 0.02 0.   3.23] Loss_P: [2.31 1.84 1.55 0.31 0.67 0.04 0.19 0.02 6.93]\n",
      "Loss_Q: [0.86 0.98 0.28 0.68 0.04 0.24 0.02 0.   3.1 ] Loss_P: [2.32 1.71 1.64 0.36 0.68 0.06 0.26 0.01 7.05]\n",
      "Loss_Q: [0.93 0.96 0.29 0.71 0.04 0.21 0.02 0.   3.15] Loss_P: [2.3  1.81 1.57 0.3  0.73 0.04 0.24 0.02 7.  ]\n",
      "Loss_Q: [0.95 0.99 0.27 0.71 0.04 0.26 0.02 0.   3.24] Loss_P: [2.27 1.75 1.64 0.36 0.7  0.04 0.24 0.01 7.01]\n",
      "Loss_Q: [0.9  1.03 0.3  0.69 0.04 0.24 0.03 0.   3.23] Loss_P: [2.35 1.77 1.56 0.34 0.68 0.05 0.25 0.02 7.01]\n",
      "Loss_Q: [0.95 0.96 0.28 0.65 0.05 0.27 0.02 0.   3.18] Loss_P: [2.27 1.82 1.62 0.31 0.69 0.05 0.26 0.01 7.04]\n",
      "Loss_Q: [0.86 0.96 0.32 0.69 0.03 0.24 0.02 0.   3.12] Loss_P: [2.29 1.8  1.58 0.32 0.73 0.06 0.22 0.02 7.02]\n",
      "Loss_Q: [0.86 0.98 0.32 0.71 0.05 0.26 0.02 0.   3.2 ] Loss_P: [2.31 1.77 1.56 0.31 0.73 0.05 0.22 0.01 6.97]\n",
      "Loss_Q: [0.88 0.94 0.32 0.7  0.03 0.31 0.02 0.   3.19] Loss_P: [2.3  1.78 1.59 0.34 0.74 0.05 0.27 0.01 7.09]\n",
      "Loss_Q: [0.9  1.02 0.29 0.77 0.05 0.28 0.01 0.   3.33] Loss_P: [2.28 1.74 1.61 0.36 0.71 0.03 0.28 0.02 7.04]\n",
      "Loss_Q: [0.83 0.97 0.32 0.72 0.03 0.28 0.03 0.   3.18] Loss_P: [2.27 1.74 1.57 0.34 0.74 0.05 0.28 0.02 7.01]\n",
      "Loss_Q: [0.86 1.01 0.3  0.74 0.04 0.24 0.02 0.   3.19] Loss_P: [2.32 1.75 1.56 0.33 0.72 0.05 0.28 0.02 7.05]\n",
      "Loss_Q: [0.85 0.99 0.33 0.74 0.04 0.26 0.01 0.   3.22] Loss_P: [2.31 1.76 1.55 0.35 0.74 0.06 0.28 0.03 7.07]\n",
      "Loss_Q: [0.82 0.98 0.28 0.7  0.04 0.27 0.01 0.   3.11] Loss_P: [2.28 1.74 1.54 0.36 0.73 0.03 0.25 0.02 6.95]\n",
      "Loss_Q: [0.87 0.93 0.29 0.76 0.04 0.31 0.03 0.   3.23] Loss_P: [2.29 1.76 1.57 0.31 0.72 0.04 0.3  0.02 7.01]\n",
      "Loss_Q: [0.88 0.93 0.26 0.68 0.04 0.3  0.01 0.   3.1 ] Loss_P: [2.3  1.74 1.57 0.38 0.68 0.04 0.28 0.01 6.99]\n",
      "Loss_Q: [0.88 1.   0.28 0.72 0.05 0.28 0.02 0.   3.23] Loss_P: [2.31 1.78 1.53 0.37 0.71 0.05 0.28 0.01 7.04]\n",
      "Loss_Q: [0.96 0.91 0.27 0.7  0.04 0.3  0.03 0.   3.2 ] Loss_P: [2.28 1.8  1.55 0.39 0.73 0.04 0.27 0.02 7.07]\n",
      "Loss_Q: [0.94 0.91 0.33 0.72 0.03 0.32 0.02 0.   3.27] Loss_P: [2.31 1.78 1.46 0.37 0.71 0.05 0.3  0.02 7.  ]\n",
      "Loss_Q: [0.9  0.97 0.3  0.68 0.04 0.25 0.01 0.   3.15] Loss_P: [2.31 1.77 1.55 0.37 0.73 0.03 0.28 0.01 7.05]\n",
      "Loss_Q: [0.95 0.97 0.3  0.7  0.03 0.27 0.01 0.   3.24] Loss_P: [2.3  1.78 1.54 0.34 0.69 0.05 0.28 0.02 7.  ]\n",
      "Loss_Q: [0.92 0.97 0.3  0.69 0.04 0.28 0.02 0.   3.22] Loss_P: [2.3  1.74 1.6  0.33 0.67 0.03 0.3  0.01 6.99]\n",
      "Loss_Q: [0.98 0.92 0.31 0.72 0.03 0.29 0.02 0.   3.27] Loss_P: [2.29 1.79 1.6  0.34 0.7  0.03 0.27 0.02 7.04]\n",
      "Loss_Q: [0.94 0.87 0.3  0.67 0.05 0.28 0.02 0.   3.14] Loss_P: [2.31 1.83 1.56 0.37 0.66 0.04 0.26 0.02 7.04]\n",
      "Loss_Q: [1.04 0.91 0.34 0.68 0.02 0.26 0.03 0.   3.27] Loss_P: [2.28 1.77 1.65 0.36 0.72 0.03 0.26 0.02 7.09]\n",
      "Loss_Q: [0.94 0.94 0.32 0.71 0.04 0.26 0.02 0.   3.22] Loss_P: [2.33 1.79 1.59 0.37 0.67 0.05 0.3  0.01 7.11]\n",
      "Loss_Q: [0.92 0.96 0.31 0.66 0.03 0.26 0.02 0.   3.16] Loss_P: [2.36 1.8  1.58 0.31 0.69 0.03 0.28 0.02 7.07]\n",
      "Loss_Q: [1.05 0.92 0.26 0.7  0.04 0.27 0.01 0.   3.26] Loss_P: [2.29 1.83 1.65 0.33 0.68 0.04 0.27 0.02 7.09]\n",
      "Loss_Q: [1.   0.93 0.3  0.67 0.02 0.27 0.02 0.   3.21] Loss_P: [2.33 1.81 1.58 0.33 0.68 0.04 0.27 0.02 7.06]\n",
      "Loss_Q: [0.94 0.95 0.28 0.67 0.05 0.24 0.02 0.   3.15] Loss_P: [2.29 1.79 1.59 0.34 0.68 0.04 0.27 0.02 7.03]\n",
      "Loss_Q: [0.95 0.91 0.32 0.7  0.04 0.28 0.03 0.   3.23] Loss_P: [2.25 1.81 1.64 0.32 0.66 0.04 0.23 0.03 6.99]\n",
      "Loss_Q: [1.04 1.01 0.29 0.67 0.04 0.26 0.02 0.   3.33] Loss_P: [2.29 1.85 1.66 0.41 0.68 0.03 0.29 0.01 7.23]\n",
      "Loss_Q: [0.98 0.98 0.28 0.67 0.04 0.27 0.02 0.   3.23] Loss_P: [2.24 1.77 1.67 0.37 0.67 0.03 0.28 0.01 7.06]\n",
      "Loss_Q: [1.1  0.98 0.3  0.66 0.02 0.25 0.02 0.   3.33] Loss_P: [2.27 1.73 1.61 0.31 0.7  0.05 0.26 0.01 6.94]\n",
      "Loss_Q: [0.9  0.88 0.26 0.63 0.04 0.25 0.02 0.   2.99] Loss_P: [2.32 1.73 1.67 0.36 0.68 0.03 0.22 0.02 7.04]\n",
      "Loss_Q: [0.93 0.97 0.3  0.66 0.05 0.23 0.02 0.   3.17] Loss_P: [2.28 1.73 1.62 0.33 0.66 0.03 0.23 0.02 6.9 ]\n",
      "Loss_Q: [0.86 0.88 0.29 0.72 0.03 0.25 0.02 0.   3.04] Loss_P: [2.39 1.71 1.54 0.37 0.65 0.03 0.24 0.02 6.96]\n",
      "Loss_Q: [0.9  0.87 0.32 0.67 0.04 0.25 0.02 0.   3.06] Loss_P: [2.25 1.76 1.61 0.34 0.62 0.03 0.22 0.02 6.85]\n",
      "Loss_Q: [0.95 0.96 0.3  0.67 0.04 0.22 0.02 0.   3.15] Loss_P: [2.29 1.69 1.63 0.33 0.63 0.03 0.25 0.01 6.87]\n",
      "Loss_Q: [1.02 0.96 0.3  0.65 0.04 0.22 0.01 0.   3.2 ] Loss_P: [2.32 1.74 1.6  0.3  0.63 0.04 0.24 0.02 6.89]\n",
      "Loss_Q: [0.93 0.95 0.3  0.68 0.03 0.25 0.02 0.   3.17] Loss_P: [2.28 1.82 1.66 0.32 0.66 0.04 0.23 0.02 7.03]\n",
      "Loss_Q: [0.98 0.95 0.29 0.67 0.03 0.24 0.02 0.   3.19] Loss_P: [2.27 1.86 1.68 0.39 0.67 0.03 0.23 0.02 7.15]\n",
      "Loss_Q: [0.95 0.91 0.3  0.69 0.03 0.26 0.01 0.   3.16] Loss_P: [2.26 1.83 1.74 0.32 0.65 0.04 0.23 0.02 7.08]\n",
      "Loss_Q: [0.99 0.95 0.31 0.68 0.04 0.25 0.02 0.   3.23] Loss_P: [2.28 1.81 1.68 0.34 0.66 0.03 0.23 0.03 7.06]\n",
      "Loss_Q: [0.94 0.91 0.29 0.68 0.03 0.23 0.02 0.   3.1 ] Loss_P: [2.29 1.75 1.65 0.34 0.65 0.04 0.23 0.01 6.96]\n",
      "Loss_Q: [1.05 0.92 0.31 0.67 0.05 0.26 0.02 0.   3.27] Loss_P: [2.31 1.82 1.68 0.37 0.67 0.03 0.2  0.02 7.09]\n",
      "Loss_Q: [0.96 0.92 0.31 0.68 0.04 0.23 0.02 0.   3.17] Loss_P: [2.27 1.82 1.58 0.32 0.66 0.04 0.23 0.02 6.95]\n",
      "Loss_Q: [0.98 0.91 0.29 0.67 0.05 0.21 0.02 0.   3.13] Loss_P: [2.28 1.78 1.58 0.38 0.65 0.04 0.22 0.01 6.94]\n",
      "Loss_Q: [1.01 0.94 0.31 0.66 0.04 0.19 0.02 0.   3.18] Loss_P: [2.27 1.81 1.65 0.34 0.68 0.04 0.23 0.02 7.06]\n",
      "Loss_Q: [0.96 0.93 0.29 0.67 0.04 0.19 0.02 0.   3.1 ] Loss_P: [2.23 1.86 1.58 0.32 0.66 0.05 0.23 0.02 6.95]\n",
      "Loss_Q: [0.99 0.97 0.27 0.64 0.04 0.23 0.02 0.   3.16] Loss_P: [2.29 1.75 1.61 0.35 0.71 0.03 0.18 0.02 6.93]\n",
      "Loss_Q: [1.   0.87 0.3  0.68 0.02 0.21 0.02 0.   3.09] Loss_P: [2.32 1.81 1.63 0.38 0.71 0.04 0.25 0.05 7.19]\n",
      "Loss_Q: [0.96 0.9  0.28 0.63 0.02 0.22 0.01 0.   3.03] Loss_P: [2.28 1.79 1.55 0.33 0.64 0.03 0.24 0.02 6.89]\n",
      "Loss_Q: [0.98 0.87 0.3  0.63 0.04 0.25 0.02 0.   3.09] Loss_P: [2.32 1.77 1.59 0.37 0.67 0.05 0.23 0.02 7.02]\n",
      "Loss_Q: [0.97 0.97 0.29 0.64 0.04 0.23 0.03 0.   3.17] Loss_P: [2.25 1.79 1.6  0.37 0.66 0.05 0.25 0.02 6.98]\n",
      "Loss_Q: [0.98 0.98 0.3  0.69 0.03 0.24 0.01 0.   3.23] Loss_P: [2.25 1.79 1.64 0.38 0.67 0.06 0.24 0.02 7.05]\n",
      "Loss_Q: [0.95 0.95 0.37 0.66 0.03 0.28 0.02 0.   3.26] Loss_P: [2.28 1.8  1.63 0.33 0.66 0.03 0.24 0.01 6.99]\n",
      "Loss_Q: [0.95 0.98 0.32 0.66 0.05 0.27 0.02 0.   3.25] Loss_P: [2.28 1.72 1.63 0.36 0.66 0.03 0.28 0.02 6.98]\n",
      "Loss_Q: [0.93 0.93 0.32 0.66 0.05 0.27 0.02 0.   3.19] Loss_P: [2.36 1.76 1.64 0.42 0.66 0.04 0.24 0.02 7.13]\n",
      "Loss_Q: [0.94 0.92 0.27 0.65 0.05 0.26 0.03 0.   3.12] Loss_P: [2.28 1.73 1.54 0.32 0.67 0.04 0.26 0.02 6.87]\n",
      "Loss_Q: [0.91 0.92 0.31 0.67 0.06 0.27 0.02 0.   3.15] Loss_P: [2.25 1.73 1.59 0.31 0.64 0.05 0.23 0.01 6.81]\n",
      "Loss_Q: [0.98 0.93 0.26 0.61 0.03 0.3  0.02 0.   3.12] Loss_P: [2.32 1.81 1.59 0.33 0.62 0.04 0.28 0.02 7.  ]\n",
      "Loss_Q: [1.11 0.92 0.32 0.64 0.04 0.27 0.01 0.   3.31] Loss_P: [2.26 1.77 1.63 0.34 0.64 0.03 0.27 0.01 6.95]\n",
      "Loss_Q: [0.96 0.96 0.29 0.65 0.04 0.27 0.03 0.   3.2 ] Loss_P: [2.32 1.82 1.62 0.34 0.66 0.04 0.24 0.02 7.07]\n",
      "Loss_Q: [1.01 0.97 0.28 0.64 0.03 0.24 0.02 0.   3.2 ] Loss_P: [2.29 1.73 1.62 0.3  0.64 0.03 0.25 0.02 6.89]\n",
      "Loss_Q: [0.97 0.97 0.28 0.66 0.04 0.23 0.01 0.   3.16] Loss_P: [2.19 1.84 1.65 0.35 0.67 0.03 0.25 0.03 7.  ]\n",
      "Loss_Q: [0.97 0.92 0.29 0.66 0.05 0.25 0.02 0.   3.16] Loss_P: [2.31 1.77 1.65 0.31 0.64 0.04 0.22 0.02 6.95]\n",
      "Loss_Q: [0.96 0.98 0.3  0.66 0.02 0.21 0.01 0.   3.14] Loss_P: [2.31 1.75 1.65 0.34 0.65 0.03 0.22 0.02 6.97]\n",
      "Loss_Q: [0.96 0.94 0.27 0.65 0.05 0.18 0.01 0.   3.06] Loss_P: [2.31 1.81 1.62 0.27 0.64 0.04 0.23 0.01 6.94]\n",
      "Loss_Q: [0.96 0.94 0.27 0.63 0.03 0.19 0.02 0.   3.04] Loss_P: [2.34 1.74 1.63 0.35 0.66 0.04 0.22 0.01 7.  ]\n",
      "Loss_Q: [0.94 0.93 0.29 0.62 0.05 0.23 0.02 0.   3.08] Loss_P: [2.29 1.79 1.59 0.4  0.64 0.04 0.22 0.02 6.99]\n",
      "Loss_Q: [0.94 0.92 0.28 0.65 0.04 0.22 0.01 0.   3.06] Loss_P: [2.28 1.84 1.53 0.3  0.65 0.04 0.2  0.01 6.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.   0.93 0.31 0.68 0.04 0.21 0.02 0.   3.18] Loss_P: [2.24 1.8  1.58 0.31 0.64 0.03 0.17 0.01 6.78]\n",
      "Loss_Q: [0.96 0.91 0.25 0.64 0.03 0.18 0.01 0.   2.99] Loss_P: [2.27 1.79 1.55 0.31 0.66 0.03 0.22 0.02 6.86]\n",
      "Loss_Q: [0.86 0.94 0.28 0.63 0.04 0.2  0.02 0.   2.96] Loss_P: [2.28 1.82 1.57 0.34 0.64 0.05 0.21 0.01 6.92]\n",
      "Loss_Q: [0.96 0.9  0.26 0.64 0.03 0.21 0.02 0.   3.02] Loss_P: [2.31 1.79 1.57 0.31 0.65 0.03 0.2  0.01 6.88]\n",
      "Loss_Q: [0.94 0.85 0.25 0.63 0.04 0.22 0.02 0.   2.94] Loss_P: [2.33 1.85 1.52 0.29 0.67 0.03 0.2  0.02 6.91]\n",
      "Loss_Q: [1.02 1.01 0.26 0.66 0.03 0.22 0.02 0.   3.22] Loss_P: [2.3  1.8  1.58 0.28 0.65 0.05 0.25 0.02 6.94]\n",
      "Loss_Q: [1.02 0.92 0.24 0.63 0.04 0.22 0.02 0.   3.09] Loss_P: [2.29 1.79 1.54 0.32 0.67 0.05 0.23 0.02 6.9 ]\n",
      "Loss_Q: [0.96 0.98 0.25 0.66 0.04 0.2  0.01 0.   3.1 ] Loss_P: [2.36 1.74 1.59 0.3  0.64 0.03 0.24 0.02 6.91]\n",
      "Loss_Q: [0.89 0.94 0.25 0.67 0.06 0.24 0.01 0.   3.05] Loss_P: [2.37 1.73 1.62 0.34 0.7  0.04 0.2  0.01 7.02]\n",
      "Loss_Q: [1.   0.97 0.23 0.65 0.04 0.25 0.01 0.   3.16] Loss_P: [2.35 1.76 1.53 0.3  0.68 0.04 0.23 0.04 6.93]\n",
      "Loss_Q: [0.94 0.87 0.24 0.65 0.03 0.23 0.02 0.   2.99] Loss_P: [2.3  1.77 1.52 0.28 0.63 0.05 0.22 0.02 6.8 ]\n",
      "Loss_Q: [1.   0.98 0.26 0.66 0.04 0.22 0.01 0.   3.16] Loss_P: [2.3  1.8  1.58 0.29 0.61 0.04 0.22 0.02 6.87]\n",
      "Loss_Q: [0.93 0.94 0.29 0.67 0.03 0.22 0.02 0.   3.1 ] Loss_P: [2.36 1.72 1.51 0.27 0.63 0.03 0.21 0.02 6.75]\n",
      "Loss_Q: [0.9  0.91 0.3  0.67 0.04 0.2  0.02 0.   3.04] Loss_P: [2.31 1.75 1.54 0.35 0.65 0.03 0.19 0.03 6.86]\n",
      "Loss_Q: [0.95 0.98 0.31 0.67 0.05 0.23 0.02 0.   3.22] Loss_P: [2.29 1.81 1.57 0.29 0.68 0.05 0.2  0.01 6.9 ]\n",
      "Loss_Q: [1.03 0.99 0.3  0.68 0.03 0.19 0.02 0.   3.23] Loss_P: [2.38 1.77 1.57 0.29 0.64 0.04 0.22 0.03 6.95]\n",
      "Loss_Q: [0.89 0.93 0.25 0.64 0.03 0.26 0.01 0.   3.02] Loss_P: [2.41 1.67 1.54 0.32 0.66 0.04 0.19 0.02 6.85]\n",
      "Loss_Q: [0.95 0.95 0.26 0.64 0.03 0.2  0.03 0.   3.06] Loss_P: [2.33 1.76 1.57 0.34 0.67 0.05 0.21 0.02 6.94]\n",
      "Loss_Q: [0.92 0.88 0.27 0.62 0.04 0.2  0.02 0.   2.95] Loss_P: [2.37 1.77 1.53 0.29 0.63 0.03 0.19 0.02 6.82]\n",
      "Loss_Q: [1.04 0.92 0.28 0.59 0.03 0.22 0.02 0.   3.1 ] Loss_P: [2.31 1.73 1.55 0.36 0.63 0.03 0.21 0.01 6.84]\n",
      "Loss_Q: [0.92 0.8  0.27 0.64 0.05 0.22 0.01 0.   2.91] Loss_P: [2.31 1.8  1.48 0.32 0.63 0.04 0.24 0.01 6.84]\n",
      "Loss_Q: [0.93 0.84 0.28 0.61 0.03 0.22 0.02 0.   2.93] Loss_P: [2.34 1.7  1.45 0.38 0.62 0.04 0.2  0.02 6.75]\n",
      "Loss_Q: [0.98 0.93 0.27 0.62 0.05 0.24 0.02 0.   3.09] Loss_P: [2.35 1.79 1.54 0.35 0.61 0.05 0.24 0.02 6.95]\n",
      "Loss_Q: [0.9  0.9  0.33 0.64 0.05 0.23 0.02 0.   3.09] Loss_P: [2.39 1.76 1.56 0.31 0.62 0.05 0.25 0.02 6.94]\n",
      "Loss_Q: [1.02 0.88 0.35 0.61 0.03 0.24 0.02 0.   3.16] Loss_P: [2.35 1.71 1.54 0.35 0.64 0.03 0.26 0.03 6.91]\n",
      "Loss_Q: [0.94 0.9  0.26 0.64 0.04 0.24 0.01 0.   3.03] Loss_P: [2.34 1.75 1.54 0.37 0.6  0.04 0.25 0.02 6.9 ]\n",
      "Loss_Q: [0.97 0.96 0.27 0.62 0.03 0.25 0.01 0.   3.11] Loss_P: [2.34 1.72 1.55 0.33 0.66 0.03 0.26 0.02 6.9 ]\n",
      "Loss_Q: [0.92 0.92 0.33 0.64 0.04 0.23 0.02 0.   3.09] Loss_P: [2.29 1.77 1.58 0.32 0.63 0.04 0.23 0.02 6.89]\n",
      "Loss_Q: [0.98 0.87 0.3  0.66 0.04 0.21 0.01 0.   3.07] Loss_P: [2.34 1.76 1.6  0.35 0.66 0.04 0.25 0.02 7.01]\n",
      "Loss_Q: [0.86 0.89 0.29 0.66 0.03 0.19 0.02 0.   2.94] Loss_P: [2.35 1.71 1.56 0.31 0.64 0.03 0.23 0.02 6.85]\n",
      "Loss_Q: [0.96 0.86 0.28 0.65 0.04 0.21 0.02 0.   3.02] Loss_P: [2.35 1.68 1.6  0.34 0.65 0.04 0.22 0.02 6.9 ]\n",
      "Loss_Q: [0.86 0.88 0.32 0.66 0.04 0.2  0.01 0.   2.97] Loss_P: [2.35 1.66 1.53 0.34 0.64 0.03 0.23 0.02 6.8 ]\n",
      "Loss_Q: [0.88 0.9  0.34 0.63 0.03 0.23 0.01 0.   3.02] Loss_P: [2.28 1.75 1.54 0.36 0.63 0.04 0.22 0.02 6.83]\n",
      "Loss_Q: [0.93 0.9  0.37 0.63 0.03 0.21 0.02 0.   3.09] Loss_P: [2.3  1.74 1.56 0.36 0.66 0.04 0.22 0.02 6.89]\n",
      "Loss_Q: [0.89 0.84 0.29 0.64 0.05 0.25 0.02 0.   2.98] Loss_P: [2.35 1.73 1.5  0.33 0.65 0.04 0.21 0.01 6.82]\n",
      "Loss_Q: [0.85 0.97 0.29 0.63 0.04 0.19 0.02 0.   2.98] Loss_P: [2.32 1.8  1.56 0.36 0.65 0.05 0.19 0.01 6.95]\n",
      "Loss_Q: [0.94 0.92 0.32 0.64 0.03 0.19 0.01 0.   3.06] Loss_P: [2.37 1.75 1.57 0.35 0.66 0.02 0.18 0.02 6.92]\n",
      "Loss_Q: [0.95 0.89 0.31 0.64 0.04 0.22 0.01 0.   3.06] Loss_P: [2.34 1.68 1.55 0.32 0.67 0.02 0.21 0.01 6.8 ]\n",
      "Loss_Q: [0.91 0.93 0.32 0.64 0.03 0.22 0.01 0.   3.07] Loss_P: [2.33 1.8  1.54 0.35 0.65 0.04 0.21 0.02 6.94]\n",
      "Loss_Q: [0.95 0.91 0.31 0.64 0.03 0.22 0.01 0.   3.06] Loss_P: [2.29 1.8  1.59 0.32 0.63 0.03 0.2  0.01 6.86]\n",
      "Loss_Q: [0.99 0.93 0.32 0.63 0.04 0.23 0.02 0.   3.16] Loss_P: [2.32 1.74 1.53 0.32 0.65 0.04 0.2  0.01 6.82]\n",
      "Loss_Q: [0.91 0.93 0.33 0.61 0.03 0.2  0.02 0.   3.03] Loss_P: [2.32 1.81 1.58 0.36 0.59 0.05 0.23 0.01 6.95]\n",
      "Loss_Q: [0.92 0.91 0.28 0.61 0.03 0.24 0.03 0.   3.03] Loss_P: [2.31 1.75 1.53 0.39 0.65 0.03 0.24 0.02 6.91]\n",
      "Loss_Q: [0.98 0.92 0.32 0.64 0.03 0.22 0.02 0.   3.14] Loss_P: [2.29 1.77 1.57 0.31 0.62 0.04 0.24 0.02 6.87]\n",
      "Loss_Q: [0.98 0.92 0.36 0.62 0.04 0.22 0.01 0.   3.15] Loss_P: [2.31 1.69 1.57 0.32 0.64 0.03 0.22 0.02 6.8 ]\n",
      "Loss_Q: [0.91 0.93 0.32 0.63 0.02 0.2  0.01 0.   3.02] Loss_P: [2.36 1.74 1.55 0.33 0.64 0.03 0.22 0.01 6.89]\n",
      "Loss_Q: [0.93 0.99 0.3  0.64 0.06 0.18 0.01 0.   3.11] Loss_P: [2.29 1.77 1.53 0.33 0.64 0.03 0.18 0.02 6.79]\n",
      "Loss_Q: [0.87 0.91 0.31 0.66 0.03 0.18 0.02 0.   2.98] Loss_P: [2.27 1.78 1.52 0.38 0.65 0.03 0.16 0.02 6.81]\n",
      "Loss_Q: [0.96 0.97 0.34 0.63 0.05 0.21 0.02 0.   3.18] Loss_P: [2.35 1.75 1.54 0.4  0.66 0.03 0.19 0.02 6.94]\n",
      "Loss_Q: [0.94 0.91 0.34 0.61 0.02 0.21 0.02 0.   3.06] Loss_P: [2.29 1.77 1.59 0.37 0.63 0.04 0.18 0.01 6.89]\n",
      "Loss_Q: [0.86 0.96 0.33 0.62 0.04 0.2  0.02 0.   3.04] Loss_P: [2.33 1.7  1.53 0.35 0.64 0.04 0.19 0.02 6.8 ]\n",
      "Loss_Q: [0.94 0.95 0.34 0.64 0.02 0.2  0.02 0.   3.11] Loss_P: [2.33 1.73 1.59 0.42 0.66 0.03 0.16 0.01 6.93]\n",
      "Loss_Q: [0.87 0.91 0.37 0.66 0.03 0.16 0.03 0.   3.02] Loss_P: [2.29 1.7  1.57 0.36 0.65 0.05 0.18 0.01 6.82]\n",
      "Loss_Q: [0.81 0.85 0.33 0.63 0.04 0.18 0.02 0.   2.86] Loss_P: [2.32 1.65 1.52 0.42 0.64 0.03 0.18 0.01 6.78]\n",
      "Loss_Q: [0.8  0.91 0.38 0.66 0.05 0.19 0.02 0.   3.  ] Loss_P: [2.34 1.68 1.58 0.38 0.64 0.03 0.15 0.01 6.8 ]\n",
      "Loss_Q: [0.89 0.91 0.33 0.65 0.03 0.15 0.01 0.   2.97] Loss_P: [2.36 1.65 1.53 0.39 0.66 0.04 0.19 0.02 6.85]\n",
      "Loss_Q: [0.96 0.86 0.33 0.62 0.04 0.18 0.02 0.   3.  ] Loss_P: [2.33 1.66 1.51 0.39 0.66 0.02 0.15 0.01 6.74]\n",
      "Loss_Q: [0.9  0.88 0.3  0.64 0.04 0.2  0.02 0.   2.99] Loss_P: [2.34 1.67 1.54 0.35 0.67 0.04 0.19 0.01 6.82]\n",
      "Loss_Q: [0.89 0.94 0.33 0.66 0.04 0.19 0.03 0.   3.08] Loss_P: [2.29 1.68 1.59 0.4  0.64 0.03 0.2  0.02 6.85]\n",
      "Loss_Q: [0.9  0.85 0.32 0.62 0.03 0.2  0.02 0.   2.94] Loss_P: [2.3  1.72 1.49 0.38 0.65 0.03 0.17 0.02 6.78]\n",
      "Loss_Q: [0.87 0.92 0.37 0.62 0.04 0.19 0.02 0.   3.04] Loss_P: [2.3  1.68 1.58 0.41 0.64 0.04 0.17 0.01 6.83]\n",
      "Loss_Q: [0.86 0.89 0.34 0.64 0.03 0.18 0.01 0.   2.95] Loss_P: [2.35 1.67 1.56 0.44 0.64 0.03 0.2  0.02 6.91]\n",
      "Loss_Q: [0.95 0.86 0.3  0.6  0.02 0.2  0.02 0.   2.95] Loss_P: [2.29 1.7  1.52 0.41 0.68 0.04 0.21 0.01 6.86]\n",
      "Loss_Q: [0.84 0.93 0.29 0.62 0.04 0.17 0.02 0.   2.92] Loss_P: [2.29 1.7  1.53 0.43 0.69 0.04 0.17 0.02 6.88]\n",
      "Loss_Q: [0.9  0.88 0.36 0.61 0.03 0.17 0.02 0.   2.97] Loss_P: [2.32 1.72 1.59 0.4  0.62 0.03 0.19 0.03 6.9 ]\n",
      "Loss_Q: [0.82 0.94 0.37 0.67 0.04 0.18 0.02 0.   3.03] Loss_P: [2.32 1.7  1.58 0.34 0.65 0.04 0.15 0.01 6.79]\n",
      "Loss_Q: [0.86 0.92 0.33 0.65 0.05 0.18 0.03 0.   3.02] Loss_P: [2.36 1.69 1.53 0.34 0.66 0.05 0.18 0.01 6.81]\n",
      "Loss_Q: [0.82 0.95 0.3  0.63 0.04 0.2  0.02 0.   2.96] Loss_P: [2.33 1.67 1.55 0.4  0.67 0.05 0.21 0.01 6.88]\n",
      "Loss_Q: [0.89 0.93 0.35 0.63 0.04 0.16 0.01 0.   3.02] Loss_P: [2.32 1.72 1.55 0.39 0.65 0.03 0.19 0.01 6.86]\n",
      "Loss_Q: [0.87 0.9  0.33 0.61 0.03 0.18 0.01 0.   2.93] Loss_P: [2.41 1.69 1.56 0.38 0.63 0.03 0.17 0.03 6.9 ]\n",
      "Loss_Q: [0.85 0.96 0.34 0.64 0.05 0.2  0.02 0.   3.06] Loss_P: [2.3  1.68 1.55 0.35 0.66 0.03 0.16 0.03 6.76]\n",
      "Loss_Q: [0.87 0.96 0.3  0.64 0.03 0.2  0.02 0.   3.01] Loss_P: [2.3  1.63 1.55 0.37 0.65 0.02 0.19 0.01 6.72]\n",
      "Loss_Q: [0.79 0.94 0.29 0.64 0.05 0.18 0.02 0.   2.91] Loss_P: [2.38 1.66 1.5  0.38 0.65 0.05 0.21 0.01 6.85]\n",
      "Loss_Q: [0.83 0.95 0.35 0.66 0.02 0.17 0.02 0.   3.01] Loss_P: [2.39 1.66 1.57 0.39 0.65 0.05 0.18 0.02 6.9 ]\n",
      "Loss_Q: [0.78 0.93 0.34 0.63 0.02 0.18 0.01 0.   2.89] Loss_P: [2.34 1.65 1.6  0.47 0.68 0.04 0.18 0.01 6.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.79 0.95 0.38 0.67 0.04 0.18 0.02 0.   3.02] Loss_P: [2.27 1.75 1.57 0.42 0.64 0.05 0.17 0.01 6.87]\n",
      "Loss_Q: [0.85 0.91 0.37 0.63 0.05 0.16 0.01 0.   2.98] Loss_P: [2.34 1.65 1.56 0.45 0.63 0.03 0.17 0.02 6.85]\n",
      "Loss_Q: [0.78 0.88 0.35 0.63 0.04 0.17 0.02 0.   2.89] Loss_P: [2.34 1.62 1.5  0.43 0.67 0.03 0.17 0.01 6.76]\n",
      "Loss_Q: [0.78 0.92 0.35 0.6  0.04 0.18 0.02 0.   2.9 ] Loss_P: [2.32 1.71 1.53 0.41 0.62 0.03 0.16 0.02 6.8 ]\n",
      "Loss_Q: [0.9  0.96 0.36 0.62 0.04 0.15 0.02 0.   3.05] Loss_P: [2.3  1.72 1.55 0.39 0.61 0.04 0.14 0.01 6.76]\n",
      "Loss_Q: [0.82 0.89 0.36 0.62 0.04 0.13 0.02 0.   2.88] Loss_P: [2.33 1.66 1.6  0.45 0.65 0.06 0.17 0.02 6.94]\n",
      "Loss_Q: [0.78 0.94 0.34 0.63 0.05 0.15 0.02 0.   2.9 ] Loss_P: [2.34 1.68 1.55 0.42 0.61 0.03 0.14 0.02 6.8 ]\n",
      "Loss_Q: [0.79 0.91 0.33 0.62 0.05 0.15 0.01 0.   2.85] Loss_P: [2.3  1.69 1.62 0.37 0.63 0.03 0.13 0.01 6.79]\n",
      "Loss_Q: [0.81 0.95 0.34 0.61 0.04 0.15 0.02 0.   2.91] Loss_P: [2.31 1.65 1.59 0.38 0.63 0.03 0.17 0.01 6.77]\n",
      "Loss_Q: [0.78 0.98 0.33 0.61 0.04 0.16 0.01 0.   2.92] Loss_P: [2.34 1.75 1.59 0.41 0.62 0.04 0.14 0.02 6.91]\n",
      "Loss_Q: [0.82 0.95 0.36 0.62 0.04 0.17 0.02 0.   2.97] Loss_P: [2.4  1.58 1.6  0.4  0.61 0.04 0.17 0.01 6.8 ]\n",
      "Loss_Q: [0.86 0.95 0.36 0.59 0.05 0.18 0.02 0.   3.01] Loss_P: [2.29 1.66 1.61 0.4  0.56 0.04 0.17 0.02 6.75]\n",
      "Loss_Q: [0.79 0.91 0.37 0.62 0.06 0.14 0.02 0.   2.9 ] Loss_P: [2.32 1.67 1.55 0.43 0.59 0.03 0.16 0.02 6.77]\n",
      "Loss_Q: [0.74 0.89 0.33 0.61 0.05 0.16 0.01 0.   2.78] Loss_P: [2.34 1.6  1.53 0.38 0.61 0.04 0.17 0.02 6.68]\n",
      "Loss_Q: [0.8  0.89 0.37 0.6  0.05 0.16 0.02 0.   2.9 ] Loss_P: [2.36 1.68 1.5  0.34 0.59 0.03 0.13 0.02 6.66]\n",
      "Loss_Q: [0.8  0.96 0.37 0.62 0.06 0.17 0.02 0.   3.01] Loss_P: [2.36 1.64 1.52 0.42 0.59 0.04 0.17 0.03 6.77]\n",
      "Loss_Q: [0.72 0.9  0.38 0.59 0.07 0.16 0.02 0.   2.83] Loss_P: [2.34 1.64 1.6  0.39 0.6  0.06 0.18 0.01 6.82]\n",
      "Loss_Q: [0.74 0.87 0.41 0.62 0.05 0.18 0.01 0.   2.87] Loss_P: [2.38 1.65 1.58 0.45 0.6  0.03 0.18 0.02 6.88]\n",
      "Loss_Q: [0.81 0.93 0.37 0.61 0.03 0.2  0.02 0.   2.97] Loss_P: [2.34 1.68 1.61 0.44 0.58 0.06 0.17 0.03 6.91]\n",
      "Loss_Q: [0.76 0.93 0.4  0.58 0.05 0.16 0.01 0.   2.9 ] Loss_P: [2.31 1.66 1.58 0.4  0.62 0.05 0.17 0.02 6.8 ]\n",
      "Loss_Q: [0.88 0.91 0.35 0.61 0.04 0.18 0.02 0.   2.98] Loss_P: [2.33 1.62 1.59 0.4  0.59 0.07 0.17 0.02 6.79]\n",
      "Loss_Q: [0.86 0.94 0.36 0.62 0.04 0.18 0.02 0.   3.02] Loss_P: [2.35 1.6  1.57 0.42 0.58 0.04 0.18 0.02 6.76]\n",
      "Loss_Q: [0.86 0.94 0.35 0.61 0.04 0.17 0.01 0.   2.98] Loss_P: [2.34 1.7  1.63 0.39 0.62 0.04 0.19 0.04 6.97]\n",
      "Loss_Q: [0.86 0.98 0.35 0.68 0.05 0.18 0.02 0.   3.11] Loss_P: [2.34 1.65 1.58 0.37 0.66 0.06 0.18 0.02 6.85]\n",
      "Loss_Q: [0.82 0.97 0.34 0.61 0.03 0.18 0.01 0.   2.96] Loss_P: [2.35 1.64 1.63 0.38 0.66 0.03 0.18 0.01 6.87]\n",
      "Loss_Q: [0.78 0.89 0.35 0.59 0.02 0.18 0.02 0.   2.83] Loss_P: [2.36 1.6  1.58 0.41 0.65 0.05 0.19 0.03 6.87]\n",
      "Loss_Q: [0.82 0.99 0.32 0.63 0.05 0.15 0.01 0.   2.97] Loss_P: [2.28 1.67 1.65 0.39 0.62 0.06 0.17 0.01 6.85]\n",
      "Loss_Q: [0.95 0.92 0.38 0.61 0.03 0.2  0.02 0.   3.12] Loss_P: [2.29 1.61 1.61 0.4  0.65 0.04 0.17 0.02 6.79]\n",
      "Loss_Q: [0.81 0.93 0.36 0.63 0.06 0.15 0.02 0.   2.96] Loss_P: [2.38 1.65 1.58 0.38 0.62 0.05 0.18 0.01 6.85]\n",
      "Loss_Q: [0.81 0.93 0.34 0.62 0.03 0.15 0.01 0.   2.9 ] Loss_P: [2.4  1.65 1.56 0.38 0.61 0.05 0.16 0.02 6.82]\n",
      "Loss_Q: [0.81 0.88 0.34 0.66 0.03 0.16 0.01 0.   2.9 ] Loss_P: [2.39 1.62 1.54 0.39 0.66 0.04 0.15 0.01 6.8 ]\n",
      "Loss_Q: [0.72 0.91 0.36 0.65 0.05 0.17 0.02 0.   2.87] Loss_P: [2.35 1.62 1.56 0.38 0.64 0.05 0.16 0.01 6.75]\n",
      "Loss_Q: [0.79 0.91 0.39 0.68 0.04 0.13 0.01 0.   2.95] Loss_P: [2.35 1.61 1.55 0.39 0.64 0.04 0.15 0.04 6.77]\n",
      "Loss_Q: [0.75 0.95 0.38 0.66 0.03 0.16 0.02 0.   2.94] Loss_P: [2.32 1.64 1.57 0.43 0.66 0.03 0.13 0.01 6.79]\n",
      "Loss_Q: [0.78 0.94 0.37 0.68 0.04 0.14 0.02 0.   2.98] Loss_P: [2.31 1.58 1.58 0.44 0.63 0.04 0.15 0.02 6.75]\n",
      "Loss_Q: [0.76 0.87 0.37 0.66 0.04 0.15 0.02 0.   2.86] Loss_P: [2.36 1.61 1.56 0.4  0.68 0.04 0.17 0.01 6.83]\n",
      "Loss_Q: [0.78 0.97 0.41 0.68 0.05 0.14 0.03 0.   3.06] Loss_P: [2.4  1.6  1.54 0.4  0.7  0.03 0.19 0.02 6.85]\n",
      "Loss_Q: [0.88 0.94 0.42 0.65 0.02 0.15 0.02 0.   3.08] Loss_P: [2.33 1.62 1.59 0.46 0.66 0.04 0.16 0.02 6.87]\n",
      "Loss_Q: [0.78 0.89 0.41 0.63 0.04 0.14 0.02 0.   2.92] Loss_P: [2.35 1.6  1.64 0.43 0.63 0.03 0.16 0.01 6.85]\n",
      "Loss_Q: [0.74 0.9  0.41 0.61 0.06 0.14 0.02 0.   2.88] Loss_P: [2.33 1.64 1.58 0.48 0.62 0.03 0.12 0.02 6.83]\n",
      "Loss_Q: [0.86 0.96 0.45 0.6  0.06 0.15 0.02 0.   3.09] Loss_P: [2.27 1.64 1.61 0.42 0.56 0.05 0.15 0.02 6.73]\n",
      "Loss_Q: [0.8  0.89 0.42 0.56 0.05 0.15 0.01 0.   2.88] Loss_P: [2.28 1.64 1.58 0.47 0.61 0.05 0.12 0.02 6.76]\n",
      "Loss_Q: [0.77 0.94 0.37 0.59 0.04 0.15 0.01 0.   2.86] Loss_P: [2.28 1.67 1.64 0.4  0.59 0.05 0.14 0.02 6.78]\n",
      "Loss_Q: [0.86 0.95 0.39 0.61 0.06 0.16 0.01 0.   3.04] Loss_P: [2.31 1.68 1.64 0.43 0.59 0.05 0.15 0.02 6.87]\n",
      "Loss_Q: [0.88 0.97 0.4  0.58 0.05 0.1  0.01 0.   2.99] Loss_P: [2.29 1.75 1.64 0.4  0.62 0.05 0.15 0.02 6.91]\n",
      "Loss_Q: [0.83 1.03 0.38 0.6  0.05 0.15 0.02 0.   3.06] Loss_P: [2.34 1.69 1.67 0.45 0.6  0.04 0.12 0.01 6.93]\n",
      "Loss_Q: [0.86 0.97 0.37 0.58 0.05 0.14 0.02 0.   2.99] Loss_P: [2.28 1.71 1.67 0.4  0.6  0.04 0.12 0.02 6.83]\n",
      "Loss_Q: [0.91 1.01 0.39 0.62 0.05 0.13 0.02 0.   3.13] Loss_P: [2.33 1.62 1.65 0.45 0.64 0.05 0.14 0.03 6.91]\n",
      "Loss_Q: [0.81 0.94 0.42 0.67 0.03 0.16 0.02 0.   3.06] Loss_P: [2.28 1.65 1.62 0.45 0.66 0.05 0.12 0.01 6.84]\n",
      "Loss_Q: [0.85 0.94 0.4  0.61 0.05 0.14 0.02 0.   3.  ] Loss_P: [2.29 1.74 1.61 0.42 0.64 0.04 0.15 0.02 6.91]\n",
      "Loss_Q: [0.82 1.   0.37 0.62 0.03 0.12 0.02 0.   2.98] Loss_P: [2.3  1.7  1.67 0.44 0.62 0.04 0.1  0.02 6.89]\n",
      "Loss_Q: [0.81 0.97 0.41 0.61 0.04 0.16 0.02 0.   3.02] Loss_P: [2.28 1.73 1.65 0.48 0.67 0.06 0.12 0.01 6.98]\n",
      "Loss_Q: [0.85 1.   0.42 0.58 0.05 0.15 0.02 0.   3.06] Loss_P: [2.3  1.66 1.66 0.54 0.6  0.05 0.15 0.02 6.98]\n",
      "Loss_Q: [0.82 0.97 0.41 0.61 0.05 0.14 0.02 0.   3.03] Loss_P: [2.3  1.66 1.67 0.51 0.64 0.06 0.13 0.02 6.97]\n",
      "Loss_Q: [0.82 1.01 0.43 0.61 0.05 0.12 0.02 0.   3.05] Loss_P: [2.31 1.68 1.71 0.44 0.6  0.03 0.13 0.02 6.92]\n",
      "Loss_Q: [0.9  1.04 0.42 0.57 0.05 0.12 0.02 0.   3.13] Loss_P: [2.34 1.72 1.72 0.48 0.62 0.04 0.13 0.01 7.06]\n",
      "Loss_Q: [0.83 1.01 0.41 0.59 0.03 0.13 0.02 0.   3.03] Loss_P: [2.29 1.7  1.63 0.44 0.59 0.04 0.16 0.01 6.86]\n",
      "Loss_Q: [0.84 0.96 0.39 0.59 0.04 0.12 0.02 0.   2.98] Loss_P: [2.33 1.7  1.65 0.43 0.64 0.05 0.15 0.01 6.97]\n",
      "Loss_Q: [0.91 0.99 0.4  0.61 0.04 0.14 0.02 0.   3.12] Loss_P: [2.31 1.69 1.67 0.47 0.59 0.05 0.14 0.02 6.94]\n",
      "Loss_Q: [0.86 0.96 0.42 0.57 0.04 0.12 0.02 0.   2.99] Loss_P: [2.33 1.75 1.71 0.46 0.58 0.05 0.14 0.02 7.04]\n",
      "Loss_Q: [0.8  1.   0.41 0.54 0.05 0.11 0.02 0.   2.92] Loss_P: [2.34 1.71 1.65 0.47 0.55 0.04 0.13 0.01 6.9 ]\n",
      "Loss_Q: [0.84 0.99 0.42 0.51 0.05 0.13 0.01 0.   2.95] Loss_P: [2.34 1.72 1.71 0.41 0.52 0.04 0.14 0.01 6.9 ]\n",
      "Loss_Q: [0.88 1.04 0.48 0.55 0.03 0.15 0.02 0.   3.15] Loss_P: [2.36 1.64 1.66 0.45 0.53 0.04 0.14 0.01 6.83]\n",
      "Loss_Q: [0.82 1.   0.41 0.55 0.05 0.14 0.01 0.   2.98] Loss_P: [2.26 1.66 1.69 0.39 0.53 0.03 0.12 0.02 6.7 ]\n",
      "Loss_Q: [0.83 1.03 0.4  0.54 0.04 0.12 0.02 0.   2.99] Loss_P: [2.33 1.68 1.69 0.43 0.55 0.04 0.1  0.01 6.82]\n",
      "Loss_Q: [0.84 1.02 0.38 0.52 0.05 0.13 0.02 0.   2.96] Loss_P: [2.31 1.69 1.72 0.39 0.52 0.04 0.15 0.02 6.82]\n",
      "Loss_Q: [0.8  1.   0.39 0.54 0.05 0.15 0.02 0.   2.94] Loss_P: [2.35 1.66 1.67 0.44 0.55 0.05 0.14 0.01 6.89]\n",
      "Loss_Q: [0.86 0.98 0.41 0.52 0.04 0.13 0.02 0.   2.97] Loss_P: [2.35 1.68 1.72 0.41 0.53 0.05 0.15 0.03 6.93]\n",
      "Loss_Q: [0.8  1.04 0.41 0.58 0.04 0.14 0.02 0.   3.03] Loss_P: [2.33 1.62 1.71 0.39 0.52 0.04 0.14 0.02 6.77]\n",
      "Loss_Q: [0.88 0.99 0.37 0.58 0.06 0.15 0.02 0.   3.05] Loss_P: [2.34 1.68 1.71 0.41 0.6  0.04 0.16 0.02 6.96]\n",
      "Loss_Q: [0.84 1.05 0.39 0.6  0.05 0.14 0.02 0.   3.09] Loss_P: [2.38 1.64 1.7  0.41 0.62 0.03 0.18 0.02 6.98]\n",
      "Loss_Q: [0.89 1.04 0.4  0.6  0.04 0.18 0.02 0.   3.17] Loss_P: [2.38 1.62 1.66 0.43 0.59 0.04 0.15 0.02 6.89]\n",
      "Loss_Q: [0.83 0.97 0.38 0.58 0.05 0.16 0.02 0.   3.  ] Loss_P: [2.38 1.7  1.64 0.43 0.6  0.05 0.18 0.01 6.99]\n",
      "Loss_Q: [0.86 0.96 0.4  0.59 0.04 0.16 0.03 0.   3.04] Loss_P: [2.33 1.68 1.67 0.39 0.59 0.03 0.14 0.02 6.86]\n",
      "Loss_Q: [0.79 1.08 0.39 0.55 0.04 0.15 0.02 0.   3.03] Loss_P: [2.35 1.72 1.71 0.34 0.56 0.04 0.15 0.01 6.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.86 1.05 0.36 0.61 0.06 0.13 0.01 0.   3.08] Loss_P: [2.33 1.69 1.74 0.37 0.57 0.03 0.14 0.02 6.88]\n",
      "Loss_Q: [0.82 1.04 0.38 0.57 0.05 0.13 0.01 0.   3.  ] Loss_P: [2.38 1.7  1.74 0.44 0.55 0.05 0.17 0.01 7.05]\n",
      "Loss_Q: [0.8  1.09 0.39 0.55 0.05 0.16 0.01 0.   3.05] Loss_P: [2.32 1.66 1.75 0.36 0.55 0.05 0.15 0.02 6.87]\n",
      "Loss_Q: [0.83 1.   0.35 0.58 0.04 0.15 0.02 0.   2.98] Loss_P: [2.33 1.62 1.71 0.42 0.55 0.04 0.13 0.02 6.84]\n",
      "Loss_Q: [0.79 1.03 0.34 0.52 0.07 0.15 0.02 0.   2.92] Loss_P: [2.4  1.6  1.67 0.37 0.55 0.06 0.14 0.01 6.79]\n",
      "Loss_Q: [0.82 1.02 0.36 0.49 0.06 0.15 0.01 0.   2.92] Loss_P: [2.39 1.69 1.66 0.41 0.52 0.04 0.18 0.01 6.91]\n",
      "Loss_Q: [0.85 1.   0.35 0.55 0.04 0.14 0.02 0.   2.94] Loss_P: [2.36 1.68 1.68 0.41 0.57 0.06 0.16 0.01 6.93]\n",
      "Loss_Q: [0.87 1.06 0.38 0.53 0.03 0.14 0.02 0.   3.02] Loss_P: [2.26 1.66 1.67 0.4  0.52 0.05 0.16 0.02 6.74]\n",
      "Loss_Q: [0.87 1.05 0.38 0.55 0.06 0.18 0.02 0.   3.1 ] Loss_P: [2.3  1.67 1.7  0.39 0.49 0.04 0.12 0.01 6.73]\n",
      "Loss_Q: [0.88 1.08 0.34 0.56 0.05 0.13 0.02 0.   3.06] Loss_P: [2.29 1.71 1.75 0.37 0.56 0.03 0.13 0.01 6.87]\n",
      "Loss_Q: [0.85 1.03 0.31 0.54 0.04 0.12 0.02 0.   2.89] Loss_P: [2.37 1.69 1.71 0.33 0.55 0.05 0.13 0.03 6.86]\n",
      "Loss_Q: [0.85 0.98 0.31 0.55 0.04 0.15 0.02 0.   2.9 ] Loss_P: [2.37 1.69 1.71 0.34 0.56 0.04 0.13 0.02 6.86]\n",
      "Loss_Q: [0.86 1.01 0.29 0.55 0.04 0.14 0.01 0.   2.89] Loss_P: [2.34 1.6  1.71 0.37 0.52 0.04 0.16 0.01 6.74]\n",
      "Loss_Q: [0.85 1.   0.29 0.54 0.04 0.14 0.02 0.   2.88] Loss_P: [2.29 1.7  1.74 0.39 0.52 0.04 0.15 0.01 6.85]\n",
      "Loss_Q: [0.86 1.04 0.35 0.48 0.03 0.13 0.02 0.   2.91] Loss_P: [2.35 1.68 1.74 0.35 0.52 0.03 0.13 0.01 6.83]\n",
      "Loss_Q: [0.82 1.05 0.32 0.45 0.06 0.14 0.01 0.   2.85] Loss_P: [2.34 1.7  1.72 0.37 0.53 0.05 0.15 0.02 6.88]\n",
      "Loss_Q: [0.83 1.01 0.31 0.55 0.04 0.13 0.01 0.   2.88] Loss_P: [2.39 1.6  1.72 0.39 0.49 0.04 0.14 0.01 6.78]\n",
      "Loss_Q: [0.84 1.01 0.33 0.54 0.05 0.12 0.02 0.   2.91] Loss_P: [2.31 1.71 1.68 0.38 0.53 0.04 0.15 0.02 6.83]\n",
      "Loss_Q: [0.9  1.03 0.36 0.48 0.06 0.13 0.01 0.   2.97] Loss_P: [2.31 1.73 1.73 0.39 0.5  0.06 0.13 0.01 6.87]\n",
      "Loss_Q: [0.78 1.09 0.31 0.53 0.04 0.14 0.01 0.   2.91] Loss_P: [2.34 1.67 1.73 0.34 0.52 0.05 0.13 0.03 6.82]\n",
      "Loss_Q: [0.85 1.07 0.38 0.5  0.06 0.14 0.02 0.   3.02] Loss_P: [2.27 1.71 1.74 0.35 0.46 0.05 0.14 0.02 6.74]\n",
      "Loss_Q: [0.83 1.05 0.33 0.46 0.03 0.16 0.02 0.   2.87] Loss_P: [2.27 1.71 1.72 0.42 0.49 0.03 0.14 0.02 6.79]\n",
      "Loss_Q: [0.84 1.02 0.31 0.5  0.04 0.14 0.02 0.   2.87] Loss_P: [2.28 1.67 1.7  0.39 0.45 0.04 0.12 0.01 6.66]\n",
      "Loss_Q: [0.84 1.03 0.33 0.49 0.05 0.14 0.02 0.   2.88] Loss_P: [2.33 1.63 1.72 0.41 0.51 0.03 0.13 0.01 6.75]\n",
      "Loss_Q: [0.76 0.99 0.32 0.51 0.03 0.15 0.01 0.   2.77] Loss_P: [2.36 1.66 1.64 0.37 0.52 0.03 0.12 0.01 6.72]\n",
      "Loss_Q: [0.86 1.02 0.36 0.55 0.05 0.15 0.01 0.   3.  ] Loss_P: [2.34 1.6  1.66 0.35 0.51 0.04 0.15 0.02 6.68]\n",
      "Loss_Q: [0.81 1.02 0.33 0.52 0.04 0.13 0.02 0.   2.87] Loss_P: [2.29 1.65 1.66 0.39 0.5  0.04 0.12 0.01 6.66]\n",
      "Loss_Q: [0.82 0.98 0.32 0.51 0.06 0.13 0.01 0.   2.83] Loss_P: [2.32 1.66 1.64 0.38 0.5  0.04 0.14 0.02 6.69]\n",
      "Loss_Q: [0.83 0.97 0.3  0.51 0.03 0.13 0.02 0.   2.8 ] Loss_P: [2.34 1.67 1.63 0.35 0.51 0.03 0.1  0.01 6.64]\n",
      "Loss_Q: [0.81 0.98 0.36 0.49 0.05 0.15 0.02 0.   2.84] Loss_P: [2.33 1.69 1.65 0.39 0.48 0.04 0.15 0.01 6.75]\n",
      "Loss_Q: [0.91 1.   0.36 0.5  0.04 0.12 0.02 0.   2.95] Loss_P: [2.31 1.69 1.59 0.41 0.53 0.04 0.15 0.02 6.74]\n",
      "Loss_Q: [0.86 1.   0.37 0.51 0.03 0.15 0.02 0.   2.93] Loss_P: [2.3  1.67 1.65 0.46 0.49 0.04 0.15 0.02 6.76]\n",
      "Loss_Q: [0.84 1.02 0.37 0.48 0.04 0.14 0.02 0.   2.92] Loss_P: [2.29 1.71 1.64 0.41 0.48 0.03 0.16 0.02 6.75]\n",
      "Loss_Q: [0.82 0.95 0.32 0.51 0.07 0.13 0.01 0.   2.8 ] Loss_P: [2.29 1.66 1.61 0.38 0.47 0.04 0.15 0.02 6.62]\n",
      "Loss_Q: [0.84 0.96 0.36 0.47 0.05 0.1  0.01 0.   2.79] Loss_P: [2.33 1.65 1.64 0.41 0.48 0.04 0.13 0.02 6.7 ]\n",
      "Loss_Q: [0.84 0.98 0.33 0.43 0.04 0.14 0.02 0.   2.77] Loss_P: [2.26 1.68 1.62 0.31 0.42 0.03 0.11 0.01 6.45]\n",
      "Loss_Q: [0.82 1.03 0.37 0.44 0.06 0.14 0.02 0.   2.87] Loss_P: [2.25 1.75 1.7  0.38 0.42 0.06 0.14 0.02 6.72]\n",
      "Loss_Q: [0.79 1.   0.34 0.43 0.03 0.14 0.01 0.   2.75] Loss_P: [2.28 1.71 1.68 0.39 0.46 0.05 0.15 0.02 6.73]\n",
      "Loss_Q: [0.83 0.96 0.31 0.46 0.03 0.13 0.01 0.   2.72] Loss_P: [2.34 1.63 1.58 0.33 0.43 0.04 0.14 0.01 6.5 ]\n",
      "Loss_Q: [0.84 0.95 0.34 0.46 0.03 0.14 0.02 0.   2.77] Loss_P: [2.32 1.68 1.57 0.37 0.44 0.04 0.12 0.02 6.55]\n",
      "Loss_Q: [0.84 0.95 0.33 0.44 0.04 0.11 0.01 0.   2.72] Loss_P: [2.33 1.61 1.59 0.34 0.49 0.03 0.13 0.02 6.54]\n",
      "Loss_Q: [0.84 0.99 0.33 0.47 0.03 0.13 0.01 0.   2.8 ] Loss_P: [2.36 1.69 1.61 0.38 0.44 0.04 0.16 0.02 6.7 ]\n",
      "Loss_Q: [0.82 0.98 0.38 0.47 0.04 0.14 0.01 0.   2.84] Loss_P: [2.35 1.66 1.61 0.37 0.47 0.05 0.15 0.01 6.67]\n",
      "Loss_Q: [0.82 0.94 0.31 0.38 0.04 0.14 0.02 0.   2.65] Loss_P: [2.35 1.67 1.59 0.37 0.42 0.05 0.13 0.02 6.58]\n",
      "Loss_Q: [0.8  0.93 0.27 0.43 0.05 0.14 0.01 0.   2.62] Loss_P: [2.31 1.71 1.6  0.35 0.41 0.04 0.14 0.01 6.57]\n",
      "Loss_Q: [0.78 1.   0.34 0.39 0.03 0.09 0.01 0.   2.65] Loss_P: [2.33 1.69 1.65 0.32 0.44 0.07 0.12 0.02 6.63]\n",
      "Loss_Q: [0.84 0.97 0.31 0.4  0.04 0.13 0.02 0.   2.7 ] Loss_P: [2.33 1.63 1.68 0.35 0.46 0.04 0.14 0.02 6.64]\n",
      "Loss_Q: [0.77 1.   0.29 0.43 0.04 0.14 0.03 0.   2.69] Loss_P: [2.31 1.61 1.64 0.35 0.43 0.04 0.11 0.02 6.51]\n",
      "Loss_Q: [0.82 1.01 0.33 0.43 0.04 0.11 0.02 0.   2.76] Loss_P: [2.29 1.66 1.59 0.39 0.41 0.04 0.12 0.02 6.52]\n",
      "Loss_Q: [0.78 0.97 0.33 0.39 0.03 0.11 0.02 0.   2.63] Loss_P: [2.31 1.64 1.61 0.39 0.42 0.05 0.14 0.02 6.56]\n",
      "Loss_Q: [0.84 0.91 0.34 0.39 0.04 0.12 0.02 0.   2.67] Loss_P: [2.27 1.72 1.65 0.42 0.43 0.02 0.1  0.02 6.63]\n",
      "Loss_Q: [0.85 0.97 0.35 0.4  0.03 0.12 0.02 0.   2.73] Loss_P: [2.38 1.62 1.59 0.38 0.42 0.02 0.1  0.01 6.53]\n",
      "Loss_Q: [0.8  0.88 0.32 0.38 0.03 0.14 0.02 0.   2.57] Loss_P: [2.38 1.64 1.62 0.42 0.44 0.04 0.14 0.02 6.7 ]\n",
      "Loss_Q: [0.84 0.96 0.35 0.42 0.03 0.16 0.03 0.   2.78] Loss_P: [2.35 1.67 1.59 0.39 0.38 0.03 0.14 0.02 6.57]\n",
      "Loss_Q: [0.82 0.91 0.37 0.46 0.04 0.12 0.01 0.   2.73] Loss_P: [2.35 1.62 1.58 0.42 0.43 0.03 0.14 0.02 6.58]\n",
      "Loss_Q: [0.81 0.9  0.31 0.41 0.03 0.12 0.02 0.   2.6 ] Loss_P: [2.28 1.61 1.6  0.41 0.42 0.04 0.17 0.01 6.54]\n",
      "Loss_Q: [0.79 0.88 0.32 0.45 0.03 0.13 0.02 0.   2.63] Loss_P: [2.4  1.62 1.61 0.47 0.44 0.04 0.12 0.01 6.72]\n",
      "Loss_Q: [0.9  0.99 0.33 0.4  0.05 0.13 0.02 0.   2.82] Loss_P: [2.35 1.68 1.58 0.41 0.38 0.05 0.15 0.02 6.62]\n",
      "Loss_Q: [0.78 0.9  0.37 0.37 0.05 0.11 0.02 0.   2.59] Loss_P: [2.28 1.68 1.64 0.51 0.41 0.04 0.14 0.02 6.71]\n",
      "Loss_Q: [0.87 0.91 0.41 0.34 0.04 0.13 0.01 0.   2.71] Loss_P: [2.33 1.61 1.56 0.48 0.39 0.03 0.14 0.01 6.56]\n",
      "Loss_Q: [0.82 0.96 0.41 0.43 0.05 0.13 0.02 0.   2.82] Loss_P: [2.38 1.57 1.58 0.41 0.41 0.04 0.1  0.01 6.51]\n",
      "Loss_Q: [0.78 0.92 0.39 0.44 0.05 0.13 0.01 0.   2.71] Loss_P: [2.34 1.63 1.63 0.43 0.39 0.06 0.12 0.01 6.61]\n",
      "Loss_Q: [0.78 0.96 0.38 0.39 0.04 0.13 0.02 0.   2.7 ] Loss_P: [2.34 1.6  1.63 0.43 0.39 0.02 0.11 0.01 6.54]\n",
      "Loss_Q: [0.82 0.95 0.38 0.36 0.04 0.13 0.02 0.   2.69] Loss_P: [2.37 1.59 1.6  0.45 0.4  0.04 0.13 0.01 6.58]\n",
      "Loss_Q: [0.79 0.94 0.38 0.4  0.04 0.13 0.02 0.   2.68] Loss_P: [2.33 1.61 1.61 0.46 0.37 0.04 0.13 0.01 6.57]\n",
      "Loss_Q: [0.9  0.99 0.41 0.49 0.04 0.12 0.02 0.   2.97] Loss_P: [2.38 1.57 1.61 0.45 0.43 0.04 0.12 0.02 6.62]\n",
      "Loss_Q: [0.86 0.99 0.36 0.46 0.02 0.09 0.01 0.   2.79] Loss_P: [2.33 1.62 1.61 0.45 0.48 0.04 0.11 0.01 6.65]\n",
      "Loss_Q: [0.81 0.94 0.39 0.43 0.04 0.13 0.02 0.   2.75] Loss_P: [2.36 1.59 1.6  0.44 0.45 0.03 0.1  0.01 6.58]\n",
      "Loss_Q: [0.86 0.9  0.38 0.49 0.04 0.13 0.01 0.   2.81] Loss_P: [2.35 1.65 1.56 0.44 0.47 0.04 0.13 0.02 6.67]\n",
      "Loss_Q: [0.81 0.93 0.36 0.46 0.06 0.12 0.02 0.   2.76] Loss_P: [2.32 1.63 1.6  0.39 0.53 0.04 0.12 0.01 6.64]\n",
      "Loss_Q: [0.86 0.94 0.35 0.46 0.05 0.11 0.02 0.   2.79] Loss_P: [2.32 1.67 1.51 0.4  0.47 0.05 0.14 0.02 6.55]\n",
      "Loss_Q: [0.78 0.92 0.34 0.47 0.04 0.11 0.02 0.   2.68] Loss_P: [2.37 1.63 1.54 0.41 0.52 0.04 0.13 0.02 6.66]\n",
      "Loss_Q: [0.78 0.93 0.37 0.49 0.03 0.13 0.02 0.   2.76] Loss_P: [2.39 1.64 1.56 0.35 0.5  0.06 0.13 0.02 6.64]\n",
      "Loss_Q: [0.88 0.9  0.32 0.55 0.02 0.14 0.02 0.   2.82] Loss_P: [2.41 1.56 1.48 0.44 0.46 0.03 0.13 0.02 6.52]\n",
      "Loss_Q: [0.75 0.91 0.32 0.48 0.04 0.12 0.03 0.   2.65] Loss_P: [2.35 1.63 1.56 0.39 0.46 0.03 0.12 0.01 6.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.86 0.92 0.35 0.48 0.03 0.13 0.01 0.   2.78] Loss_P: [2.38 1.62 1.49 0.39 0.46 0.05 0.12 0.02 6.52]\n",
      "Loss_Q: [0.76 0.86 0.33 0.5  0.04 0.14 0.02 0.   2.66] Loss_P: [2.39 1.57 1.51 0.43 0.51 0.06 0.12 0.01 6.6 ]\n",
      "Loss_Q: [0.84 0.98 0.37 0.5  0.05 0.11 0.02 0.   2.86] Loss_P: [2.34 1.65 1.59 0.42 0.55 0.03 0.12 0.01 6.7 ]\n",
      "Loss_Q: [0.78 0.85 0.28 0.49 0.04 0.12 0.02 0.   2.57] Loss_P: [2.39 1.63 1.49 0.41 0.52 0.03 0.17 0.02 6.66]\n",
      "Loss_Q: [0.85 0.89 0.33 0.5  0.03 0.14 0.02 0.   2.75] Loss_P: [2.33 1.66 1.47 0.4  0.51 0.05 0.14 0.02 6.57]\n",
      "Loss_Q: [0.8  0.94 0.36 0.54 0.03 0.12 0.01 0.   2.81] Loss_P: [2.38 1.66 1.5  0.4  0.48 0.04 0.12 0.01 6.58]\n",
      "Loss_Q: [0.84 0.89 0.36 0.53 0.02 0.14 0.01 0.   2.79] Loss_P: [2.3  1.68 1.49 0.35 0.5  0.05 0.1  0.01 6.47]\n",
      "Loss_Q: [0.78 0.9  0.34 0.47 0.04 0.12 0.01 0.   2.67] Loss_P: [2.32 1.7  1.58 0.39 0.49 0.03 0.12 0.02 6.66]\n",
      "Loss_Q: [0.75 0.91 0.33 0.48 0.03 0.12 0.01 0.   2.64] Loss_P: [2.42 1.64 1.5  0.36 0.49 0.03 0.12 0.01 6.57]\n",
      "Loss_Q: [0.83 0.96 0.36 0.54 0.04 0.12 0.01 0.   2.86] Loss_P: [2.35 1.66 1.55 0.41 0.54 0.03 0.14 0.02 6.72]\n",
      "Loss_Q: [0.82 0.93 0.33 0.56 0.04 0.11 0.02 0.   2.8 ] Loss_P: [2.34 1.66 1.55 0.43 0.58 0.04 0.14 0.01 6.76]\n",
      "Loss_Q: [0.81 0.98 0.32 0.53 0.08 0.12 0.02 0.   2.86] Loss_P: [2.27 1.68 1.58 0.39 0.54 0.03 0.11 0.01 6.61]\n",
      "Loss_Q: [0.81 0.93 0.32 0.53 0.03 0.14 0.02 0.   2.78] Loss_P: [2.42 1.54 1.54 0.38 0.55 0.04 0.12 0.01 6.6 ]\n",
      "Loss_Q: [0.8  0.9  0.29 0.5  0.03 0.13 0.02 0.   2.68] Loss_P: [2.34 1.62 1.51 0.34 0.5  0.04 0.15 0.03 6.53]\n",
      "Loss_Q: [0.82 0.96 0.31 0.49 0.03 0.12 0.02 0.   2.75] Loss_P: [2.35 1.64 1.53 0.41 0.51 0.04 0.13 0.02 6.62]\n",
      "Loss_Q: [0.84 0.91 0.35 0.48 0.04 0.12 0.02 0.   2.75] Loss_P: [2.33 1.65 1.6  0.39 0.52 0.05 0.12 0.01 6.66]\n",
      "Loss_Q: [0.82 0.95 0.34 0.51 0.05 0.15 0.01 0.   2.83] Loss_P: [2.33 1.67 1.51 0.39 0.53 0.05 0.14 0.02 6.63]\n",
      "Loss_Q: [0.74 0.95 0.33 0.48 0.04 0.14 0.02 0.   2.69] Loss_P: [2.3  1.68 1.51 0.32 0.46 0.05 0.13 0.01 6.45]\n",
      "Loss_Q: [0.86 0.94 0.31 0.45 0.04 0.14 0.03 0.   2.78] Loss_P: [2.35 1.66 1.6  0.39 0.48 0.04 0.16 0.02 6.71]\n",
      "Loss_Q: [0.77 0.94 0.29 0.47 0.03 0.16 0.02 0.   2.67] Loss_P: [2.28 1.66 1.63 0.35 0.45 0.05 0.17 0.02 6.61]\n",
      "Loss_Q: [0.8  0.91 0.3  0.48 0.05 0.15 0.02 0.   2.7 ] Loss_P: [2.36 1.55 1.56 0.34 0.47 0.04 0.11 0.01 6.44]\n",
      "Loss_Q: [0.76 0.99 0.3  0.52 0.04 0.14 0.02 0.   2.78] Loss_P: [2.31 1.59 1.57 0.34 0.49 0.04 0.16 0.02 6.5 ]\n",
      "Loss_Q: [0.85 0.99 0.31 0.52 0.05 0.15 0.02 0.   2.88] Loss_P: [2.31 1.7  1.61 0.38 0.52 0.04 0.18 0.01 6.75]\n",
      "Loss_Q: [0.83 1.01 0.32 0.52 0.04 0.19 0.02 0.   2.93] Loss_P: [2.32 1.6  1.63 0.34 0.5  0.06 0.17 0.02 6.63]\n",
      "Loss_Q: [0.84 0.99 0.37 0.52 0.04 0.18 0.01 0.   2.96] Loss_P: [2.32 1.72 1.58 0.38 0.48 0.04 0.17 0.03 6.7 ]\n",
      "Loss_Q: [0.84 1.   0.33 0.52 0.04 0.17 0.02 0.   2.91] Loss_P: [2.33 1.7  1.6  0.39 0.46 0.05 0.14 0.02 6.7 ]\n",
      "Loss_Q: [0.8  0.98 0.28 0.48 0.04 0.2  0.02 0.   2.79] Loss_P: [2.32 1.64 1.61 0.39 0.5  0.03 0.17 0.01 6.67]\n",
      "Loss_Q: [0.89 1.   0.37 0.46 0.04 0.17 0.02 0.   2.94] Loss_P: [2.32 1.61 1.6  0.33 0.46 0.04 0.17 0.02 6.55]\n",
      "Loss_Q: [0.86 1.01 0.36 0.49 0.04 0.17 0.02 0.   2.95] Loss_P: [2.38 1.59 1.64 0.42 0.51 0.04 0.14 0.02 6.73]\n",
      "Loss_Q: [0.83 1.06 0.32 0.45 0.04 0.16 0.02 0.   2.88] Loss_P: [2.35 1.67 1.6  0.39 0.49 0.03 0.16 0.02 6.71]\n",
      "Loss_Q: [0.84 1.03 0.3  0.48 0.06 0.19 0.01 0.   2.92] Loss_P: [2.37 1.66 1.64 0.36 0.51 0.04 0.18 0.03 6.78]\n",
      "Loss_Q: [0.87 1.02 0.32 0.5  0.03 0.16 0.01 0.   2.91] Loss_P: [2.29 1.67 1.68 0.41 0.5  0.04 0.17 0.02 6.78]\n",
      "Loss_Q: [0.81 1.03 0.33 0.49 0.04 0.19 0.02 0.   2.91] Loss_P: [2.33 1.72 1.68 0.38 0.52 0.05 0.18 0.03 6.89]\n",
      "Loss_Q: [0.84 1.07 0.31 0.54 0.04 0.18 0.01 0.   2.98] Loss_P: [2.35 1.67 1.64 0.31 0.54 0.04 0.18 0.01 6.75]\n",
      "Loss_Q: [0.85 1.01 0.33 0.52 0.03 0.17 0.02 0.   2.93] Loss_P: [2.32 1.71 1.64 0.33 0.53 0.05 0.18 0.02 6.77]\n",
      "Loss_Q: [0.79 1.08 0.34 0.52 0.05 0.19 0.01 0.   2.99] Loss_P: [2.29 1.71 1.64 0.36 0.52 0.03 0.17 0.02 6.73]\n",
      "Loss_Q: [0.85 1.04 0.32 0.51 0.04 0.15 0.02 0.   2.93] Loss_P: [2.34 1.66 1.61 0.37 0.55 0.04 0.19 0.02 6.78]\n",
      "Loss_Q: [0.88 1.01 0.3  0.56 0.04 0.2  0.01 0.   3.  ] Loss_P: [2.27 1.69 1.72 0.38 0.52 0.04 0.17 0.02 6.81]\n",
      "Loss_Q: [0.78 1.   0.27 0.5  0.03 0.13 0.01 0.   2.72] Loss_P: [2.34 1.73 1.65 0.38 0.51 0.02 0.17 0.02 6.83]\n",
      "Loss_Q: [0.81 1.04 0.32 0.47 0.03 0.19 0.02 0.   2.88] Loss_P: [2.32 1.69 1.57 0.33 0.51 0.07 0.15 0.01 6.66]\n",
      "Loss_Q: [0.8  0.99 0.33 0.5  0.03 0.19 0.03 0.   2.86] Loss_P: [2.33 1.71 1.62 0.37 0.5  0.03 0.17 0.02 6.74]\n",
      "Loss_Q: [0.8  1.   0.31 0.48 0.03 0.15 0.02 0.   2.8 ] Loss_P: [2.3  1.68 1.62 0.39 0.52 0.04 0.16 0.02 6.72]\n",
      "Loss_Q: [0.9  1.04 0.33 0.46 0.04 0.19 0.02 0.   2.98] Loss_P: [2.33 1.66 1.67 0.39 0.47 0.05 0.19 0.01 6.78]\n",
      "Loss_Q: [0.88 1.04 0.39 0.5  0.04 0.22 0.02 0.   3.09] Loss_P: [2.33 1.7  1.61 0.39 0.49 0.04 0.19 0.02 6.76]\n",
      "Loss_Q: [0.83 1.05 0.3  0.53 0.05 0.18 0.02 0.   2.96] Loss_P: [2.36 1.62 1.64 0.38 0.51 0.05 0.17 0.01 6.74]\n",
      "Loss_Q: [0.84 1.02 0.33 0.46 0.04 0.17 0.01 0.   2.88] Loss_P: [2.32 1.75 1.63 0.33 0.47 0.07 0.18 0.02 6.76]\n",
      "Loss_Q: [0.8  1.07 0.35 0.47 0.03 0.18 0.02 0.   2.93] Loss_P: [2.32 1.71 1.58 0.36 0.49 0.03 0.18 0.02 6.69]\n",
      "Loss_Q: [0.82 1.06 0.33 0.45 0.02 0.2  0.01 0.   2.9 ] Loss_P: [2.33 1.68 1.65 0.39 0.47 0.04 0.19 0.02 6.77]\n",
      "Loss_Q: [0.83 1.03 0.33 0.45 0.06 0.19 0.02 0.   2.91] Loss_P: [2.34 1.67 1.6  0.44 0.42 0.03 0.15 0.02 6.65]\n",
      "Loss_Q: [0.83 1.09 0.38 0.42 0.05 0.16 0.02 0.   2.94] Loss_P: [2.33 1.69 1.67 0.47 0.41 0.03 0.22 0.02 6.84]\n",
      "Loss_Q: [0.8  1.02 0.33 0.44 0.03 0.19 0.03 0.   2.84] Loss_P: [2.41 1.68 1.6  0.41 0.43 0.03 0.17 0.01 6.73]\n",
      "Loss_Q: [0.82 1.04 0.34 0.44 0.03 0.17 0.01 0.   2.85] Loss_P: [2.38 1.64 1.62 0.4  0.44 0.05 0.17 0.01 6.7 ]\n",
      "Loss_Q: [0.83 1.07 0.32 0.42 0.05 0.2  0.01 0.   2.92] Loss_P: [2.32 1.63 1.6  0.41 0.4  0.03 0.19 0.01 6.59]\n",
      "Loss_Q: [0.79 1.05 0.36 0.42 0.03 0.19 0.02 0.   2.87] Loss_P: [2.34 1.65 1.63 0.36 0.43 0.05 0.19 0.02 6.68]\n",
      "Loss_Q: [0.81 1.01 0.38 0.46 0.05 0.17 0.01 0.   2.89] Loss_P: [2.36 1.61 1.62 0.46 0.44 0.03 0.18 0.02 6.72]\n",
      "Loss_Q: [0.73 1.03 0.33 0.4  0.06 0.19 0.03 0.   2.77] Loss_P: [2.3  1.66 1.66 0.44 0.43 0.03 0.21 0.02 6.76]\n",
      "Loss_Q: [0.8  1.08 0.35 0.38 0.05 0.18 0.01 0.   2.85] Loss_P: [2.35 1.59 1.58 0.39 0.41 0.05 0.16 0.02 6.54]\n",
      "Loss_Q: [0.78 1.03 0.31 0.44 0.02 0.15 0.02 0.   2.75] Loss_P: [2.37 1.64 1.6  0.43 0.42 0.04 0.17 0.02 6.69]\n",
      "Loss_Q: [0.84 1.07 0.39 0.41 0.03 0.15 0.03 0.   2.92] Loss_P: [2.35 1.64 1.64 0.42 0.43 0.04 0.19 0.02 6.72]\n",
      "Loss_Q: [0.84 1.06 0.38 0.43 0.05 0.19 0.02 0.   2.99] Loss_P: [2.38 1.62 1.61 0.44 0.42 0.02 0.19 0.03 6.72]\n",
      "Loss_Q: [0.88 1.01 0.38 0.42 0.03 0.18 0.01 0.   2.91] Loss_P: [2.37 1.7  1.63 0.39 0.42 0.03 0.2  0.02 6.77]\n",
      "Loss_Q: [0.82 1.11 0.39 0.4  0.04 0.17 0.02 0.   2.94] Loss_P: [2.32 1.65 1.7  0.45 0.42 0.04 0.15 0.01 6.74]\n",
      "Loss_Q: [0.87 1.03 0.44 0.46 0.03 0.19 0.02 0.   3.04] Loss_P: [2.33 1.66 1.65 0.42 0.44 0.04 0.17 0.02 6.71]\n",
      "Loss_Q: [0.88 1.01 0.36 0.46 0.04 0.18 0.02 0.   2.95] Loss_P: [2.38 1.59 1.67 0.4  0.43 0.04 0.19 0.01 6.72]\n",
      "Loss_Q: [0.77 1.04 0.37 0.44 0.04 0.19 0.02 0.   2.88] Loss_P: [2.32 1.7  1.6  0.44 0.47 0.05 0.18 0.01 6.78]\n",
      "Loss_Q: [0.88 1.   0.39 0.46 0.03 0.2  0.02 0.   2.97] Loss_P: [2.36 1.67 1.61 0.44 0.46 0.04 0.19 0.02 6.78]\n",
      "Loss_Q: [0.79 1.02 0.39 0.45 0.03 0.19 0.01 0.   2.87] Loss_P: [2.41 1.63 1.65 0.44 0.46 0.05 0.15 0.02 6.81]\n",
      "Loss_Q: [0.85 1.04 0.35 0.48 0.03 0.17 0.02 0.   2.93] Loss_P: [2.41 1.62 1.6  0.38 0.45 0.04 0.16 0.02 6.68]\n",
      "Loss_Q: [0.83 1.1  0.35 0.43 0.04 0.17 0.02 0.   2.94] Loss_P: [2.33 1.61 1.56 0.39 0.49 0.04 0.14 0.02 6.58]\n",
      "Loss_Q: [0.8  1.06 0.37 0.43 0.05 0.16 0.02 0.   2.88] Loss_P: [2.36 1.68 1.68 0.46 0.48 0.05 0.18 0.01 6.89]\n",
      "Loss_Q: [0.85 1.04 0.34 0.5  0.04 0.17 0.02 0.   2.95] Loss_P: [2.35 1.64 1.63 0.43 0.46 0.05 0.18 0.01 6.74]\n",
      "Loss_Q: [0.81 1.06 0.33 0.46 0.03 0.18 0.02 0.   2.9 ] Loss_P: [2.38 1.7  1.55 0.36 0.49 0.02 0.15 0.02 6.66]\n",
      "Loss_Q: [0.82 1.02 0.31 0.44 0.05 0.14 0.02 0.   2.79] Loss_P: [2.39 1.66 1.57 0.4  0.44 0.03 0.13 0.01 6.63]\n",
      "Loss_Q: [0.76 1.05 0.34 0.43 0.05 0.16 0.02 0.   2.8 ] Loss_P: [2.38 1.68 1.56 0.4  0.5  0.04 0.16 0.01 6.73]\n",
      "Loss_Q: [0.8  1.02 0.38 0.42 0.05 0.18 0.02 0.   2.86] Loss_P: [2.38 1.62 1.55 0.37 0.44 0.04 0.16 0.02 6.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.72 0.99 0.31 0.46 0.05 0.17 0.01 0.   2.72] Loss_P: [2.36 1.54 1.55 0.4  0.42 0.04 0.16 0.02 6.49]\n",
      "Loss_Q: [0.69 0.95 0.29 0.38 0.04 0.19 0.01 0.   2.55] Loss_P: [2.4  1.57 1.54 0.43 0.42 0.04 0.18 0.02 6.6 ]\n",
      "Loss_Q: [0.81 1.   0.33 0.4  0.06 0.21 0.02 0.   2.84] Loss_P: [2.35 1.69 1.53 0.41 0.41 0.04 0.18 0.02 6.62]\n",
      "Loss_Q: [0.86 1.04 0.32 0.39 0.04 0.14 0.02 0.   2.81] Loss_P: [2.42 1.67 1.44 0.34 0.38 0.04 0.19 0.02 6.5 ]\n",
      "Loss_Q: [0.79 1.06 0.3  0.41 0.04 0.22 0.01 0.   2.83] Loss_P: [2.38 1.64 1.52 0.37 0.4  0.03 0.17 0.02 6.53]\n",
      "Loss_Q: [0.79 0.98 0.28 0.4  0.04 0.15 0.01 0.   2.66] Loss_P: [2.34 1.66 1.51 0.37 0.36 0.05 0.18 0.01 6.48]\n",
      "Loss_Q: [0.8  0.98 0.35 0.37 0.06 0.18 0.02 0.   2.76] Loss_P: [2.36 1.73 1.58 0.4  0.39 0.05 0.14 0.03 6.67]\n",
      "Loss_Q: [0.81 0.98 0.35 0.36 0.03 0.16 0.01 0.   2.71] Loss_P: [2.4  1.56 1.56 0.43 0.41 0.04 0.21 0.01 6.63]\n",
      "Loss_Q: [0.77 0.93 0.35 0.37 0.04 0.17 0.02 0.   2.64] Loss_P: [2.37 1.65 1.61 0.41 0.38 0.07 0.19 0.01 6.69]\n",
      "Loss_Q: [0.73 0.97 0.35 0.39 0.04 0.2  0.02 0.   2.7 ] Loss_P: [2.38 1.6  1.58 0.38 0.37 0.04 0.17 0.01 6.53]\n",
      "Loss_Q: [0.82 1.02 0.32 0.34 0.05 0.18 0.03 0.   2.76] Loss_P: [2.36 1.67 1.63 0.39 0.33 0.04 0.2  0.02 6.63]\n",
      "Loss_Q: [0.83 1.06 0.35 0.33 0.04 0.22 0.02 0.   2.86] Loss_P: [2.36 1.64 1.65 0.42 0.31 0.03 0.23 0.01 6.64]\n",
      "Loss_Q: [0.89 1.   0.32 0.35 0.06 0.22 0.02 0.   2.86] Loss_P: [2.41 1.61 1.61 0.39 0.37 0.03 0.18 0.02 6.63]\n",
      "Loss_Q: [0.81 0.96 0.31 0.4  0.04 0.21 0.03 0.   2.75] Loss_P: [2.34 1.67 1.63 0.42 0.37 0.06 0.21 0.01 6.72]\n",
      "Loss_Q: [0.83 1.03 0.32 0.41 0.05 0.24 0.02 0.   2.9 ] Loss_P: [2.4  1.68 1.6  0.42 0.41 0.05 0.22 0.02 6.78]\n",
      "Loss_Q: [0.88 1.1  0.32 0.45 0.03 0.22 0.01 0.   3.01] Loss_P: [2.38 1.63 1.67 0.38 0.39 0.05 0.2  0.02 6.7 ]\n",
      "Loss_Q: [0.84 1.   0.31 0.38 0.06 0.2  0.02 0.   2.82] Loss_P: [2.41 1.66 1.65 0.41 0.38 0.05 0.21 0.03 6.79]\n",
      "Loss_Q: [0.9  0.99 0.32 0.38 0.04 0.19 0.02 0.   2.85] Loss_P: [2.38 1.67 1.63 0.42 0.38 0.04 0.22 0.02 6.76]\n",
      "Loss_Q: [0.89 0.96 0.34 0.38 0.04 0.21 0.02 0.   2.85] Loss_P: [2.38 1.68 1.55 0.41 0.38 0.05 0.19 0.02 6.66]\n",
      "Loss_Q: [0.89 1.09 0.3  0.43 0.03 0.21 0.02 0.   2.97] Loss_P: [2.35 1.71 1.62 0.37 0.34 0.04 0.21 0.02 6.65]\n",
      "Loss_Q: [0.9  1.02 0.34 0.41 0.03 0.24 0.03 0.   2.98] Loss_P: [2.37 1.68 1.54 0.39 0.44 0.05 0.24 0.01 6.72]\n",
      "Loss_Q: [0.84 0.98 0.33 0.38 0.04 0.18 0.01 0.   2.76] Loss_P: [2.36 1.68 1.63 0.36 0.39 0.05 0.2  0.01 6.67]\n",
      "Loss_Q: [0.83 0.96 0.31 0.34 0.05 0.21 0.01 0.   2.72] Loss_P: [2.28 1.79 1.63 0.38 0.37 0.05 0.24 0.02 6.75]\n",
      "Loss_Q: [0.87 1.06 0.36 0.32 0.05 0.21 0.02 0.   2.89] Loss_P: [2.42 1.66 1.58 0.39 0.39 0.04 0.19 0.02 6.69]\n",
      "Loss_Q: [0.82 1.01 0.34 0.3  0.04 0.21 0.02 0.   2.74] Loss_P: [2.37 1.69 1.56 0.33 0.34 0.03 0.21 0.02 6.53]\n",
      "Loss_Q: [0.8  0.96 0.28 0.34 0.02 0.22 0.02 0.   2.64] Loss_P: [2.33 1.7  1.54 0.35 0.35 0.05 0.23 0.01 6.56]\n",
      "Loss_Q: [0.82 1.01 0.34 0.36 0.04 0.23 0.01 0.   2.81] Loss_P: [2.43 1.64 1.57 0.37 0.37 0.03 0.2  0.01 6.62]\n",
      "Loss_Q: [0.74 0.93 0.35 0.41 0.04 0.23 0.01 0.   2.71] Loss_P: [2.37 1.66 1.48 0.36 0.36 0.03 0.2  0.02 6.48]\n",
      "Loss_Q: [0.87 1.   0.32 0.36 0.04 0.2  0.02 0.   2.82] Loss_P: [2.38 1.7  1.49 0.35 0.37 0.06 0.19 0.02 6.57]\n",
      "Loss_Q: [0.7  0.99 0.3  0.4  0.03 0.21 0.01 0.   2.65] Loss_P: [2.38 1.7  1.5  0.37 0.35 0.03 0.21 0.02 6.57]\n",
      "Loss_Q: [0.78 1.02 0.35 0.42 0.04 0.21 0.01 0.   2.82] Loss_P: [2.3  1.67 1.51 0.35 0.43 0.03 0.21 0.01 6.51]\n",
      "Loss_Q: [0.84 1.02 0.33 0.41 0.04 0.18 0.01 0.   2.83] Loss_P: [2.37 1.65 1.55 0.36 0.41 0.04 0.2  0.01 6.59]\n",
      "Loss_Q: [0.85 1.01 0.34 0.4  0.04 0.22 0.02 0.   2.88] Loss_P: [2.36 1.64 1.46 0.43 0.44 0.04 0.19 0.01 6.58]\n",
      "Loss_Q: [0.76 0.99 0.36 0.36 0.03 0.21 0.01 0.   2.73] Loss_P: [2.35 1.7  1.49 0.39 0.39 0.05 0.21 0.02 6.6 ]\n",
      "Loss_Q: [0.81 0.99 0.35 0.32 0.04 0.21 0.02 0.   2.74] Loss_P: [2.39 1.67 1.54 0.37 0.37 0.04 0.21 0.03 6.61]\n",
      "Loss_Q: [0.82 0.99 0.35 0.39 0.04 0.25 0.02 0.   2.84] Loss_P: [2.39 1.65 1.51 0.39 0.35 0.04 0.24 0.04 6.61]\n",
      "Loss_Q: [0.84 1.07 0.33 0.36 0.04 0.21 0.02 0.   2.88] Loss_P: [2.39 1.65 1.52 0.42 0.39 0.04 0.21 0.01 6.63]\n",
      "Loss_Q: [0.84 1.02 0.38 0.4  0.04 0.22 0.01 0.   2.91] Loss_P: [2.37 1.73 1.51 0.39 0.35 0.05 0.22 0.01 6.63]\n",
      "Loss_Q: [0.87 0.93 0.31 0.32 0.02 0.2  0.02 0.   2.66] Loss_P: [2.38 1.67 1.48 0.43 0.38 0.03 0.24 0.02 6.62]\n",
      "Loss_Q: [0.83 0.96 0.4  0.36 0.03 0.21 0.01 0.   2.81] Loss_P: [2.41 1.63 1.54 0.43 0.36 0.03 0.23 0.02 6.65]\n",
      "Loss_Q: [0.86 0.99 0.34 0.34 0.03 0.21 0.02 0.   2.8 ] Loss_P: [2.36 1.69 1.49 0.43 0.33 0.03 0.23 0.02 6.58]\n",
      "Loss_Q: [0.82 0.98 0.37 0.32 0.05 0.24 0.01 0.   2.8 ] Loss_P: [2.34 1.67 1.45 0.42 0.37 0.05 0.22 0.02 6.55]\n",
      "Loss_Q: [0.78 0.97 0.39 0.34 0.05 0.23 0.02 0.   2.78] Loss_P: [2.4  1.68 1.41 0.42 0.35 0.03 0.22 0.02 6.53]\n",
      "Loss_Q: [0.75 0.95 0.36 0.32 0.05 0.21 0.01 0.   2.65] Loss_P: [2.38 1.7  1.49 0.49 0.34 0.04 0.2  0.02 6.66]\n",
      "Loss_Q: [0.78 0.9  0.36 0.33 0.05 0.24 0.02 0.   2.69] Loss_P: [2.33 1.65 1.5  0.46 0.33 0.03 0.21 0.01 6.53]\n",
      "Loss_Q: [0.83 0.99 0.36 0.33 0.03 0.22 0.01 0.   2.77] Loss_P: [2.29 1.66 1.5  0.43 0.36 0.04 0.26 0.01 6.55]\n",
      "Loss_Q: [0.8  0.96 0.38 0.31 0.02 0.24 0.01 0.   2.72] Loss_P: [2.47 1.59 1.53 0.47 0.33 0.05 0.2  0.01 6.65]\n",
      "Loss_Q: [0.83 1.01 0.42 0.3  0.04 0.24 0.02 0.   2.86] Loss_P: [2.33 1.72 1.54 0.46 0.34 0.04 0.24 0.01 6.67]\n",
      "Loss_Q: [0.88 0.95 0.34 0.31 0.03 0.22 0.01 0.   2.74] Loss_P: [2.33 1.68 1.53 0.41 0.37 0.04 0.25 0.02 6.62]\n",
      "Loss_Q: [0.84 1.01 0.29 0.32 0.03 0.27 0.01 0.   2.77] Loss_P: [2.33 1.73 1.57 0.39 0.34 0.04 0.24 0.01 6.65]\n",
      "Loss_Q: [0.79 1.05 0.35 0.31 0.04 0.29 0.01 0.   2.84] Loss_P: [2.34 1.65 1.56 0.39 0.29 0.04 0.25 0.01 6.54]\n",
      "Loss_Q: [0.79 0.98 0.34 0.32 0.04 0.23 0.03 0.   2.73] Loss_P: [2.31 1.65 1.55 0.43 0.31 0.05 0.23 0.02 6.54]\n",
      "Loss_Q: [0.88 1.02 0.44 0.36 0.04 0.21 0.02 0.   2.96] Loss_P: [2.32 1.65 1.66 0.46 0.36 0.05 0.25 0.02 6.77]\n",
      "Loss_Q: [0.84 1.06 0.39 0.31 0.07 0.26 0.02 0.   2.96] Loss_P: [2.36 1.7  1.62 0.57 0.36 0.03 0.24 0.02 6.89]\n",
      "Loss_Q: [0.89 1.01 0.43 0.35 0.04 0.2  0.02 0.   2.94] Loss_P: [2.33 1.69 1.63 0.47 0.34 0.03 0.23 0.02 6.75]\n",
      "Loss_Q: [0.83 1.02 0.37 0.33 0.06 0.2  0.03 0.   2.83] Loss_P: [2.34 1.69 1.66 0.49 0.34 0.05 0.21 0.02 6.8 ]\n",
      "Loss_Q: [0.94 0.98 0.38 0.31 0.04 0.28 0.02 0.   2.94] Loss_P: [2.33 1.64 1.56 0.5  0.28 0.03 0.24 0.02 6.6 ]\n",
      "Loss_Q: [0.87 0.97 0.37 0.3  0.04 0.21 0.02 0.   2.78] Loss_P: [2.38 1.64 1.49 0.45 0.28 0.04 0.19 0.01 6.47]\n",
      "Loss_Q: [0.89 0.98 0.39 0.3  0.05 0.23 0.02 0.   2.85] Loss_P: [2.35 1.65 1.59 0.44 0.33 0.04 0.21 0.02 6.62]\n",
      "Loss_Q: [0.85 0.98 0.37 0.3  0.03 0.21 0.01 0.   2.75] Loss_P: [2.34 1.71 1.59 0.39 0.29 0.05 0.21 0.01 6.59]\n",
      "Loss_Q: [0.87 0.99 0.38 0.35 0.06 0.21 0.02 0.   2.86] Loss_P: [2.36 1.64 1.59 0.4  0.32 0.05 0.18 0.01 6.55]\n",
      "Loss_Q: [0.86 0.93 0.39 0.33 0.05 0.23 0.02 0.   2.8 ] Loss_P: [2.38 1.7  1.56 0.44 0.34 0.04 0.2  0.02 6.68]\n",
      "Loss_Q: [0.86 0.93 0.32 0.31 0.04 0.22 0.03 0.   2.7 ] Loss_P: [2.41 1.63 1.54 0.38 0.27 0.03 0.22 0.02 6.51]\n",
      "Loss_Q: [0.83 0.99 0.36 0.28 0.04 0.19 0.01 0.   2.68] Loss_P: [2.34 1.61 1.49 0.41 0.31 0.04 0.17 0.02 6.38]\n",
      "Loss_Q: [0.83 0.95 0.39 0.26 0.03 0.2  0.01 0.   2.68] Loss_P: [2.39 1.62 1.53 0.43 0.3  0.04 0.19 0.02 6.51]\n",
      "Loss_Q: [0.81 0.92 0.36 0.31 0.03 0.21 0.03 0.   2.67] Loss_P: [2.36 1.64 1.54 0.44 0.34 0.04 0.24 0.02 6.61]\n",
      "Loss_Q: [0.89 0.89 0.34 0.27 0.03 0.21 0.02 0.   2.65] Loss_P: [2.31 1.65 1.59 0.44 0.26 0.04 0.24 0.02 6.54]\n",
      "Loss_Q: [0.81 0.94 0.34 0.33 0.04 0.22 0.02 0.   2.7 ] Loss_P: [2.34 1.66 1.61 0.37 0.31 0.04 0.2  0.01 6.53]\n",
      "Loss_Q: [0.88 0.94 0.29 0.31 0.04 0.19 0.01 0.   2.66] Loss_P: [2.37 1.58 1.5  0.4  0.35 0.03 0.19 0.01 6.43]\n",
      "Loss_Q: [0.83 0.92 0.32 0.33 0.05 0.2  0.03 0.   2.69] Loss_P: [2.35 1.62 1.54 0.39 0.34 0.05 0.19 0.02 6.5 ]\n",
      "Loss_Q: [0.81 0.91 0.29 0.34 0.03 0.21 0.02 0.   2.61] Loss_P: [2.37 1.65 1.66 0.41 0.33 0.06 0.26 0.02 6.75]\n",
      "Loss_Q: [0.91 0.94 0.34 0.39 0.04 0.2  0.02 0.   2.85] Loss_P: [2.35 1.54 1.56 0.45 0.35 0.05 0.24 0.02 6.55]\n",
      "Loss_Q: [0.77 0.94 0.38 0.4  0.05 0.19 0.02 0.   2.75] Loss_P: [2.35 1.61 1.55 0.42 0.37 0.05 0.22 0.01 6.59]\n",
      "Loss_Q: [0.86 0.94 0.39 0.39 0.04 0.21 0.03 0.   2.86] Loss_P: [2.38 1.63 1.54 0.41 0.39 0.04 0.2  0.01 6.6 ]\n",
      "Loss_Q: [0.79 0.98 0.4  0.38 0.03 0.22 0.02 0.   2.82] Loss_P: [2.33 1.62 1.54 0.43 0.38 0.06 0.21 0.02 6.59]\n",
      "Loss_Q: [0.81 0.89 0.35 0.4  0.03 0.18 0.01 0.   2.68] Loss_P: [2.33 1.68 1.52 0.42 0.37 0.04 0.21 0.02 6.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.87 0.98 0.34 0.37 0.04 0.2  0.02 0.   2.82] Loss_P: [2.37 1.63 1.62 0.41 0.39 0.04 0.19 0.02 6.66]\n",
      "Loss_Q: [0.81 1.   0.31 0.39 0.05 0.21 0.02 0.   2.79] Loss_P: [2.33 1.61 1.61 0.43 0.37 0.03 0.21 0.03 6.62]\n",
      "Loss_Q: [0.81 0.93 0.36 0.37 0.04 0.22 0.02 0.   2.75] Loss_P: [2.39 1.66 1.54 0.44 0.37 0.04 0.22 0.02 6.69]\n",
      "Loss_Q: [0.76 1.   0.38 0.41 0.04 0.2  0.01 0.   2.8 ] Loss_P: [2.35 1.63 1.53 0.42 0.37 0.04 0.22 0.02 6.59]\n",
      "Loss_Q: [0.79 0.94 0.36 0.39 0.05 0.21 0.01 0.   2.74] Loss_P: [2.35 1.58 1.61 0.44 0.39 0.03 0.21 0.01 6.63]\n",
      "Loss_Q: [0.81 0.92 0.35 0.39 0.04 0.21 0.02 0.   2.74] Loss_P: [2.39 1.62 1.5  0.37 0.39 0.04 0.19 0.02 6.51]\n",
      "Loss_Q: [0.88 0.95 0.35 0.38 0.05 0.2  0.01 0.   2.81] Loss_P: [2.37 1.55 1.48 0.41 0.38 0.04 0.2  0.01 6.44]\n",
      "Loss_Q: [0.78 0.95 0.38 0.37 0.05 0.18 0.01 0.   2.72] Loss_P: [2.37 1.63 1.5  0.47 0.33 0.04 0.22 0.03 6.58]\n",
      "Loss_Q: [0.87 0.96 0.35 0.39 0.04 0.2  0.03 0.   2.84] Loss_P: [2.34 1.61 1.49 0.43 0.37 0.03 0.21 0.02 6.5 ]\n",
      "Loss_Q: [0.76 0.85 0.34 0.37 0.04 0.23 0.01 0.   2.6 ] Loss_P: [2.38 1.56 1.51 0.46 0.4  0.02 0.2  0.02 6.56]\n",
      "Loss_Q: [0.77 0.94 0.37 0.36 0.04 0.24 0.02 0.   2.73] Loss_P: [2.34 1.67 1.53 0.42 0.41 0.04 0.2  0.01 6.62]\n",
      "Loss_Q: [0.78 0.99 0.34 0.4  0.05 0.19 0.01 0.   2.76] Loss_P: [2.34 1.61 1.54 0.43 0.37 0.05 0.19 0.02 6.56]\n",
      "Loss_Q: [0.79 1.01 0.37 0.39 0.03 0.22 0.02 0.   2.84] Loss_P: [2.39 1.58 1.51 0.41 0.4  0.04 0.18 0.02 6.53]\n",
      "Loss_Q: [0.85 0.99 0.32 0.4  0.02 0.19 0.01 0.   2.78] Loss_P: [2.4  1.61 1.58 0.4  0.39 0.04 0.22 0.02 6.65]\n",
      "Loss_Q: [0.82 1.05 0.35 0.36 0.04 0.18 0.02 0.   2.82] Loss_P: [2.32 1.65 1.57 0.41 0.39 0.04 0.18 0.02 6.57]\n",
      "Loss_Q: [0.81 0.93 0.35 0.42 0.03 0.21 0.02 0.   2.77] Loss_P: [2.36 1.6  1.52 0.44 0.39 0.03 0.18 0.01 6.52]\n",
      "Loss_Q: [0.76 0.91 0.32 0.43 0.04 0.22 0.02 0.   2.69] Loss_P: [2.37 1.62 1.51 0.43 0.44 0.03 0.21 0.04 6.65]\n",
      "Loss_Q: [0.74 1.02 0.36 0.4  0.03 0.23 0.02 0.   2.8 ] Loss_P: [2.34 1.57 1.6  0.37 0.41 0.04 0.21 0.03 6.57]\n",
      "Loss_Q: [0.81 0.99 0.31 0.45 0.04 0.21 0.02 0.   2.82] Loss_P: [2.35 1.55 1.5  0.42 0.49 0.04 0.23 0.02 6.6 ]\n",
      "Loss_Q: [0.84 0.96 0.3  0.43 0.03 0.2  0.01 0.   2.77] Loss_P: [2.38 1.59 1.48 0.32 0.4  0.03 0.22 0.03 6.45]\n",
      "Loss_Q: [0.81 0.99 0.31 0.45 0.05 0.21 0.02 0.   2.83] Loss_P: [2.42 1.58 1.55 0.36 0.41 0.03 0.18 0.01 6.54]\n",
      "Loss_Q: [0.78 0.99 0.33 0.41 0.04 0.21 0.01 0.   2.78] Loss_P: [2.39 1.59 1.51 0.35 0.42 0.05 0.22 0.01 6.54]\n",
      "Loss_Q: [0.87 1.01 0.34 0.45 0.03 0.22 0.02 0.   2.94] Loss_P: [2.32 1.62 1.56 0.37 0.44 0.05 0.24 0.02 6.62]\n",
      "Loss_Q: [0.76 0.94 0.32 0.42 0.04 0.23 0.03 0.   2.73] Loss_P: [2.37 1.6  1.53 0.36 0.38 0.04 0.23 0.02 6.51]\n",
      "Loss_Q: [0.67 0.95 0.34 0.38 0.03 0.25 0.02 0.   2.65] Loss_P: [2.34 1.59 1.5  0.32 0.36 0.03 0.24 0.02 6.4 ]\n",
      "Loss_Q: [0.71 1.02 0.33 0.37 0.05 0.23 0.02 0.   2.72] Loss_P: [2.43 1.63 1.52 0.36 0.32 0.04 0.25 0.03 6.58]\n",
      "Loss_Q: [0.79 1.05 0.3  0.33 0.05 0.24 0.03 0.   2.79] Loss_P: [2.37 1.59 1.56 0.37 0.33 0.04 0.26 0.02 6.53]\n",
      "Loss_Q: [0.7  0.96 0.26 0.31 0.03 0.24 0.03 0.   2.54] Loss_P: [2.36 1.63 1.57 0.3  0.3  0.03 0.25 0.02 6.46]\n",
      "Loss_Q: [0.76 1.01 0.29 0.34 0.05 0.25 0.03 0.   2.74] Loss_P: [2.34 1.6  1.63 0.37 0.35 0.05 0.25 0.02 6.62]\n",
      "Loss_Q: [0.76 0.97 0.3  0.3  0.05 0.25 0.02 0.   2.65] Loss_P: [2.4  1.56 1.55 0.32 0.33 0.03 0.26 0.03 6.48]\n",
      "Loss_Q: [0.77 1.   0.3  0.31 0.05 0.22 0.03 0.   2.67] Loss_P: [2.36 1.63 1.65 0.4  0.3  0.04 0.24 0.02 6.63]\n",
      "Loss_Q: [0.77 0.96 0.29 0.33 0.05 0.2  0.01 0.   2.61] Loss_P: [2.34 1.58 1.59 0.35 0.33 0.04 0.24 0.02 6.48]\n",
      "Loss_Q: [0.78 1.01 0.32 0.33 0.04 0.22 0.02 0.   2.73] Loss_P: [2.36 1.64 1.67 0.28 0.31 0.04 0.23 0.02 6.53]\n",
      "Loss_Q: [0.71 0.99 0.3  0.29 0.03 0.21 0.02 0.   2.56] Loss_P: [2.4  1.57 1.57 0.3  0.31 0.05 0.19 0.01 6.42]\n",
      "Loss_Q: [0.69 0.97 0.31 0.35 0.04 0.2  0.01 0.   2.58] Loss_P: [2.39 1.6  1.56 0.28 0.35 0.04 0.22 0.02 6.45]\n",
      "Loss_Q: [0.73 1.   0.27 0.34 0.06 0.21 0.02 0.   2.63] Loss_P: [2.39 1.57 1.59 0.34 0.35 0.06 0.23 0.02 6.55]\n",
      "Loss_Q: [0.78 0.9  0.3  0.32 0.05 0.27 0.02 0.   2.63] Loss_P: [2.41 1.62 1.51 0.29 0.34 0.04 0.2  0.01 6.42]\n",
      "Loss_Q: [0.73 0.97 0.29 0.37 0.04 0.2  0.02 0.   2.61] Loss_P: [2.39 1.65 1.57 0.32 0.34 0.04 0.21 0.01 6.53]\n",
      "Loss_Q: [0.8  1.01 0.31 0.33 0.04 0.22 0.03 0.   2.73] Loss_P: [2.38 1.6  1.58 0.32 0.32 0.06 0.19 0.02 6.47]\n",
      "Loss_Q: [0.74 1.   0.27 0.39 0.04 0.21 0.01 0.   2.67] Loss_P: [2.41 1.65 1.58 0.36 0.36 0.04 0.21 0.01 6.63]\n",
      "Loss_Q: [0.78 1.03 0.32 0.32 0.05 0.21 0.02 0.   2.74] Loss_P: [2.32 1.67 1.64 0.38 0.3  0.04 0.22 0.01 6.58]\n",
      "Loss_Q: [0.77 1.02 0.33 0.31 0.04 0.23 0.04 0.   2.74] Loss_P: [2.34 1.64 1.68 0.36 0.29 0.03 0.22 0.01 6.56]\n",
      "Loss_Q: [0.77 0.96 0.3  0.33 0.04 0.21 0.02 0.   2.62] Loss_P: [2.46 1.58 1.58 0.34 0.35 0.05 0.24 0.01 6.62]\n",
      "Loss_Q: [0.75 1.01 0.31 0.32 0.04 0.2  0.02 0.   2.65] Loss_P: [2.3  1.64 1.59 0.32 0.33 0.05 0.24 0.02 6.49]\n",
      "Loss_Q: [0.8  0.94 0.25 0.32 0.05 0.21 0.02 0.   2.58] Loss_P: [2.38 1.62 1.59 0.35 0.31 0.08 0.23 0.01 6.58]\n",
      "Loss_Q: [0.8  0.94 0.33 0.31 0.04 0.21 0.03 0.   2.66] Loss_P: [2.37 1.62 1.54 0.35 0.28 0.04 0.19 0.02 6.42]\n",
      "Loss_Q: [0.77 1.03 0.34 0.33 0.04 0.18 0.01 0.   2.69] Loss_P: [2.33 1.64 1.6  0.32 0.32 0.05 0.21 0.02 6.48]\n",
      "Loss_Q: [0.82 1.02 0.33 0.3  0.04 0.2  0.02 0.   2.73] Loss_P: [2.32 1.66 1.6  0.34 0.35 0.03 0.22 0.03 6.55]\n",
      "Loss_Q: [0.77 1.02 0.31 0.34 0.05 0.21 0.02 0.   2.73] Loss_P: [2.44 1.61 1.55 0.38 0.34 0.05 0.22 0.02 6.61]\n",
      "Loss_Q: [0.76 0.99 0.31 0.3  0.02 0.25 0.03 0.   2.67] Loss_P: [2.36 1.67 1.62 0.36 0.33 0.05 0.24 0.03 6.66]\n",
      "Loss_Q: [0.77 1.01 0.3  0.32 0.04 0.27 0.02 0.   2.73] Loss_P: [2.37 1.63 1.59 0.37 0.31 0.03 0.24 0.01 6.56]\n",
      "Loss_Q: [0.81 1.   0.29 0.28 0.04 0.22 0.02 0.   2.66] Loss_P: [2.34 1.63 1.62 0.36 0.34 0.05 0.26 0.02 6.63]\n",
      "Loss_Q: [0.75 1.07 0.31 0.31 0.04 0.26 0.02 0.   2.75] Loss_P: [2.35 1.68 1.63 0.35 0.28 0.05 0.23 0.01 6.57]\n",
      "Loss_Q: [0.73 1.02 0.32 0.31 0.04 0.26 0.01 0.   2.7 ] Loss_P: [2.35 1.63 1.6  0.36 0.31 0.04 0.23 0.01 6.54]\n",
      "Loss_Q: [0.74 1.03 0.31 0.32 0.04 0.24 0.01 0.   2.69] Loss_P: [2.39 1.63 1.54 0.35 0.32 0.04 0.23 0.02 6.54]\n",
      "Loss_Q: [0.77 1.03 0.33 0.24 0.04 0.24 0.01 0.   2.67] Loss_P: [2.36 1.61 1.54 0.39 0.29 0.04 0.22 0.02 6.46]\n",
      "Loss_Q: [0.8  1.07 0.32 0.33 0.05 0.23 0.01 0.   2.81] Loss_P: [2.36 1.69 1.62 0.36 0.29 0.04 0.23 0.02 6.62]\n",
      "Loss_Q: [0.87 0.98 0.38 0.31 0.06 0.23 0.01 0.   2.83] Loss_P: [2.36 1.72 1.58 0.36 0.32 0.03 0.22 0.01 6.59]\n",
      "Loss_Q: [0.8  1.05 0.33 0.31 0.05 0.23 0.01 0.   2.79] Loss_P: [2.35 1.63 1.59 0.39 0.3  0.03 0.24 0.02 6.56]\n",
      "Loss_Q: [0.81 1.01 0.32 0.31 0.05 0.25 0.01 0.   2.77] Loss_P: [2.42 1.69 1.59 0.39 0.37 0.04 0.26 0.02 6.77]\n",
      "Loss_Q: [0.76 1.11 0.37 0.33 0.04 0.24 0.01 0.   2.87] Loss_P: [2.34 1.67 1.61 0.37 0.31 0.05 0.23 0.02 6.59]\n",
      "Loss_Q: [0.77 1.09 0.4  0.31 0.06 0.21 0.01 0.   2.86] Loss_P: [2.42 1.62 1.61 0.41 0.31 0.04 0.23 0.02 6.65]\n",
      "Loss_Q: [0.74 1.01 0.38 0.38 0.07 0.21 0.01 0.   2.8 ] Loss_P: [2.34 1.7  1.58 0.41 0.35 0.06 0.21 0.02 6.67]\n",
      "Loss_Q: [0.75 1.06 0.37 0.35 0.09 0.21 0.02 0.   2.85] Loss_P: [2.4  1.61 1.57 0.44 0.36 0.04 0.22 0.01 6.65]\n",
      "Loss_Q: [0.75 1.03 0.38 0.37 0.04 0.2  0.01 0.   2.78] Loss_P: [2.36 1.67 1.61 0.43 0.35 0.06 0.21 0.01 6.71]\n",
      "Loss_Q: [0.77 1.13 0.38 0.31 0.04 0.2  0.01 0.   2.83] Loss_P: [2.39 1.61 1.56 0.42 0.32 0.04 0.2  0.01 6.55]\n",
      "Loss_Q: [0.83 1.01 0.34 0.3  0.06 0.22 0.01 0.   2.78] Loss_P: [2.41 1.63 1.63 0.43 0.36 0.04 0.21 0.01 6.73]\n",
      "Loss_Q: [0.74 1.05 0.35 0.39 0.04 0.19 0.01 0.   2.78] Loss_P: [2.33 1.63 1.65 0.41 0.34 0.04 0.21 0.02 6.63]\n",
      "Loss_Q: [0.79 1.04 0.35 0.43 0.04 0.25 0.02 0.   2.91] Loss_P: [2.41 1.64 1.63 0.44 0.37 0.04 0.24 0.01 6.78]\n",
      "Loss_Q: [0.77 1.08 0.36 0.35 0.04 0.24 0.03 0.   2.87] Loss_P: [2.35 1.63 1.61 0.44 0.37 0.06 0.21 0.01 6.67]\n",
      "Loss_Q: [0.71 1.03 0.35 0.35 0.04 0.2  0.02 0.   2.7 ] Loss_P: [2.34 1.64 1.56 0.44 0.35 0.06 0.23 0.01 6.63]\n",
      "Loss_Q: [0.76 1.03 0.36 0.35 0.05 0.22 0.02 0.   2.79] Loss_P: [2.37 1.66 1.56 0.42 0.35 0.03 0.25 0.02 6.66]\n",
      "Loss_Q: [0.8  1.06 0.4  0.38 0.04 0.22 0.01 0.   2.92] Loss_P: [2.33 1.65 1.61 0.47 0.38 0.05 0.23 0.02 6.75]\n",
      "Loss_Q: [0.82 1.09 0.43 0.37 0.04 0.22 0.02 0.   2.99] Loss_P: [2.35 1.64 1.66 0.48 0.37 0.05 0.24 0.02 6.81]\n",
      "Loss_Q: [0.8  1.06 0.46 0.35 0.05 0.23 0.02 0.   2.97] Loss_P: [2.34 1.63 1.59 0.49 0.38 0.06 0.22 0.02 6.72]\n",
      "Loss_Q: [0.8  1.05 0.39 0.36 0.05 0.23 0.01 0.   2.9 ] Loss_P: [2.32 1.66 1.62 0.46 0.34 0.04 0.22 0.02 6.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.78 1.08 0.37 0.31 0.06 0.24 0.02 0.   2.86] Loss_P: [2.35 1.71 1.61 0.45 0.37 0.06 0.24 0.03 6.81]\n",
      "Loss_Q: [0.79 1.07 0.4  0.33 0.04 0.26 0.01 0.   2.92] Loss_P: [2.33 1.65 1.6  0.47 0.35 0.04 0.24 0.02 6.7 ]\n",
      "Loss_Q: [0.79 1.05 0.39 0.32 0.05 0.21 0.01 0.   2.81] Loss_P: [2.35 1.67 1.63 0.45 0.31 0.03 0.22 0.02 6.68]\n",
      "Loss_Q: [0.83 1.1  0.38 0.34 0.05 0.19 0.02 0.   2.92] Loss_P: [2.29 1.63 1.64 0.46 0.34 0.05 0.23 0.03 6.68]\n",
      "Loss_Q: [0.77 1.07 0.38 0.38 0.07 0.26 0.01 0.   2.94] Loss_P: [2.33 1.6  1.66 0.42 0.34 0.04 0.24 0.02 6.65]\n",
      "Loss_Q: [0.78 1.09 0.35 0.34 0.06 0.25 0.02 0.   2.89] Loss_P: [2.32 1.69 1.63 0.48 0.31 0.03 0.22 0.02 6.71]\n",
      "Loss_Q: [0.81 1.08 0.38 0.31 0.05 0.25 0.01 0.   2.89] Loss_P: [2.32 1.65 1.66 0.42 0.36 0.03 0.26 0.02 6.74]\n",
      "Loss_Q: [0.83 1.08 0.36 0.3  0.06 0.26 0.02 0.   2.91] Loss_P: [2.41 1.61 1.61 0.44 0.32 0.05 0.23 0.03 6.7 ]\n",
      "Loss_Q: [0.74 1.07 0.35 0.3  0.05 0.24 0.01 0.   2.76] Loss_P: [2.34 1.71 1.66 0.45 0.36 0.05 0.27 0.03 6.87]\n",
      "Loss_Q: [0.8  1.08 0.36 0.33 0.04 0.25 0.01 0.   2.87] Loss_P: [2.33 1.63 1.67 0.4  0.3  0.05 0.26 0.02 6.67]\n",
      "Loss_Q: [0.86 1.03 0.36 0.32 0.04 0.25 0.02 0.   2.89] Loss_P: [2.37 1.64 1.71 0.43 0.27 0.05 0.25 0.01 6.73]\n",
      "Loss_Q: [0.77 1.06 0.36 0.37 0.04 0.25 0.02 0.   2.87] Loss_P: [2.38 1.65 1.68 0.4  0.32 0.04 0.25 0.02 6.75]\n",
      "Loss_Q: [0.76 1.06 0.35 0.33 0.04 0.24 0.02 0.   2.81] Loss_P: [2.36 1.68 1.65 0.43 0.32 0.03 0.26 0.01 6.74]\n",
      "Loss_Q: [0.82 1.08 0.37 0.36 0.04 0.24 0.02 0.   2.92] Loss_P: [2.35 1.63 1.71 0.44 0.32 0.04 0.25 0.01 6.76]\n",
      "Loss_Q: [0.78 1.06 0.32 0.31 0.05 0.26 0.01 0.   2.78] Loss_P: [2.38 1.61 1.71 0.46 0.34 0.03 0.29 0.01 6.84]\n",
      "Loss_Q: [0.8  1.06 0.4  0.32 0.07 0.25 0.02 0.   2.92] Loss_P: [2.32 1.6  1.67 0.42 0.32 0.04 0.25 0.02 6.65]\n",
      "Loss_Q: [0.79 1.11 0.38 0.31 0.03 0.26 0.02 0.   2.9 ] Loss_P: [2.29 1.68 1.72 0.45 0.29 0.04 0.24 0.02 6.73]\n",
      "Loss_Q: [0.91 1.09 0.39 0.26 0.05 0.23 0.02 0.   2.95] Loss_P: [2.3  1.65 1.69 0.46 0.32 0.04 0.28 0.01 6.75]\n",
      "Loss_Q: [0.91 1.07 0.35 0.29 0.04 0.26 0.02 0.   2.94] Loss_P: [2.34 1.66 1.75 0.41 0.27 0.06 0.26 0.01 6.76]\n",
      "Loss_Q: [0.8  1.05 0.36 0.28 0.06 0.28 0.02 0.   2.86] Loss_P: [2.29 1.58 1.68 0.44 0.31 0.05 0.26 0.02 6.63]\n",
      "Loss_Q: [0.77 1.09 0.34 0.31 0.05 0.24 0.01 0.   2.82] Loss_P: [2.33 1.61 1.68 0.42 0.27 0.05 0.26 0.02 6.63]\n",
      "Loss_Q: [0.72 1.05 0.33 0.28 0.06 0.27 0.01 0.   2.72] Loss_P: [2.31 1.66 1.68 0.46 0.31 0.04 0.26 0.02 6.75]\n",
      "Loss_Q: [0.85 1.06 0.37 0.32 0.04 0.24 0.02 0.   2.9 ] Loss_P: [2.33 1.63 1.67 0.41 0.31 0.04 0.27 0.02 6.67]\n",
      "Loss_Q: [0.86 1.09 0.38 0.28 0.03 0.27 0.02 0.   2.92] Loss_P: [2.33 1.7  1.66 0.44 0.31 0.06 0.25 0.01 6.76]\n",
      "Loss_Q: [0.85 1.1  0.4  0.33 0.06 0.26 0.01 0.   3.02] Loss_P: [2.36 1.62 1.66 0.45 0.32 0.04 0.26 0.02 6.73]\n",
      "Loss_Q: [0.83 1.03 0.38 0.32 0.04 0.26 0.02 0.   2.88] Loss_P: [2.4  1.62 1.68 0.41 0.3  0.04 0.24 0.02 6.72]\n",
      "Loss_Q: [0.81 1.07 0.37 0.36 0.05 0.27 0.01 0.   2.95] Loss_P: [2.35 1.64 1.68 0.45 0.33 0.06 0.27 0.03 6.8 ]\n",
      "Loss_Q: [0.81 1.13 0.37 0.33 0.05 0.26 0.03 0.   2.97] Loss_P: [2.28 1.67 1.69 0.39 0.31 0.06 0.26 0.01 6.66]\n",
      "Loss_Q: [0.86 1.12 0.36 0.36 0.06 0.28 0.02 0.   3.04] Loss_P: [2.33 1.74 1.7  0.44 0.34 0.03 0.25 0.02 6.85]\n",
      "Loss_Q: [0.8  1.1  0.4  0.34 0.04 0.27 0.03 0.   2.98] Loss_P: [2.33 1.62 1.66 0.41 0.38 0.06 0.27 0.02 6.75]\n",
      "Loss_Q: [0.81 1.1  0.36 0.3  0.06 0.26 0.01 0.   2.91] Loss_P: [2.31 1.68 1.71 0.43 0.29 0.03 0.25 0.01 6.7 ]\n",
      "Loss_Q: [0.82 1.09 0.37 0.34 0.05 0.22 0.02 0.   2.91] Loss_P: [2.31 1.72 1.64 0.4  0.33 0.05 0.21 0.02 6.68]\n",
      "Loss_Q: [0.86 1.13 0.41 0.31 0.04 0.24 0.02 0.   3.01] Loss_P: [2.38 1.69 1.62 0.41 0.34 0.06 0.23 0.01 6.73]\n",
      "Loss_Q: [0.77 1.06 0.33 0.28 0.04 0.23 0.01 0.   2.74] Loss_P: [2.37 1.66 1.59 0.35 0.34 0.04 0.26 0.01 6.62]\n",
      "Loss_Q: [0.79 1.04 0.33 0.31 0.04 0.24 0.01 0.   2.76] Loss_P: [2.34 1.74 1.61 0.37 0.33 0.05 0.23 0.02 6.68]\n",
      "Loss_Q: [0.85 1.07 0.36 0.28 0.05 0.27 0.02 0.   2.9 ] Loss_P: [2.38 1.71 1.62 0.38 0.35 0.05 0.27 0.02 6.78]\n",
      "Loss_Q: [0.8  1.03 0.36 0.33 0.05 0.25 0.02 0.   2.83] Loss_P: [2.28 1.71 1.67 0.45 0.34 0.06 0.25 0.02 6.78]\n",
      "Loss_Q: [0.83 1.08 0.39 0.28 0.03 0.23 0.02 0.   2.87] Loss_P: [2.37 1.78 1.62 0.47 0.31 0.04 0.22 0.02 6.83]\n",
      "Loss_Q: [0.86 1.09 0.38 0.39 0.05 0.24 0.02 0.   3.02] Loss_P: [2.38 1.68 1.66 0.43 0.35 0.04 0.27 0.02 6.83]\n",
      "Loss_Q: [0.82 1.08 0.39 0.37 0.06 0.23 0.02 0.   2.95] Loss_P: [2.31 1.67 1.72 0.5  0.31 0.05 0.24 0.02 6.82]\n",
      "Loss_Q: [0.79 1.06 0.38 0.34 0.05 0.21 0.01 0.   2.84] Loss_P: [2.35 1.69 1.61 0.47 0.33 0.03 0.25 0.02 6.76]\n",
      "Loss_Q: [0.79 1.05 0.36 0.32 0.04 0.26 0.02 0.   2.84] Loss_P: [2.33 1.69 1.6  0.39 0.36 0.05 0.24 0.01 6.68]\n",
      "Loss_Q: [0.8  1.04 0.36 0.31 0.05 0.23 0.01 0.   2.8 ] Loss_P: [2.29 1.69 1.67 0.39 0.31 0.04 0.24 0.02 6.65]\n",
      "Loss_Q: [0.83 1.04 0.37 0.33 0.04 0.22 0.02 0.   2.87] Loss_P: [2.39 1.58 1.67 0.39 0.34 0.06 0.25 0.01 6.68]\n",
      "Loss_Q: [0.78 1.08 0.37 0.34 0.04 0.23 0.03 0.   2.87] Loss_P: [2.3  1.69 1.65 0.41 0.3  0.04 0.26 0.02 6.66]\n",
      "Loss_Q: [0.83 1.12 0.43 0.31 0.04 0.25 0.01 0.   2.98] Loss_P: [2.31 1.71 1.69 0.38 0.3  0.05 0.23 0.02 6.71]\n",
      "Loss_Q: [0.81 1.08 0.34 0.32 0.04 0.25 0.02 0.   2.86] Loss_P: [2.33 1.71 1.62 0.41 0.34 0.07 0.22 0.02 6.7 ]\n",
      "Loss_Q: [0.85 1.01 0.34 0.33 0.04 0.23 0.02 0.   2.82] Loss_P: [2.35 1.71 1.65 0.39 0.31 0.05 0.2  0.02 6.68]\n",
      "Loss_Q: [0.9  1.07 0.33 0.33 0.05 0.24 0.02 0.   2.94] Loss_P: [2.27 1.74 1.65 0.4  0.31 0.05 0.2  0.02 6.64]\n",
      "Loss_Q: [0.78 1.07 0.37 0.33 0.05 0.2  0.02 0.   2.82] Loss_P: [2.29 1.7  1.63 0.47 0.35 0.04 0.22 0.01 6.71]\n",
      "Loss_Q: [0.85 1.03 0.43 0.37 0.05 0.21 0.01 0.   2.94] Loss_P: [2.33 1.71 1.64 0.42 0.32 0.03 0.24 0.02 6.71]\n",
      "Loss_Q: [0.84 1.11 0.38 0.34 0.04 0.25 0.01 0.   2.97] Loss_P: [2.3  1.66 1.7  0.38 0.32 0.03 0.26 0.01 6.65]\n",
      "Loss_Q: [0.84 1.1  0.42 0.36 0.05 0.24 0.02 0.   3.04] Loss_P: [2.4  1.62 1.65 0.41 0.33 0.05 0.23 0.01 6.69]\n",
      "Loss_Q: [0.85 1.13 0.35 0.35 0.06 0.23 0.01 0.   2.98] Loss_P: [2.34 1.66 1.7  0.44 0.32 0.05 0.25 0.01 6.78]\n",
      "Loss_Q: [0.88 1.1  0.41 0.32 0.03 0.25 0.02 0.   3.01] Loss_P: [2.31 1.79 1.73 0.38 0.35 0.06 0.25 0.02 6.9 ]\n",
      "Loss_Q: [0.85 1.14 0.36 0.36 0.05 0.27 0.02 0.   3.05] Loss_P: [2.28 1.72 1.69 0.41 0.35 0.05 0.28 0.02 6.81]\n",
      "Loss_Q: [0.79 1.09 0.38 0.36 0.05 0.25 0.01 0.   2.93] Loss_P: [2.33 1.7  1.71 0.45 0.38 0.04 0.25 0.02 6.87]\n",
      "Loss_Q: [0.8  1.09 0.38 0.33 0.04 0.25 0.02 0.   2.9 ] Loss_P: [2.35 1.66 1.68 0.5  0.35 0.04 0.21 0.01 6.8 ]\n",
      "Loss_Q: [0.8  1.08 0.36 0.34 0.04 0.23 0.02 0.   2.87] Loss_P: [2.32 1.63 1.72 0.44 0.33 0.07 0.23 0.02 6.77]\n",
      "Loss_Q: [0.84 1.06 0.43 0.32 0.04 0.23 0.02 0.   2.94] Loss_P: [2.3  1.73 1.77 0.47 0.31 0.04 0.23 0.02 6.86]\n",
      "Loss_Q: [0.84 1.1  0.38 0.31 0.04 0.19 0.02 0.   2.88] Loss_P: [2.28 1.63 1.72 0.47 0.32 0.04 0.22 0.02 6.71]\n",
      "Loss_Q: [0.81 1.04 0.41 0.31 0.04 0.23 0.02 0.   2.85] Loss_P: [2.36 1.66 1.62 0.46 0.32 0.05 0.23 0.02 6.73]\n",
      "Loss_Q: [0.83 1.15 0.45 0.33 0.05 0.23 0.01 0.   3.05] Loss_P: [2.34 1.64 1.77 0.55 0.31 0.03 0.21 0.02 6.86]\n",
      "Loss_Q: [0.82 1.08 0.44 0.31 0.04 0.23 0.02 0.   2.93] Loss_P: [2.33 1.67 1.66 0.51 0.3  0.04 0.26 0.02 6.8 ]\n",
      "Loss_Q: [0.77 1.12 0.43 0.32 0.03 0.24 0.02 0.   2.94] Loss_P: [2.34 1.62 1.71 0.53 0.35 0.04 0.21 0.01 6.81]\n",
      "Loss_Q: [0.78 1.1  0.46 0.31 0.03 0.26 0.02 0.   2.96] Loss_P: [2.28 1.69 1.73 0.54 0.31 0.04 0.25 0.03 6.86]\n",
      "Loss_Q: [0.77 1.11 0.45 0.31 0.04 0.29 0.02 0.   2.99] Loss_P: [2.34 1.65 1.66 0.55 0.32 0.04 0.27 0.01 6.83]\n",
      "Loss_Q: [0.82 1.1  0.45 0.31 0.05 0.26 0.02 0.   3.01] Loss_P: [2.3  1.67 1.7  0.51 0.31 0.05 0.27 0.01 6.82]\n",
      "Loss_Q: [0.82 1.16 0.46 0.32 0.06 0.26 0.01 0.   3.08] Loss_P: [2.38 1.65 1.68 0.57 0.33 0.05 0.28 0.02 6.96]\n",
      "Loss_Q: [0.79 1.12 0.47 0.34 0.05 0.24 0.01 0.   3.02] Loss_P: [2.32 1.63 1.7  0.51 0.33 0.05 0.25 0.01 6.8 ]\n",
      "Loss_Q: [0.79 1.14 0.46 0.3  0.04 0.27 0.01 0.   3.01] Loss_P: [2.34 1.69 1.67 0.51 0.33 0.04 0.28 0.01 6.86]\n",
      "Loss_Q: [0.7  1.12 0.43 0.28 0.04 0.27 0.02 0.   2.87] Loss_P: [2.35 1.62 1.66 0.48 0.27 0.06 0.27 0.02 6.74]\n",
      "Loss_Q: [0.74 1.08 0.4  0.29 0.04 0.25 0.02 0.   2.83] Loss_P: [2.33 1.63 1.66 0.49 0.34 0.05 0.26 0.02 6.77]\n",
      "Loss_Q: [0.76 1.07 0.41 0.31 0.04 0.28 0.01 0.   2.88] Loss_P: [2.33 1.68 1.62 0.47 0.3  0.05 0.25 0.01 6.72]\n",
      "Loss_Q: [0.78 1.11 0.43 0.31 0.05 0.24 0.02 0.   2.94] Loss_P: [2.33 1.65 1.57 0.47 0.32 0.05 0.24 0.02 6.66]\n",
      "Loss_Q: [0.75 1.1  0.49 0.31 0.05 0.3  0.01 0.   3.01] Loss_P: [2.34 1.65 1.67 0.53 0.34 0.04 0.27 0.02 6.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.75 1.11 0.52 0.28 0.06 0.25 0.02 0.   2.98] Loss_P: [2.3  1.71 1.64 0.52 0.3  0.05 0.3  0.02 6.85]\n",
      "Loss_Q: [0.81 1.08 0.46 0.29 0.07 0.26 0.01 0.   2.98] Loss_P: [2.3  1.7  1.63 0.51 0.27 0.05 0.28 0.02 6.76]\n",
      "Loss_Q: [0.77 1.07 0.4  0.26 0.05 0.26 0.01 0.   2.82] Loss_P: [2.37 1.61 1.63 0.43 0.3  0.05 0.29 0.01 6.69]\n",
      "Loss_Q: [0.76 1.04 0.42 0.28 0.06 0.26 0.02 0.   2.83] Loss_P: [2.29 1.71 1.63 0.5  0.27 0.04 0.25 0.01 6.69]\n",
      "Loss_Q: [0.74 1.06 0.43 0.3  0.05 0.26 0.01 0.   2.85] Loss_P: [2.29 1.64 1.58 0.49 0.31 0.07 0.28 0.03 6.7 ]\n",
      "Loss_Q: [0.77 1.02 0.41 0.3  0.07 0.25 0.02 0.   2.83] Loss_P: [2.35 1.65 1.65 0.47 0.26 0.03 0.28 0.02 6.72]\n",
      "Loss_Q: [0.86 1.06 0.48 0.29 0.04 0.29 0.02 0.   3.04] Loss_P: [2.34 1.63 1.66 0.49 0.25 0.05 0.26 0.01 6.72]\n",
      "Loss_Q: [0.72 1.05 0.47 0.3  0.05 0.28 0.01 0.   2.88] Loss_P: [2.38 1.63 1.63 0.51 0.25 0.04 0.25 0.02 6.7 ]\n",
      "Loss_Q: [0.81 1.06 0.38 0.29 0.04 0.26 0.02 0.   2.87] Loss_P: [2.38 1.66 1.59 0.49 0.27 0.04 0.26 0.02 6.71]\n",
      "Loss_Q: [0.81 1.1  0.47 0.31 0.05 0.24 0.02 0.   3.  ] Loss_P: [2.39 1.63 1.59 0.51 0.32 0.04 0.28 0.01 6.76]\n",
      "Loss_Q: [0.78 1.05 0.41 0.33 0.04 0.25 0.01 0.   2.88] Loss_P: [2.39 1.61 1.57 0.49 0.29 0.04 0.24 0.02 6.65]\n",
      "Loss_Q: [0.79 1.14 0.43 0.32 0.05 0.23 0.01 0.   2.98] Loss_P: [2.37 1.69 1.59 0.49 0.31 0.04 0.26 0.01 6.75]\n",
      "Loss_Q: [0.8  1.04 0.43 0.37 0.05 0.26 0.01 0.   2.96] Loss_P: [2.39 1.59 1.53 0.48 0.29 0.04 0.23 0.02 6.58]\n",
      "Loss_Q: [0.75 1.1  0.42 0.31 0.04 0.26 0.02 0.   2.9 ] Loss_P: [2.4  1.61 1.55 0.5  0.32 0.04 0.27 0.01 6.7 ]\n",
      "Loss_Q: [0.69 1.11 0.46 0.3  0.04 0.22 0.01 0.   2.83] Loss_P: [2.4  1.62 1.58 0.46 0.38 0.04 0.25 0.02 6.75]\n",
      "Loss_Q: [0.75 1.1  0.43 0.35 0.07 0.25 0.02 0.   2.95] Loss_P: [2.38 1.66 1.54 0.46 0.39 0.06 0.25 0.01 6.75]\n",
      "Loss_Q: [0.76 1.12 0.37 0.39 0.03 0.25 0.02 0.   2.94] Loss_P: [2.4  1.61 1.53 0.47 0.36 0.04 0.28 0.02 6.73]\n",
      "Loss_Q: [0.75 1.06 0.43 0.39 0.04 0.28 0.02 0.   2.96] Loss_P: [2.4  1.59 1.52 0.47 0.37 0.04 0.28 0.02 6.68]\n",
      "Loss_Q: [0.73 1.08 0.4  0.35 0.06 0.23 0.01 0.   2.88] Loss_P: [2.38 1.7  1.54 0.51 0.32 0.06 0.24 0.01 6.76]\n",
      "Loss_Q: [0.8  1.09 0.43 0.36 0.04 0.24 0.01 0.   2.98] Loss_P: [2.37 1.69 1.6  0.52 0.36 0.05 0.27 0.02 6.87]\n",
      "Loss_Q: [0.68 1.04 0.44 0.34 0.05 0.22 0.01 0.   2.77] Loss_P: [2.39 1.66 1.52 0.51 0.36 0.06 0.24 0.02 6.77]\n",
      "Loss_Q: [0.71 1.12 0.4  0.34 0.05 0.23 0.01 0.   2.86] Loss_P: [2.4  1.62 1.57 0.48 0.35 0.04 0.23 0.02 6.7 ]\n",
      "Loss_Q: [0.82 1.12 0.41 0.32 0.04 0.23 0.01 0.   2.96] Loss_P: [2.39 1.71 1.63 0.49 0.37 0.05 0.23 0.02 6.89]\n",
      "Loss_Q: [0.73 1.06 0.37 0.37 0.04 0.23 0.01 0.   2.8 ] Loss_P: [2.4  1.65 1.64 0.43 0.36 0.05 0.25 0.02 6.8 ]\n",
      "Loss_Q: [0.76 1.1  0.39 0.45 0.07 0.25 0.01 0.   3.03] Loss_P: [2.42 1.59 1.63 0.43 0.36 0.05 0.23 0.01 6.72]\n",
      "Loss_Q: [0.78 1.13 0.44 0.35 0.04 0.27 0.02 0.   3.02] Loss_P: [2.38 1.73 1.56 0.48 0.37 0.05 0.22 0.03 6.8 ]\n",
      "Loss_Q: [0.77 1.02 0.4  0.41 0.05 0.25 0.01 0.   2.91] Loss_P: [2.37 1.68 1.64 0.42 0.39 0.04 0.25 0.02 6.81]\n",
      "Loss_Q: [0.86 1.13 0.42 0.39 0.04 0.28 0.02 0.   3.13] Loss_P: [2.33 1.72 1.65 0.47 0.38 0.05 0.24 0.02 6.86]\n",
      "Loss_Q: [0.77 1.12 0.37 0.35 0.06 0.26 0.02 0.   2.97] Loss_P: [2.39 1.65 1.62 0.43 0.39 0.05 0.26 0.02 6.8 ]\n",
      "Loss_Q: [0.83 1.13 0.42 0.33 0.07 0.22 0.02 0.   3.03] Loss_P: [2.33 1.68 1.68 0.47 0.41 0.04 0.25 0.03 6.88]\n",
      "Loss_Q: [0.82 1.1  0.45 0.43 0.05 0.23 0.02 0.   3.1 ] Loss_P: [2.38 1.68 1.67 0.44 0.37 0.05 0.25 0.02 6.86]\n",
      "Loss_Q: [0.79 1.05 0.41 0.38 0.04 0.26 0.02 0.   2.95] Loss_P: [2.37 1.7  1.58 0.4  0.37 0.05 0.27 0.01 6.74]\n",
      "Loss_Q: [0.77 1.13 0.41 0.39 0.05 0.23 0.02 0.   2.99] Loss_P: [2.38 1.72 1.58 0.44 0.4  0.04 0.25 0.01 6.83]\n",
      "Loss_Q: [0.85 1.12 0.43 0.34 0.05 0.26 0.01 0.   3.07] Loss_P: [2.39 1.66 1.6  0.5  0.41 0.04 0.26 0.01 6.86]\n",
      "Loss_Q: [0.8  1.05 0.45 0.41 0.05 0.24 0.02 0.   3.01] Loss_P: [2.41 1.68 1.54 0.5  0.4  0.04 0.26 0.02 6.84]\n",
      "Loss_Q: [0.74 1.1  0.47 0.43 0.05 0.24 0.01 0.   3.03] Loss_P: [2.34 1.67 1.58 0.47 0.4  0.06 0.24 0.01 6.76]\n",
      "Loss_Q: [0.72 1.04 0.44 0.4  0.04 0.27 0.02 0.   2.95] Loss_P: [2.4  1.74 1.58 0.48 0.41 0.04 0.24 0.02 6.91]\n",
      "Loss_Q: [0.76 1.13 0.39 0.41 0.04 0.28 0.02 0.   3.04] Loss_P: [2.36 1.68 1.58 0.52 0.35 0.04 0.24 0.02 6.79]\n",
      "Loss_Q: [0.73 1.07 0.41 0.42 0.06 0.25 0.03 0.   2.96] Loss_P: [2.4  1.65 1.59 0.48 0.4  0.03 0.25 0.02 6.83]\n",
      "Loss_Q: [0.74 1.1  0.47 0.41 0.05 0.25 0.03 0.   3.05] Loss_P: [2.37 1.68 1.52 0.46 0.4  0.05 0.28 0.02 6.78]\n",
      "Loss_Q: [0.74 1.07 0.42 0.4  0.06 0.27 0.04 0.   3.01] Loss_P: [2.4  1.68 1.57 0.52 0.46 0.04 0.25 0.02 6.94]\n",
      "Loss_Q: [0.8  1.09 0.45 0.4  0.06 0.27 0.03 0.   3.1 ] Loss_P: [2.39 1.66 1.55 0.49 0.41 0.04 0.28 0.02 6.83]\n",
      "Loss_Q: [0.71 1.03 0.42 0.45 0.05 0.25 0.01 0.   2.92] Loss_P: [2.38 1.66 1.55 0.49 0.43 0.05 0.24 0.02 6.82]\n",
      "Loss_Q: [0.76 0.99 0.44 0.47 0.06 0.22 0.03 0.   2.96] Loss_P: [2.39 1.66 1.53 0.48 0.44 0.04 0.25 0.01 6.8 ]\n",
      "Loss_Q: [0.75 0.99 0.43 0.45 0.04 0.24 0.02 0.   2.92] Loss_P: [2.35 1.67 1.55 0.51 0.43 0.04 0.24 0.02 6.8 ]\n",
      "Loss_Q: [0.79 1.08 0.49 0.43 0.06 0.26 0.01 0.   3.13] Loss_P: [2.31 1.7  1.55 0.51 0.45 0.04 0.24 0.02 6.83]\n",
      "Loss_Q: [0.83 1.11 0.5  0.51 0.04 0.29 0.02 0.   3.3 ] Loss_P: [2.3  1.73 1.57 0.49 0.46 0.04 0.28 0.02 6.89]\n",
      "Loss_Q: [0.8  1.06 0.48 0.44 0.04 0.29 0.01 0.   3.14] Loss_P: [2.37 1.7  1.58 0.58 0.53 0.05 0.28 0.02 7.11]\n",
      "Loss_Q: [0.81 1.06 0.5  0.44 0.05 0.26 0.02 0.   3.13] Loss_P: [2.35 1.77 1.56 0.49 0.48 0.04 0.27 0.03 7.  ]\n",
      "Loss_Q: [0.76 1.09 0.51 0.47 0.05 0.3  0.03 0.   3.21] Loss_P: [2.39 1.67 1.57 0.54 0.48 0.03 0.26 0.03 6.96]\n",
      "Loss_Q: [0.75 1.05 0.5  0.48 0.05 0.28 0.01 0.   3.13] Loss_P: [2.36 1.73 1.52 0.53 0.5  0.04 0.27 0.02 6.96]\n",
      "Loss_Q: [0.8  1.05 0.46 0.46 0.05 0.27 0.02 0.   3.12] Loss_P: [2.36 1.78 1.51 0.51 0.51 0.04 0.27 0.02 7.01]\n",
      "Loss_Q: [0.8  1.11 0.46 0.51 0.06 0.26 0.03 0.   3.23] Loss_P: [2.32 1.74 1.59 0.5  0.48 0.04 0.28 0.02 6.99]\n",
      "Loss_Q: [0.79 1.05 0.48 0.46 0.04 0.27 0.01 0.   3.12] Loss_P: [2.34 1.72 1.62 0.56 0.51 0.03 0.31 0.01 7.1 ]\n",
      "Loss_Q: [0.77 1.14 0.52 0.5  0.05 0.31 0.02 0.   3.3 ] Loss_P: [2.3  1.72 1.61 0.55 0.5  0.04 0.29 0.02 7.03]\n",
      "Loss_Q: [0.85 1.1  0.47 0.5  0.04 0.32 0.01 0.   3.3 ] Loss_P: [2.27 1.76 1.65 0.5  0.49 0.04 0.33 0.01 7.06]\n",
      "Loss_Q: [0.85 1.05 0.48 0.5  0.04 0.32 0.01 0.   3.24] Loss_P: [2.36 1.71 1.61 0.55 0.52 0.04 0.33 0.01 7.14]\n",
      "Loss_Q: [0.8  1.07 0.43 0.5  0.03 0.31 0.01 0.   3.14] Loss_P: [2.35 1.76 1.63 0.54 0.53 0.05 0.33 0.01 7.18]\n",
      "Loss_Q: [0.74 1.06 0.45 0.53 0.04 0.34 0.02 0.   3.18] Loss_P: [2.28 1.67 1.56 0.54 0.52 0.04 0.33 0.02 6.95]\n",
      "Loss_Q: [0.78 1.02 0.5  0.51 0.04 0.32 0.01 0.   3.19] Loss_P: [2.38 1.62 1.54 0.62 0.54 0.03 0.33 0.01 7.07]\n",
      "Loss_Q: [0.81 1.01 0.53 0.51 0.05 0.32 0.02 0.   3.25] Loss_P: [2.32 1.72 1.58 0.55 0.49 0.04 0.29 0.02 7.01]\n",
      "Loss_Q: [0.89 1.13 0.55 0.51 0.04 0.29 0.02 0.   3.44] Loss_P: [2.32 1.77 1.58 0.54 0.51 0.04 0.31 0.02 7.08]\n",
      "Loss_Q: [0.84 1.06 0.46 0.51 0.04 0.31 0.02 0.   3.23] Loss_P: [2.34 1.72 1.57 0.52 0.5  0.04 0.33 0.02 7.05]\n",
      "Loss_Q: [0.81 1.05 0.47 0.53 0.04 0.34 0.02 0.   3.25] Loss_P: [2.26 1.77 1.63 0.58 0.5  0.04 0.33 0.02 7.13]\n",
      "Loss_Q: [0.78 1.12 0.46 0.5  0.03 0.37 0.01 0.   3.27] Loss_P: [2.33 1.79 1.54 0.54 0.48 0.03 0.35 0.02 7.08]\n",
      "Loss_Q: [0.84 1.04 0.49 0.52 0.05 0.36 0.02 0.   3.31] Loss_P: [2.36 1.74 1.54 0.53 0.53 0.04 0.32 0.01 7.09]\n",
      "Loss_Q: [0.74 1.12 0.44 0.56 0.03 0.36 0.01 0.   3.26] Loss_P: [2.33 1.72 1.62 0.54 0.56 0.04 0.34 0.02 7.17]\n",
      "Loss_Q: [0.82 1.09 0.48 0.6  0.03 0.33 0.02 0.   3.37] Loss_P: [2.3  1.7  1.58 0.56 0.58 0.04 0.33 0.01 7.12]\n",
      "Loss_Q: [0.84 1.14 0.48 0.56 0.06 0.34 0.01 0.   3.42] Loss_P: [2.3  1.74 1.58 0.53 0.58 0.04 0.34 0.03 7.15]\n",
      "Loss_Q: [0.82 1.08 0.51 0.55 0.06 0.33 0.02 0.   3.36] Loss_P: [2.3  1.77 1.57 0.58 0.55 0.05 0.33 0.02 7.17]\n",
      "Loss_Q: [0.86 1.12 0.5  0.54 0.04 0.34 0.02 0.   3.43] Loss_P: [2.37 1.78 1.66 0.56 0.55 0.05 0.33 0.01 7.32]\n",
      "Loss_Q: [0.82 1.1  0.53 0.57 0.05 0.32 0.01 0.   3.41] Loss_P: [2.28 1.76 1.58 0.57 0.56 0.04 0.32 0.01 7.13]\n",
      "Loss_Q: [0.85 1.13 0.48 0.55 0.03 0.33 0.02 0.   3.39] Loss_P: [2.3  1.81 1.55 0.51 0.57 0.05 0.33 0.02 7.15]\n",
      "Loss_Q: [0.82 1.16 0.52 0.54 0.05 0.29 0.02 0.   3.4 ] Loss_P: [2.24 1.83 1.59 0.56 0.58 0.06 0.33 0.02 7.22]\n",
      "Loss_Q: [0.84 1.13 0.55 0.53 0.04 0.33 0.01 0.   3.44] Loss_P: [2.28 1.74 1.61 0.63 0.53 0.04 0.35 0.01 7.2 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.81 1.18 0.54 0.5  0.04 0.32 0.02 0.   3.4 ] Loss_P: [2.28 1.81 1.66 0.6  0.53 0.05 0.34 0.02 7.28]\n",
      "Loss_Q: [0.84 1.12 0.49 0.51 0.05 0.35 0.03 0.   3.38] Loss_P: [2.37 1.75 1.68 0.6  0.51 0.04 0.35 0.02 7.3 ]\n",
      "Loss_Q: [0.76 1.09 0.52 0.5  0.05 0.31 0.02 0.   3.25] Loss_P: [2.3  1.73 1.66 0.59 0.51 0.03 0.33 0.02 7.17]\n",
      "Loss_Q: [0.81 1.14 0.51 0.45 0.04 0.32 0.02 0.   3.28] Loss_P: [2.34 1.7  1.64 0.63 0.51 0.06 0.32 0.01 7.2 ]\n",
      "Loss_Q: [0.79 1.13 0.51 0.52 0.04 0.33 0.01 0.   3.33] Loss_P: [2.29 1.75 1.64 0.6  0.46 0.04 0.31 0.01 7.11]\n",
      "Loss_Q: [0.83 1.19 0.57 0.54 0.04 0.33 0.01 0.   3.51] Loss_P: [2.31 1.73 1.69 0.6  0.52 0.04 0.32 0.01 7.21]\n",
      "Loss_Q: [0.87 1.18 0.47 0.49 0.05 0.33 0.01 0.   3.41] Loss_P: [2.38 1.69 1.7  0.62 0.52 0.06 0.31 0.02 7.3 ]\n",
      "Loss_Q: [0.81 1.11 0.51 0.54 0.05 0.3  0.02 0.   3.33] Loss_P: [2.37 1.74 1.74 0.56 0.5  0.06 0.31 0.01 7.29]\n",
      "Loss_Q: [0.78 1.14 0.56 0.56 0.04 0.3  0.02 0.   3.39] Loss_P: [2.33 1.75 1.73 0.55 0.54 0.05 0.31 0.01 7.27]\n",
      "Loss_Q: [0.86 1.1  0.52 0.58 0.03 0.3  0.02 0.   3.42] Loss_P: [2.31 1.64 1.66 0.58 0.58 0.03 0.31 0.02 7.13]\n",
      "Loss_Q: [0.87 1.11 0.53 0.57 0.04 0.31 0.03 0.   3.46] Loss_P: [2.3  1.72 1.64 0.59 0.54 0.04 0.3  0.01 7.15]\n",
      "Loss_Q: [0.84 1.16 0.5  0.55 0.04 0.3  0.02 0.   3.42] Loss_P: [2.28 1.77 1.69 0.59 0.56 0.04 0.34 0.02 7.29]\n",
      "Loss_Q: [0.87 1.11 0.55 0.56 0.05 0.31 0.02 0.   3.47] Loss_P: [2.35 1.76 1.69 0.57 0.56 0.04 0.31 0.01 7.29]\n",
      "Loss_Q: [0.87 1.15 0.49 0.55 0.07 0.32 0.03 0.   3.49] Loss_P: [2.33 1.76 1.69 0.53 0.55 0.05 0.3  0.02 7.21]\n",
      "Loss_Q: [0.82 1.13 0.49 0.53 0.05 0.31 0.02 0.   3.34] Loss_P: [2.31 1.7  1.72 0.57 0.54 0.05 0.28 0.02 7.19]\n",
      "Loss_Q: [0.89 1.18 0.51 0.54 0.06 0.3  0.02 0.   3.5 ] Loss_P: [2.29 1.75 1.73 0.55 0.55 0.04 0.3  0.01 7.2 ]\n",
      "Loss_Q: [0.87 1.12 0.5  0.54 0.05 0.32 0.02 0.   3.43] Loss_P: [2.35 1.7  1.72 0.58 0.53 0.04 0.28 0.02 7.22]\n",
      "Loss_Q: [0.83 1.13 0.56 0.55 0.04 0.31 0.01 0.   3.43] Loss_P: [2.28 1.69 1.69 0.6  0.53 0.09 0.3  0.02 7.2 ]\n",
      "Loss_Q: [0.87 1.14 0.54 0.55 0.06 0.32 0.01 0.   3.49] Loss_P: [2.29 1.75 1.64 0.58 0.54 0.05 0.29 0.02 7.17]\n",
      "Loss_Q: [0.85 1.07 0.5  0.54 0.06 0.28 0.02 0.   3.31] Loss_P: [2.27 1.78 1.7  0.59 0.53 0.05 0.26 0.01 7.2 ]\n",
      "Loss_Q: [0.82 1.12 0.46 0.5  0.04 0.31 0.01 0.   3.26] Loss_P: [2.29 1.69 1.69 0.55 0.52 0.05 0.27 0.01 7.07]\n",
      "Loss_Q: [0.87 1.05 0.51 0.51 0.05 0.27 0.01 0.   3.27] Loss_P: [2.34 1.74 1.66 0.6  0.51 0.05 0.32 0.02 7.25]\n",
      "Loss_Q: [0.85 1.15 0.5  0.44 0.05 0.28 0.02 0.   3.29] Loss_P: [2.34 1.67 1.64 0.58 0.46 0.04 0.26 0.01 7.01]\n",
      "Loss_Q: [0.84 1.09 0.47 0.49 0.05 0.28 0.02 0.   3.24] Loss_P: [2.29 1.66 1.65 0.58 0.46 0.04 0.25 0.02 6.96]\n",
      "Loss_Q: [0.82 1.15 0.43 0.42 0.04 0.24 0.01 0.   3.11] Loss_P: [2.29 1.72 1.69 0.54 0.43 0.04 0.26 0.01 6.97]\n",
      "Loss_Q: [0.84 1.09 0.48 0.48 0.05 0.26 0.01 0.   3.21] Loss_P: [2.36 1.73 1.68 0.55 0.44 0.06 0.26 0.02 7.11]\n",
      "Loss_Q: [0.75 1.15 0.51 0.46 0.03 0.23 0.02 0.   3.15] Loss_P: [2.4  1.6  1.68 0.56 0.49 0.06 0.24 0.02 7.04]\n",
      "Loss_Q: [0.78 1.19 0.47 0.46 0.05 0.26 0.04 0.   3.24] Loss_P: [2.37 1.64 1.77 0.54 0.48 0.04 0.24 0.02 7.1 ]\n",
      "Loss_Q: [0.88 1.22 0.53 0.5  0.05 0.23 0.03 0.   3.45] Loss_P: [2.35 1.67 1.73 0.52 0.48 0.05 0.21 0.01 7.02]\n",
      "Loss_Q: [0.86 1.18 0.52 0.46 0.05 0.25 0.02 0.   3.34] Loss_P: [2.36 1.75 1.72 0.58 0.5  0.05 0.22 0.01 7.18]\n",
      "Loss_Q: [0.86 1.16 0.51 0.51 0.05 0.23 0.02 0.   3.33] Loss_P: [2.36 1.67 1.7  0.59 0.47 0.04 0.25 0.01 7.1 ]\n",
      "Loss_Q: [0.89 1.13 0.56 0.48 0.05 0.23 0.01 0.   3.36] Loss_P: [2.33 1.76 1.73 0.55 0.5  0.04 0.23 0.03 7.17]\n",
      "Loss_Q: [0.86 1.14 0.51 0.48 0.06 0.25 0.01 0.   3.31] Loss_P: [2.36 1.7  1.72 0.57 0.51 0.06 0.26 0.01 7.19]\n",
      "Loss_Q: [0.82 1.15 0.5  0.52 0.05 0.24 0.03 0.   3.31] Loss_P: [2.36 1.68 1.76 0.59 0.55 0.05 0.26 0.02 7.26]\n",
      "Loss_Q: [0.79 1.18 0.54 0.53 0.04 0.24 0.02 0.   3.34] Loss_P: [2.37 1.64 1.74 0.56 0.55 0.03 0.21 0.02 7.13]\n",
      "Loss_Q: [0.82 1.1  0.48 0.53 0.05 0.22 0.01 0.   3.21] Loss_P: [2.37 1.64 1.71 0.56 0.5  0.06 0.26 0.01 7.1 ]\n",
      "Loss_Q: [0.81 1.12 0.5  0.54 0.07 0.21 0.01 0.   3.25] Loss_P: [2.37 1.58 1.76 0.5  0.56 0.04 0.25 0.02 7.08]\n",
      "Loss_Q: [0.77 1.14 0.49 0.5  0.04 0.23 0.01 0.   3.18] Loss_P: [2.35 1.68 1.74 0.58 0.52 0.04 0.21 0.02 7.13]\n",
      "Loss_Q: [0.81 1.13 0.53 0.58 0.07 0.22 0.01 0.   3.34] Loss_P: [2.36 1.6  1.74 0.61 0.55 0.06 0.21 0.02 7.16]\n",
      "Loss_Q: [0.74 1.17 0.53 0.55 0.06 0.22 0.02 0.   3.29] Loss_P: [2.42 1.64 1.67 0.6  0.62 0.07 0.23 0.01 7.26]\n",
      "Loss_Q: [0.79 1.19 0.48 0.57 0.04 0.22 0.02 0.   3.32] Loss_P: [2.41 1.6  1.73 0.54 0.56 0.05 0.2  0.02 7.1 ]\n",
      "Loss_Q: [0.82 1.17 0.5  0.53 0.04 0.22 0.02 0.   3.3 ] Loss_P: [2.39 1.6  1.69 0.52 0.5  0.03 0.21 0.02 6.96]\n",
      "Loss_Q: [0.74 1.12 0.47 0.54 0.08 0.24 0.01 0.   3.2 ] Loss_P: [2.37 1.61 1.67 0.52 0.53 0.05 0.24 0.02 7.01]\n",
      "Loss_Q: [0.83 1.11 0.48 0.53 0.04 0.26 0.02 0.   3.26] Loss_P: [2.35 1.68 1.7  0.54 0.6  0.05 0.2  0.02 7.14]\n",
      "Loss_Q: [0.7  1.11 0.47 0.53 0.06 0.22 0.01 0.   3.11] Loss_P: [2.37 1.61 1.73 0.58 0.58 0.06 0.22 0.01 7.15]\n",
      "Loss_Q: [0.74 1.05 0.44 0.58 0.04 0.21 0.01 0.   3.09] Loss_P: [2.36 1.68 1.73 0.45 0.56 0.05 0.23 0.01 7.09]\n",
      "Loss_Q: [0.8  1.12 0.39 0.59 0.05 0.21 0.02 0.   3.18] Loss_P: [2.37 1.62 1.74 0.47 0.57 0.04 0.21 0.01 7.04]\n",
      "Loss_Q: [0.75 1.09 0.41 0.63 0.05 0.21 0.02 0.   3.15] Loss_P: [2.34 1.68 1.71 0.49 0.59 0.04 0.2  0.01 7.06]\n",
      "Loss_Q: [0.78 1.12 0.45 0.63 0.03 0.22 0.02 0.   3.24] Loss_P: [2.36 1.58 1.67 0.47 0.58 0.06 0.21 0.02 6.95]\n",
      "Loss_Q: [0.74 1.05 0.42 0.57 0.03 0.23 0.02 0.   3.06] Loss_P: [2.38 1.61 1.64 0.47 0.6  0.05 0.24 0.01 6.99]\n",
      "Loss_Q: [0.64 1.08 0.42 0.57 0.04 0.23 0.01 0.   2.99] Loss_P: [2.34 1.59 1.66 0.47 0.58 0.04 0.22 0.01 6.91]\n",
      "Loss_Q: [0.74 1.07 0.41 0.57 0.03 0.22 0.01 0.   3.07] Loss_P: [2.43 1.6  1.7  0.46 0.59 0.05 0.2  0.01 7.04]\n",
      "Loss_Q: [0.8  1.15 0.44 0.6  0.07 0.2  0.02 0.   3.26] Loss_P: [2.37 1.62 1.69 0.47 0.62 0.06 0.21 0.02 7.05]\n",
      "Loss_Q: [0.84 1.05 0.43 0.55 0.04 0.22 0.01 0.   3.14] Loss_P: [2.31 1.64 1.71 0.49 0.58 0.04 0.22 0.02 7.  ]\n",
      "Loss_Q: [0.78 1.1  0.39 0.59 0.03 0.19 0.01 0.   3.1 ] Loss_P: [2.35 1.59 1.64 0.46 0.6  0.04 0.18 0.02 6.88]\n",
      "Loss_Q: [0.81 1.09 0.49 0.61 0.05 0.19 0.01 0.   3.25] Loss_P: [2.34 1.72 1.68 0.44 0.59 0.05 0.21 0.02 7.06]\n",
      "Loss_Q: [0.81 1.08 0.39 0.56 0.06 0.21 0.02 0.   3.11] Loss_P: [2.37 1.62 1.69 0.45 0.57 0.05 0.19 0.02 6.97]\n",
      "Loss_Q: [0.78 1.11 0.41 0.56 0.05 0.18 0.01 0.   3.11] Loss_P: [2.33 1.65 1.74 0.36 0.55 0.05 0.16 0.02 6.86]\n",
      "Loss_Q: [0.81 1.1  0.41 0.59 0.05 0.23 0.02 0.   3.2 ] Loss_P: [2.38 1.64 1.75 0.42 0.6  0.04 0.17 0.02 7.02]\n",
      "Loss_Q: [0.8  1.13 0.4  0.61 0.04 0.2  0.02 0.   3.2 ] Loss_P: [2.35 1.66 1.7  0.46 0.58 0.06 0.21 0.01 7.02]\n",
      "Loss_Q: [0.87 1.11 0.43 0.58 0.04 0.22 0.02 0.   3.28] Loss_P: [2.32 1.63 1.79 0.42 0.57 0.06 0.21 0.02 7.02]\n",
      "Loss_Q: [0.82 1.08 0.38 0.59 0.05 0.18 0.02 0.   3.13] Loss_P: [2.35 1.64 1.77 0.49 0.59 0.03 0.21 0.02 7.11]\n",
      "Loss_Q: [0.71 1.09 0.43 0.59 0.04 0.24 0.02 0.   3.11] Loss_P: [2.31 1.66 1.75 0.47 0.59 0.05 0.19 0.03 7.05]\n",
      "Loss_Q: [0.77 1.12 0.44 0.58 0.06 0.21 0.02 0.   3.21] Loss_P: [2.32 1.61 1.74 0.49 0.61 0.07 0.21 0.02 7.08]\n",
      "Loss_Q: [0.79 1.14 0.43 0.52 0.05 0.21 0.01 0.   3.15] Loss_P: [2.37 1.65 1.73 0.48 0.59 0.05 0.23 0.02 7.12]\n",
      "Loss_Q: [0.71 1.1  0.38 0.6  0.05 0.18 0.02 0.   3.04] Loss_P: [2.36 1.56 1.75 0.49 0.6  0.04 0.19 0.01 7.01]\n",
      "Loss_Q: [0.78 1.16 0.43 0.54 0.04 0.19 0.01 0.   3.14] Loss_P: [2.38 1.6  1.73 0.47 0.58 0.05 0.2  0.02 7.04]\n",
      "Loss_Q: [0.77 1.08 0.45 0.62 0.05 0.18 0.01 0.   3.16] Loss_P: [2.37 1.62 1.7  0.45 0.57 0.04 0.21 0.01 6.97]\n",
      "Loss_Q: [0.73 1.07 0.44 0.56 0.05 0.2  0.02 0.   3.08] Loss_P: [2.37 1.62 1.7  0.48 0.59 0.06 0.2  0.02 7.04]\n",
      "Loss_Q: [0.77 1.05 0.43 0.58 0.05 0.2  0.02 0.   3.09] Loss_P: [2.41 1.57 1.68 0.47 0.58 0.05 0.21 0.02 6.99]\n",
      "Loss_Q: [0.76 1.14 0.5  0.58 0.04 0.23 0.02 0.   3.27] Loss_P: [2.35 1.6  1.64 0.51 0.58 0.03 0.21 0.02 6.93]\n",
      "Loss_Q: [0.84 1.03 0.41 0.59 0.04 0.23 0.01 0.   3.15] Loss_P: [2.31 1.67 1.73 0.46 0.54 0.04 0.2  0.02 6.97]\n",
      "Loss_Q: [0.77 1.09 0.44 0.61 0.06 0.21 0.01 0.   3.19] Loss_P: [2.29 1.57 1.66 0.51 0.64 0.04 0.22 0.01 6.94]\n",
      "Loss_Q: [0.77 1.04 0.4  0.58 0.05 0.19 0.01 0.   3.04] Loss_P: [2.31 1.65 1.7  0.49 0.62 0.03 0.21 0.01 7.03]\n",
      "Loss_Q: [0.75 1.08 0.47 0.6  0.05 0.23 0.02 0.   3.2 ] Loss_P: [2.34 1.57 1.67 0.49 0.61 0.06 0.21 0.01 6.96]\n",
      "Loss_Q: [0.75 1.07 0.44 0.59 0.04 0.21 0.02 0.   3.11] Loss_P: [2.38 1.62 1.62 0.53 0.62 0.06 0.2  0.01 7.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.78 1.03 0.42 0.61 0.04 0.21 0.02 0.   3.12] Loss_P: [2.35 1.6  1.67 0.52 0.59 0.04 0.18 0.02 6.97]\n",
      "Loss_Q: [0.82 0.98 0.47 0.6  0.04 0.21 0.01 0.   3.12] Loss_P: [2.29 1.61 1.67 0.51 0.62 0.08 0.23 0.02 7.02]\n",
      "Loss_Q: [0.84 1.05 0.46 0.6  0.06 0.23 0.01 0.   3.24] Loss_P: [2.4  1.59 1.61 0.46 0.6  0.05 0.19 0.02 6.92]\n",
      "Loss_Q: [0.82 1.01 0.42 0.6  0.07 0.19 0.02 0.   3.14] Loss_P: [2.41 1.62 1.68 0.46 0.6  0.04 0.18 0.02 7.01]\n",
      "Loss_Q: [0.79 1.04 0.45 0.57 0.05 0.21 0.02 0.   3.14] Loss_P: [2.34 1.63 1.61 0.46 0.57 0.05 0.19 0.01 6.87]\n",
      "Loss_Q: [0.8  1.   0.45 0.56 0.04 0.23 0.02 0.   3.1 ] Loss_P: [2.35 1.62 1.57 0.51 0.58 0.06 0.25 0.02 6.95]\n",
      "Loss_Q: [0.83 1.07 0.5  0.61 0.05 0.24 0.02 0.   3.31] Loss_P: [2.33 1.69 1.67 0.56 0.58 0.05 0.23 0.02 7.11]\n",
      "Loss_Q: [0.86 1.02 0.43 0.61 0.04 0.24 0.03 0.   3.22] Loss_P: [2.32 1.63 1.67 0.53 0.59 0.05 0.24 0.02 7.05]\n",
      "Loss_Q: [0.86 0.98 0.42 0.61 0.03 0.24 0.02 0.   3.15] Loss_P: [2.27 1.72 1.71 0.54 0.6  0.05 0.23 0.02 7.15]\n",
      "Loss_Q: [0.92 1.09 0.41 0.57 0.06 0.22 0.01 0.   3.29] Loss_P: [2.34 1.69 1.7  0.51 0.58 0.05 0.24 0.02 7.14]\n",
      "Loss_Q: [0.79 1.07 0.42 0.64 0.04 0.24 0.02 0.   3.23] Loss_P: [2.26 1.66 1.73 0.49 0.62 0.06 0.24 0.01 7.07]\n",
      "Loss_Q: [0.88 1.08 0.43 0.6  0.04 0.23 0.02 0.   3.27] Loss_P: [2.27 1.68 1.77 0.53 0.63 0.06 0.23 0.02 7.18]\n",
      "Loss_Q: [0.86 1.14 0.43 0.63 0.06 0.2  0.02 0.   3.34] Loss_P: [2.34 1.66 1.67 0.49 0.59 0.05 0.24 0.02 7.06]\n",
      "Loss_Q: [0.81 1.09 0.47 0.62 0.06 0.25 0.02 0.   3.32] Loss_P: [2.36 1.65 1.65 0.49 0.6  0.04 0.2  0.02 7.01]\n",
      "Loss_Q: [0.85 1.11 0.44 0.63 0.03 0.23 0.02 0.   3.31] Loss_P: [2.31 1.76 1.68 0.49 0.59 0.04 0.23 0.02 7.11]\n",
      "Loss_Q: [0.81 1.1  0.46 0.64 0.04 0.24 0.01 0.   3.3 ] Loss_P: [2.32 1.71 1.74 0.5  0.62 0.05 0.24 0.02 7.18]\n",
      "Loss_Q: [0.78 1.07 0.45 0.59 0.04 0.24 0.02 0.   3.2 ] Loss_P: [2.31 1.66 1.69 0.52 0.56 0.05 0.24 0.02 7.05]\n",
      "Loss_Q: [0.9  1.15 0.46 0.6  0.05 0.27 0.02 0.   3.45] Loss_P: [2.32 1.64 1.76 0.53 0.6  0.04 0.25 0.02 7.16]\n",
      "Loss_Q: [0.81 1.13 0.48 0.61 0.05 0.24 0.02 0.   3.33] Loss_P: [2.33 1.67 1.75 0.52 0.62 0.03 0.25 0.02 7.2 ]\n",
      "Loss_Q: [0.86 1.15 0.52 0.62 0.04 0.23 0.02 0.   3.44] Loss_P: [2.35 1.64 1.76 0.5  0.64 0.05 0.22 0.02 7.18]\n",
      "Loss_Q: [0.85 1.07 0.45 0.58 0.05 0.25 0.02 0.   3.27] Loss_P: [2.3  1.64 1.67 0.59 0.61 0.06 0.23 0.01 7.11]\n",
      "Loss_Q: [0.82 1.11 0.48 0.61 0.05 0.24 0.01 0.   3.31] Loss_P: [2.37 1.59 1.69 0.55 0.63 0.04 0.23 0.01 7.12]\n",
      "Loss_Q: [0.78 1.09 0.42 0.63 0.04 0.25 0.01 0.   3.22] Loss_P: [2.28 1.67 1.8  0.54 0.64 0.03 0.25 0.01 7.22]\n",
      "Loss_Q: [0.85 1.04 0.44 0.61 0.04 0.22 0.01 0.   3.22] Loss_P: [2.31 1.61 1.71 0.5  0.63 0.05 0.24 0.02 7.07]\n",
      "Loss_Q: [0.81 1.1  0.45 0.64 0.05 0.25 0.02 0.   3.33] Loss_P: [2.38 1.65 1.71 0.5  0.58 0.05 0.27 0.02 7.16]\n",
      "Loss_Q: [0.8  1.17 0.48 0.64 0.05 0.21 0.01 0.   3.37] Loss_P: [2.31 1.64 1.74 0.48 0.65 0.03 0.22 0.01 7.08]\n",
      "Loss_Q: [0.92 1.1  0.5  0.62 0.05 0.24 0.01 0.   3.44] Loss_P: [2.32 1.6  1.71 0.51 0.68 0.05 0.24 0.02 7.13]\n",
      "Loss_Q: [0.83 1.06 0.47 0.64 0.06 0.23 0.01 0.   3.29] Loss_P: [2.33 1.7  1.73 0.55 0.67 0.05 0.22 0.02 7.26]\n",
      "Loss_Q: [0.89 1.09 0.44 0.65 0.05 0.26 0.01 0.   3.39] Loss_P: [2.29 1.64 1.72 0.51 0.68 0.07 0.25 0.02 7.17]\n",
      "Loss_Q: [0.85 1.1  0.46 0.64 0.04 0.25 0.01 0.   3.34] Loss_P: [2.32 1.63 1.63 0.54 0.71 0.04 0.29 0.02 7.17]\n",
      "Loss_Q: [0.87 1.1  0.44 0.68 0.05 0.26 0.01 0.   3.41] Loss_P: [2.36 1.6  1.72 0.56 0.63 0.04 0.27 0.03 7.21]\n",
      "Loss_Q: [0.89 1.06 0.43 0.68 0.05 0.25 0.02 0.   3.38] Loss_P: [2.33 1.68 1.71 0.51 0.71 0.06 0.24 0.02 7.24]\n",
      "Loss_Q: [0.83 1.09 0.4  0.63 0.07 0.26 0.02 0.   3.28] Loss_P: [2.35 1.59 1.74 0.52 0.66 0.07 0.26 0.02 7.21]\n",
      "Loss_Q: [0.87 1.05 0.47 0.64 0.04 0.28 0.01 0.   3.36] Loss_P: [2.28 1.75 1.78 0.51 0.63 0.05 0.26 0.02 7.27]\n",
      "Loss_Q: [0.89 1.04 0.42 0.64 0.07 0.22 0.01 0.   3.3 ] Loss_P: [2.35 1.66 1.75 0.55 0.69 0.05 0.23 0.01 7.29]\n",
      "Loss_Q: [0.87 1.07 0.43 0.69 0.05 0.25 0.02 0.   3.39] Loss_P: [2.32 1.61 1.66 0.48 0.64 0.05 0.27 0.01 7.04]\n",
      "Loss_Q: [0.8  1.06 0.45 0.66 0.06 0.26 0.01 0.   3.3 ] Loss_P: [2.34 1.64 1.71 0.54 0.68 0.04 0.25 0.02 7.22]\n",
      "Loss_Q: [0.84 1.01 0.48 0.64 0.03 0.24 0.02 0.   3.26] Loss_P: [2.33 1.61 1.68 0.55 0.67 0.06 0.25 0.01 7.15]\n",
      "Loss_Q: [0.85 1.03 0.47 0.67 0.06 0.27 0.02 0.   3.37] Loss_P: [2.29 1.67 1.7  0.58 0.66 0.09 0.27 0.02 7.27]\n",
      "Loss_Q: [0.84 1.06 0.51 0.67 0.04 0.27 0.01 0.   3.4 ] Loss_P: [2.32 1.62 1.74 0.61 0.67 0.06 0.3  0.02 7.34]\n",
      "Loss_Q: [0.86 1.07 0.48 0.65 0.04 0.29 0.02 0.   3.41] Loss_P: [2.33 1.66 1.76 0.56 0.7  0.05 0.28 0.02 7.38]\n",
      "Loss_Q: [0.91 1.07 0.43 0.63 0.06 0.25 0.02 0.   3.37] Loss_P: [2.31 1.68 1.74 0.47 0.64 0.06 0.28 0.02 7.19]\n",
      "Loss_Q: [0.83 1.08 0.43 0.62 0.06 0.29 0.01 0.   3.32] Loss_P: [2.36 1.71 1.77 0.53 0.67 0.07 0.28 0.01 7.4 ]\n",
      "Loss_Q: [0.85 1.09 0.41 0.63 0.04 0.29 0.01 0.   3.32] Loss_P: [2.42 1.55 1.71 0.49 0.65 0.05 0.29 0.02 7.16]\n",
      "Loss_Q: [0.89 1.02 0.44 0.64 0.04 0.26 0.01 0.   3.29] Loss_P: [2.34 1.7  1.69 0.52 0.65 0.04 0.27 0.01 7.23]\n",
      "Loss_Q: [0.89 1.07 0.44 0.66 0.04 0.29 0.03 0.   3.42] Loss_P: [2.31 1.65 1.73 0.51 0.64 0.04 0.28 0.01 7.18]\n",
      "Loss_Q: [0.89 1.09 0.48 0.65 0.06 0.28 0.02 0.   3.46] Loss_P: [2.37 1.61 1.72 0.52 0.66 0.04 0.28 0.02 7.23]\n",
      "Loss_Q: [0.86 1.08 0.4  0.62 0.06 0.28 0.02 0.   3.33] Loss_P: [2.27 1.64 1.76 0.54 0.62 0.05 0.27 0.02 7.17]\n",
      "Loss_Q: [0.89 1.07 0.45 0.63 0.03 0.29 0.02 0.   3.37] Loss_P: [2.3  1.65 1.74 0.53 0.65 0.06 0.25 0.01 7.18]\n",
      "Loss_Q: [0.88 1.17 0.44 0.66 0.04 0.26 0.01 0.   3.46] Loss_P: [2.36 1.63 1.77 0.5  0.65 0.04 0.27 0.02 7.23]\n",
      "Loss_Q: [0.9  1.11 0.47 0.68 0.04 0.28 0.01 0.   3.49] Loss_P: [2.28 1.64 1.76 0.53 0.66 0.06 0.27 0.02 7.22]\n",
      "Loss_Q: [0.87 1.09 0.46 0.65 0.05 0.28 0.01 0.   3.41] Loss_P: [2.39 1.65 1.74 0.52 0.66 0.05 0.29 0.02 7.32]\n",
      "Loss_Q: [0.88 1.05 0.46 0.62 0.05 0.29 0.02 0.   3.37] Loss_P: [2.35 1.61 1.69 0.61 0.65 0.04 0.27 0.01 7.23]\n",
      "Loss_Q: [0.88 1.11 0.44 0.61 0.05 0.29 0.02 0.   3.39] Loss_P: [2.32 1.63 1.84 0.57 0.69 0.06 0.27 0.02 7.39]\n",
      "Loss_Q: [0.91 1.09 0.47 0.64 0.04 0.26 0.01 0.   3.42] Loss_P: [2.32 1.64 1.77 0.57 0.65 0.05 0.26 0.01 7.27]\n",
      "Loss_Q: [0.94 1.1  0.46 0.61 0.06 0.25 0.01 0.   3.43] Loss_P: [2.36 1.65 1.76 0.5  0.62 0.04 0.25 0.02 7.2 ]\n",
      "Loss_Q: [0.89 1.1  0.43 0.6  0.04 0.28 0.02 0.   3.35] Loss_P: [2.37 1.63 1.72 0.52 0.62 0.04 0.28 0.03 7.2 ]\n",
      "Loss_Q: [0.83 1.11 0.43 0.62 0.05 0.27 0.02 0.   3.33] Loss_P: [2.29 1.65 1.78 0.52 0.64 0.05 0.27 0.02 7.22]\n",
      "Loss_Q: [0.85 1.09 0.48 0.63 0.05 0.29 0.02 0.   3.4 ] Loss_P: [2.34 1.63 1.74 0.56 0.62 0.05 0.29 0.02 7.24]\n",
      "Loss_Q: [0.92 1.07 0.42 0.63 0.06 0.23 0.03 0.   3.36] Loss_P: [2.35 1.64 1.7  0.56 0.63 0.04 0.27 0.01 7.21]\n",
      "Loss_Q: [0.97 1.14 0.46 0.63 0.06 0.26 0.01 0.   3.53] Loss_P: [2.38 1.61 1.71 0.51 0.62 0.05 0.24 0.01 7.14]\n",
      "Loss_Q: [0.91 1.12 0.45 0.63 0.04 0.27 0.02 0.   3.44] Loss_P: [2.34 1.59 1.72 0.57 0.65 0.04 0.26 0.01 7.18]\n",
      "Loss_Q: [0.9  1.09 0.47 0.66 0.04 0.27 0.02 0.   3.45] Loss_P: [2.38 1.71 1.79 0.51 0.64 0.04 0.3  0.01 7.38]\n",
      "Loss_Q: [0.87 1.13 0.46 0.6  0.03 0.3  0.02 0.   3.4 ] Loss_P: [2.39 1.68 1.72 0.53 0.62 0.03 0.3  0.02 7.3 ]\n",
      "Loss_Q: [0.88 1.16 0.52 0.63 0.04 0.3  0.01 0.   3.55] Loss_P: [2.37 1.7  1.79 0.57 0.67 0.04 0.27 0.01 7.43]\n",
      "Loss_Q: [0.84 1.06 0.48 0.63 0.04 0.28 0.02 0.   3.34] Loss_P: [2.36 1.67 1.69 0.55 0.65 0.05 0.28 0.02 7.26]\n",
      "Loss_Q: [0.9  1.09 0.4  0.66 0.07 0.27 0.03 0.   3.42] Loss_P: [2.38 1.62 1.69 0.54 0.69 0.04 0.26 0.01 7.23]\n",
      "Loss_Q: [0.89 1.07 0.45 0.62 0.04 0.29 0.02 0.   3.37] Loss_P: [2.34 1.61 1.65 0.48 0.64 0.04 0.28 0.01 7.07]\n",
      "Loss_Q: [0.95 1.11 0.46 0.67 0.05 0.27 0.01 0.   3.52] Loss_P: [2.41 1.64 1.66 0.52 0.65 0.04 0.31 0.02 7.25]\n",
      "Loss_Q: [0.81 1.07 0.47 0.67 0.05 0.27 0.02 0.   3.38] Loss_P: [2.39 1.63 1.64 0.54 0.68 0.06 0.27 0.02 7.24]\n",
      "Loss_Q: [0.88 1.05 0.46 0.65 0.03 0.27 0.01 0.   3.35] Loss_P: [2.36 1.63 1.74 0.53 0.67 0.05 0.3  0.01 7.3 ]\n",
      "Loss_Q: [0.9  1.09 0.45 0.69 0.05 0.3  0.01 0.   3.48] Loss_P: [2.4  1.64 1.74 0.49 0.69 0.05 0.32 0.02 7.35]\n",
      "Loss_Q: [0.93 1.11 0.46 0.63 0.05 0.29 0.01 0.   3.46] Loss_P: [2.38 1.64 1.72 0.56 0.66 0.05 0.29 0.02 7.31]\n",
      "Loss_Q: [0.88 1.11 0.43 0.66 0.05 0.33 0.01 0.   3.46] Loss_P: [2.38 1.68 1.72 0.44 0.66 0.04 0.29 0.02 7.22]\n",
      "Loss_Q: [0.96 1.15 0.43 0.66 0.05 0.31 0.02 0.   3.56] Loss_P: [2.4  1.63 1.69 0.51 0.67 0.05 0.3  0.01 7.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.87 1.08 0.46 0.7  0.06 0.31 0.01 0.   3.49] Loss_P: [2.36 1.69 1.72 0.48 0.68 0.04 0.29 0.02 7.28]\n",
      "Loss_Q: [0.9  1.16 0.49 0.68 0.05 0.3  0.02 0.   3.6 ] Loss_P: [2.42 1.64 1.75 0.5  0.66 0.05 0.28 0.01 7.3 ]\n",
      "Loss_Q: [0.86 1.14 0.39 0.66 0.04 0.29 0.01 0.   3.39] Loss_P: [2.37 1.61 1.69 0.48 0.7  0.06 0.25 0.02 7.19]\n",
      "Loss_Q: [0.83 1.07 0.41 0.67 0.05 0.27 0.02 0.   3.32] Loss_P: [2.44 1.59 1.7  0.47 0.72 0.04 0.3  0.02 7.29]\n",
      "Loss_Q: [0.87 1.15 0.47 0.68 0.05 0.25 0.02 0.   3.49] Loss_P: [2.43 1.65 1.71 0.51 0.68 0.06 0.26 0.02 7.31]\n",
      "Loss_Q: [0.89 1.11 0.44 0.68 0.05 0.26 0.02 0.   3.45] Loss_P: [2.37 1.61 1.8  0.53 0.69 0.04 0.28 0.02 7.35]\n",
      "Loss_Q: [0.88 1.11 0.42 0.68 0.06 0.29 0.01 0.   3.46] Loss_P: [2.38 1.66 1.71 0.48 0.67 0.06 0.28 0.01 7.24]\n",
      "Loss_Q: [0.87 1.12 0.5  0.72 0.05 0.27 0.03 0.   3.55] Loss_P: [2.35 1.64 1.78 0.57 0.7  0.06 0.26 0.02 7.37]\n",
      "Loss_Q: [0.88 1.16 0.5  0.69 0.05 0.25 0.02 0.   3.56] Loss_P: [2.35 1.63 1.75 0.57 0.72 0.03 0.26 0.01 7.33]\n",
      "Loss_Q: [0.89 1.12 0.48 0.69 0.03 0.26 0.02 0.   3.5 ] Loss_P: [2.35 1.71 1.73 0.53 0.72 0.04 0.29 0.01 7.38]\n",
      "Loss_Q: [0.82 1.06 0.38 0.66 0.05 0.24 0.02 0.   3.23] Loss_P: [2.4  1.64 1.68 0.56 0.71 0.05 0.27 0.02 7.32]\n",
      "Loss_Q: [0.84 1.03 0.45 0.67 0.06 0.26 0.02 0.   3.32] Loss_P: [2.36 1.65 1.68 0.51 0.71 0.03 0.25 0.01 7.2 ]\n",
      "Loss_Q: [0.85 1.13 0.42 0.66 0.04 0.25 0.01 0.   3.35] Loss_P: [2.41 1.62 1.72 0.51 0.67 0.05 0.28 0.02 7.28]\n",
      "Loss_Q: [0.86 1.15 0.49 0.69 0.04 0.22 0.02 0.   3.47] Loss_P: [2.37 1.7  1.76 0.52 0.67 0.03 0.27 0.02 7.32]\n",
      "Loss_Q: [0.86 1.2  0.46 0.68 0.06 0.24 0.02 0.   3.52] Loss_P: [2.35 1.64 1.71 0.52 0.68 0.05 0.25 0.02 7.22]\n",
      "Loss_Q: [0.86 1.14 0.42 0.68 0.04 0.28 0.02 0.   3.42] Loss_P: [2.37 1.64 1.76 0.52 0.71 0.04 0.26 0.02 7.32]\n",
      "Loss_Q: [0.88 1.1  0.44 0.65 0.04 0.24 0.02 0.   3.37] Loss_P: [2.38 1.63 1.77 0.54 0.65 0.04 0.25 0.01 7.27]\n",
      "Loss_Q: [0.87 1.15 0.45 0.63 0.04 0.24 0.01 0.   3.39] Loss_P: [2.36 1.67 1.76 0.48 0.64 0.03 0.24 0.02 7.2 ]\n",
      "Loss_Q: [0.85 1.21 0.4  0.62 0.05 0.24 0.02 0.   3.4 ] Loss_P: [2.44 1.67 1.77 0.5  0.64 0.04 0.24 0.02 7.32]\n",
      "Loss_Q: [0.86 1.12 0.41 0.67 0.05 0.21 0.02 0.   3.36] Loss_P: [2.38 1.69 1.73 0.53 0.7  0.05 0.22 0.02 7.31]\n",
      "Loss_Q: [0.86 1.11 0.43 0.64 0.05 0.21 0.02 0.   3.32] Loss_P: [2.35 1.68 1.74 0.5  0.68 0.05 0.22 0.02 7.23]\n",
      "Loss_Q: [0.86 1.18 0.4  0.65 0.04 0.2  0.01 0.   3.34] Loss_P: [2.4  1.67 1.76 0.51 0.66 0.04 0.2  0.01 7.25]\n",
      "Loss_Q: [0.9  1.17 0.4  0.61 0.03 0.22 0.02 0.   3.35] Loss_P: [2.42 1.69 1.77 0.47 0.66 0.03 0.21 0.02 7.26]\n",
      "Loss_Q: [0.84 1.14 0.42 0.62 0.03 0.23 0.02 0.   3.29] Loss_P: [2.4  1.71 1.76 0.52 0.63 0.04 0.2  0.01 7.28]\n",
      "Loss_Q: [0.93 1.16 0.41 0.63 0.04 0.23 0.01 0.   3.42] Loss_P: [2.37 1.7  1.74 0.45 0.66 0.04 0.25 0.01 7.22]\n",
      "Loss_Q: [0.83 1.17 0.37 0.67 0.04 0.22 0.02 0.   3.32] Loss_P: [2.38 1.71 1.74 0.49 0.67 0.03 0.22 0.02 7.26]\n",
      "Loss_Q: [0.86 1.21 0.4  0.63 0.05 0.22 0.02 0.   3.39] Loss_P: [2.46 1.64 1.76 0.47 0.65 0.05 0.24 0.02 7.28]\n",
      "Loss_Q: [0.87 1.23 0.39 0.64 0.04 0.23 0.01 0.   3.42] Loss_P: [2.39 1.69 1.82 0.43 0.63 0.05 0.25 0.03 7.3 ]\n",
      "Loss_Q: [0.92 1.21 0.44 0.63 0.03 0.21 0.02 0.   3.47] Loss_P: [2.36 1.71 1.88 0.47 0.67 0.05 0.24 0.01 7.4 ]\n",
      "Loss_Q: [0.9  1.15 0.4  0.66 0.05 0.21 0.01 0.   3.39] Loss_P: [2.42 1.68 1.84 0.44 0.63 0.06 0.21 0.01 7.3 ]\n",
      "Loss_Q: [0.93 1.2  0.41 0.67 0.05 0.24 0.02 0.   3.52] Loss_P: [2.33 1.66 1.81 0.45 0.68 0.06 0.23 0.01 7.23]\n",
      "Loss_Q: [0.86 1.13 0.39 0.66 0.03 0.21 0.01 0.   3.29] Loss_P: [2.35 1.72 1.87 0.45 0.68 0.04 0.23 0.01 7.34]\n",
      "Loss_Q: [0.88 1.17 0.45 0.66 0.05 0.23 0.02 0.   3.44] Loss_P: [2.33 1.71 1.83 0.56 0.68 0.04 0.23 0.03 7.41]\n",
      "Loss_Q: [0.89 1.18 0.49 0.64 0.05 0.25 0.02 0.   3.51] Loss_P: [2.37 1.64 1.73 0.5  0.64 0.04 0.23 0.01 7.16]\n",
      "Loss_Q: [0.86 1.17 0.47 0.68 0.04 0.22 0.01 0.   3.45] Loss_P: [2.39 1.64 1.76 0.46 0.64 0.03 0.23 0.02 7.15]\n",
      "Loss_Q: [0.88 1.23 0.43 0.66 0.05 0.26 0.01 0.   3.53] Loss_P: [2.43 1.69 1.81 0.47 0.62 0.05 0.25 0.01 7.33]\n",
      "Loss_Q: [0.92 1.23 0.42 0.61 0.04 0.24 0.03 0.   3.48] Loss_P: [2.44 1.65 1.8  0.48 0.66 0.07 0.23 0.02 7.33]\n",
      "Loss_Q: [0.83 1.14 0.41 0.63 0.05 0.23 0.01 0.   3.3 ] Loss_P: [2.38 1.7  1.73 0.43 0.66 0.06 0.23 0.01 7.2 ]\n",
      "Loss_Q: [0.86 1.22 0.4  0.59 0.03 0.24 0.02 0.   3.36] Loss_P: [2.36 1.66 1.81 0.5  0.64 0.04 0.22 0.01 7.23]\n",
      "Loss_Q: [0.89 1.2  0.39 0.59 0.05 0.24 0.02 0.   3.38] Loss_P: [2.36 1.69 1.85 0.45 0.58 0.05 0.22 0.02 7.23]\n",
      "Loss_Q: [0.87 1.23 0.43 0.65 0.04 0.23 0.01 0.   3.47] Loss_P: [2.35 1.65 1.82 0.46 0.66 0.05 0.18 0.02 7.19]\n",
      "Loss_Q: [0.85 1.09 0.46 0.67 0.05 0.23 0.02 0.   3.36] Loss_P: [2.35 1.68 1.78 0.47 0.64 0.04 0.2  0.01 7.17]\n",
      "Loss_Q: [0.87 1.17 0.49 0.61 0.04 0.2  0.01 0.   3.39] Loss_P: [2.33 1.7  1.86 0.46 0.63 0.05 0.22 0.02 7.26]\n",
      "Loss_Q: [0.89 1.13 0.42 0.62 0.05 0.24 0.01 0.   3.35] Loss_P: [2.33 1.7  1.85 0.47 0.62 0.04 0.21 0.02 7.24]\n",
      "Loss_Q: [0.86 1.2  0.42 0.64 0.07 0.22 0.02 0.   3.42] Loss_P: [2.41 1.61 1.77 0.46 0.6  0.05 0.23 0.02 7.15]\n",
      "Loss_Q: [0.88 1.16 0.4  0.62 0.05 0.23 0.01 0.   3.34] Loss_P: [2.43 1.57 1.7  0.48 0.62 0.05 0.2  0.02 7.07]\n",
      "Loss_Q: [0.84 1.13 0.46 0.61 0.04 0.24 0.02 0.   3.34] Loss_P: [2.35 1.65 1.79 0.52 0.61 0.04 0.21 0.02 7.19]\n",
      "Loss_Q: [0.89 1.11 0.44 0.63 0.05 0.2  0.03 0.   3.35] Loss_P: [2.33 1.68 1.77 0.46 0.68 0.03 0.2  0.02 7.16]\n",
      "Loss_Q: [0.87 1.18 0.4  0.58 0.05 0.19 0.03 0.   3.3 ] Loss_P: [2.43 1.65 1.77 0.45 0.64 0.04 0.21 0.02 7.21]\n",
      "Loss_Q: [0.88 1.08 0.42 0.6  0.04 0.21 0.01 0.   3.24] Loss_P: [2.35 1.67 1.78 0.44 0.63 0.07 0.19 0.01 7.14]\n",
      "Loss_Q: [0.87 1.2  0.43 0.58 0.03 0.2  0.02 0.   3.33] Loss_P: [2.32 1.74 1.8  0.41 0.59 0.04 0.21 0.01 7.14]\n",
      "Loss_Q: [0.92 1.17 0.42 0.57 0.05 0.19 0.02 0.   3.33] Loss_P: [2.39 1.69 1.86 0.47 0.6  0.05 0.23 0.01 7.29]\n",
      "Loss_Q: [0.91 1.18 0.42 0.6  0.05 0.2  0.01 0.   3.37] Loss_P: [2.38 1.66 1.8  0.53 0.61 0.05 0.17 0.02 7.21]\n",
      "Loss_Q: [0.89 1.2  0.38 0.58 0.05 0.18 0.02 0.   3.29] Loss_P: [2.4  1.74 1.81 0.43 0.63 0.05 0.18 0.02 7.26]\n",
      "Loss_Q: [0.87 1.18 0.39 0.59 0.06 0.23 0.01 0.   3.32] Loss_P: [2.33 1.72 1.76 0.44 0.59 0.05 0.18 0.02 7.09]\n",
      "Loss_Q: [0.92 1.11 0.38 0.58 0.04 0.22 0.02 0.   3.27] Loss_P: [2.31 1.68 1.8  0.47 0.62 0.04 0.22 0.01 7.14]\n",
      "Loss_Q: [0.83 1.09 0.38 0.61 0.05 0.19 0.01 0.   3.16] Loss_P: [2.42 1.58 1.76 0.42 0.59 0.06 0.2  0.02 7.05]\n",
      "Loss_Q: [0.91 1.11 0.35 0.61 0.05 0.18 0.01 0.   3.22] Loss_P: [2.36 1.66 1.67 0.45 0.66 0.06 0.19 0.01 7.05]\n",
      "Loss_Q: [0.88 1.13 0.4  0.63 0.04 0.2  0.02 0.   3.31] Loss_P: [2.39 1.69 1.76 0.45 0.65 0.05 0.21 0.01 7.2 ]\n",
      "Loss_Q: [0.83 1.12 0.39 0.61 0.03 0.19 0.02 0.   3.19] Loss_P: [2.35 1.65 1.73 0.44 0.61 0.03 0.2  0.01 7.02]\n",
      "Loss_Q: [0.89 1.18 0.39 0.57 0.04 0.18 0.01 0.   3.25] Loss_P: [2.38 1.7  1.75 0.49 0.6  0.04 0.18 0.02 7.16]\n",
      "Loss_Q: [0.83 1.16 0.42 0.59 0.06 0.19 0.01 0.   3.27] Loss_P: [2.39 1.65 1.72 0.46 0.61 0.04 0.16 0.01 7.04]\n",
      "Loss_Q: [0.83 1.18 0.38 0.59 0.07 0.18 0.01 0.   3.24] Loss_P: [2.35 1.7  1.73 0.44 0.65 0.05 0.18 0.02 7.12]\n",
      "Loss_Q: [0.83 1.21 0.42 0.63 0.05 0.18 0.02 0.   3.33] Loss_P: [2.35 1.7  1.78 0.51 0.66 0.07 0.18 0.01 7.26]\n",
      "Loss_Q: [0.82 1.26 0.37 0.59 0.02 0.19 0.03 0.   3.28] Loss_P: [2.39 1.69 1.82 0.46 0.59 0.05 0.18 0.01 7.2 ]\n",
      "Loss_Q: [0.89 1.25 0.35 0.57 0.04 0.13 0.02 0.   3.25] Loss_P: [2.35 1.73 1.85 0.46 0.59 0.04 0.17 0.03 7.22]\n",
      "Loss_Q: [0.89 1.2  0.38 0.56 0.04 0.17 0.01 0.   3.25] Loss_P: [2.38 1.71 1.79 0.42 0.59 0.03 0.12 0.01 7.05]\n",
      "Loss_Q: [0.88 1.29 0.36 0.61 0.03 0.17 0.02 0.   3.35] Loss_P: [2.37 1.7  1.79 0.43 0.58 0.05 0.18 0.02 7.11]\n",
      "Loss_Q: [0.88 1.26 0.32 0.59 0.05 0.15 0.01 0.   3.25] Loss_P: [2.35 1.74 1.8  0.36 0.62 0.04 0.15 0.01 7.07]\n",
      "Loss_Q: [0.92 1.29 0.35 0.55 0.04 0.22 0.02 0.   3.38] Loss_P: [2.45 1.71 1.78 0.38 0.61 0.06 0.16 0.02 7.17]\n",
      "Loss_Q: [0.81 1.21 0.37 0.59 0.03 0.18 0.01 0.   3.21] Loss_P: [2.39 1.71 1.76 0.36 0.6  0.04 0.17 0.02 7.05]\n",
      "Loss_Q: [0.86 1.26 0.34 0.62 0.04 0.16 0.01 0.   3.28] Loss_P: [2.37 1.71 1.86 0.41 0.58 0.04 0.21 0.02 7.19]\n",
      "Loss_Q: [0.91 1.21 0.29 0.58 0.06 0.2  0.01 0.   3.27] Loss_P: [2.37 1.69 1.83 0.37 0.6  0.03 0.17 0.02 7.09]\n",
      "Loss_Q: [0.94 1.23 0.35 0.56 0.04 0.18 0.02 0.   3.31] Loss_P: [2.4  1.73 1.83 0.39 0.53 0.05 0.18 0.02 7.12]\n",
      "Loss_Q: [0.91 1.29 0.36 0.6  0.05 0.15 0.01 0.   3.36] Loss_P: [2.4  1.78 1.88 0.39 0.58 0.03 0.17 0.02 7.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.86 1.3  0.34 0.61 0.05 0.17 0.01 0.   3.33] Loss_P: [2.39 1.7  1.84 0.39 0.58 0.04 0.16 0.01 7.12]\n",
      "Loss_Q: [0.86 1.27 0.36 0.61 0.04 0.14 0.01 0.   3.29] Loss_P: [2.37 1.74 1.86 0.35 0.57 0.05 0.17 0.02 7.12]\n",
      "Loss_Q: [0.86 1.31 0.36 0.58 0.03 0.16 0.01 0.   3.31] Loss_P: [2.38 1.8  1.85 0.37 0.57 0.05 0.18 0.01 7.21]\n",
      "Loss_Q: [0.95 1.24 0.31 0.55 0.02 0.17 0.01 0.   3.25] Loss_P: [2.43 1.66 1.77 0.37 0.55 0.04 0.18 0.02 7.01]\n",
      "Loss_Q: [0.87 1.28 0.3  0.61 0.04 0.19 0.01 0.   3.3 ] Loss_P: [2.4  1.74 1.87 0.4  0.59 0.04 0.19 0.01 7.25]\n",
      "Loss_Q: [0.87 1.23 0.34 0.62 0.04 0.17 0.02 0.   3.28] Loss_P: [2.42 1.68 1.78 0.39 0.58 0.04 0.15 0.02 7.06]\n",
      "Loss_Q: [0.8  1.26 0.3  0.56 0.04 0.14 0.01 0.   3.1 ] Loss_P: [2.39 1.69 1.8  0.43 0.59 0.03 0.17 0.01 7.11]\n",
      "Loss_Q: [0.9  1.2  0.35 0.65 0.05 0.15 0.02 0.   3.32] Loss_P: [2.41 1.67 1.83 0.45 0.65 0.06 0.15 0.01 7.22]\n",
      "Loss_Q: [0.89 1.26 0.38 0.64 0.05 0.15 0.01 0.   3.39] Loss_P: [2.37 1.73 1.83 0.41 0.65 0.04 0.14 0.02 7.18]\n",
      "Loss_Q: [0.93 1.26 0.35 0.61 0.05 0.16 0.02 0.   3.39] Loss_P: [2.36 1.69 1.8  0.44 0.6  0.04 0.14 0.03 7.1 ]\n",
      "Loss_Q: [0.87 1.24 0.36 0.61 0.05 0.14 0.01 0.   3.28] Loss_P: [2.34 1.75 1.84 0.4  0.61 0.06 0.17 0.02 7.21]\n",
      "Loss_Q: [0.89 1.16 0.35 0.62 0.06 0.14 0.01 0.   3.24] Loss_P: [2.38 1.7  1.82 0.4  0.64 0.06 0.16 0.01 7.19]\n",
      "Loss_Q: [0.96 1.28 0.35 0.61 0.06 0.14 0.01 0.   3.42] Loss_P: [2.39 1.72 1.95 0.45 0.61 0.04 0.15 0.01 7.32]\n",
      "Loss_Q: [0.91 1.23 0.39 0.59 0.04 0.14 0.01 0.   3.32] Loss_P: [2.4  1.66 1.85 0.38 0.59 0.05 0.15 0.02 7.08]\n",
      "Loss_Q: [0.95 1.25 0.35 0.6  0.03 0.19 0.02 0.   3.39] Loss_P: [2.4  1.73 1.76 0.42 0.6  0.04 0.17 0.03 7.14]\n",
      "Loss_Q: [0.87 1.24 0.36 0.62 0.05 0.16 0.01 0.   3.29] Loss_P: [2.4  1.73 1.79 0.44 0.6  0.04 0.15 0.02 7.16]\n",
      "Loss_Q: [0.9  1.19 0.35 0.56 0.04 0.13 0.01 0.   3.18] Loss_P: [2.44 1.73 1.78 0.45 0.62 0.05 0.16 0.01 7.24]\n",
      "Loss_Q: [0.85 1.15 0.33 0.59 0.04 0.15 0.01 0.   3.12] Loss_P: [2.39 1.68 1.71 0.44 0.63 0.04 0.17 0.01 7.06]\n",
      "Loss_Q: [0.86 1.23 0.4  0.58 0.04 0.15 0.01 0.   3.27] Loss_P: [2.4  1.75 1.71 0.41 0.62 0.05 0.15 0.02 7.1 ]\n",
      "Loss_Q: [0.83 1.15 0.39 0.57 0.03 0.15 0.01 0.   3.12] Loss_P: [2.4  1.67 1.71 0.42 0.59 0.04 0.15 0.01 6.98]\n",
      "Loss_Q: [0.93 1.23 0.37 0.62 0.04 0.15 0.02 0.   3.36] Loss_P: [2.37 1.81 1.83 0.43 0.62 0.04 0.14 0.01 7.25]\n",
      "Loss_Q: [0.9  1.28 0.4  0.64 0.05 0.15 0.02 0.   3.44] Loss_P: [2.4  1.73 1.8  0.42 0.62 0.05 0.16 0.02 7.19]\n",
      "Loss_Q: [0.91 1.29 0.36 0.55 0.03 0.13 0.03 0.   3.31] Loss_P: [2.38 1.75 1.77 0.47 0.59 0.03 0.13 0.01 7.13]\n",
      "Loss_Q: [0.88 1.3  0.34 0.57 0.03 0.16 0.02 0.   3.3 ] Loss_P: [2.45 1.72 1.75 0.41 0.53 0.04 0.13 0.01 7.04]\n",
      "Loss_Q: [0.93 1.23 0.37 0.59 0.03 0.13 0.01 0.   3.28] Loss_P: [2.44 1.71 1.78 0.43 0.59 0.06 0.14 0.02 7.16]\n",
      "Loss_Q: [0.94 1.27 0.38 0.6  0.04 0.16 0.02 0.   3.4 ] Loss_P: [2.38 1.72 1.75 0.41 0.6  0.06 0.16 0.03 7.12]\n",
      "Loss_Q: [0.93 1.26 0.37 0.62 0.04 0.11 0.01 0.   3.34] Loss_P: [2.36 1.83 1.76 0.42 0.58 0.04 0.14 0.01 7.15]\n",
      "Loss_Q: [0.87 1.21 0.38 0.6  0.05 0.14 0.01 0.   3.26] Loss_P: [2.4  1.71 1.78 0.43 0.64 0.03 0.13 0.02 7.13]\n",
      "Loss_Q: [0.95 1.25 0.36 0.57 0.04 0.14 0.03 0.   3.34] Loss_P: [2.44 1.76 1.75 0.4  0.55 0.04 0.14 0.02 7.09]\n",
      "Loss_Q: [0.9  1.28 0.39 0.52 0.03 0.15 0.02 0.   3.29] Loss_P: [2.39 1.74 1.71 0.42 0.58 0.04 0.17 0.02 7.07]\n",
      "Loss_Q: [0.88 1.28 0.36 0.58 0.03 0.13 0.02 0.   3.28] Loss_P: [2.47 1.8  1.74 0.35 0.63 0.04 0.15 0.02 7.19]\n",
      "Loss_Q: [0.98 1.24 0.35 0.59 0.04 0.15 0.02 0.   3.37] Loss_P: [2.39 1.81 1.76 0.35 0.58 0.04 0.13 0.02 7.07]\n",
      "Loss_Q: [0.93 1.26 0.34 0.58 0.04 0.14 0.01 0.   3.3 ] Loss_P: [2.43 1.74 1.75 0.37 0.6  0.04 0.14 0.02 7.1 ]\n",
      "Loss_Q: [0.92 1.27 0.36 0.57 0.06 0.17 0.02 0.   3.37] Loss_P: [2.35 1.79 1.82 0.41 0.58 0.03 0.15 0.01 7.15]\n",
      "Loss_Q: [0.98 1.28 0.38 0.56 0.05 0.15 0.01 0.   3.39] Loss_P: [2.36 1.84 1.84 0.39 0.58 0.04 0.11 0.02 7.17]\n",
      "Loss_Q: [0.92 1.27 0.34 0.61 0.06 0.13 0.01 0.   3.34] Loss_P: [2.4  1.74 1.87 0.38 0.57 0.04 0.13 0.01 7.15]\n",
      "Loss_Q: [0.91 1.19 0.32 0.56 0.06 0.11 0.01 0.   3.17] Loss_P: [2.33 1.72 1.88 0.38 0.57 0.04 0.15 0.01 7.09]\n",
      "Loss_Q: [0.95 1.31 0.36 0.58 0.04 0.17 0.02 0.   3.41] Loss_P: [2.43 1.75 1.94 0.4  0.6  0.04 0.18 0.02 7.37]\n",
      "Loss_Q: [0.92 1.38 0.4  0.57 0.05 0.16 0.01 0.   3.5 ] Loss_P: [2.33 1.74 1.95 0.4  0.58 0.03 0.17 0.02 7.22]\n",
      "Loss_Q: [0.97 1.32 0.37 0.53 0.05 0.15 0.01 0.   3.4 ] Loss_P: [2.37 1.75 1.92 0.43 0.55 0.05 0.16 0.01 7.24]\n",
      "Loss_Q: [1.   1.33 0.41 0.57 0.03 0.18 0.02 0.   3.54] Loss_P: [2.38 1.73 1.93 0.46 0.61 0.03 0.11 0.01 7.26]\n",
      "Loss_Q: [0.88 1.26 0.34 0.54 0.04 0.17 0.01 0.   3.24] Loss_P: [2.39 1.7  1.91 0.39 0.55 0.04 0.17 0.02 7.16]\n",
      "Loss_Q: [0.91 1.34 0.33 0.56 0.05 0.14 0.02 0.   3.35] Loss_P: [2.37 1.79 1.93 0.37 0.58 0.05 0.15 0.02 7.25]\n",
      "Loss_Q: [0.98 1.3  0.37 0.53 0.05 0.14 0.01 0.   3.38] Loss_P: [2.4  1.73 1.9  0.42 0.56 0.05 0.18 0.02 7.25]\n",
      "Loss_Q: [1.05 1.24 0.31 0.5  0.03 0.14 0.02 0.   3.29] Loss_P: [2.38 1.77 1.91 0.38 0.5  0.04 0.16 0.03 7.16]\n",
      "Loss_Q: [0.93 1.32 0.36 0.54 0.06 0.13 0.02 0.   3.36] Loss_P: [2.37 1.71 1.95 0.39 0.53 0.05 0.17 0.02 7.2 ]\n",
      "Loss_Q: [0.98 1.28 0.3  0.54 0.03 0.2  0.02 0.   3.35] Loss_P: [2.33 1.78 1.92 0.38 0.58 0.04 0.18 0.02 7.23]\n",
      "Loss_Q: [0.95 1.29 0.32 0.6  0.06 0.2  0.01 0.   3.42] Loss_P: [2.34 1.77 1.94 0.42 0.59 0.06 0.17 0.02 7.29]\n",
      "Loss_Q: [0.94 1.27 0.3  0.61 0.05 0.19 0.01 0.   3.35] Loss_P: [2.37 1.74 1.94 0.38 0.59 0.04 0.19 0.02 7.27]\n",
      "Loss_Q: [0.88 1.32 0.38 0.59 0.04 0.17 0.01 0.   3.39] Loss_P: [2.34 1.79 1.93 0.35 0.62 0.04 0.15 0.02 7.24]\n",
      "Loss_Q: [0.92 1.33 0.3  0.59 0.05 0.18 0.01 0.   3.39] Loss_P: [2.41 1.67 1.85 0.4  0.6  0.03 0.2  0.03 7.2 ]\n",
      "Loss_Q: [0.93 1.31 0.3  0.6  0.06 0.16 0.01 0.   3.37] Loss_P: [2.46 1.66 1.83 0.32 0.58 0.04 0.16 0.02 7.08]\n",
      "Loss_Q: [0.87 1.32 0.28 0.63 0.05 0.16 0.01 0.   3.32] Loss_P: [2.41 1.72 1.92 0.38 0.62 0.05 0.15 0.02 7.27]\n",
      "Loss_Q: [0.93 1.35 0.33 0.61 0.04 0.18 0.01 0.   3.46] Loss_P: [2.45 1.63 1.89 0.36 0.65 0.05 0.19 0.01 7.23]\n",
      "Loss_Q: [0.91 1.37 0.32 0.63 0.05 0.16 0.01 0.   3.46] Loss_P: [2.41 1.69 1.96 0.36 0.65 0.05 0.16 0.01 7.28]\n",
      "Loss_Q: [0.93 1.3  0.29 0.63 0.03 0.17 0.01 0.   3.36] Loss_P: [2.35 1.65 1.93 0.39 0.64 0.05 0.18 0.02 7.2 ]\n",
      "Loss_Q: [0.88 1.32 0.33 0.65 0.05 0.15 0.02 0.   3.41] Loss_P: [2.36 1.66 1.92 0.34 0.65 0.06 0.17 0.02 7.17]\n",
      "Loss_Q: [0.91 1.28 0.32 0.62 0.05 0.16 0.02 0.   3.36] Loss_P: [2.41 1.64 1.9  0.39 0.64 0.04 0.16 0.01 7.17]\n",
      "Loss_Q: [0.88 1.29 0.36 0.65 0.07 0.17 0.01 0.   3.43] Loss_P: [2.39 1.67 1.88 0.41 0.65 0.04 0.18 0.03 7.24]\n",
      "Loss_Q: [0.86 1.23 0.34 0.64 0.05 0.2  0.02 0.   3.33] Loss_P: [2.4  1.72 1.85 0.37 0.64 0.06 0.16 0.02 7.22]\n",
      "Loss_Q: [0.92 1.35 0.33 0.65 0.05 0.18 0.01 0.   3.48] Loss_P: [2.43 1.68 1.91 0.39 0.67 0.06 0.2  0.01 7.35]\n",
      "Loss_Q: [0.93 1.3  0.34 0.63 0.05 0.17 0.01 0.   3.43] Loss_P: [2.38 1.68 1.89 0.37 0.69 0.07 0.19 0.01 7.28]\n",
      "Loss_Q: [0.91 1.21 0.37 0.66 0.05 0.16 0.03 0.   3.38] Loss_P: [2.36 1.67 1.94 0.37 0.65 0.06 0.19 0.02 7.27]\n",
      "Loss_Q: [0.89 1.32 0.35 0.66 0.04 0.17 0.02 0.   3.43] Loss_P: [2.39 1.69 1.89 0.38 0.65 0.04 0.17 0.02 7.21]\n",
      "Loss_Q: [0.87 1.28 0.33 0.65 0.05 0.18 0.01 0.   3.37] Loss_P: [2.41 1.73 1.87 0.43 0.65 0.05 0.19 0.02 7.35]\n",
      "Loss_Q: [0.86 1.28 0.35 0.67 0.06 0.18 0.02 0.   3.42] Loss_P: [2.43 1.66 1.88 0.36 0.67 0.04 0.17 0.02 7.22]\n",
      "Loss_Q: [0.92 1.29 0.38 0.67 0.05 0.16 0.01 0.   3.47] Loss_P: [2.43 1.63 1.91 0.39 0.66 0.07 0.19 0.01 7.29]\n",
      "Loss_Q: [0.89 1.22 0.37 0.66 0.05 0.18 0.02 0.   3.4 ] Loss_P: [2.37 1.6  1.96 0.43 0.67 0.05 0.2  0.01 7.27]\n",
      "Loss_Q: [0.91 1.32 0.35 0.68 0.05 0.16 0.01 0.   3.49] Loss_P: [2.43 1.59 1.99 0.44 0.67 0.04 0.21 0.02 7.38]\n",
      "Loss_Q: [1.01 1.32 0.38 0.62 0.04 0.16 0.01 0.   3.54] Loss_P: [2.37 1.69 1.97 0.44 0.69 0.05 0.18 0.02 7.41]\n",
      "Loss_Q: [0.96 1.33 0.41 0.65 0.07 0.18 0.02 0.   3.62] Loss_P: [2.36 1.68 1.97 0.48 0.67 0.05 0.19 0.02 7.41]\n",
      "Loss_Q: [0.93 1.33 0.37 0.65 0.07 0.18 0.01 0.   3.55] Loss_P: [2.4  1.69 1.96 0.47 0.67 0.05 0.2  0.01 7.45]\n",
      "Loss_Q: [0.94 1.3  0.38 0.67 0.05 0.19 0.02 0.   3.55] Loss_P: [2.42 1.66 1.99 0.42 0.67 0.05 0.19 0.01 7.41]\n",
      "Loss_Q: [0.96 1.22 0.36 0.63 0.04 0.15 0.01 0.   3.37] Loss_P: [2.37 1.61 1.92 0.43 0.68 0.04 0.19 0.01 7.24]\n",
      "Loss_Q: [0.96 1.21 0.38 0.69 0.08 0.16 0.01 0.   3.5 ] Loss_P: [2.43 1.6  1.86 0.46 0.69 0.05 0.16 0.01 7.25]\n",
      "Loss_Q: [0.85 1.25 0.36 0.65 0.03 0.2  0.02 0.   3.35] Loss_P: [2.4  1.65 1.93 0.43 0.7  0.08 0.16 0.01 7.36]\n",
      "Loss_Q: [0.89 1.31 0.42 0.7  0.06 0.19 0.01 0.   3.58] Loss_P: [2.35 1.65 1.97 0.47 0.71 0.05 0.16 0.02 7.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.01 1.31 0.43 0.69 0.05 0.16 0.02 0.   3.67] Loss_P: [2.38 1.58 1.92 0.41 0.68 0.06 0.19 0.01 7.23]\n",
      "Loss_Q: [0.86 1.3  0.4  0.67 0.06 0.17 0.02 0.   3.47] Loss_P: [2.37 1.66 1.96 0.4  0.66 0.05 0.17 0.02 7.29]\n",
      "Loss_Q: [0.93 1.25 0.36 0.68 0.06 0.17 0.02 0.   3.46] Loss_P: [2.38 1.73 1.93 0.47 0.72 0.06 0.18 0.02 7.5 ]\n",
      "Loss_Q: [0.94 1.29 0.38 0.7  0.07 0.16 0.01 0.   3.56] Loss_P: [2.4  1.69 1.96 0.46 0.69 0.06 0.17 0.02 7.44]\n",
      "Loss_Q: [0.87 1.29 0.39 0.71 0.07 0.17 0.02 0.   3.53] Loss_P: [2.32 1.7  2.   0.45 0.67 0.04 0.17 0.01 7.36]\n",
      "Loss_Q: [0.88 1.29 0.36 0.67 0.05 0.15 0.02 0.   3.42] Loss_P: [2.43 1.63 1.93 0.47 0.66 0.06 0.18 0.02 7.4 ]\n",
      "Loss_Q: [0.95 1.34 0.36 0.68 0.04 0.18 0.01 0.   3.56] Loss_P: [2.4  1.67 1.95 0.44 0.69 0.05 0.16 0.02 7.39]\n",
      "Loss_Q: [0.91 1.28 0.4  0.65 0.04 0.19 0.02 0.   3.48] Loss_P: [2.39 1.59 1.98 0.48 0.71 0.04 0.19 0.01 7.39]\n",
      "Loss_Q: [0.91 1.24 0.39 0.68 0.05 0.18 0.01 0.   3.46] Loss_P: [2.33 1.67 1.86 0.46 0.7  0.04 0.14 0.02 7.22]\n",
      "Loss_Q: [0.91 1.27 0.37 0.67 0.05 0.16 0.01 0.   3.44] Loss_P: [2.4  1.66 1.89 0.45 0.67 0.06 0.16 0.01 7.29]\n",
      "Loss_Q: [0.93 1.14 0.35 0.7  0.05 0.16 0.01 0.   3.35] Loss_P: [2.43 1.65 1.89 0.41 0.63 0.04 0.18 0.02 7.24]\n",
      "Loss_Q: [0.93 1.17 0.41 0.69 0.04 0.16 0.01 0.   3.41] Loss_P: [2.38 1.65 1.84 0.45 0.68 0.06 0.18 0.02 7.26]\n",
      "Loss_Q: [0.95 1.2  0.41 0.67 0.06 0.15 0.01 0.   3.45] Loss_P: [2.39 1.63 1.89 0.47 0.7  0.03 0.16 0.03 7.3 ]\n",
      "Loss_Q: [0.94 1.23 0.37 0.68 0.05 0.17 0.01 0.   3.44] Loss_P: [2.34 1.71 1.89 0.47 0.7  0.05 0.19 0.01 7.36]\n",
      "Loss_Q: [0.91 1.22 0.48 0.67 0.04 0.15 0.01 0.   3.49] Loss_P: [2.43 1.65 1.9  0.49 0.7  0.04 0.17 0.01 7.39]\n",
      "Loss_Q: [0.93 1.19 0.37 0.68 0.04 0.17 0.03 0.   3.41] Loss_P: [2.36 1.64 1.88 0.47 0.71 0.06 0.16 0.01 7.3 ]\n",
      "Loss_Q: [0.86 1.29 0.42 0.69 0.06 0.15 0.01 0.   3.47] Loss_P: [2.36 1.65 1.91 0.53 0.69 0.06 0.16 0.02 7.38]\n",
      "Loss_Q: [0.95 1.23 0.49 0.69 0.06 0.15 0.02 0.   3.59] Loss_P: [2.42 1.64 1.9  0.48 0.65 0.04 0.13 0.02 7.27]\n",
      "Loss_Q: [0.94 1.17 0.47 0.67 0.06 0.13 0.01 0.   3.46] Loss_P: [2.37 1.69 1.92 0.51 0.68 0.04 0.15 0.02 7.37]\n",
      "Loss_Q: [0.96 1.24 0.48 0.68 0.06 0.16 0.01 0.   3.59] Loss_P: [2.34 1.69 1.92 0.51 0.69 0.05 0.15 0.02 7.37]\n",
      "Loss_Q: [0.98 1.19 0.42 0.68 0.05 0.16 0.01 0.   3.5 ] Loss_P: [2.32 1.68 1.95 0.5  0.67 0.07 0.17 0.01 7.37]\n",
      "Loss_Q: [0.89 1.23 0.45 0.69 0.03 0.14 0.02 0.   3.46] Loss_P: [2.36 1.68 1.87 0.47 0.67 0.03 0.13 0.02 7.25]\n",
      "Loss_Q: [0.91 1.16 0.46 0.69 0.04 0.16 0.02 0.   3.43] Loss_P: [2.4  1.63 1.91 0.51 0.69 0.05 0.16 0.02 7.36]\n",
      "Loss_Q: [0.97 1.27 0.45 0.61 0.05 0.2  0.02 0.   3.58] Loss_P: [2.4  1.68 1.9  0.56 0.67 0.05 0.17 0.01 7.43]\n",
      "Loss_Q: [0.92 1.25 0.43 0.62 0.06 0.16 0.02 0.   3.46] Loss_P: [2.37 1.68 1.88 0.46 0.63 0.05 0.16 0.01 7.25]\n",
      "Loss_Q: [0.93 1.19 0.41 0.67 0.05 0.18 0.02 0.   3.44] Loss_P: [2.35 1.63 1.87 0.52 0.68 0.05 0.16 0.02 7.29]\n",
      "Loss_Q: [0.89 1.25 0.47 0.66 0.05 0.16 0.01 0.   3.48] Loss_P: [2.34 1.67 1.85 0.58 0.7  0.04 0.16 0.02 7.36]\n",
      "Loss_Q: [0.9  1.18 0.43 0.67 0.05 0.15 0.02 0.   3.4 ] Loss_P: [2.42 1.62 1.85 0.53 0.7  0.03 0.17 0.02 7.34]\n",
      "Loss_Q: [0.91 1.25 0.48 0.66 0.04 0.17 0.01 0.   3.52] Loss_P: [2.35 1.66 1.8  0.48 0.71 0.05 0.2  0.02 7.26]\n",
      "Loss_Q: [0.95 1.26 0.46 0.64 0.05 0.16 0.01 0.   3.53] Loss_P: [2.34 1.67 1.87 0.51 0.67 0.05 0.16 0.03 7.31]\n",
      "Loss_Q: [0.88 1.22 0.42 0.64 0.05 0.17 0.01 0.   3.39] Loss_P: [2.34 1.66 1.89 0.53 0.72 0.05 0.14 0.01 7.34]\n",
      "Loss_Q: [0.87 1.28 0.44 0.67 0.05 0.12 0.01 0.   3.45] Loss_P: [2.45 1.65 1.93 0.48 0.69 0.04 0.13 0.01 7.38]\n",
      "Loss_Q: [0.93 1.23 0.43 0.63 0.07 0.19 0.02 0.   3.51] Loss_P: [2.38 1.67 1.88 0.48 0.64 0.06 0.15 0.02 7.27]\n",
      "Loss_Q: [0.94 1.26 0.45 0.62 0.05 0.18 0.02 0.   3.51] Loss_P: [2.36 1.71 1.89 0.4  0.63 0.05 0.17 0.02 7.24]\n",
      "Loss_Q: [1.04 1.24 0.46 0.58 0.05 0.14 0.02 0.   3.52] Loss_P: [2.32 1.67 1.92 0.45 0.61 0.05 0.17 0.02 7.21]\n",
      "Loss_Q: [0.94 1.15 0.39 0.55 0.04 0.16 0.01 0.   3.24] Loss_P: [2.32 1.75 1.94 0.45 0.59 0.04 0.15 0.02 7.26]\n",
      "Loss_Q: [0.91 1.19 0.38 0.61 0.04 0.18 0.03 0.   3.35] Loss_P: [2.39 1.78 1.89 0.43 0.61 0.04 0.16 0.02 7.33]\n",
      "Loss_Q: [1.01 1.2  0.43 0.59 0.04 0.15 0.01 0.   3.44] Loss_P: [2.37 1.67 1.85 0.41 0.58 0.07 0.17 0.02 7.15]\n",
      "Loss_Q: [1.04 1.23 0.39 0.64 0.05 0.16 0.01 0.   3.52] Loss_P: [2.35 1.72 1.81 0.44 0.61 0.04 0.16 0.02 7.16]\n",
      "Loss_Q: [1.   1.22 0.45 0.63 0.04 0.16 0.01 0.   3.5 ] Loss_P: [2.45 1.68 1.84 0.46 0.59 0.06 0.18 0.02 7.29]\n",
      "Loss_Q: [0.94 1.19 0.47 0.65 0.07 0.19 0.03 0.   3.55] Loss_P: [2.35 1.69 1.98 0.46 0.56 0.06 0.2  0.01 7.3 ]\n",
      "Loss_Q: [1.07 1.3  0.4  0.57 0.04 0.17 0.02 0.   3.56] Loss_P: [2.35 1.76 1.98 0.44 0.62 0.05 0.15 0.02 7.36]\n",
      "Loss_Q: [0.9  1.35 0.43 0.61 0.05 0.17 0.02 0.   3.52] Loss_P: [2.37 1.68 2.03 0.46 0.6  0.04 0.15 0.01 7.35]\n",
      "Loss_Q: [0.93 1.39 0.41 0.54 0.04 0.18 0.02 0.   3.5 ] Loss_P: [2.34 1.73 2.01 0.48 0.6  0.05 0.17 0.02 7.4 ]\n",
      "Loss_Q: [1.   1.33 0.45 0.6  0.05 0.19 0.02 0.   3.64] Loss_P: [2.43 1.67 1.92 0.48 0.64 0.04 0.17 0.01 7.37]\n",
      "Loss_Q: [1.01 1.29 0.41 0.57 0.03 0.17 0.01 0.   3.49] Loss_P: [2.4  1.67 2.05 0.5  0.64 0.06 0.16 0.01 7.49]\n",
      "Loss_Q: [0.9  1.33 0.42 0.6  0.05 0.16 0.02 0.   3.47] Loss_P: [2.46 1.69 1.91 0.43 0.6  0.05 0.18 0.01 7.33]\n",
      "Loss_Q: [0.91 1.22 0.4  0.65 0.05 0.17 0.02 0.   3.42] Loss_P: [2.39 1.72 1.95 0.53 0.66 0.04 0.16 0.02 7.46]\n",
      "Loss_Q: [0.98 1.28 0.47 0.62 0.03 0.2  0.02 0.   3.61] Loss_P: [2.41 1.63 1.94 0.47 0.62 0.05 0.18 0.02 7.31]\n",
      "Loss_Q: [0.89 1.3  0.45 0.64 0.07 0.17 0.02 0.   3.53] Loss_P: [2.4  1.68 1.93 0.53 0.62 0.03 0.18 0.02 7.4 ]\n",
      "Loss_Q: [0.92 1.29 0.43 0.6  0.04 0.16 0.02 0.   3.45] Loss_P: [2.37 1.7  1.93 0.46 0.66 0.05 0.18 0.02 7.38]\n",
      "Loss_Q: [1.01 1.25 0.43 0.64 0.04 0.17 0.01 0.   3.55] Loss_P: [2.38 1.75 1.93 0.51 0.67 0.06 0.18 0.02 7.5 ]\n",
      "Loss_Q: [1.   1.29 0.42 0.6  0.04 0.17 0.02 0.   3.53] Loss_P: [2.39 1.67 1.94 0.48 0.61 0.05 0.17 0.02 7.32]\n",
      "Loss_Q: [0.98 1.28 0.45 0.62 0.04 0.15 0.01 0.   3.54] Loss_P: [2.37 1.68 1.97 0.52 0.67 0.04 0.16 0.04 7.45]\n",
      "Loss_Q: [0.99 1.26 0.46 0.64 0.05 0.18 0.03 0.   3.62] Loss_P: [2.36 1.71 1.9  0.53 0.66 0.03 0.16 0.01 7.36]\n",
      "Loss_Q: [0.97 1.27 0.48 0.62 0.05 0.15 0.01 0.   3.56] Loss_P: [2.36 1.72 1.94 0.51 0.66 0.06 0.15 0.01 7.41]\n",
      "Loss_Q: [0.94 1.25 0.42 0.63 0.05 0.17 0.02 0.   3.48] Loss_P: [2.41 1.69 1.92 0.52 0.66 0.06 0.17 0.01 7.45]\n",
      "Loss_Q: [0.87 1.24 0.42 0.62 0.03 0.14 0.01 0.   3.35] Loss_P: [2.34 1.72 1.94 0.49 0.64 0.04 0.17 0.01 7.35]\n",
      "Loss_Q: [0.93 1.29 0.46 0.61 0.04 0.15 0.01 0.   3.49] Loss_P: [2.38 1.71 1.87 0.49 0.64 0.05 0.2  0.02 7.36]\n",
      "Loss_Q: [0.97 1.18 0.37 0.66 0.05 0.16 0.01 0.   3.4 ] Loss_P: [2.37 1.7  1.83 0.48 0.59 0.04 0.18 0.02 7.19]\n",
      "Loss_Q: [0.89 1.26 0.45 0.63 0.04 0.17 0.02 0.   3.46] Loss_P: [2.4  1.69 1.84 0.43 0.62 0.04 0.19 0.02 7.23]\n",
      "Loss_Q: [0.9  1.19 0.36 0.62 0.05 0.21 0.02 0.   3.34] Loss_P: [2.41 1.69 1.88 0.51 0.67 0.05 0.2  0.02 7.42]\n",
      "Loss_Q: [0.85 1.2  0.4  0.69 0.04 0.19 0.02 0.   3.39] Loss_P: [2.37 1.69 1.81 0.5  0.66 0.04 0.16 0.01 7.25]\n",
      "Loss_Q: [0.97 1.22 0.43 0.67 0.05 0.16 0.01 0.   3.51] Loss_P: [2.35 1.71 1.89 0.53 0.67 0.04 0.16 0.02 7.37]\n",
      "Loss_Q: [0.99 1.18 0.4  0.69 0.04 0.18 0.02 0.   3.5 ] Loss_P: [2.4  1.69 1.81 0.53 0.72 0.05 0.17 0.02 7.37]\n",
      "Loss_Q: [0.94 1.17 0.43 0.71 0.05 0.18 0.01 0.   3.48] Loss_P: [2.34 1.69 1.89 0.51 0.75 0.05 0.2  0.02 7.45]\n",
      "Loss_Q: [0.92 1.18 0.45 0.7  0.06 0.19 0.03 0.   3.53] Loss_P: [2.4  1.64 1.88 0.46 0.67 0.03 0.19 0.02 7.3 ]\n",
      "Loss_Q: [0.95 1.18 0.44 0.7  0.06 0.16 0.02 0.   3.52] Loss_P: [2.4  1.8  1.9  0.51 0.72 0.06 0.18 0.01 7.58]\n",
      "Loss_Q: [1.05 1.28 0.45 0.65 0.05 0.18 0.02 0.   3.68] Loss_P: [2.34 1.73 1.91 0.52 0.69 0.05 0.17 0.03 7.46]\n",
      "Loss_Q: [0.92 1.2  0.46 0.63 0.06 0.18 0.01 0.   3.47] Loss_P: [2.38 1.73 1.9  0.46 0.65 0.05 0.16 0.01 7.34]\n",
      "Loss_Q: [1.   1.27 0.44 0.67 0.06 0.17 0.02 0.   3.63] Loss_P: [2.39 1.61 1.85 0.59 0.65 0.04 0.2  0.02 7.35]\n",
      "Loss_Q: [0.92 1.29 0.5  0.66 0.06 0.17 0.02 0.   3.61] Loss_P: [2.37 1.75 1.88 0.52 0.67 0.05 0.17 0.03 7.44]\n",
      "Loss_Q: [0.96 1.26 0.45 0.66 0.05 0.19 0.01 0.   3.58] Loss_P: [2.42 1.77 1.89 0.57 0.69 0.05 0.18 0.01 7.57]\n",
      "Loss_Q: [0.93 1.25 0.46 0.7  0.04 0.16 0.01 0.   3.54] Loss_P: [2.38 1.71 1.94 0.55 0.72 0.04 0.17 0.02 7.53]\n",
      "Loss_Q: [0.97 1.23 0.42 0.67 0.04 0.17 0.02 0.   3.52] Loss_P: [2.4  1.72 1.9  0.52 0.7  0.04 0.17 0.02 7.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.99 1.19 0.47 0.68 0.04 0.15 0.01 0.   3.53] Loss_P: [2.31 1.71 1.98 0.5  0.69 0.05 0.17 0.02 7.43]\n",
      "Loss_Q: [0.97 1.26 0.44 0.69 0.04 0.18 0.03 0.   3.6 ] Loss_P: [2.35 1.76 1.96 0.49 0.68 0.06 0.21 0.02 7.53]\n",
      "Loss_Q: [0.95 1.25 0.42 0.67 0.05 0.17 0.01 0.   3.53] Loss_P: [2.39 1.68 1.95 0.5  0.72 0.04 0.17 0.02 7.47]\n",
      "Loss_Q: [0.97 1.21 0.4  0.67 0.06 0.18 0.02 0.   3.49] Loss_P: [2.38 1.72 1.9  0.5  0.67 0.04 0.18 0.01 7.4 ]\n",
      "Loss_Q: [0.92 1.22 0.48 0.69 0.03 0.18 0.01 0.   3.54] Loss_P: [2.4  1.74 1.86 0.52 0.68 0.04 0.17 0.02 7.44]\n",
      "Loss_Q: [0.95 1.17 0.47 0.69 0.04 0.17 0.02 0.   3.51] Loss_P: [2.35 1.76 1.91 0.56 0.69 0.05 0.2  0.01 7.54]\n",
      "Loss_Q: [0.96 1.2  0.51 0.72 0.04 0.22 0.02 0.   3.68] Loss_P: [2.34 1.82 1.87 0.54 0.7  0.04 0.19 0.01 7.51]\n",
      "Loss_Q: [0.95 1.27 0.47 0.69 0.03 0.17 0.02 0.   3.6 ] Loss_P: [2.4  1.69 1.9  0.51 0.69 0.04 0.19 0.01 7.43]\n",
      "Loss_Q: [1.02 1.22 0.49 0.68 0.06 0.15 0.01 0.   3.63] Loss_P: [2.38 1.76 1.86 0.53 0.68 0.05 0.18 0.02 7.46]\n",
      "Loss_Q: [0.98 1.2  0.47 0.69 0.04 0.18 0.02 0.   3.58] Loss_P: [2.38 1.73 1.84 0.47 0.67 0.05 0.18 0.02 7.33]\n",
      "Loss_Q: [0.94 1.15 0.43 0.69 0.05 0.2  0.03 0.   3.49] Loss_P: [2.43 1.7  1.85 0.55 0.69 0.07 0.17 0.02 7.49]\n",
      "Loss_Q: [0.98 1.15 0.46 0.71 0.06 0.21 0.01 0.   3.58] Loss_P: [2.41 1.78 1.8  0.55 0.67 0.04 0.17 0.02 7.44]\n",
      "Loss_Q: [0.91 1.09 0.46 0.68 0.04 0.19 0.01 0.   3.37] Loss_P: [2.39 1.69 1.78 0.55 0.7  0.05 0.17 0.02 7.36]\n",
      "Loss_Q: [0.92 1.18 0.45 0.69 0.07 0.18 0.02 0.   3.51] Loss_P: [2.38 1.71 1.73 0.51 0.74 0.05 0.16 0.02 7.3 ]\n",
      "Loss_Q: [0.98 1.07 0.54 0.75 0.06 0.21 0.02 0.   3.64] Loss_P: [2.4  1.77 1.66 0.56 0.68 0.06 0.18 0.02 7.33]\n",
      "Loss_Q: [0.92 1.   0.5  0.75 0.04 0.19 0.02 0.   3.43] Loss_P: [2.37 1.71 1.67 0.57 0.68 0.06 0.17 0.03 7.26]\n",
      "Loss_Q: [0.99 1.02 0.5  0.73 0.06 0.17 0.03 0.   3.49] Loss_P: [2.35 1.78 1.62 0.54 0.75 0.06 0.19 0.02 7.31]\n",
      "Loss_Q: [0.93 1.07 0.5  0.68 0.04 0.18 0.01 0.   3.42] Loss_P: [2.41 1.72 1.61 0.53 0.71 0.06 0.17 0.02 7.2 ]\n",
      "Loss_Q: [0.92 1.03 0.47 0.75 0.05 0.17 0.02 0.   3.42] Loss_P: [2.35 1.72 1.66 0.51 0.68 0.05 0.18 0.02 7.18]\n",
      "Loss_Q: [0.94 1.17 0.49 0.74 0.06 0.18 0.02 0.   3.59] Loss_P: [2.4  1.76 1.76 0.52 0.7  0.07 0.2  0.02 7.43]\n",
      "Loss_Q: [0.9  1.12 0.43 0.67 0.08 0.2  0.01 0.   3.4 ] Loss_P: [2.37 1.71 1.79 0.55 0.7  0.05 0.17 0.02 7.35]\n",
      "Loss_Q: [0.89 1.09 0.48 0.69 0.05 0.18 0.01 0.   3.4 ] Loss_P: [2.42 1.7  1.7  0.49 0.67 0.05 0.2  0.02 7.24]\n",
      "Loss_Q: [0.97 1.14 0.44 0.69 0.05 0.2  0.01 0.   3.49] Loss_P: [2.42 1.77 1.71 0.51 0.71 0.04 0.2  0.01 7.37]\n",
      "Loss_Q: [0.89 1.15 0.43 0.7  0.06 0.19 0.02 0.   3.45] Loss_P: [2.4  1.67 1.7  0.47 0.71 0.04 0.22 0.02 7.24]\n",
      "Loss_Q: [0.99 1.07 0.47 0.7  0.04 0.19 0.01 0.   3.48] Loss_P: [2.41 1.71 1.69 0.48 0.68 0.06 0.22 0.02 7.27]\n",
      "Loss_Q: [0.95 1.13 0.44 0.65 0.07 0.21 0.01 0.   3.46] Loss_P: [2.36 1.7  1.67 0.48 0.69 0.05 0.2  0.01 7.17]\n",
      "Loss_Q: [0.94 1.15 0.38 0.64 0.07 0.2  0.01 0.   3.39] Loss_P: [2.34 1.81 1.73 0.49 0.71 0.07 0.2  0.01 7.36]\n",
      "Loss_Q: [0.96 1.18 0.44 0.64 0.04 0.2  0.02 0.   3.49] Loss_P: [2.36 1.77 1.72 0.47 0.66 0.05 0.18 0.02 7.22]\n",
      "Loss_Q: [0.93 1.17 0.47 0.67 0.05 0.21 0.02 0.   3.53] Loss_P: [2.33 1.86 1.79 0.49 0.69 0.06 0.18 0.01 7.39]\n",
      "Loss_Q: [0.98 1.18 0.43 0.67 0.05 0.19 0.01 0.   3.51] Loss_P: [2.33 1.86 1.78 0.49 0.69 0.06 0.19 0.02 7.42]\n",
      "Loss_Q: [0.97 1.15 0.44 0.71 0.07 0.2  0.02 0.   3.56] Loss_P: [2.38 1.84 1.72 0.48 0.7  0.05 0.21 0.02 7.4 ]\n",
      "Loss_Q: [0.94 1.23 0.44 0.72 0.05 0.21 0.02 0.   3.61] Loss_P: [2.37 1.87 1.72 0.49 0.72 0.04 0.21 0.02 7.43]\n",
      "Loss_Q: [0.96 1.14 0.39 0.69 0.05 0.19 0.01 0.   3.44] Loss_P: [2.34 1.84 1.72 0.45 0.7  0.06 0.21 0.01 7.34]\n",
      "Loss_Q: [0.98 1.13 0.36 0.71 0.06 0.22 0.01 0.   3.46] Loss_P: [2.36 1.84 1.72 0.48 0.73 0.03 0.21 0.02 7.39]\n",
      "Loss_Q: [0.97 1.15 0.38 0.7  0.05 0.2  0.02 0.   3.47] Loss_P: [2.37 1.8  1.74 0.44 0.73 0.04 0.21 0.03 7.36]\n",
      "Loss_Q: [0.99 1.17 0.38 0.7  0.05 0.2  0.02 0.   3.52] Loss_P: [2.36 1.84 1.78 0.47 0.75 0.05 0.21 0.02 7.48]\n",
      "Loss_Q: [1.   1.17 0.38 0.69 0.04 0.2  0.02 0.   3.51] Loss_P: [2.38 1.83 1.76 0.45 0.73 0.06 0.2  0.01 7.41]\n",
      "Loss_Q: [0.98 1.15 0.39 0.7  0.06 0.19 0.01 0.   3.47] Loss_P: [2.32 1.84 1.74 0.41 0.77 0.05 0.19 0.02 7.35]\n",
      "Loss_Q: [0.94 1.16 0.36 0.73 0.09 0.21 0.02 0.   3.5 ] Loss_P: [2.36 1.8  1.72 0.43 0.75 0.08 0.21 0.01 7.36]\n",
      "Loss_Q: [0.96 1.16 0.39 0.72 0.05 0.23 0.01 0.   3.51] Loss_P: [2.39 1.81 1.73 0.46 0.74 0.05 0.24 0.01 7.44]\n",
      "Loss_Q: [0.91 1.17 0.38 0.66 0.06 0.22 0.01 0.   3.4 ] Loss_P: [2.42 1.78 1.73 0.43 0.74 0.05 0.23 0.01 7.4 ]\n",
      "Loss_Q: [0.89 1.23 0.41 0.68 0.05 0.21 0.02 0.   3.49] Loss_P: [2.39 1.82 1.78 0.5  0.76 0.06 0.21 0.02 7.55]\n",
      "Loss_Q: [0.88 1.14 0.43 0.72 0.06 0.21 0.02 0.   3.46] Loss_P: [2.38 1.79 1.7  0.48 0.71 0.05 0.22 0.02 7.35]\n",
      "Loss_Q: [0.87 1.11 0.42 0.75 0.04 0.2  0.01 0.   3.41] Loss_P: [2.36 1.82 1.71 0.46 0.74 0.04 0.24 0.02 7.39]\n",
      "Loss_Q: [0.92 1.14 0.4  0.68 0.04 0.21 0.02 0.   3.39] Loss_P: [2.35 1.76 1.69 0.46 0.72 0.06 0.21 0.02 7.27]\n",
      "Loss_Q: [0.94 1.07 0.41 0.7  0.04 0.23 0.02 0.   3.42] Loss_P: [2.36 1.86 1.7  0.44 0.72 0.06 0.23 0.02 7.39]\n",
      "Loss_Q: [0.97 1.06 0.41 0.72 0.08 0.23 0.01 0.   3.47] Loss_P: [2.36 1.78 1.71 0.43 0.73 0.06 0.2  0.02 7.28]\n",
      "Loss_Q: [0.94 1.13 0.4  0.68 0.05 0.23 0.01 0.   3.43] Loss_P: [2.37 1.79 1.66 0.47 0.7  0.04 0.22 0.02 7.27]\n",
      "Loss_Q: [0.94 1.04 0.38 0.71 0.05 0.22 0.02 0.   3.35] Loss_P: [2.36 1.8  1.61 0.51 0.73 0.06 0.21 0.02 7.31]\n",
      "Loss_Q: [0.95 1.09 0.4  0.72 0.05 0.24 0.02 0.   3.46] Loss_P: [2.37 1.81 1.65 0.42 0.71 0.04 0.24 0.01 7.24]\n",
      "Loss_Q: [0.91 1.08 0.38 0.68 0.04 0.19 0.01 0.   3.3 ] Loss_P: [2.39 1.8  1.63 0.46 0.74 0.05 0.23 0.01 7.31]\n",
      "Loss_Q: [0.92 1.11 0.39 0.74 0.04 0.22 0.03 0.   3.45] Loss_P: [2.37 1.8  1.58 0.48 0.73 0.05 0.22 0.02 7.24]\n",
      "Loss_Q: [0.97 1.1  0.39 0.7  0.05 0.22 0.01 0.   3.45] Loss_P: [2.39 1.84 1.66 0.45 0.74 0.06 0.19 0.03 7.35]\n",
      "Loss_Q: [0.97 1.15 0.42 0.68 0.05 0.2  0.01 0.   3.49] Loss_P: [2.36 1.85 1.71 0.46 0.73 0.05 0.19 0.02 7.37]\n",
      "Loss_Q: [0.97 1.19 0.43 0.76 0.05 0.2  0.01 0.   3.6 ] Loss_P: [2.4  1.85 1.68 0.43 0.71 0.04 0.2  0.02 7.32]\n",
      "Loss_Q: [0.98 1.2  0.44 0.7  0.07 0.19 0.01 0.   3.59] Loss_P: [2.37 1.78 1.78 0.51 0.74 0.06 0.25 0.01 7.51]\n",
      "Loss_Q: [0.94 1.23 0.44 0.72 0.04 0.23 0.01 0.   3.62] Loss_P: [2.35 1.78 1.78 0.5  0.76 0.08 0.23 0.02 7.5 ]\n",
      "Loss_Q: [0.96 1.14 0.4  0.72 0.04 0.18 0.01 0.   3.46] Loss_P: [2.37 1.74 1.72 0.48 0.72 0.06 0.22 0.01 7.31]\n",
      "Loss_Q: [0.89 1.12 0.45 0.73 0.07 0.23 0.02 0.   3.51] Loss_P: [2.4  1.81 1.72 0.46 0.76 0.06 0.24 0.02 7.46]\n",
      "Loss_Q: [1.   1.11 0.46 0.72 0.04 0.24 0.01 0.   3.58] Loss_P: [2.33 1.79 1.75 0.5  0.71 0.07 0.24 0.01 7.4 ]\n",
      "Loss_Q: [0.97 1.23 0.45 0.7  0.06 0.24 0.01 0.   3.66] Loss_P: [2.39 1.83 1.75 0.5  0.69 0.05 0.25 0.02 7.49]\n",
      "Loss_Q: [0.92 1.15 0.45 0.67 0.04 0.25 0.02 0.   3.49] Loss_P: [2.35 1.84 1.69 0.54 0.67 0.05 0.25 0.01 7.41]\n",
      "Loss_Q: [1.   1.2  0.43 0.69 0.04 0.25 0.01 0.   3.62] Loss_P: [2.33 1.8  1.74 0.5  0.7  0.06 0.26 0.01 7.4 ]\n",
      "Loss_Q: [0.92 1.21 0.41 0.67 0.05 0.22 0.03 0.   3.51] Loss_P: [2.34 1.8  1.71 0.54 0.71 0.07 0.23 0.02 7.41]\n",
      "Loss_Q: [1.01 1.17 0.4  0.7  0.04 0.23 0.01 0.   3.56] Loss_P: [2.34 1.83 1.78 0.55 0.75 0.05 0.21 0.01 7.52]\n",
      "Loss_Q: [0.96 1.12 0.4  0.71 0.06 0.26 0.03 0.   3.53] Loss_P: [2.33 1.82 1.68 0.49 0.73 0.05 0.24 0.01 7.35]\n",
      "Loss_Q: [0.88 1.19 0.4  0.73 0.07 0.26 0.02 0.   3.54] Loss_P: [2.33 1.85 1.7  0.48 0.76 0.05 0.23 0.01 7.4 ]\n",
      "Loss_Q: [0.96 1.24 0.44 0.71 0.05 0.23 0.01 0.   3.63] Loss_P: [2.37 1.91 1.74 0.49 0.73 0.06 0.27 0.02 7.58]\n",
      "Loss_Q: [0.95 1.17 0.39 0.75 0.06 0.24 0.01 0.   3.57] Loss_P: [2.37 1.87 1.76 0.47 0.7  0.06 0.22 0.01 7.46]\n",
      "Loss_Q: [0.94 1.24 0.45 0.72 0.06 0.24 0.01 0.   3.66] Loss_P: [2.36 1.77 1.74 0.45 0.71 0.05 0.24 0.03 7.35]\n",
      "Loss_Q: [0.99 1.21 0.4  0.66 0.06 0.26 0.01 0.   3.59] Loss_P: [2.35 1.88 1.72 0.47 0.7  0.04 0.28 0.01 7.44]\n",
      "Loss_Q: [0.95 1.19 0.41 0.69 0.09 0.24 0.01 0.   3.57] Loss_P: [2.4  1.84 1.74 0.43 0.71 0.04 0.25 0.02 7.43]\n",
      "Loss_Q: [0.97 1.21 0.36 0.68 0.05 0.24 0.01 0.   3.52] Loss_P: [2.35 1.81 1.71 0.42 0.68 0.07 0.24 0.02 7.3 ]\n",
      "Loss_Q: [0.95 1.18 0.39 0.69 0.05 0.27 0.02 0.   3.55] Loss_P: [2.44 1.79 1.74 0.48 0.72 0.04 0.27 0.02 7.51]\n",
      "Loss_Q: [0.82 1.21 0.42 0.71 0.07 0.21 0.01 0.   3.45] Loss_P: [2.39 1.87 1.77 0.52 0.69 0.05 0.25 0.02 7.56]\n",
      "Loss_Q: [0.94 1.16 0.45 0.7  0.05 0.26 0.01 0.   3.57] Loss_P: [2.36 1.88 1.69 0.47 0.71 0.06 0.25 0.01 7.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.95 1.24 0.42 0.69 0.04 0.27 0.02 0.   3.63] Loss_P: [2.32 1.86 1.73 0.5  0.68 0.05 0.24 0.01 7.39]\n",
      "Loss_Q: [1.02 1.2  0.4  0.68 0.04 0.27 0.02 0.   3.63] Loss_P: [2.39 1.84 1.8  0.54 0.7  0.06 0.24 0.02 7.6 ]\n",
      "Loss_Q: [0.95 1.22 0.41 0.71 0.08 0.25 0.01 0.   3.63] Loss_P: [2.36 1.85 1.69 0.47 0.71 0.06 0.25 0.02 7.41]\n",
      "Loss_Q: [0.95 1.2  0.43 0.69 0.04 0.27 0.02 0.   3.59] Loss_P: [2.32 1.91 1.69 0.49 0.68 0.05 0.3  0.02 7.46]\n",
      "Loss_Q: [0.87 1.27 0.41 0.74 0.04 0.26 0.02 0.   3.59] Loss_P: [2.38 1.83 1.73 0.44 0.7  0.04 0.27 0.02 7.41]\n",
      "Loss_Q: [0.93 1.22 0.44 0.73 0.03 0.26 0.01 0.   3.61] Loss_P: [2.37 1.78 1.66 0.49 0.74 0.05 0.27 0.02 7.38]\n",
      "Loss_Q: [0.9  1.21 0.44 0.72 0.06 0.3  0.03 0.   3.66] Loss_P: [2.36 1.81 1.69 0.47 0.73 0.04 0.29 0.02 7.41]\n",
      "Loss_Q: [1.01 1.15 0.4  0.78 0.07 0.27 0.02 0.   3.7 ] Loss_P: [2.35 1.84 1.67 0.5  0.79 0.05 0.28 0.02 7.51]\n",
      "Loss_Q: [0.92 1.21 0.49 0.76 0.06 0.29 0.02 0.   3.75] Loss_P: [2.39 1.79 1.69 0.53 0.78 0.06 0.31 0.01 7.57]\n",
      "Loss_Q: [0.95 1.24 0.46 0.7  0.07 0.28 0.02 0.   3.72] Loss_P: [2.39 1.89 1.69 0.54 0.73 0.07 0.28 0.01 7.6 ]\n",
      "Loss_Q: [0.92 1.19 0.5  0.76 0.05 0.29 0.01 0.   3.72] Loss_P: [2.35 1.87 1.62 0.48 0.76 0.05 0.29 0.01 7.43]\n",
      "Loss_Q: [1.07 1.22 0.44 0.73 0.05 0.26 0.02 0.   3.79] Loss_P: [2.39 1.86 1.7  0.5  0.75 0.04 0.28 0.03 7.55]\n",
      "Loss_Q: [0.95 1.22 0.47 0.8  0.06 0.29 0.02 0.   3.82] Loss_P: [2.34 1.83 1.66 0.6  0.82 0.05 0.28 0.01 7.59]\n",
      "Loss_Q: [0.91 1.18 0.48 0.76 0.05 0.29 0.02 0.   3.69] Loss_P: [2.38 1.93 1.61 0.5  0.79 0.06 0.28 0.02 7.55]\n",
      "Loss_Q: [0.84 1.13 0.41 0.76 0.04 0.28 0.01 0.   3.48] Loss_P: [2.41 1.83 1.63 0.47 0.75 0.05 0.27 0.01 7.41]\n",
      "Loss_Q: [0.93 1.13 0.42 0.74 0.06 0.27 0.01 0.   3.55] Loss_P: [2.38 1.86 1.64 0.48 0.76 0.05 0.25 0.02 7.44]\n",
      "Loss_Q: [1.03 1.11 0.39 0.73 0.05 0.28 0.02 0.   3.59] Loss_P: [2.36 1.92 1.64 0.48 0.75 0.07 0.27 0.02 7.5 ]\n",
      "Loss_Q: [0.94 1.18 0.39 0.76 0.03 0.27 0.01 0.   3.57] Loss_P: [2.37 1.88 1.61 0.43 0.77 0.04 0.25 0.02 7.37]\n",
      "Loss_Q: [0.89 1.2  0.32 0.76 0.06 0.27 0.02 0.   3.52] Loss_P: [2.42 1.96 1.54 0.45 0.75 0.06 0.3  0.02 7.49]\n",
      "Loss_Q: [0.95 1.14 0.35 0.74 0.05 0.27 0.01 0.   3.52] Loss_P: [2.36 1.95 1.56 0.46 0.79 0.07 0.29 0.02 7.5 ]\n",
      "Loss_Q: [1.03 1.16 0.4  0.72 0.05 0.27 0.03 0.   3.66] Loss_P: [2.41 1.9  1.57 0.37 0.73 0.05 0.25 0.01 7.29]\n",
      "Loss_Q: [0.96 1.15 0.37 0.74 0.06 0.26 0.01 0.   3.55] Loss_P: [2.42 1.85 1.61 0.38 0.73 0.06 0.24 0.02 7.3 ]\n",
      "Loss_Q: [0.99 1.06 0.38 0.74 0.05 0.26 0.03 0.   3.5 ] Loss_P: [2.4  1.89 1.49 0.37 0.75 0.06 0.23 0.01 7.2 ]\n",
      "Loss_Q: [1.01 1.11 0.37 0.77 0.07 0.25 0.01 0.   3.6 ] Loss_P: [2.38 1.89 1.62 0.4  0.76 0.04 0.24 0.02 7.35]\n",
      "Loss_Q: [1.01 1.12 0.38 0.75 0.08 0.28 0.01 0.   3.65] Loss_P: [2.39 1.95 1.55 0.4  0.72 0.06 0.25 0.01 7.34]\n",
      "Loss_Q: [0.87 1.13 0.34 0.74 0.05 0.24 0.02 0.   3.38] Loss_P: [2.41 1.91 1.52 0.38 0.76 0.07 0.27 0.02 7.35]\n",
      "Loss_Q: [0.94 1.09 0.32 0.74 0.05 0.26 0.01 0.   3.42] Loss_P: [2.4  1.87 1.54 0.4  0.75 0.06 0.26 0.02 7.31]\n",
      "Loss_Q: [0.89 1.13 0.37 0.68 0.07 0.22 0.02 0.   3.38] Loss_P: [2.36 1.88 1.6  0.41 0.73 0.07 0.22 0.02 7.27]\n",
      "Loss_Q: [0.9  1.12 0.34 0.68 0.04 0.24 0.01 0.   3.34] Loss_P: [2.44 1.86 1.57 0.43 0.72 0.07 0.24 0.02 7.35]\n",
      "Loss_Q: [0.96 1.06 0.34 0.67 0.05 0.23 0.02 0.   3.33] Loss_P: [2.43 1.93 1.59 0.44 0.7  0.06 0.22 0.01 7.36]\n",
      "Loss_Q: [0.94 1.1  0.4  0.74 0.05 0.23 0.02 0.   3.49] Loss_P: [2.38 1.94 1.51 0.39 0.75 0.06 0.25 0.01 7.3 ]\n",
      "Loss_Q: [0.92 1.09 0.39 0.71 0.05 0.2  0.02 0.   3.37] Loss_P: [2.38 1.98 1.55 0.41 0.75 0.07 0.19 0.02 7.34]\n",
      "Loss_Q: [0.94 1.1  0.4  0.72 0.07 0.19 0.01 0.   3.43] Loss_P: [2.45 1.91 1.55 0.42 0.71 0.07 0.2  0.02 7.33]\n",
      "Loss_Q: [0.98 1.09 0.38 0.7  0.06 0.22 0.02 0.   3.44] Loss_P: [2.39 1.9  1.52 0.43 0.74 0.07 0.22 0.01 7.27]\n",
      "Loss_Q: [0.98 1.11 0.38 0.73 0.05 0.18 0.02 0.   3.43] Loss_P: [2.34 1.9  1.58 0.43 0.74 0.05 0.21 0.01 7.26]\n",
      "Loss_Q: [0.97 1.14 0.41 0.7  0.05 0.21 0.02 0.   3.51] Loss_P: [2.41 1.92 1.52 0.47 0.7  0.06 0.18 0.02 7.27]\n",
      "Loss_Q: [0.97 1.11 0.35 0.72 0.06 0.2  0.02 0.   3.42] Loss_P: [2.44 1.91 1.54 0.45 0.71 0.05 0.17 0.01 7.29]\n",
      "Loss_Q: [0.93 1.12 0.38 0.71 0.06 0.21 0.01 0.   3.42] Loss_P: [2.4  2.01 1.52 0.45 0.71 0.05 0.21 0.02 7.36]\n",
      "Loss_Q: [0.97 1.14 0.34 0.7  0.06 0.21 0.01 0.   3.43] Loss_P: [2.4  1.9  1.53 0.43 0.74 0.06 0.21 0.02 7.28]\n",
      "Loss_Q: [0.9  1.05 0.36 0.71 0.05 0.19 0.01 0.   3.27] Loss_P: [2.48 1.91 1.52 0.43 0.68 0.07 0.23 0.01 7.32]\n",
      "Loss_Q: [0.95 1.07 0.37 0.68 0.06 0.23 0.02 0.   3.38] Loss_P: [2.33 1.87 1.54 0.45 0.72 0.07 0.18 0.02 7.18]\n",
      "Loss_Q: [0.97 1.07 0.37 0.66 0.05 0.19 0.01 0.   3.33] Loss_P: [2.43 1.89 1.53 0.43 0.68 0.06 0.2  0.01 7.22]\n",
      "Loss_Q: [0.98 1.03 0.42 0.69 0.07 0.19 0.01 0.   3.39] Loss_P: [2.45 1.87 1.46 0.45 0.68 0.06 0.23 0.02 7.23]\n",
      "Loss_Q: [0.98 1.11 0.42 0.68 0.07 0.19 0.01 0.   3.46] Loss_P: [2.36 1.96 1.51 0.42 0.72 0.08 0.2  0.02 7.27]\n",
      "Loss_Q: [1.   0.98 0.32 0.66 0.07 0.2  0.03 0.   3.26] Loss_P: [2.43 1.94 1.52 0.4  0.72 0.07 0.21 0.01 7.3 ]\n",
      "Loss_Q: [0.99 1.03 0.37 0.69 0.06 0.2  0.01 0.   3.36] Loss_P: [2.42 1.97 1.47 0.39 0.7  0.07 0.22 0.01 7.25]\n",
      "Loss_Q: [0.99 0.96 0.39 0.67 0.06 0.22 0.01 0.   3.29] Loss_P: [2.33 1.94 1.47 0.4  0.72 0.06 0.21 0.01 7.14]\n",
      "Loss_Q: [1.02 1.05 0.38 0.7  0.07 0.22 0.02 0.   3.46] Loss_P: [2.39 1.93 1.48 0.45 0.73 0.09 0.19 0.01 7.27]\n",
      "Loss_Q: [1.01 0.99 0.42 0.69 0.05 0.19 0.02 0.   3.37] Loss_P: [2.4  1.93 1.45 0.49 0.73 0.08 0.23 0.02 7.32]\n",
      "Loss_Q: [1.02 1.03 0.4  0.72 0.07 0.22 0.01 0.   3.47] Loss_P: [2.39 1.96 1.45 0.45 0.69 0.05 0.24 0.01 7.23]\n",
      "Loss_Q: [0.95 1.02 0.39 0.67 0.06 0.22 0.03 0.   3.32] Loss_P: [2.37 1.96 1.47 0.42 0.74 0.05 0.2  0.02 7.22]\n",
      "Loss_Q: [1.03 1.07 0.42 0.68 0.08 0.23 0.02 0.   3.51] Loss_P: [2.37 1.93 1.55 0.44 0.7  0.07 0.22 0.01 7.3 ]\n",
      "Loss_Q: [1.09 1.12 0.44 0.68 0.07 0.23 0.03 0.   3.65] Loss_P: [2.43 1.93 1.54 0.51 0.73 0.05 0.22 0.03 7.44]\n",
      "Loss_Q: [1.03 1.04 0.45 0.67 0.06 0.21 0.01 0.   3.48] Loss_P: [2.4  1.91 1.51 0.5  0.69 0.06 0.22 0.02 7.31]\n",
      "Loss_Q: [0.98 1.05 0.48 0.72 0.08 0.25 0.02 0.   3.56] Loss_P: [2.41 1.98 1.49 0.5  0.7  0.05 0.2  0.02 7.34]\n",
      "Loss_Q: [1.06 1.14 0.41 0.71 0.07 0.25 0.01 0.   3.66] Loss_P: [2.4  2.   1.59 0.46 0.69 0.05 0.21 0.03 7.44]\n",
      "Loss_Q: [1.06 1.07 0.44 0.74 0.06 0.23 0.02 0.   3.61] Loss_P: [2.34 1.97 1.5  0.47 0.69 0.06 0.23 0.02 7.27]\n",
      "Loss_Q: [1.02 1.1  0.45 0.66 0.05 0.22 0.01 0.   3.5 ] Loss_P: [2.35 1.99 1.53 0.44 0.71 0.06 0.23 0.01 7.33]\n",
      "Loss_Q: [1.03 1.04 0.47 0.7  0.06 0.22 0.01 0.   3.52] Loss_P: [2.44 2.   1.53 0.41 0.7  0.05 0.23 0.01 7.38]\n",
      "Loss_Q: [1.07 1.08 0.38 0.66 0.06 0.23 0.02 0.   3.5 ] Loss_P: [2.33 1.95 1.54 0.49 0.69 0.04 0.24 0.02 7.3 ]\n",
      "Loss_Q: [1.02 1.08 0.42 0.65 0.07 0.23 0.01 0.   3.47] Loss_P: [2.35 2.01 1.52 0.51 0.67 0.08 0.24 0.02 7.38]\n",
      "Loss_Q: [1.1  1.09 0.41 0.65 0.05 0.22 0.01 0.   3.53] Loss_P: [2.34 2.02 1.61 0.5  0.68 0.05 0.21 0.01 7.42]\n",
      "Loss_Q: [1.03 1.13 0.45 0.64 0.05 0.24 0.02 0.   3.55] Loss_P: [2.35 1.94 1.59 0.48 0.61 0.05 0.22 0.02 7.26]\n",
      "Loss_Q: [1.05 1.12 0.47 0.68 0.04 0.22 0.01 0.   3.58] Loss_P: [2.4  1.98 1.56 0.48 0.68 0.07 0.21 0.01 7.37]\n",
      "Loss_Q: [0.98 1.14 0.48 0.63 0.07 0.23 0.02 0.   3.55] Loss_P: [2.34 1.94 1.59 0.54 0.66 0.06 0.23 0.02 7.39]\n",
      "Loss_Q: [0.99 1.13 0.45 0.62 0.06 0.24 0.01 0.   3.5 ] Loss_P: [2.36 1.9  1.58 0.5  0.64 0.06 0.25 0.02 7.31]\n",
      "Loss_Q: [1.02 1.11 0.46 0.6  0.04 0.23 0.01 0.   3.47] Loss_P: [2.3  1.95 1.6  0.48 0.65 0.06 0.26 0.01 7.31]\n",
      "Loss_Q: [1.04 1.06 0.48 0.62 0.05 0.22 0.01 0.   3.48] Loss_P: [2.38 1.98 1.56 0.57 0.69 0.07 0.25 0.01 7.51]\n",
      "Loss_Q: [1.01 1.07 0.41 0.63 0.06 0.22 0.01 0.   3.41] Loss_P: [2.39 1.97 1.59 0.46 0.62 0.06 0.23 0.02 7.33]\n",
      "Loss_Q: [1.07 1.05 0.47 0.66 0.06 0.21 0.01 0.   3.53] Loss_P: [2.33 1.98 1.56 0.56 0.67 0.07 0.26 0.02 7.45]\n",
      "Loss_Q: [1.02 1.15 0.48 0.65 0.06 0.22 0.01 0.   3.59] Loss_P: [2.34 2.   1.58 0.51 0.67 0.05 0.24 0.01 7.4 ]\n",
      "Loss_Q: [1.04 1.15 0.49 0.6  0.04 0.22 0.01 0.   3.55] Loss_P: [2.34 2.04 1.62 0.5  0.63 0.04 0.23 0.01 7.41]\n",
      "Loss_Q: [1.03 1.12 0.42 0.57 0.04 0.21 0.02 0.   3.4 ] Loss_P: [2.29 2.   1.68 0.49 0.61 0.05 0.2  0.01 7.32]\n",
      "Loss_Q: [1.03 1.12 0.46 0.59 0.06 0.23 0.02 0.   3.5 ] Loss_P: [2.34 1.98 1.62 0.5  0.62 0.07 0.21 0.02 7.36]\n",
      "Loss_Q: [1.05 1.09 0.41 0.6  0.05 0.19 0.01 0.   3.41] Loss_P: [2.4  1.92 1.65 0.46 0.59 0.05 0.26 0.02 7.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.99 1.11 0.47 0.63 0.04 0.25 0.01 0.   3.5 ] Loss_P: [2.37 1.95 1.66 0.55 0.64 0.04 0.27 0.01 7.48]\n",
      "Loss_Q: [0.99 1.11 0.45 0.64 0.04 0.26 0.02 0.   3.5 ] Loss_P: [2.37 1.94 1.72 0.52 0.64 0.07 0.23 0.02 7.52]\n",
      "Loss_Q: [0.99 1.13 0.49 0.61 0.04 0.26 0.02 0.   3.53] Loss_P: [2.37 1.84 1.65 0.54 0.61 0.06 0.27 0.01 7.35]\n",
      "Loss_Q: [0.97 1.13 0.49 0.6  0.05 0.3  0.01 0.   3.55] Loss_P: [2.34 1.88 1.72 0.56 0.65 0.06 0.28 0.01 7.51]\n",
      "Loss_Q: [1.01 1.14 0.52 0.6  0.06 0.28 0.02 0.   3.62] Loss_P: [2.35 1.85 1.7  0.54 0.6  0.04 0.3  0.01 7.39]\n",
      "Loss_Q: [0.93 1.18 0.52 0.62 0.07 0.27 0.01 0.   3.6 ] Loss_P: [2.38 1.89 1.71 0.62 0.67 0.05 0.25 0.02 7.59]\n",
      "Loss_Q: [0.97 1.15 0.5  0.6  0.06 0.26 0.02 0.   3.56] Loss_P: [2.34 1.86 1.72 0.55 0.61 0.04 0.27 0.01 7.4 ]\n",
      "Loss_Q: [1.03 1.15 0.5  0.56 0.06 0.23 0.01 0.   3.56] Loss_P: [2.38 1.81 1.67 0.56 0.66 0.06 0.29 0.03 7.45]\n",
      "Loss_Q: [0.99 1.12 0.5  0.6  0.04 0.27 0.02 0.   3.54] Loss_P: [2.32 1.9  1.65 0.53 0.64 0.05 0.29 0.01 7.39]\n",
      "Loss_Q: [1.02 1.11 0.42 0.64 0.07 0.27 0.02 0.   3.54] Loss_P: [2.3  1.91 1.76 0.55 0.62 0.05 0.27 0.01 7.48]\n",
      "Loss_Q: [1.05 1.23 0.52 0.64 0.05 0.26 0.02 0.   3.76] Loss_P: [2.37 1.88 1.76 0.56 0.66 0.05 0.28 0.02 7.57]\n",
      "Loss_Q: [0.87 1.15 0.52 0.63 0.05 0.29 0.02 0.   3.53] Loss_P: [2.36 1.89 1.77 0.55 0.67 0.05 0.29 0.02 7.58]\n",
      "Loss_Q: [0.96 1.12 0.52 0.63 0.06 0.25 0.01 0.   3.55] Loss_P: [2.37 1.83 1.67 0.56 0.7  0.05 0.28 0.01 7.47]\n",
      "Loss_Q: [0.96 1.16 0.45 0.68 0.06 0.26 0.01 0.   3.57] Loss_P: [2.39 1.86 1.74 0.55 0.68 0.06 0.22 0.02 7.52]\n",
      "Loss_Q: [1.   1.15 0.47 0.64 0.05 0.27 0.01 0.   3.58] Loss_P: [2.37 1.84 1.73 0.54 0.67 0.06 0.26 0.02 7.49]\n",
      "Loss_Q: [1.08 1.11 0.49 0.64 0.05 0.26 0.01 0.   3.64] Loss_P: [2.31 1.91 1.68 0.5  0.69 0.07 0.29 0.02 7.46]\n",
      "Loss_Q: [1.04 1.12 0.46 0.64 0.05 0.26 0.01 0.   3.58] Loss_P: [2.36 1.92 1.72 0.5  0.69 0.04 0.27 0.02 7.51]\n",
      "Loss_Q: [1.04 1.1  0.49 0.63 0.06 0.23 0.01 0.   3.56] Loss_P: [2.35 1.86 1.7  0.53 0.65 0.06 0.25 0.01 7.4 ]\n",
      "Loss_Q: [0.98 1.09 0.43 0.6  0.04 0.22 0.02 0.   3.38] Loss_P: [2.31 1.87 1.75 0.56 0.65 0.06 0.23 0.01 7.45]\n",
      "Loss_Q: [1.07 1.16 0.49 0.61 0.04 0.22 0.02 0.   3.61] Loss_P: [2.35 1.88 1.76 0.53 0.7  0.05 0.22 0.02 7.5 ]\n",
      "Loss_Q: [0.96 1.18 0.49 0.65 0.04 0.21 0.01 0.   3.53] Loss_P: [2.35 1.83 1.73 0.49 0.66 0.05 0.22 0.02 7.34]\n",
      "Loss_Q: [0.98 1.15 0.51 0.67 0.03 0.24 0.02 0.   3.6 ] Loss_P: [2.36 1.84 1.72 0.49 0.69 0.04 0.19 0.01 7.34]\n",
      "Loss_Q: [0.99 1.15 0.46 0.7  0.07 0.23 0.01 0.   3.62] Loss_P: [2.33 1.85 1.74 0.54 0.67 0.06 0.21 0.01 7.4 ]\n",
      "Loss_Q: [1.07 1.23 0.48 0.62 0.03 0.24 0.02 0.   3.68] Loss_P: [2.32 1.9  1.73 0.49 0.67 0.05 0.24 0.02 7.42]\n",
      "Loss_Q: [1.03 1.1  0.5  0.64 0.05 0.24 0.02 0.   3.57] Loss_P: [2.33 1.83 1.74 0.57 0.67 0.04 0.22 0.02 7.42]\n",
      "Loss_Q: [0.94 1.12 0.53 0.64 0.05 0.21 0.01 0.   3.5 ] Loss_P: [2.37 1.77 1.67 0.56 0.66 0.05 0.21 0.01 7.3 ]\n",
      "Loss_Q: [0.87 1.06 0.44 0.68 0.04 0.23 0.02 0.   3.33] Loss_P: [2.35 1.82 1.69 0.54 0.66 0.04 0.24 0.02 7.36]\n",
      "Loss_Q: [0.9  1.11 0.5  0.7  0.05 0.22 0.01 0.   3.5 ] Loss_P: [2.35 1.8  1.74 0.5  0.71 0.05 0.25 0.01 7.42]\n",
      "Loss_Q: [1.06 1.16 0.54 0.72 0.05 0.27 0.03 0.   3.83] Loss_P: [2.31 1.88 1.67 0.59 0.74 0.06 0.23 0.02 7.5 ]\n",
      "Loss_Q: [1.02 1.15 0.5  0.74 0.05 0.23 0.01 0.   3.71] Loss_P: [2.38 1.83 1.74 0.52 0.73 0.07 0.24 0.02 7.54]\n",
      "Loss_Q: [0.95 1.14 0.45 0.69 0.05 0.26 0.01 0.   3.56] Loss_P: [2.34 1.78 1.72 0.5  0.72 0.05 0.23 0.03 7.37]\n",
      "Loss_Q: [1.   1.09 0.43 0.7  0.05 0.23 0.01 0.   3.5 ] Loss_P: [2.4  1.85 1.74 0.49 0.73 0.05 0.22 0.01 7.5 ]\n",
      "Loss_Q: [0.99 1.15 0.47 0.69 0.06 0.22 0.01 0.   3.59] Loss_P: [2.31 1.88 1.74 0.53 0.71 0.07 0.23 0.02 7.48]\n",
      "Loss_Q: [1.04 1.08 0.44 0.72 0.04 0.21 0.02 0.   3.55] Loss_P: [2.28 1.83 1.68 0.5  0.76 0.05 0.22 0.01 7.33]\n",
      "Loss_Q: [0.99 1.18 0.43 0.76 0.04 0.2  0.01 0.   3.62] Loss_P: [2.43 1.86 1.7  0.51 0.78 0.05 0.19 0.02 7.52]\n",
      "Loss_Q: [0.98 1.16 0.46 0.68 0.03 0.22 0.02 0.   3.56] Loss_P: [2.3  1.9  1.79 0.48 0.72 0.03 0.23 0.01 7.46]\n",
      "Loss_Q: [0.96 1.15 0.46 0.77 0.06 0.21 0.02 0.   3.62] Loss_P: [2.36 1.86 1.73 0.48 0.78 0.05 0.22 0.01 7.48]\n",
      "Loss_Q: [1.01 1.21 0.46 0.77 0.05 0.2  0.01 0.   3.72] Loss_P: [2.34 1.87 1.82 0.56 0.78 0.07 0.23 0.01 7.69]\n",
      "Loss_Q: [1.09 1.19 0.52 0.74 0.06 0.25 0.01 0.   3.85] Loss_P: [2.37 1.84 1.8  0.55 0.76 0.05 0.19 0.01 7.56]\n",
      "Loss_Q: [1.02 1.18 0.49 0.73 0.06 0.22 0.03 0.   3.72] Loss_P: [2.4  1.81 1.75 0.52 0.76 0.06 0.21 0.02 7.52]\n",
      "Loss_Q: [1.03 1.17 0.44 0.77 0.04 0.23 0.02 0.   3.69] Loss_P: [2.35 1.84 1.76 0.49 0.78 0.06 0.21 0.02 7.51]\n",
      "Loss_Q: [0.96 1.14 0.43 0.73 0.05 0.24 0.02 0.   3.58] Loss_P: [2.38 1.81 1.76 0.54 0.79 0.08 0.22 0.01 7.59]\n",
      "Loss_Q: [1.01 1.14 0.45 0.76 0.04 0.24 0.02 0.   3.65] Loss_P: [2.34 1.82 1.74 0.47 0.76 0.05 0.23 0.02 7.43]\n",
      "Loss_Q: [1.   1.11 0.51 0.75 0.05 0.24 0.02 0.   3.68] Loss_P: [2.31 1.83 1.78 0.53 0.76 0.05 0.25 0.01 7.53]\n",
      "Loss_Q: [0.97 1.09 0.47 0.79 0.06 0.21 0.02 0.   3.62] Loss_P: [2.36 1.82 1.65 0.45 0.75 0.04 0.24 0.01 7.32]\n",
      "Loss_Q: [1.02 1.13 0.43 0.78 0.05 0.24 0.02 0.   3.67] Loss_P: [2.38 1.81 1.7  0.53 0.77 0.06 0.24 0.01 7.5 ]\n",
      "Loss_Q: [1.04 1.13 0.45 0.72 0.05 0.24 0.01 0.   3.65] Loss_P: [2.37 1.85 1.68 0.52 0.78 0.07 0.25 0.02 7.54]\n",
      "Loss_Q: [1.06 1.17 0.47 0.76 0.03 0.23 0.02 0.   3.74] Loss_P: [2.4  1.83 1.72 0.44 0.79 0.08 0.25 0.02 7.52]\n",
      "Loss_Q: [1.02 1.08 0.42 0.78 0.06 0.2  0.02 0.   3.58] Loss_P: [2.35 1.85 1.71 0.5  0.79 0.05 0.25 0.01 7.5 ]\n",
      "Loss_Q: [0.98 1.11 0.46 0.77 0.06 0.21 0.02 0.   3.61] Loss_P: [2.41 1.87 1.66 0.49 0.78 0.07 0.23 0.01 7.52]\n",
      "Loss_Q: [0.96 1.1  0.4  0.74 0.08 0.22 0.01 0.   3.51] Loss_P: [2.35 1.8  1.62 0.47 0.75 0.05 0.23 0.01 7.27]\n",
      "Loss_Q: [1.03 1.15 0.42 0.75 0.05 0.27 0.02 0.   3.69] Loss_P: [2.29 1.85 1.73 0.49 0.73 0.05 0.24 0.01 7.39]\n",
      "Loss_Q: [1.02 1.18 0.39 0.73 0.04 0.25 0.03 0.   3.64] Loss_P: [2.37 1.85 1.71 0.47 0.73 0.05 0.22 0.01 7.42]\n",
      "Loss_Q: [0.96 1.16 0.41 0.75 0.07 0.25 0.02 0.   3.61] Loss_P: [2.34 1.91 1.68 0.45 0.75 0.06 0.27 0.03 7.49]\n",
      "Loss_Q: [1.05 1.21 0.4  0.72 0.05 0.24 0.02 0.   3.69] Loss_P: [2.39 1.86 1.73 0.51 0.74 0.06 0.25 0.02 7.55]\n",
      "Loss_Q: [0.93 1.17 0.44 0.76 0.06 0.26 0.02 0.   3.63] Loss_P: [2.38 1.94 1.63 0.44 0.77 0.07 0.27 0.03 7.53]\n",
      "Loss_Q: [1.07 1.12 0.45 0.76 0.04 0.26 0.02 0.   3.71] Loss_P: [2.4  1.83 1.69 0.49 0.75 0.03 0.29 0.02 7.49]\n",
      "Loss_Q: [0.98 1.15 0.43 0.77 0.06 0.3  0.03 0.   3.72] Loss_P: [2.36 1.83 1.73 0.45 0.73 0.06 0.27 0.01 7.44]\n",
      "Loss_Q: [0.93 1.14 0.46 0.74 0.05 0.27 0.02 0.   3.61] Loss_P: [2.42 1.86 1.63 0.41 0.73 0.07 0.25 0.02 7.38]\n",
      "Loss_Q: [1.   1.05 0.43 0.76 0.06 0.27 0.02 0.   3.58] Loss_P: [2.35 1.91 1.72 0.5  0.78 0.06 0.27 0.02 7.59]\n",
      "Loss_Q: [0.96 1.16 0.47 0.72 0.06 0.25 0.01 0.   3.63] Loss_P: [2.35 1.93 1.66 0.49 0.74 0.06 0.25 0.02 7.49]\n",
      "Loss_Q: [0.99 1.09 0.44 0.77 0.06 0.26 0.02 0.   3.64] Loss_P: [2.33 1.95 1.7  0.48 0.72 0.04 0.25 0.02 7.48]\n",
      "Loss_Q: [1.   1.19 0.49 0.74 0.06 0.25 0.03 0.   3.75] Loss_P: [2.33 1.89 1.7  0.49 0.74 0.06 0.24 0.01 7.46]\n",
      "Loss_Q: [1.04 1.16 0.52 0.71 0.06 0.22 0.01 0.   3.71] Loss_P: [2.31 1.98 1.69 0.56 0.75 0.06 0.22 0.01 7.58]\n",
      "Loss_Q: [1.01 1.2  0.47 0.72 0.07 0.26 0.02 0.   3.75] Loss_P: [2.38 1.92 1.67 0.48 0.73 0.06 0.24 0.02 7.5 ]\n",
      "Loss_Q: [1.   1.17 0.43 0.72 0.06 0.23 0.02 0.   3.62] Loss_P: [2.31 1.94 1.71 0.49 0.74 0.04 0.23 0.01 7.47]\n",
      "Loss_Q: [1.04 1.16 0.45 0.72 0.04 0.22 0.02 0.   3.64] Loss_P: [2.34 1.97 1.73 0.46 0.75 0.08 0.24 0.01 7.57]\n",
      "Loss_Q: [0.95 1.2  0.43 0.74 0.06 0.21 0.01 0.   3.59] Loss_P: [2.36 1.87 1.67 0.52 0.73 0.05 0.23 0.02 7.46]\n",
      "Loss_Q: [1.1  1.14 0.49 0.73 0.07 0.24 0.01 0.   3.77] Loss_P: [2.35 1.91 1.6  0.45 0.75 0.06 0.25 0.01 7.37]\n",
      "Loss_Q: [1.07 1.15 0.43 0.72 0.06 0.22 0.01 0.   3.65] Loss_P: [2.33 1.9  1.59 0.45 0.71 0.06 0.22 0.01 7.27]\n",
      "Loss_Q: [0.98 1.06 0.43 0.72 0.06 0.22 0.01 0.   3.48] Loss_P: [2.32 1.99 1.65 0.49 0.72 0.05 0.21 0.01 7.44]\n",
      "Loss_Q: [1.02 1.05 0.45 0.72 0.06 0.22 0.01 0.   3.53] Loss_P: [2.36 1.97 1.49 0.52 0.71 0.06 0.2  0.02 7.33]\n",
      "Loss_Q: [0.93 1.1  0.42 0.72 0.06 0.21 0.02 0.   3.47] Loss_P: [2.39 1.9  1.62 0.47 0.75 0.06 0.22 0.02 7.41]\n",
      "Loss_Q: [0.95 1.09 0.46 0.67 0.06 0.22 0.01 0.   3.46] Loss_P: [2.37 1.91 1.56 0.5  0.74 0.04 0.22 0.01 7.36]\n",
      "Loss_Q: [1.01 1.11 0.47 0.74 0.06 0.21 0.02 0.   3.61] Loss_P: [2.37 1.92 1.62 0.49 0.74 0.05 0.23 0.02 7.43]\n",
      "Loss_Q: [0.99 1.13 0.48 0.73 0.05 0.24 0.02 0.   3.64] Loss_P: [2.32 1.97 1.6  0.49 0.68 0.04 0.21 0.02 7.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.07 1.09 0.51 0.69 0.04 0.24 0.02 0.   3.65] Loss_P: [2.31 1.96 1.58 0.49 0.69 0.05 0.19 0.01 7.28]\n",
      "Loss_Q: [0.98 1.02 0.49 0.7  0.03 0.22 0.03 0.   3.46] Loss_P: [2.35 1.9  1.49 0.55 0.74 0.05 0.22 0.02 7.31]\n",
      "Loss_Q: [0.99 1.1  0.52 0.71 0.07 0.19 0.02 0.   3.61] Loss_P: [2.34 1.93 1.46 0.48 0.71 0.06 0.21 0.02 7.2 ]\n",
      "Loss_Q: [0.93 1.01 0.49 0.7  0.06 0.2  0.02 0.   3.4 ] Loss_P: [2.34 1.96 1.5  0.55 0.73 0.05 0.2  0.01 7.34]\n",
      "Loss_Q: [0.95 1.03 0.46 0.68 0.04 0.21 0.02 0.   3.39] Loss_P: [2.34 1.91 1.45 0.53 0.67 0.06 0.21 0.02 7.19]\n",
      "Loss_Q: [0.99 0.96 0.48 0.66 0.04 0.19 0.02 0.   3.34] Loss_P: [2.37 1.93 1.47 0.49 0.67 0.05 0.2  0.02 7.2 ]\n",
      "Loss_Q: [1.03 0.97 0.45 0.69 0.05 0.19 0.01 0.   3.39] Loss_P: [2.34 1.98 1.43 0.52 0.72 0.06 0.22 0.02 7.27]\n",
      "Loss_Q: [1.   0.97 0.47 0.74 0.05 0.24 0.01 0.   3.47] Loss_P: [2.36 1.95 1.36 0.5  0.69 0.05 0.2  0.02 7.13]\n",
      "Loss_Q: [0.95 1.03 0.5  0.68 0.05 0.18 0.01 0.   3.4 ] Loss_P: [2.32 2.01 1.45 0.5  0.7  0.06 0.19 0.02 7.26]\n",
      "Loss_Q: [0.99 0.96 0.46 0.68 0.05 0.24 0.01 0.   3.4 ] Loss_P: [2.32 2.02 1.39 0.54 0.65 0.05 0.21 0.01 7.19]\n",
      "Loss_Q: [0.92 0.96 0.44 0.65 0.06 0.21 0.01 0.   3.25] Loss_P: [2.34 1.98 1.44 0.57 0.65 0.04 0.2  0.02 7.24]\n",
      "Loss_Q: [1.05 1.01 0.48 0.67 0.05 0.2  0.01 0.   3.46] Loss_P: [2.29 2.02 1.41 0.53 0.68 0.06 0.21 0.02 7.22]\n",
      "Loss_Q: [0.97 1.04 0.45 0.65 0.04 0.24 0.02 0.   3.41] Loss_P: [2.37 2.02 1.4  0.54 0.61 0.03 0.28 0.02 7.27]\n",
      "Loss_Q: [1.03 1.01 0.45 0.64 0.06 0.23 0.02 0.   3.43] Loss_P: [2.31 1.99 1.48 0.5  0.66 0.05 0.25 0.02 7.27]\n",
      "Loss_Q: [0.87 1.03 0.43 0.67 0.06 0.24 0.02 0.   3.33] Loss_P: [2.38 1.95 1.45 0.47 0.66 0.06 0.22 0.03 7.23]\n",
      "Loss_Q: [0.92 1.03 0.45 0.73 0.04 0.23 0.02 0.   3.43] Loss_P: [2.38 1.95 1.44 0.45 0.69 0.07 0.25 0.02 7.23]\n",
      "Loss_Q: [0.94 1.06 0.42 0.71 0.05 0.24 0.01 0.   3.43] Loss_P: [2.32 1.99 1.52 0.51 0.65 0.04 0.25 0.02 7.32]\n",
      "Loss_Q: [0.94 1.04 0.42 0.67 0.05 0.25 0.02 0.   3.41] Loss_P: [2.38 1.95 1.47 0.46 0.66 0.05 0.21 0.02 7.2 ]\n",
      "Loss_Q: [0.93 1.11 0.43 0.67 0.06 0.25 0.01 0.   3.47] Loss_P: [2.36 1.98 1.5  0.49 0.7  0.04 0.25 0.01 7.33]\n",
      "Loss_Q: [0.88 1.13 0.5  0.74 0.07 0.23 0.01 0.   3.55] Loss_P: [2.32 1.99 1.55 0.5  0.69 0.06 0.22 0.02 7.34]\n",
      "Loss_Q: [0.88 1.07 0.44 0.69 0.05 0.21 0.02 0.   3.36] Loss_P: [2.33 1.94 1.47 0.55 0.7  0.06 0.21 0.01 7.27]\n",
      "Loss_Q: [0.9  1.09 0.48 0.69 0.06 0.24 0.01 0.   3.47] Loss_P: [2.37 1.92 1.52 0.51 0.73 0.06 0.23 0.01 7.35]\n",
      "Loss_Q: [0.98 1.12 0.51 0.67 0.07 0.24 0.02 0.   3.61] Loss_P: [2.27 1.98 1.54 0.52 0.69 0.04 0.22 0.02 7.28]\n",
      "Loss_Q: [0.9  1.08 0.45 0.69 0.07 0.27 0.01 0.   3.47] Loss_P: [2.34 2.01 1.44 0.52 0.71 0.04 0.24 0.02 7.31]\n",
      "Loss_Q: [0.96 1.08 0.48 0.73 0.04 0.26 0.01 0.   3.54] Loss_P: [2.29 1.97 1.52 0.56 0.72 0.07 0.25 0.01 7.38]\n",
      "Loss_Q: [0.87 1.12 0.48 0.74 0.05 0.24 0.02 0.   3.52] Loss_P: [2.39 1.93 1.49 0.52 0.7  0.05 0.27 0.02 7.38]\n",
      "Loss_Q: [0.85 1.04 0.48 0.69 0.06 0.26 0.01 0.   3.39] Loss_P: [2.38 1.88 1.54 0.56 0.72 0.05 0.28 0.01 7.42]\n",
      "Loss_Q: [0.93 1.11 0.47 0.64 0.07 0.29 0.02 0.   3.52] Loss_P: [2.3  1.97 1.54 0.54 0.64 0.04 0.28 0.02 7.33]\n",
      "Loss_Q: [0.95 1.13 0.48 0.68 0.05 0.3  0.01 0.   3.6 ] Loss_P: [2.34 1.94 1.49 0.52 0.71 0.07 0.3  0.02 7.39]\n",
      "Loss_Q: [0.89 1.1  0.5  0.69 0.05 0.3  0.01 0.   3.53] Loss_P: [2.33 1.9  1.51 0.55 0.68 0.07 0.29 0.02 7.35]\n",
      "Loss_Q: [0.98 1.11 0.46 0.65 0.04 0.29 0.01 0.   3.54] Loss_P: [2.33 1.92 1.52 0.55 0.74 0.04 0.28 0.02 7.4 ]\n",
      "Loss_Q: [0.86 1.11 0.49 0.7  0.06 0.31 0.02 0.   3.54] Loss_P: [2.31 1.88 1.54 0.48 0.7  0.04 0.32 0.02 7.29]\n",
      "Loss_Q: [0.92 1.13 0.51 0.69 0.06 0.31 0.03 0.   3.64] Loss_P: [2.29 1.92 1.51 0.52 0.69 0.05 0.31 0.02 7.31]\n",
      "Loss_Q: [0.91 1.06 0.51 0.64 0.06 0.32 0.02 0.   3.51] Loss_P: [2.35 1.92 1.45 0.55 0.68 0.05 0.27 0.02 7.29]\n",
      "Loss_Q: [0.86 1.1  0.48 0.64 0.05 0.34 0.02 0.   3.48] Loss_P: [2.34 1.9  1.52 0.54 0.67 0.03 0.28 0.01 7.3 ]\n",
      "Loss_Q: [0.84 1.11 0.47 0.7  0.05 0.3  0.01 0.   3.48] Loss_P: [2.33 1.96 1.54 0.51 0.7  0.05 0.3  0.02 7.4 ]\n",
      "Loss_Q: [0.97 1.12 0.52 0.66 0.06 0.3  0.01 0.   3.63] Loss_P: [2.35 1.89 1.49 0.55 0.66 0.06 0.29 0.01 7.3 ]\n",
      "Loss_Q: [0.92 1.04 0.48 0.73 0.06 0.29 0.01 0.   3.54] Loss_P: [2.34 1.94 1.43 0.53 0.7  0.04 0.23 0.02 7.24]\n",
      "Loss_Q: [0.91 1.08 0.49 0.73 0.08 0.29 0.01 0.   3.59] Loss_P: [2.34 1.99 1.47 0.53 0.65 0.04 0.27 0.01 7.3 ]\n",
      "Loss_Q: [1.01 1.09 0.52 0.66 0.04 0.28 0.02 0.   3.61] Loss_P: [2.39 1.96 1.48 0.52 0.67 0.05 0.25 0.02 7.33]\n",
      "Loss_Q: [0.93 1.08 0.48 0.64 0.04 0.26 0.03 0.   3.48] Loss_P: [2.32 2.06 1.46 0.54 0.69 0.04 0.3  0.01 7.43]\n",
      "Loss_Q: [1.05 1.15 0.53 0.68 0.06 0.31 0.01 0.   3.78] Loss_P: [2.35 2.08 1.45 0.5  0.65 0.05 0.25 0.03 7.36]\n",
      "Loss_Q: [0.98 1.08 0.44 0.62 0.04 0.28 0.01 0.   3.45] Loss_P: [2.4  1.98 1.49 0.47 0.64 0.05 0.27 0.02 7.32]\n",
      "Loss_Q: [1.02 1.11 0.46 0.63 0.06 0.24 0.01 0.   3.54] Loss_P: [2.36 1.97 1.54 0.5  0.69 0.04 0.27 0.02 7.4 ]\n",
      "Loss_Q: [0.95 1.09 0.52 0.67 0.05 0.28 0.01 0.   3.58] Loss_P: [2.36 1.99 1.54 0.49 0.63 0.05 0.22 0.01 7.29]\n",
      "Loss_Q: [0.93 1.08 0.42 0.66 0.04 0.27 0.02 0.   3.42] Loss_P: [2.4  1.94 1.52 0.51 0.67 0.06 0.23 0.01 7.32]\n",
      "Loss_Q: [0.96 1.05 0.44 0.65 0.06 0.27 0.02 0.   3.44] Loss_P: [2.38 2.02 1.47 0.51 0.67 0.05 0.28 0.02 7.41]\n",
      "Loss_Q: [0.93 1.14 0.42 0.61 0.05 0.28 0.02 0.   3.44] Loss_P: [2.38 2.02 1.47 0.5  0.64 0.06 0.25 0.02 7.33]\n",
      "Loss_Q: [1.02 1.07 0.39 0.64 0.05 0.28 0.02 0.   3.46] Loss_P: [2.37 1.96 1.46 0.45 0.63 0.04 0.24 0.01 7.16]\n",
      "Loss_Q: [0.96 1.06 0.43 0.61 0.07 0.3  0.02 0.   3.45] Loss_P: [2.33 2.02 1.42 0.43 0.63 0.05 0.28 0.02 7.19]\n",
      "Loss_Q: [0.99 1.08 0.48 0.63 0.06 0.29 0.02 0.   3.54] Loss_P: [2.33 1.96 1.57 0.52 0.61 0.04 0.26 0.02 7.31]\n",
      "Loss_Q: [1.07 1.12 0.48 0.61 0.05 0.26 0.01 0.   3.59] Loss_P: [2.42 1.98 1.53 0.48 0.58 0.05 0.28 0.01 7.34]\n",
      "Loss_Q: [0.91 1.06 0.46 0.53 0.04 0.27 0.01 0.   3.29] Loss_P: [2.36 1.97 1.55 0.49 0.6  0.05 0.26 0.02 7.31]\n",
      "Loss_Q: [0.96 1.11 0.44 0.57 0.04 0.24 0.01 0.   3.36] Loss_P: [2.38 1.97 1.51 0.46 0.56 0.04 0.23 0.02 7.18]\n",
      "Loss_Q: [0.96 1.11 0.43 0.56 0.04 0.25 0.01 0.   3.36] Loss_P: [2.41 1.92 1.55 0.5  0.55 0.06 0.23 0.01 7.24]\n",
      "Loss_Q: [0.97 1.13 0.46 0.56 0.05 0.27 0.01 0.   3.43] Loss_P: [2.31 1.96 1.56 0.49 0.61 0.05 0.22 0.01 7.21]\n",
      "Loss_Q: [1.01 1.15 0.44 0.58 0.05 0.27 0.02 0.   3.52] Loss_P: [2.37 1.98 1.57 0.45 0.59 0.04 0.22 0.02 7.24]\n",
      "Loss_Q: [1.   1.08 0.42 0.58 0.06 0.21 0.01 0.   3.36] Loss_P: [2.38 1.92 1.53 0.44 0.57 0.04 0.24 0.02 7.15]\n",
      "Loss_Q: [0.95 1.06 0.44 0.6  0.05 0.22 0.01 0.   3.32] Loss_P: [2.31 1.95 1.6  0.47 0.61 0.05 0.26 0.01 7.27]\n",
      "Loss_Q: [0.88 1.1  0.39 0.56 0.05 0.25 0.01 0.   3.26] Loss_P: [2.33 1.98 1.51 0.47 0.63 0.05 0.24 0.02 7.22]\n",
      "Loss_Q: [0.97 1.12 0.43 0.6  0.04 0.29 0.02 0.   3.46] Loss_P: [2.37 1.99 1.52 0.47 0.59 0.05 0.29 0.01 7.3 ]\n",
      "Loss_Q: [0.91 1.09 0.41 0.64 0.04 0.32 0.02 0.   3.44] Loss_P: [2.4  1.98 1.54 0.45 0.64 0.05 0.26 0.01 7.33]\n",
      "Loss_Q: [0.97 1.11 0.41 0.61 0.04 0.28 0.01 0.   3.43] Loss_P: [2.35 1.96 1.5  0.46 0.61 0.06 0.25 0.01 7.21]\n",
      "Loss_Q: [0.97 1.09 0.37 0.58 0.05 0.28 0.02 0.   3.36] Loss_P: [2.35 1.96 1.49 0.42 0.57 0.06 0.3  0.01 7.17]\n",
      "Loss_Q: [0.94 1.05 0.38 0.59 0.04 0.26 0.01 0.   3.27] Loss_P: [2.35 1.92 1.54 0.44 0.56 0.06 0.3  0.02 7.18]\n",
      "Loss_Q: [0.91 1.11 0.39 0.54 0.04 0.28 0.02 0.   3.3 ] Loss_P: [2.37 1.97 1.51 0.4  0.58 0.05 0.28 0.02 7.18]\n",
      "Loss_Q: [0.88 1.12 0.39 0.56 0.05 0.27 0.01 0.   3.28] Loss_P: [2.36 1.97 1.51 0.38 0.54 0.05 0.25 0.01 7.08]\n",
      "Loss_Q: [0.93 1.03 0.38 0.53 0.05 0.26 0.01 0.   3.2 ] Loss_P: [2.39 1.97 1.49 0.44 0.59 0.07 0.21 0.02 7.17]\n",
      "Loss_Q: [0.96 1.09 0.4  0.53 0.04 0.27 0.01 0.   3.31] Loss_P: [2.39 1.96 1.51 0.44 0.51 0.06 0.22 0.01 7.1 ]\n",
      "Loss_Q: [0.9  1.14 0.41 0.56 0.05 0.27 0.02 0.   3.34] Loss_P: [2.41 1.85 1.53 0.44 0.57 0.07 0.3  0.02 7.19]\n",
      "Loss_Q: [0.99 1.13 0.39 0.56 0.06 0.26 0.01 0.   3.4 ] Loss_P: [2.37 1.94 1.54 0.47 0.56 0.06 0.28 0.01 7.24]\n",
      "Loss_Q: [0.88 1.06 0.37 0.54 0.04 0.3  0.01 0.   3.21] Loss_P: [2.35 1.85 1.55 0.45 0.57 0.06 0.28 0.02 7.12]\n",
      "Loss_Q: [0.9  1.11 0.4  0.57 0.05 0.27 0.01 0.   3.31] Loss_P: [2.31 1.98 1.5  0.47 0.58 0.03 0.25 0.01 7.14]\n",
      "Loss_Q: [0.9  1.09 0.4  0.51 0.06 0.25 0.02 0.   3.23] Loss_P: [2.37 1.97 1.55 0.49 0.5  0.05 0.28 0.02 7.21]\n",
      "Loss_Q: [0.94 1.15 0.44 0.52 0.04 0.25 0.03 0.   3.38] Loss_P: [2.35 1.93 1.48 0.44 0.47 0.03 0.28 0.01 7.  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.88 1.13 0.4  0.51 0.05 0.28 0.02 0.   3.28] Loss_P: [2.33 1.95 1.53 0.45 0.52 0.04 0.28 0.01 7.1 ]\n",
      "Loss_Q: [0.87 1.11 0.4  0.5  0.05 0.3  0.02 0.   3.26] Loss_P: [2.36 1.93 1.59 0.5  0.53 0.06 0.29 0.02 7.28]\n",
      "Loss_Q: [0.86 1.17 0.38 0.53 0.05 0.29 0.02 0.   3.31] Loss_P: [2.36 1.95 1.56 0.46 0.55 0.04 0.26 0.01 7.18]\n",
      "Loss_Q: [0.84 1.15 0.44 0.55 0.03 0.33 0.03 0.   3.38] Loss_P: [2.34 1.88 1.58 0.5  0.55 0.07 0.33 0.01 7.27]\n",
      "Loss_Q: [0.89 1.12 0.5  0.62 0.04 0.28 0.02 0.   3.47] Loss_P: [2.36 1.89 1.52 0.47 0.54 0.04 0.3  0.02 7.14]\n",
      "Loss_Q: [0.92 1.12 0.42 0.55 0.05 0.33 0.02 0.   3.42] Loss_P: [2.34 1.94 1.57 0.57 0.55 0.03 0.32 0.02 7.34]\n",
      "Loss_Q: [0.93 1.2  0.51 0.54 0.03 0.32 0.02 0.   3.56] Loss_P: [2.37 1.93 1.61 0.53 0.57 0.04 0.32 0.02 7.39]\n",
      "Loss_Q: [0.89 1.25 0.45 0.57 0.03 0.29 0.02 0.   3.5 ] Loss_P: [2.33 1.95 1.61 0.48 0.58 0.06 0.28 0.01 7.29]\n",
      "Loss_Q: [0.83 1.17 0.42 0.55 0.04 0.3  0.02 0.   3.33] Loss_P: [2.4  1.9  1.56 0.49 0.57 0.04 0.33 0.02 7.32]\n",
      "Loss_Q: [0.86 1.16 0.43 0.53 0.04 0.34 0.02 0.   3.37] Loss_P: [2.4  1.9  1.61 0.48 0.56 0.04 0.32 0.01 7.32]\n",
      "Loss_Q: [0.82 1.14 0.41 0.57 0.05 0.31 0.02 0.   3.33] Loss_P: [2.4  1.86 1.57 0.48 0.59 0.04 0.31 0.02 7.26]\n",
      "Loss_Q: [0.89 1.12 0.43 0.58 0.06 0.34 0.02 0.   3.45] Loss_P: [2.4  1.9  1.56 0.46 0.55 0.04 0.31 0.01 7.23]\n",
      "Loss_Q: [0.8  1.19 0.43 0.53 0.06 0.3  0.01 0.   3.32] Loss_P: [2.39 1.85 1.52 0.48 0.51 0.05 0.32 0.01 7.13]\n",
      "Loss_Q: [0.87 1.11 0.42 0.55 0.05 0.34 0.01 0.   3.36] Loss_P: [2.44 1.92 1.44 0.47 0.6  0.07 0.33 0.03 7.3 ]\n",
      "Loss_Q: [0.81 1.15 0.41 0.59 0.03 0.3  0.02 0.   3.31] Loss_P: [2.42 1.86 1.52 0.5  0.57 0.05 0.31 0.02 7.25]\n",
      "Loss_Q: [0.87 1.15 0.4  0.59 0.05 0.28 0.01 0.   3.35] Loss_P: [2.44 1.9  1.46 0.48 0.57 0.06 0.32 0.02 7.24]\n",
      "Loss_Q: [0.81 1.16 0.42 0.54 0.06 0.33 0.02 0.   3.34] Loss_P: [2.38 1.79 1.54 0.45 0.54 0.05 0.32 0.02 7.08]\n",
      "Loss_Q: [0.83 1.15 0.42 0.55 0.04 0.31 0.02 0.   3.32] Loss_P: [2.45 1.83 1.46 0.45 0.6  0.05 0.34 0.02 7.19]\n",
      "Loss_Q: [0.87 1.06 0.37 0.49 0.04 0.37 0.02 0.   3.23] Loss_P: [2.39 1.85 1.53 0.42 0.56 0.05 0.36 0.02 7.17]\n",
      "Loss_Q: [0.87 1.14 0.39 0.51 0.04 0.37 0.02 0.   3.34] Loss_P: [2.42 1.89 1.52 0.42 0.54 0.04 0.32 0.02 7.17]\n",
      "Loss_Q: [0.92 1.16 0.41 0.5  0.05 0.37 0.02 0.   3.43] Loss_P: [2.43 1.83 1.51 0.42 0.55 0.07 0.33 0.02 7.15]\n",
      "Loss_Q: [0.86 1.11 0.4  0.54 0.06 0.34 0.02 0.   3.32] Loss_P: [2.4  1.83 1.57 0.4  0.59 0.06 0.34 0.01 7.21]\n",
      "Loss_Q: [0.89 1.2  0.44 0.5  0.04 0.32 0.02 0.   3.41] Loss_P: [2.39 1.86 1.62 0.46 0.54 0.06 0.3  0.02 7.25]\n",
      "Loss_Q: [0.85 1.2  0.39 0.51 0.05 0.3  0.02 0.   3.33] Loss_P: [2.37 1.83 1.63 0.44 0.54 0.06 0.34 0.02 7.25]\n",
      "Loss_Q: [0.93 1.16 0.38 0.48 0.05 0.32 0.02 0.   3.34] Loss_P: [2.37 1.88 1.62 0.43 0.47 0.05 0.31 0.01 7.14]\n",
      "Loss_Q: [0.83 1.18 0.38 0.45 0.05 0.31 0.01 0.   3.2 ] Loss_P: [2.36 1.93 1.62 0.43 0.52 0.06 0.36 0.01 7.29]\n",
      "Loss_Q: [0.91 1.19 0.43 0.43 0.05 0.33 0.02 0.   3.35] Loss_P: [2.35 1.9  1.61 0.45 0.44 0.04 0.33 0.02 7.15]\n",
      "Loss_Q: [0.86 1.22 0.44 0.5  0.05 0.31 0.02 0.   3.41] Loss_P: [2.35 1.87 1.53 0.41 0.47 0.05 0.34 0.02 7.05]\n",
      "Loss_Q: [0.9  1.15 0.39 0.5  0.06 0.31 0.02 0.   3.32] Loss_P: [2.38 1.84 1.6  0.43 0.48 0.05 0.29 0.01 7.08]\n",
      "Loss_Q: [0.87 1.15 0.41 0.47 0.06 0.3  0.01 0.   3.27] Loss_P: [2.4  1.86 1.51 0.48 0.45 0.08 0.29 0.02 7.07]\n",
      "Loss_Q: [0.77 1.15 0.46 0.46 0.05 0.32 0.02 0.   3.22] Loss_P: [2.44 1.82 1.53 0.49 0.44 0.05 0.28 0.03 7.08]\n",
      "Loss_Q: [0.92 1.12 0.4  0.48 0.06 0.28 0.01 0.   3.27] Loss_P: [2.37 1.92 1.58 0.45 0.46 0.06 0.31 0.02 7.18]\n",
      "Loss_Q: [0.93 1.16 0.47 0.46 0.05 0.28 0.02 0.   3.37] Loss_P: [2.39 1.93 1.56 0.5  0.43 0.06 0.27 0.01 7.15]\n",
      "Loss_Q: [0.9  1.2  0.38 0.46 0.07 0.33 0.01 0.   3.36] Loss_P: [2.38 1.85 1.59 0.47 0.45 0.05 0.29 0.02 7.1 ]\n",
      "Loss_Q: [0.88 1.21 0.4  0.47 0.04 0.27 0.01 0.   3.28] Loss_P: [2.41 1.76 1.69 0.47 0.43 0.04 0.29 0.02 7.12]\n",
      "Loss_Q: [0.91 1.19 0.46 0.46 0.06 0.29 0.03 0.   3.41] Loss_P: [2.37 1.88 1.62 0.57 0.41 0.06 0.29 0.02 7.22]\n",
      "Loss_Q: [0.82 1.2  0.44 0.41 0.05 0.3  0.02 0.   3.24] Loss_P: [2.45 1.77 1.61 0.49 0.37 0.04 0.26 0.02 7.01]\n",
      "Loss_Q: [1.   1.17 0.48 0.44 0.05 0.28 0.02 0.   3.44] Loss_P: [2.36 1.82 1.65 0.52 0.44 0.05 0.29 0.01 7.13]\n",
      "Loss_Q: [0.93 1.16 0.46 0.41 0.06 0.26 0.02 0.   3.3 ] Loss_P: [2.43 1.79 1.61 0.51 0.48 0.06 0.27 0.02 7.16]\n",
      "Loss_Q: [0.95 1.18 0.43 0.39 0.04 0.27 0.01 0.   3.27] Loss_P: [2.39 1.88 1.65 0.52 0.4  0.07 0.26 0.02 7.19]\n",
      "Loss_Q: [0.97 1.17 0.43 0.36 0.04 0.3  0.02 0.   3.29] Loss_P: [2.37 1.85 1.66 0.53 0.39 0.06 0.26 0.01 7.13]\n",
      "Loss_Q: [0.89 1.21 0.5  0.37 0.06 0.28 0.03 0.   3.34] Loss_P: [2.4  1.87 1.7  0.56 0.48 0.07 0.24 0.01 7.32]\n",
      "Loss_Q: [0.96 1.24 0.49 0.44 0.06 0.27 0.01 0.   3.47] Loss_P: [2.41 1.8  1.61 0.56 0.45 0.05 0.23 0.01 7.14]\n",
      "Loss_Q: [0.97 1.23 0.48 0.43 0.07 0.28 0.01 0.   3.47] Loss_P: [2.43 1.81 1.71 0.61 0.44 0.05 0.25 0.02 7.32]\n",
      "Loss_Q: [0.99 1.2  0.59 0.43 0.06 0.27 0.02 0.   3.57] Loss_P: [2.39 1.88 1.71 0.6  0.42 0.06 0.25 0.02 7.32]\n",
      "Loss_Q: [0.96 1.15 0.49 0.4  0.06 0.24 0.02 0.   3.32] Loss_P: [2.4  1.9  1.72 0.59 0.42 0.06 0.23 0.01 7.32]\n",
      "Loss_Q: [1.   1.21 0.52 0.42 0.06 0.21 0.01 0.   3.42] Loss_P: [2.37 1.8  1.72 0.61 0.44 0.07 0.27 0.01 7.28]\n",
      "Loss_Q: [0.93 1.23 0.53 0.43 0.06 0.24 0.02 0.   3.42] Loss_P: [2.33 1.84 1.69 0.54 0.44 0.07 0.25 0.02 7.18]\n",
      "Loss_Q: [0.92 1.21 0.46 0.42 0.08 0.28 0.02 0.   3.39] Loss_P: [2.37 1.84 1.69 0.63 0.44 0.07 0.21 0.02 7.27]\n",
      "Loss_Q: [0.93 1.25 0.5  0.49 0.05 0.24 0.02 0.   3.49] Loss_P: [2.37 1.87 1.67 0.56 0.48 0.05 0.21 0.02 7.23]\n",
      "Loss_Q: [0.92 1.26 0.52 0.43 0.07 0.24 0.01 0.   3.44] Loss_P: [2.32 1.93 1.71 0.59 0.45 0.04 0.22 0.02 7.29]\n",
      "Loss_Q: [0.96 1.25 0.51 0.41 0.06 0.22 0.01 0.   3.42] Loss_P: [2.42 1.82 1.68 0.51 0.4  0.08 0.25 0.02 7.18]\n",
      "Loss_Q: [0.9  1.21 0.52 0.45 0.04 0.22 0.01 0.   3.35] Loss_P: [2.36 1.84 1.73 0.51 0.42 0.05 0.24 0.01 7.17]\n",
      "Loss_Q: [0.91 1.28 0.49 0.45 0.05 0.23 0.01 0.   3.41] Loss_P: [2.33 1.84 1.73 0.56 0.42 0.06 0.22 0.02 7.18]\n",
      "Loss_Q: [0.9  1.27 0.44 0.45 0.08 0.23 0.01 0.   3.38] Loss_P: [2.38 1.89 1.72 0.53 0.45 0.06 0.23 0.02 7.28]\n",
      "Loss_Q: [0.98 1.23 0.5  0.41 0.06 0.22 0.01 0.   3.4 ] Loss_P: [2.39 1.82 1.68 0.49 0.4  0.06 0.2  0.02 7.06]\n",
      "Loss_Q: [0.95 1.16 0.47 0.4  0.07 0.23 0.01 0.   3.31] Loss_P: [2.38 1.82 1.72 0.54 0.41 0.06 0.18 0.02 7.12]\n",
      "Loss_Q: [0.92 1.24 0.49 0.41 0.06 0.22 0.01 0.   3.36] Loss_P: [2.37 1.87 1.7  0.52 0.4  0.05 0.23 0.01 7.16]\n",
      "Loss_Q: [0.85 1.26 0.47 0.37 0.05 0.22 0.02 0.   3.24] Loss_P: [2.34 1.88 1.72 0.56 0.4  0.05 0.21 0.02 7.18]\n",
      "Loss_Q: [0.91 1.26 0.47 0.38 0.04 0.23 0.03 0.   3.31] Loss_P: [2.36 1.88 1.68 0.55 0.35 0.07 0.19 0.02 7.1 ]\n",
      "Loss_Q: [0.99 1.23 0.5  0.4  0.05 0.19 0.01 0.   3.37] Loss_P: [2.34 1.93 1.62 0.52 0.41 0.07 0.22 0.02 7.13]\n",
      "Loss_Q: [0.93 1.23 0.47 0.39 0.05 0.21 0.02 0.   3.29] Loss_P: [2.32 1.9  1.77 0.59 0.4  0.07 0.22 0.01 7.27]\n",
      "Loss_Q: [0.92 1.17 0.49 0.36 0.06 0.22 0.02 0.   3.23] Loss_P: [2.35 1.87 1.76 0.59 0.4  0.05 0.2  0.01 7.22]\n",
      "Loss_Q: [0.86 1.22 0.49 0.39 0.04 0.23 0.01 0.   3.24] Loss_P: [2.32 1.88 1.75 0.6  0.37 0.06 0.19 0.01 7.17]\n",
      "Loss_Q: [0.89 1.26 0.53 0.38 0.08 0.2  0.02 0.   3.35] Loss_P: [2.27 1.96 1.7  0.57 0.41 0.06 0.19 0.01 7.17]\n",
      "Loss_Q: [0.98 1.18 0.5  0.4  0.05 0.18 0.01 0.   3.3 ] Loss_P: [2.35 1.94 1.7  0.58 0.4  0.05 0.2  0.01 7.22]\n",
      "Loss_Q: [0.98 1.24 0.55 0.35 0.06 0.21 0.02 0.   3.4 ] Loss_P: [2.33 1.92 1.66 0.59 0.36 0.05 0.19 0.02 7.13]\n",
      "Loss_Q: [0.92 1.24 0.54 0.41 0.05 0.19 0.02 0.   3.37] Loss_P: [2.35 1.87 1.68 0.53 0.42 0.07 0.18 0.02 7.12]\n",
      "Loss_Q: [1.   1.22 0.5  0.36 0.05 0.2  0.01 0.   3.35] Loss_P: [2.23 1.84 1.74 0.63 0.45 0.05 0.23 0.02 7.18]\n",
      "Loss_Q: [0.95 1.18 0.52 0.41 0.05 0.2  0.01 0.   3.32] Loss_P: [2.32 1.89 1.67 0.59 0.41 0.05 0.17 0.02 7.13]\n",
      "Loss_Q: [0.94 1.2  0.54 0.4  0.06 0.22 0.02 0.   3.37] Loss_P: [2.37 1.85 1.65 0.58 0.41 0.07 0.21 0.02 7.16]\n",
      "Loss_Q: [0.96 1.27 0.57 0.42 0.05 0.17 0.01 0.   3.45] Loss_P: [2.37 1.85 1.66 0.64 0.38 0.07 0.18 0.01 7.16]\n",
      "Loss_Q: [0.93 1.22 0.6  0.43 0.06 0.17 0.01 0.   3.41] Loss_P: [2.35 1.91 1.64 0.65 0.38 0.05 0.19 0.01 7.18]\n",
      "Loss_Q: [0.98 1.15 0.57 0.41 0.05 0.19 0.02 0.   3.37] Loss_P: [2.31 1.92 1.64 0.61 0.4  0.05 0.18 0.01 7.12]\n",
      "Loss_Q: [1.01 1.17 0.55 0.38 0.06 0.18 0.01 0.   3.37] Loss_P: [2.38 1.87 1.62 0.54 0.39 0.04 0.17 0.02 7.03]\n",
      "Loss_Q: [0.95 1.23 0.56 0.41 0.04 0.17 0.02 0.   3.37] Loss_P: [2.37 1.85 1.65 0.61 0.42 0.06 0.18 0.02 7.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.9  1.18 0.57 0.45 0.04 0.16 0.02 0.   3.32] Loss_P: [2.32 1.83 1.58 0.6  0.43 0.04 0.2  0.01 7.01]\n",
      "Loss_Q: [0.86 1.19 0.52 0.44 0.04 0.17 0.01 0.   3.23] Loss_P: [2.38 1.8  1.66 0.62 0.44 0.07 0.19 0.02 7.18]\n",
      "Loss_Q: [0.98 1.19 0.57 0.42 0.05 0.17 0.01 0.   3.4 ] Loss_P: [2.39 1.84 1.62 0.6  0.41 0.08 0.18 0.02 7.14]\n",
      "Loss_Q: [0.9  1.18 0.54 0.4  0.06 0.19 0.01 0.   3.29] Loss_P: [2.38 1.88 1.69 0.6  0.42 0.05 0.17 0.01 7.2 ]\n",
      "Loss_Q: [0.86 1.22 0.54 0.4  0.07 0.18 0.02 0.   3.28] Loss_P: [2.39 1.82 1.68 0.62 0.45 0.05 0.2  0.02 7.23]\n",
      "Loss_Q: [0.89 1.19 0.5  0.45 0.06 0.19 0.01 0.   3.3 ] Loss_P: [2.37 1.86 1.67 0.57 0.45 0.05 0.16 0.02 7.15]\n",
      "Loss_Q: [0.9  1.15 0.59 0.48 0.06 0.18 0.01 0.   3.37] Loss_P: [2.34 1.82 1.69 0.63 0.46 0.05 0.19 0.02 7.2 ]\n",
      "Loss_Q: [0.96 1.14 0.58 0.45 0.07 0.17 0.01 0.   3.36] Loss_P: [2.34 1.86 1.67 0.6  0.45 0.06 0.18 0.01 7.16]\n",
      "Loss_Q: [0.89 1.13 0.55 0.41 0.07 0.21 0.01 0.   3.28] Loss_P: [2.33 1.83 1.66 0.62 0.43 0.07 0.19 0.02 7.15]\n",
      "Loss_Q: [0.93 1.15 0.57 0.47 0.05 0.19 0.02 0.   3.38] Loss_P: [2.37 1.84 1.64 0.63 0.44 0.05 0.19 0.01 7.17]\n",
      "Loss_Q: [1.01 1.14 0.63 0.45 0.05 0.16 0.01 0.   3.45] Loss_P: [2.38 1.88 1.63 0.65 0.37 0.06 0.2  0.02 7.2 ]\n",
      "Loss_Q: [0.92 1.18 0.59 0.44 0.06 0.17 0.01 0.   3.36] Loss_P: [2.43 1.83 1.62 0.6  0.43 0.05 0.17 0.02 7.15]\n",
      "Loss_Q: [0.88 1.16 0.6  0.4  0.05 0.15 0.02 0.   3.27] Loss_P: [2.33 1.86 1.7  0.6  0.43 0.08 0.16 0.01 7.18]\n",
      "Loss_Q: [0.88 1.18 0.59 0.52 0.05 0.15 0.01 0.   3.39] Loss_P: [2.39 1.82 1.63 0.65 0.44 0.07 0.19 0.01 7.2 ]\n",
      "Loss_Q: [0.98 1.2  0.57 0.42 0.04 0.18 0.01 0.   3.41] Loss_P: [2.34 1.89 1.69 0.61 0.45 0.05 0.18 0.01 7.23]\n",
      "Loss_Q: [0.9  1.18 0.56 0.46 0.06 0.19 0.01 0.   3.36] Loss_P: [2.4  1.79 1.61 0.66 0.43 0.05 0.19 0.01 7.14]\n",
      "Loss_Q: [0.93 1.2  0.63 0.44 0.05 0.18 0.01 0.   3.43] Loss_P: [2.38 1.89 1.68 0.64 0.45 0.06 0.19 0.02 7.3 ]\n",
      "Loss_Q: [0.91 1.24 0.64 0.45 0.06 0.17 0.01 0.   3.47] Loss_P: [2.33 1.86 1.68 0.67 0.5  0.06 0.19 0.01 7.28]\n",
      "Loss_Q: [0.89 1.11 0.58 0.42 0.06 0.18 0.01 0.   3.24] Loss_P: [2.36 1.8  1.72 0.66 0.46 0.06 0.18 0.02 7.25]\n",
      "Loss_Q: [0.91 1.21 0.59 0.4  0.04 0.18 0.01 0.   3.34] Loss_P: [2.35 1.88 1.67 0.62 0.45 0.06 0.18 0.02 7.23]\n",
      "Loss_Q: [0.94 1.24 0.58 0.47 0.06 0.17 0.01 0.   3.48] Loss_P: [2.33 1.91 1.74 0.69 0.48 0.06 0.16 0.02 7.38]\n",
      "Loss_Q: [0.91 1.21 0.58 0.47 0.05 0.17 0.02 0.   3.41] Loss_P: [2.36 1.86 1.67 0.68 0.5  0.05 0.18 0.02 7.31]\n",
      "Loss_Q: [0.96 1.24 0.64 0.48 0.06 0.18 0.02 0.   3.57] Loss_P: [2.42 1.79 1.71 0.68 0.5  0.08 0.17 0.01 7.36]\n",
      "Loss_Q: [0.84 1.22 0.67 0.51 0.06 0.14 0.01 0.   3.46] Loss_P: [2.37 1.85 1.63 0.66 0.54 0.04 0.14 0.02 7.25]\n",
      "Loss_Q: [0.96 1.27 0.67 0.57 0.07 0.17 0.02 0.   3.73] Loss_P: [2.29 1.87 1.72 0.7  0.55 0.06 0.17 0.01 7.37]\n",
      "Loss_Q: [0.99 1.25 0.69 0.49 0.09 0.16 0.01 0.   3.68] Loss_P: [2.37 1.77 1.73 0.75 0.51 0.05 0.16 0.01 7.35]\n",
      "Loss_Q: [0.93 1.2  0.65 0.53 0.05 0.17 0.02 0.   3.55] Loss_P: [2.33 1.88 1.68 0.7  0.53 0.07 0.16 0.03 7.37]\n",
      "Loss_Q: [0.99 1.21 0.66 0.57 0.08 0.16 0.02 0.   3.68] Loss_P: [2.32 1.82 1.74 0.71 0.53 0.05 0.14 0.01 7.31]\n",
      "Loss_Q: [0.94 1.19 0.73 0.57 0.05 0.15 0.01 0.   3.65] Loss_P: [2.37 1.84 1.79 0.82 0.53 0.05 0.15 0.01 7.56]\n",
      "Loss_Q: [0.91 1.18 0.75 0.53 0.06 0.14 0.01 0.   3.59] Loss_P: [2.39 1.86 1.76 0.82 0.53 0.06 0.15 0.02 7.6 ]\n",
      "Loss_Q: [0.99 1.21 0.75 0.52 0.04 0.15 0.02 0.   3.69] Loss_P: [2.36 1.85 1.71 0.85 0.55 0.06 0.17 0.01 7.55]\n",
      "Loss_Q: [0.95 1.23 0.74 0.53 0.05 0.18 0.02 0.   3.69] Loss_P: [2.37 1.86 1.73 0.8  0.49 0.05 0.14 0.02 7.46]\n",
      "Loss_Q: [0.9  1.24 0.76 0.52 0.05 0.16 0.01 0.   3.63] Loss_P: [2.38 1.81 1.69 0.89 0.53 0.05 0.14 0.02 7.5 ]\n",
      "Loss_Q: [0.99 1.15 0.78 0.48 0.04 0.16 0.01 0.   3.62] Loss_P: [2.34 1.83 1.68 0.84 0.5  0.08 0.13 0.02 7.42]\n",
      "Loss_Q: [0.94 1.2  0.71 0.51 0.05 0.15 0.02 0.   3.58] Loss_P: [2.36 1.83 1.73 0.82 0.53 0.05 0.16 0.02 7.5 ]\n",
      "Loss_Q: [0.91 1.15 0.75 0.53 0.05 0.14 0.01 0.   3.56] Loss_P: [2.42 1.82 1.72 0.85 0.5  0.05 0.19 0.02 7.57]\n",
      "Loss_Q: [1.07 1.22 0.82 0.57 0.06 0.16 0.01 0.   3.92] Loss_P: [2.33 1.87 1.77 0.89 0.55 0.05 0.14 0.01 7.6 ]\n",
      "Loss_Q: [0.9  1.22 0.72 0.47 0.05 0.15 0.01 0.   3.53] Loss_P: [2.38 1.82 1.73 0.84 0.52 0.05 0.16 0.02 7.52]\n",
      "Loss_Q: [0.98 1.15 0.8  0.53 0.06 0.17 0.02 0.   3.7 ] Loss_P: [2.38 1.79 1.69 0.91 0.51 0.05 0.15 0.02 7.49]\n",
      "Loss_Q: [0.94 1.23 0.79 0.54 0.03 0.15 0.01 0.   3.69] Loss_P: [2.41 1.85 1.64 0.81 0.54 0.06 0.14 0.02 7.47]\n",
      "Loss_Q: [0.96 1.19 0.78 0.5  0.08 0.15 0.02 0.   3.68] Loss_P: [2.34 1.83 1.68 0.84 0.53 0.05 0.17 0.02 7.45]\n",
      "Loss_Q: [0.98 1.13 0.76 0.55 0.05 0.18 0.02 0.   3.67] Loss_P: [2.4  1.86 1.68 0.88 0.53 0.06 0.15 0.02 7.57]\n",
      "Loss_Q: [0.9  1.16 0.72 0.54 0.05 0.14 0.01 0.   3.52] Loss_P: [2.28 1.88 1.75 0.81 0.53 0.06 0.14 0.02 7.46]\n",
      "Loss_Q: [0.91 1.27 0.77 0.54 0.07 0.18 0.01 0.   3.74] Loss_P: [2.28 1.95 1.66 0.81 0.52 0.07 0.16 0.02 7.46]\n",
      "Loss_Q: [0.95 1.16 0.72 0.55 0.06 0.14 0.02 0.   3.6 ] Loss_P: [2.31 1.9  1.65 0.87 0.57 0.07 0.17 0.01 7.55]\n",
      "Loss_Q: [0.96 1.2  0.74 0.56 0.07 0.17 0.02 0.   3.72] Loss_P: [2.39 1.87 1.71 0.77 0.58 0.05 0.14 0.02 7.52]\n",
      "Loss_Q: [0.92 1.24 0.7  0.61 0.07 0.15 0.01 0.   3.71] Loss_P: [2.32 1.89 1.71 0.78 0.6  0.06 0.14 0.02 7.51]\n",
      "Loss_Q: [0.92 1.3  0.72 0.61 0.07 0.12 0.01 0.   3.75] Loss_P: [2.33 1.87 1.74 0.81 0.6  0.06 0.15 0.01 7.56]\n",
      "Loss_Q: [0.99 1.22 0.77 0.66 0.06 0.13 0.01 0.   3.85] Loss_P: [2.33 1.86 1.73 0.84 0.56 0.06 0.12 0.02 7.53]\n",
      "Loss_Q: [0.99 1.19 0.76 0.61 0.06 0.15 0.01 0.   3.77] Loss_P: [2.37 1.89 1.73 0.82 0.58 0.06 0.14 0.01 7.6 ]\n",
      "Loss_Q: [0.95 1.24 0.8  0.6  0.05 0.18 0.02 0.   3.85] Loss_P: [2.32 1.89 1.74 0.83 0.64 0.06 0.15 0.01 7.63]\n",
      "Loss_Q: [1.01 1.21 0.75 0.58 0.05 0.18 0.02 0.   3.81] Loss_P: [2.36 1.86 1.77 0.85 0.6  0.06 0.19 0.01 7.68]\n",
      "Loss_Q: [0.87 1.21 0.75 0.65 0.06 0.19 0.01 0.   3.75] Loss_P: [2.36 1.9  1.73 0.84 0.61 0.05 0.15 0.01 7.65]\n",
      "Loss_Q: [0.97 1.16 0.76 0.61 0.05 0.16 0.02 0.   3.73] Loss_P: [2.34 1.91 1.7  0.84 0.61 0.05 0.17 0.01 7.64]\n",
      "Loss_Q: [0.94 1.21 0.75 0.56 0.06 0.18 0.02 0.   3.74] Loss_P: [2.36 1.98 1.71 0.87 0.6  0.05 0.22 0.02 7.8 ]\n",
      "Loss_Q: [1.01 1.29 0.77 0.59 0.05 0.2  0.02 0.   3.94] Loss_P: [2.32 1.91 1.71 0.83 0.61 0.06 0.2  0.01 7.65]\n",
      "Loss_Q: [0.95 1.22 0.72 0.56 0.04 0.2  0.01 0.   3.7 ] Loss_P: [2.35 1.92 1.72 0.85 0.56 0.04 0.18 0.01 7.63]\n",
      "Loss_Q: [0.97 1.26 0.77 0.6  0.05 0.18 0.01 0.   3.84] Loss_P: [2.33 1.9  1.7  0.86 0.6  0.06 0.18 0.02 7.64]\n",
      "Loss_Q: [0.94 1.25 0.71 0.57 0.06 0.18 0.02 0.   3.72] Loss_P: [2.28 1.96 1.76 0.81 0.61 0.07 0.2  0.02 7.69]\n",
      "Loss_Q: [0.98 1.28 0.75 0.58 0.07 0.18 0.01 0.   3.85] Loss_P: [2.33 1.94 1.73 0.8  0.59 0.07 0.21 0.02 7.69]\n",
      "Loss_Q: [0.94 1.3  0.77 0.57 0.04 0.18 0.01 0.   3.81] Loss_P: [2.39 1.84 1.82 0.8  0.57 0.07 0.18 0.02 7.69]\n",
      "Loss_Q: [1.03 1.22 0.79 0.55 0.03 0.18 0.02 0.   3.82] Loss_P: [2.36 1.88 1.78 0.94 0.55 0.06 0.19 0.02 7.77]\n",
      "Loss_Q: [0.97 1.36 0.79 0.57 0.05 0.22 0.02 0.   3.97] Loss_P: [2.35 1.94 1.73 0.88 0.58 0.08 0.23 0.03 7.82]\n",
      "Loss_Q: [0.97 1.22 0.78 0.59 0.06 0.19 0.01 0.   3.82] Loss_P: [2.38 1.91 1.7  0.86 0.58 0.06 0.19 0.02 7.69]\n",
      "Loss_Q: [1.02 1.3  0.79 0.52 0.05 0.18 0.01 0.   3.88] Loss_P: [2.35 1.85 1.75 0.87 0.56 0.07 0.19 0.02 7.66]\n",
      "Loss_Q: [0.97 1.27 0.8  0.6  0.06 0.21 0.01 0.   3.92] Loss_P: [2.34 1.9  1.76 0.88 0.56 0.07 0.23 0.01 7.75]\n",
      "Loss_Q: [1.01 1.33 0.84 0.55 0.07 0.23 0.01 0.   4.04] Loss_P: [2.32 1.96 1.78 0.85 0.55 0.06 0.21 0.01 7.76]\n",
      "Loss_Q: [1.01 1.34 0.78 0.56 0.06 0.21 0.01 0.   3.98] Loss_P: [2.4  1.91 1.79 0.87 0.55 0.06 0.21 0.01 7.8 ]\n",
      "Loss_Q: [1.01 1.27 0.78 0.57 0.04 0.25 0.01 0.   3.93] Loss_P: [2.36 1.91 1.79 0.87 0.55 0.06 0.25 0.02 7.79]\n",
      "Loss_Q: [0.97 1.23 0.81 0.53 0.07 0.23 0.01 0.   3.85] Loss_P: [2.35 1.95 1.75 0.79 0.55 0.05 0.21 0.02 7.66]\n",
      "Loss_Q: [0.93 1.31 0.73 0.52 0.06 0.19 0.01 0.   3.74] Loss_P: [2.36 1.93 1.71 0.83 0.49 0.06 0.22 0.01 7.61]\n",
      "Loss_Q: [1.02 1.25 0.79 0.53 0.06 0.26 0.02 0.   3.94] Loss_P: [2.32 1.94 1.74 0.89 0.55 0.06 0.23 0.02 7.75]\n",
      "Loss_Q: [0.98 1.27 0.8  0.56 0.05 0.2  0.01 0.   3.88] Loss_P: [2.32 1.95 1.66 0.87 0.53 0.06 0.22 0.02 7.63]\n",
      "Loss_Q: [0.99 1.27 0.84 0.54 0.05 0.23 0.01 0.   3.93] Loss_P: [2.36 1.98 1.72 0.98 0.57 0.07 0.21 0.02 7.91]\n",
      "Loss_Q: [0.92 1.31 0.89 0.55 0.06 0.22 0.02 0.   3.98] Loss_P: [2.32 1.98 1.74 0.99 0.57 0.08 0.2  0.02 7.9 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.   1.36 0.89 0.54 0.04 0.23 0.03 0.   4.08] Loss_P: [2.3  1.93 1.79 0.96 0.59 0.06 0.21 0.01 7.85]\n",
      "Loss_Q: [1.02 1.26 0.82 0.55 0.07 0.23 0.02 0.   3.97] Loss_P: [2.33 1.94 1.69 0.96 0.55 0.05 0.2  0.02 7.75]\n",
      "Loss_Q: [0.98 1.31 0.84 0.56 0.08 0.21 0.02 0.   3.98] Loss_P: [2.35 1.95 1.68 0.92 0.51 0.09 0.23 0.01 7.75]\n",
      "Loss_Q: [1.   1.33 0.8  0.52 0.05 0.19 0.01 0.   3.9 ] Loss_P: [2.33 1.91 1.7  0.9  0.53 0.06 0.19 0.02 7.63]\n",
      "Loss_Q: [0.98 1.26 0.8  0.58 0.07 0.21 0.01 0.   3.9 ] Loss_P: [2.31 1.93 1.75 0.87 0.56 0.05 0.19 0.02 7.69]\n",
      "Loss_Q: [0.91 1.27 0.83 0.57 0.04 0.19 0.02 0.   3.84] Loss_P: [2.31 1.96 1.73 0.9  0.58 0.06 0.22 0.02 7.78]\n",
      "Loss_Q: [0.96 1.16 0.8  0.54 0.06 0.21 0.01 0.   3.74] Loss_P: [2.31 1.93 1.74 0.89 0.58 0.05 0.18 0.02 7.7 ]\n",
      "Loss_Q: [0.99 1.22 0.82 0.62 0.07 0.19 0.02 0.   3.93] Loss_P: [2.36 1.9  1.68 0.86 0.61 0.06 0.19 0.01 7.68]\n",
      "Loss_Q: [0.94 1.23 0.81 0.59 0.06 0.19 0.02 0.   3.83] Loss_P: [2.32 1.92 1.72 0.89 0.62 0.06 0.18 0.02 7.72]\n",
      "Loss_Q: [0.98 1.27 0.85 0.58 0.05 0.17 0.01 0.   3.91] Loss_P: [2.36 1.92 1.76 0.88 0.63 0.06 0.2  0.01 7.82]\n",
      "Loss_Q: [0.99 1.21 0.81 0.55 0.05 0.23 0.02 0.   3.87] Loss_P: [2.36 1.88 1.72 0.91 0.58 0.07 0.19 0.01 7.73]\n",
      "Loss_Q: [0.91 1.21 0.85 0.61 0.05 0.22 0.01 0.   3.86] Loss_P: [2.33 1.87 1.67 0.93 0.6  0.04 0.21 0.01 7.66]\n",
      "Loss_Q: [1.   1.21 0.83 0.56 0.06 0.23 0.01 0.   3.89] Loss_P: [2.4  1.86 1.66 0.89 0.56 0.06 0.21 0.01 7.65]\n",
      "Loss_Q: [0.95 1.21 0.88 0.64 0.05 0.19 0.01 0.   3.93] Loss_P: [2.36 1.86 1.74 0.98 0.62 0.06 0.21 0.01 7.84]\n",
      "Loss_Q: [0.97 1.25 0.87 0.64 0.06 0.21 0.02 0.   4.02] Loss_P: [2.34 1.9  1.71 1.   0.62 0.04 0.23 0.01 7.85]\n",
      "Loss_Q: [0.95 1.19 0.84 0.62 0.05 0.19 0.01 0.   3.86] Loss_P: [2.37 1.86 1.72 0.97 0.63 0.06 0.22 0.02 7.86]\n",
      "Loss_Q: [0.91 1.22 0.89 0.57 0.06 0.18 0.01 0.   3.85] Loss_P: [2.4  1.89 1.61 1.04 0.64 0.06 0.21 0.02 7.87]\n",
      "Loss_Q: [0.91 1.27 0.9  0.6  0.07 0.24 0.01 0.   3.98] Loss_P: [2.42 1.84 1.66 1.03 0.59 0.06 0.22 0.02 7.85]\n",
      "Loss_Q: [0.87 1.25 0.87 0.62 0.06 0.19 0.02 0.   3.89] Loss_P: [2.4  1.83 1.64 1.01 0.68 0.06 0.21 0.01 7.82]\n",
      "Loss_Q: [0.82 1.22 0.91 0.66 0.05 0.22 0.01 0.   3.88] Loss_P: [2.33 1.89 1.68 0.91 0.61 0.06 0.22 0.02 7.71]\n",
      "Loss_Q: [1.04 1.17 0.87 0.6  0.06 0.2  0.03 0.   3.97] Loss_P: [2.4  1.86 1.63 0.95 0.68 0.07 0.21 0.01 7.81]\n",
      "Loss_Q: [0.91 1.16 0.91 0.63 0.06 0.18 0.01 0.   3.85] Loss_P: [2.29 1.91 1.64 1.03 0.66 0.08 0.21 0.02 7.84]\n",
      "Loss_Q: [0.96 1.19 0.93 0.61 0.04 0.19 0.01 0.   3.94] Loss_P: [2.35 1.87 1.65 1.02 0.64 0.06 0.22 0.03 7.84]\n",
      "Loss_Q: [0.91 1.22 0.9  0.62 0.06 0.18 0.01 0.   3.9 ] Loss_P: [2.31 1.98 1.66 0.98 0.63 0.09 0.2  0.01 7.87]\n",
      "Loss_Q: [1.   1.21 0.96 0.66 0.07 0.19 0.01 0.   4.1 ] Loss_P: [2.34 1.97 1.66 1.02 0.62 0.06 0.22 0.02 7.9 ]\n",
      "Loss_Q: [0.98 1.19 0.93 0.62 0.06 0.21 0.04 0.   4.02] Loss_P: [2.33 1.93 1.66 1.   0.62 0.06 0.19 0.01 7.78]\n",
      "Loss_Q: [0.93 1.24 0.9  0.57 0.07 0.19 0.01 0.   3.91] Loss_P: [2.36 1.89 1.68 0.97 0.6  0.08 0.21 0.02 7.8 ]\n",
      "Loss_Q: [0.99 1.16 0.85 0.57 0.09 0.2  0.01 0.   3.88] Loss_P: [2.43 1.92 1.67 0.94 0.59 0.09 0.2  0.01 7.84]\n",
      "Loss_Q: [0.93 1.23 0.87 0.6  0.08 0.21 0.02 0.   3.92] Loss_P: [2.31 1.95 1.61 0.93 0.58 0.07 0.2  0.02 7.67]\n",
      "Loss_Q: [0.98 1.21 0.86 0.58 0.08 0.19 0.01 0.   3.9 ] Loss_P: [2.36 1.91 1.63 0.94 0.59 0.07 0.21 0.01 7.72]\n",
      "Loss_Q: [0.95 1.18 0.82 0.54 0.09 0.19 0.01 0.   3.77] Loss_P: [2.3  1.97 1.64 0.84 0.56 0.07 0.22 0.01 7.62]\n",
      "Loss_Q: [0.95 1.2  0.78 0.54 0.09 0.19 0.02 0.   3.75] Loss_P: [2.34 1.91 1.63 0.89 0.56 0.08 0.22 0.02 7.64]\n",
      "Loss_Q: [0.94 1.17 0.79 0.51 0.09 0.19 0.01 0.   3.69] Loss_P: [2.27 1.99 1.62 0.9  0.58 0.09 0.17 0.02 7.63]\n",
      "Loss_Q: [0.95 1.18 0.8  0.49 0.06 0.19 0.02 0.   3.69] Loss_P: [2.31 1.93 1.64 0.81 0.57 0.08 0.21 0.01 7.56]\n",
      "Loss_Q: [0.87 1.16 0.74 0.53 0.07 0.22 0.03 0.   3.62] Loss_P: [2.37 1.94 1.69 0.83 0.54 0.08 0.19 0.01 7.65]\n",
      "Loss_Q: [0.89 1.22 0.78 0.5  0.07 0.18 0.01 0.   3.65] Loss_P: [2.36 1.97 1.68 0.83 0.57 0.08 0.2  0.02 7.71]\n",
      "Loss_Q: [0.91 1.21 0.78 0.47 0.07 0.2  0.02 0.   3.66] Loss_P: [2.33 1.94 1.69 0.83 0.54 0.07 0.2  0.01 7.6 ]\n",
      "Loss_Q: [0.96 1.25 0.82 0.5  0.08 0.22 0.01 0.   3.84] Loss_P: [2.3  1.98 1.7  0.88 0.55 0.08 0.23 0.02 7.74]\n",
      "Loss_Q: [0.87 1.24 0.8  0.48 0.08 0.21 0.01 0.   3.69] Loss_P: [2.28 1.95 1.69 0.87 0.54 0.09 0.2  0.01 7.62]\n",
      "Loss_Q: [0.91 1.29 0.75 0.54 0.08 0.24 0.01 0.   3.82] Loss_P: [2.32 1.91 1.68 0.86 0.52 0.06 0.2  0.02 7.56]\n",
      "Loss_Q: [0.97 1.26 0.76 0.55 0.09 0.19 0.01 0.   3.82] Loss_P: [2.31 1.97 1.63 0.86 0.54 0.07 0.23 0.01 7.61]\n",
      "Loss_Q: [0.89 1.18 0.76 0.58 0.09 0.21 0.01 0.   3.73] Loss_P: [2.39 1.94 1.63 0.87 0.52 0.05 0.21 0.01 7.63]\n",
      "Loss_Q: [0.9  1.19 0.76 0.55 0.06 0.19 0.01 0.   3.68] Loss_P: [2.3  1.92 1.68 0.84 0.52 0.08 0.23 0.02 7.58]\n",
      "Loss_Q: [0.93 1.17 0.76 0.53 0.08 0.19 0.02 0.   3.67] Loss_P: [2.28 1.96 1.68 0.8  0.53 0.07 0.2  0.01 7.53]\n",
      "Loss_Q: [0.98 1.25 0.75 0.58 0.06 0.22 0.01 0.   3.85] Loss_P: [2.24 2.02 1.68 0.86 0.54 0.07 0.18 0.01 7.6 ]\n",
      "Loss_Q: [1.02 1.21 0.75 0.53 0.08 0.23 0.02 0.   3.83] Loss_P: [2.32 1.93 1.62 0.83 0.51 0.09 0.21 0.02 7.52]\n",
      "Loss_Q: [0.9  1.2  0.75 0.53 0.07 0.24 0.01 0.   3.7 ] Loss_P: [2.25 1.97 1.65 0.84 0.53 0.09 0.2  0.01 7.54]\n",
      "Loss_Q: [0.91 1.21 0.75 0.52 0.06 0.2  0.01 0.   3.66] Loss_P: [2.31 1.99 1.66 0.8  0.54 0.08 0.23 0.02 7.63]\n",
      "Loss_Q: [0.93 1.2  0.74 0.51 0.08 0.22 0.01 0.   3.69] Loss_P: [2.3  1.96 1.68 0.75 0.53 0.1  0.21 0.02 7.56]\n",
      "Loss_Q: [0.9  1.18 0.76 0.52 0.08 0.2  0.01 0.   3.65] Loss_P: [2.32 1.93 1.63 0.81 0.51 0.07 0.21 0.01 7.49]\n",
      "Loss_Q: [0.9  1.21 0.82 0.51 0.06 0.18 0.01 0.   3.7 ] Loss_P: [2.31 1.93 1.69 0.86 0.51 0.07 0.23 0.03 7.63]\n",
      "Loss_Q: [0.92 1.24 0.8  0.51 0.09 0.2  0.01 0.   3.78] Loss_P: [2.3  1.94 1.67 0.85 0.53 0.07 0.22 0.01 7.58]\n",
      "Loss_Q: [0.89 1.19 0.83 0.47 0.07 0.22 0.02 0.   3.69] Loss_P: [2.34 1.96 1.68 0.89 0.56 0.09 0.21 0.01 7.73]\n",
      "Loss_Q: [0.88 1.19 0.8  0.53 0.08 0.21 0.01 0.   3.69] Loss_P: [2.34 1.94 1.68 0.88 0.58 0.06 0.23 0.01 7.72]\n",
      "Loss_Q: [0.93 1.14 0.78 0.53 0.07 0.2  0.01 0.   3.67] Loss_P: [2.32 1.9  1.67 0.91 0.55 0.07 0.19 0.01 7.62]\n",
      "Loss_Q: [0.91 1.28 0.87 0.61 0.08 0.19 0.01 0.   3.94] Loss_P: [2.29 1.94 1.72 0.93 0.59 0.06 0.17 0.02 7.72]\n",
      "Loss_Q: [0.95 1.17 0.82 0.55 0.06 0.21 0.01 0.   3.76] Loss_P: [2.35 1.91 1.65 0.84 0.62 0.06 0.19 0.01 7.64]\n",
      "Loss_Q: [1.09 1.21 0.81 0.54 0.07 0.21 0.02 0.   3.94] Loss_P: [2.33 1.86 1.68 0.91 0.54 0.05 0.2  0.02 7.59]\n",
      "Loss_Q: [0.96 1.23 0.81 0.57 0.07 0.19 0.02 0.   3.84] Loss_P: [2.36 1.89 1.69 0.86 0.53 0.08 0.19 0.01 7.6 ]\n",
      "Loss_Q: [0.94 1.24 0.81 0.53 0.09 0.19 0.01 0.   3.81] Loss_P: [2.28 1.96 1.68 0.88 0.56 0.08 0.19 0.01 7.64]\n",
      "Loss_Q: [0.99 1.18 0.77 0.6  0.07 0.18 0.02 0.   3.8 ] Loss_P: [2.34 1.94 1.71 0.84 0.54 0.07 0.18 0.01 7.63]\n",
      "Loss_Q: [0.97 1.21 0.75 0.54 0.08 0.22 0.02 0.   3.8 ] Loss_P: [2.33 1.9  1.71 0.91 0.57 0.09 0.17 0.01 7.68]\n",
      "Loss_Q: [0.97 1.16 0.8  0.51 0.06 0.2  0.01 0.   3.71] Loss_P: [2.36 1.87 1.66 0.85 0.55 0.06 0.19 0.01 7.56]\n",
      "Loss_Q: [0.99 1.15 0.8  0.54 0.05 0.2  0.01 0.   3.76] Loss_P: [2.31 1.9  1.64 0.85 0.54 0.07 0.21 0.02 7.53]\n",
      "Loss_Q: [0.95 1.19 0.86 0.58 0.09 0.23 0.01 0.   3.92] Loss_P: [2.33 1.87 1.7  0.93 0.59 0.07 0.23 0.01 7.75]\n",
      "Loss_Q: [0.91 1.19 0.85 0.58 0.05 0.21 0.02 0.   3.82] Loss_P: [2.34 1.91 1.7  0.9  0.55 0.06 0.21 0.02 7.69]\n",
      "Loss_Q: [0.93 1.23 0.85 0.57 0.06 0.21 0.01 0.   3.85] Loss_P: [2.29 1.9  1.66 0.94 0.54 0.06 0.26 0.02 7.66]\n",
      "Loss_Q: [0.9  1.19 0.81 0.53 0.07 0.21 0.01 0.   3.72] Loss_P: [2.29 1.93 1.67 0.95 0.57 0.06 0.2  0.02 7.68]\n",
      "Loss_Q: [0.96 1.21 0.84 0.56 0.07 0.23 0.01 0.   3.89] Loss_P: [2.39 1.86 1.58 0.92 0.54 0.06 0.19 0.03 7.57]\n",
      "Loss_Q: [0.95 1.22 0.82 0.57 0.06 0.21 0.01 0.   3.84] Loss_P: [2.31 1.89 1.66 0.9  0.54 0.08 0.25 0.01 7.64]\n",
      "Loss_Q: [0.93 1.23 0.83 0.48 0.08 0.23 0.01 0.   3.79] Loss_P: [2.36 1.89 1.6  0.91 0.5  0.06 0.2  0.01 7.53]\n",
      "Loss_Q: [0.88 1.22 0.82 0.47 0.06 0.19 0.02 0.   3.65] Loss_P: [2.38 1.83 1.65 0.89 0.52 0.07 0.2  0.02 7.57]\n",
      "Loss_Q: [0.88 1.23 0.8  0.46 0.07 0.21 0.02 0.   3.67] Loss_P: [2.3  1.9  1.69 0.88 0.48 0.06 0.22 0.01 7.54]\n",
      "Loss_Q: [0.97 1.22 0.77 0.49 0.05 0.21 0.01 0.   3.72] Loss_P: [2.35 1.87 1.66 0.86 0.48 0.08 0.19 0.01 7.5 ]\n",
      "Loss_Q: [0.89 1.24 0.79 0.53 0.07 0.2  0.01 0.   3.73] Loss_P: [2.42 1.88 1.65 0.86 0.52 0.07 0.19 0.01 7.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.89 1.21 0.8  0.56 0.07 0.2  0.02 0.   3.73] Loss_P: [2.39 1.93 1.64 0.86 0.53 0.08 0.2  0.01 7.65]\n",
      "Loss_Q: [0.98 1.2  0.76 0.56 0.05 0.18 0.02 0.   3.75] Loss_P: [2.36 1.92 1.62 0.9  0.52 0.09 0.17 0.01 7.58]\n",
      "Loss_Q: [0.9  1.16 0.74 0.52 0.06 0.16 0.02 0.   3.56] Loss_P: [2.34 1.86 1.6  0.85 0.58 0.06 0.19 0.02 7.51]\n",
      "Loss_Q: [0.88 1.16 0.81 0.58 0.06 0.18 0.01 0.   3.68] Loss_P: [2.3  1.88 1.63 0.82 0.56 0.06 0.19 0.03 7.47]\n",
      "Loss_Q: [0.97 1.14 0.81 0.56 0.07 0.2  0.01 0.   3.75] Loss_P: [2.31 1.99 1.66 0.9  0.57 0.04 0.23 0.02 7.72]\n",
      "Loss_Q: [0.96 1.24 0.85 0.61 0.06 0.17 0.01 0.   3.91] Loss_P: [2.38 1.91 1.69 0.9  0.59 0.07 0.18 0.02 7.74]\n",
      "Loss_Q: [0.92 1.2  0.82 0.62 0.07 0.19 0.02 0.   3.84] Loss_P: [2.31 1.93 1.63 0.85 0.62 0.06 0.18 0.01 7.58]\n",
      "Loss_Q: [0.98 1.21 0.82 0.61 0.05 0.19 0.01 0.   3.89] Loss_P: [2.28 1.9  1.75 0.93 0.61 0.08 0.14 0.01 7.7 ]\n",
      "Loss_Q: [0.99 1.32 0.89 0.59 0.06 0.18 0.02 0.   4.04] Loss_P: [2.29 1.96 1.72 0.92 0.62 0.07 0.18 0.01 7.78]\n",
      "Loss_Q: [1.03 1.23 0.84 0.53 0.08 0.18 0.02 0.   3.91] Loss_P: [2.35 1.94 1.67 0.93 0.57 0.06 0.17 0.01 7.7 ]\n",
      "Loss_Q: [0.96 1.24 0.87 0.55 0.06 0.18 0.02 0.   3.87] Loss_P: [2.29 1.95 1.74 0.92 0.59 0.08 0.19 0.02 7.78]\n",
      "Loss_Q: [1.02 1.26 0.84 0.53 0.07 0.17 0.02 0.   3.9 ] Loss_P: [2.28 1.94 1.78 0.91 0.55 0.05 0.17 0.02 7.71]\n",
      "Loss_Q: [1.01 1.25 0.91 0.55 0.07 0.17 0.01 0.   3.97] Loss_P: [2.31 1.97 1.74 0.94 0.53 0.06 0.19 0.02 7.76]\n",
      "Loss_Q: [0.94 1.19 0.83 0.6  0.07 0.18 0.01 0.   3.83] Loss_P: [2.31 1.94 1.69 0.93 0.55 0.07 0.18 0.02 7.7 ]\n",
      "Loss_Q: [0.95 1.33 0.86 0.59 0.07 0.16 0.02 0.   3.98] Loss_P: [2.24 1.97 1.78 0.96 0.55 0.07 0.17 0.01 7.76]\n",
      "Loss_Q: [1.   1.31 0.87 0.62 0.08 0.19 0.02 0.   4.09] Loss_P: [2.25 1.93 1.81 0.94 0.63 0.07 0.15 0.01 7.8 ]\n",
      "Loss_Q: [0.96 1.28 0.86 0.6  0.05 0.17 0.01 0.   3.93] Loss_P: [2.32 1.87 1.91 0.98 0.63 0.07 0.17 0.02 7.96]\n",
      "Loss_Q: [1.09 1.29 0.88 0.61 0.05 0.18 0.01 0.   4.11] Loss_P: [2.32 1.93 1.84 0.93 0.63 0.06 0.15 0.01 7.87]\n",
      "Loss_Q: [1.   1.33 0.85 0.59 0.06 0.17 0.01 0.   4.  ] Loss_P: [2.31 1.91 1.84 0.97 0.61 0.06 0.17 0.02 7.9 ]\n",
      "Loss_Q: [1.   1.28 0.88 0.56 0.09 0.19 0.01 0.   4.01] Loss_P: [2.22 1.96 1.82 0.92 0.55 0.05 0.16 0.01 7.69]\n",
      "Loss_Q: [0.96 1.32 0.89 0.59 0.07 0.18 0.01 0.   4.02] Loss_P: [2.31 1.92 1.79 0.96 0.59 0.07 0.16 0.01 7.81]\n",
      "Loss_Q: [0.98 1.33 0.9  0.58 0.04 0.15 0.02 0.   4.  ] Loss_P: [2.31 1.93 1.8  0.99 0.61 0.07 0.16 0.01 7.89]\n",
      "Loss_Q: [1.01 1.29 0.89 0.53 0.05 0.17 0.02 0.   3.95] Loss_P: [2.28 1.93 1.71 1.01 0.56 0.06 0.18 0.01 7.74]\n",
      "Loss_Q: [0.91 1.25 0.92 0.58 0.04 0.2  0.01 0.   3.9 ] Loss_P: [2.31 1.87 1.76 1.03 0.61 0.09 0.19 0.01 7.87]\n",
      "Loss_Q: [1.03 1.23 0.88 0.58 0.06 0.17 0.01 0.   3.97] Loss_P: [2.27 1.94 1.74 1.01 0.54 0.07 0.17 0.01 7.76]\n",
      "Loss_Q: [0.96 1.19 0.85 0.54 0.09 0.18 0.01 0.   3.82] Loss_P: [2.34 1.93 1.75 1.01 0.54 0.05 0.17 0.01 7.81]\n",
      "Loss_Q: [1.04 1.24 0.9  0.54 0.07 0.19 0.01 0.   4.  ] Loss_P: [2.3  1.92 1.77 0.98 0.52 0.08 0.19 0.01 7.77]\n",
      "Loss_Q: [0.99 1.25 0.9  0.53 0.08 0.16 0.01 0.   3.93] Loss_P: [2.27 1.9  1.77 1.04 0.52 0.06 0.2  0.01 7.76]\n",
      "Loss_Q: [1.01 1.26 0.9  0.53 0.05 0.19 0.01 0.   3.95] Loss_P: [2.29 1.88 1.75 0.98 0.53 0.07 0.19 0.01 7.71]\n",
      "Loss_Q: [1.02 1.2  0.96 0.52 0.07 0.21 0.01 0.   3.98] Loss_P: [2.36 1.9  1.74 1.05 0.52 0.07 0.21 0.02 7.87]\n",
      "Loss_Q: [1.07 1.25 0.98 0.51 0.05 0.18 0.01 0.   4.07] Loss_P: [2.26 1.95 1.8  1.09 0.53 0.07 0.17 0.01 7.87]\n",
      "Loss_Q: [0.93 1.23 0.88 0.53 0.09 0.18 0.01 0.   3.84] Loss_P: [2.33 1.87 1.77 1.02 0.51 0.06 0.21 0.01 7.8 ]\n",
      "Loss_Q: [1.04 1.22 0.92 0.56 0.07 0.17 0.02 0.   4.01] Loss_P: [2.3  1.89 1.74 1.05 0.55 0.06 0.18 0.02 7.8 ]\n",
      "Loss_Q: [0.99 1.22 0.91 0.52 0.09 0.2  0.01 0.   3.93] Loss_P: [2.3  1.9  1.8  1.06 0.54 0.09 0.19 0.02 7.9 ]\n",
      "Loss_Q: [1.07 1.22 0.93 0.52 0.06 0.2  0.01 0.   4.01] Loss_P: [2.34 1.87 1.79 1.03 0.54 0.05 0.22 0.02 7.87]\n",
      "Loss_Q: [0.95 1.21 0.92 0.56 0.11 0.2  0.03 0.   3.98] Loss_P: [2.32 1.79 1.78 1.1  0.57 0.09 0.16 0.02 7.83]\n",
      "Loss_Q: [1.02 1.2  0.93 0.57 0.07 0.2  0.02 0.   4.01] Loss_P: [2.31 1.83 1.79 1.04 0.54 0.07 0.21 0.01 7.8 ]\n",
      "Loss_Q: [1.01 1.23 0.91 0.57 0.07 0.19 0.01 0.   3.99] Loss_P: [2.29 1.93 1.79 1.   0.58 0.06 0.18 0.01 7.84]\n",
      "Loss_Q: [1.01 1.24 0.89 0.56 0.07 0.2  0.01 0.   3.99] Loss_P: [2.3  1.9  1.75 1.   0.56 0.06 0.17 0.01 7.76]\n",
      "Loss_Q: [1.02 1.23 0.88 0.6  0.05 0.2  0.01 0.   4.01] Loss_P: [2.34 1.83 1.78 0.95 0.6  0.06 0.16 0.01 7.75]\n",
      "Loss_Q: [1.01 1.27 0.92 0.59 0.09 0.2  0.01 0.   4.09] Loss_P: [2.32 1.89 1.77 0.98 0.6  0.08 0.18 0.01 7.84]\n",
      "Loss_Q: [1.01 1.23 0.9  0.62 0.08 0.19 0.01 0.   4.06] Loss_P: [2.32 1.91 1.77 0.96 0.66 0.07 0.21 0.02 7.91]\n",
      "Loss_Q: [0.94 1.23 0.89 0.6  0.07 0.22 0.02 0.   3.96] Loss_P: [2.32 1.87 1.73 1.02 0.61 0.07 0.19 0.01 7.84]\n",
      "Loss_Q: [1.   1.25 0.89 0.59 0.06 0.21 0.01 0.   4.01] Loss_P: [2.3  1.87 1.76 0.99 0.59 0.1  0.21 0.02 7.84]\n",
      "Loss_Q: [1.03 1.25 0.89 0.62 0.08 0.21 0.03 0.   4.1 ] Loss_P: [2.3  1.89 1.78 0.93 0.6  0.07 0.2  0.02 7.78]\n",
      "Loss_Q: [0.97 1.19 0.84 0.59 0.08 0.18 0.02 0.   3.86] Loss_P: [2.35 1.88 1.77 0.93 0.6  0.07 0.21 0.01 7.81]\n",
      "Loss_Q: [1.03 1.27 0.88 0.56 0.08 0.2  0.02 0.   4.03] Loss_P: [2.28 1.92 1.85 0.92 0.54 0.06 0.2  0.02 7.77]\n",
      "Loss_Q: [1.03 1.23 0.88 0.56 0.06 0.19 0.02 0.   3.97] Loss_P: [2.32 1.84 1.83 0.98 0.58 0.05 0.19 0.03 7.83]\n",
      "Loss_Q: [1.03 1.19 0.82 0.59 0.07 0.21 0.01 0.   3.92] Loss_P: [2.31 1.85 1.82 0.96 0.54 0.06 0.17 0.02 7.72]\n",
      "Loss_Q: [1.   1.18 0.87 0.54 0.07 0.19 0.01 0.   3.86] Loss_P: [2.35 1.83 1.81 0.96 0.55 0.09 0.19 0.02 7.8 ]\n",
      "Loss_Q: [1.07 1.21 0.83 0.56 0.07 0.21 0.02 0.   3.97] Loss_P: [2.29 1.88 1.79 1.01 0.57 0.07 0.21 0.02 7.83]\n",
      "Loss_Q: [1.   1.25 0.83 0.52 0.08 0.19 0.02 0.   3.89] Loss_P: [2.28 1.88 1.8  0.99 0.55 0.05 0.2  0.02 7.77]\n",
      "Loss_Q: [1.01 1.25 0.87 0.55 0.07 0.16 0.01 0.   3.9 ] Loss_P: [2.31 1.86 1.79 1.02 0.51 0.09 0.2  0.01 7.79]\n",
      "Loss_Q: [1.01 1.2  0.83 0.5  0.07 0.21 0.02 0.   3.84] Loss_P: [2.34 1.87 1.75 1.   0.55 0.06 0.19 0.01 7.76]\n",
      "Loss_Q: [1.02 1.2  0.89 0.53 0.07 0.19 0.01 0.   3.92] Loss_P: [2.3  1.84 1.82 0.93 0.52 0.07 0.2  0.02 7.69]\n",
      "Loss_Q: [0.97 1.21 0.87 0.51 0.08 0.18 0.01 0.   3.83] Loss_P: [2.31 1.86 1.76 1.02 0.56 0.07 0.2  0.01 7.79]\n",
      "Loss_Q: [0.98 1.23 0.86 0.5  0.06 0.21 0.03 0.   3.87] Loss_P: [2.35 1.81 1.82 0.96 0.53 0.08 0.21 0.01 7.76]\n",
      "Loss_Q: [1.06 1.19 0.88 0.58 0.08 0.18 0.02 0.   3.99] Loss_P: [2.29 1.9  1.81 0.96 0.58 0.09 0.23 0.02 7.87]\n",
      "Loss_Q: [1.08 1.2  0.85 0.54 0.06 0.18 0.01 0.   3.93] Loss_P: [2.28 1.87 1.82 0.94 0.57 0.07 0.2  0.01 7.77]\n",
      "Loss_Q: [1.04 1.25 0.77 0.55 0.05 0.18 0.01 0.   3.85] Loss_P: [2.25 1.92 1.78 0.89 0.54 0.07 0.17 0.01 7.64]\n",
      "Loss_Q: [1.05 1.16 0.83 0.54 0.09 0.18 0.02 0.   3.87] Loss_P: [2.37 1.89 1.78 0.91 0.58 0.08 0.19 0.01 7.82]\n",
      "Loss_Q: [1.08 1.22 0.75 0.56 0.08 0.2  0.01 0.   3.91] Loss_P: [2.32 1.89 1.74 0.95 0.59 0.08 0.19 0.01 7.78]\n",
      "Loss_Q: [1.08 1.2  0.89 0.56 0.08 0.19 0.02 0.   4.04] Loss_P: [2.33 1.88 1.78 0.96 0.61 0.05 0.22 0.02 7.84]\n",
      "Loss_Q: [1.05 1.15 0.81 0.59 0.05 0.21 0.03 0.   3.88] Loss_P: [2.28 1.9  1.77 0.88 0.56 0.09 0.2  0.01 7.7 ]\n",
      "Loss_Q: [1.08 1.18 0.78 0.61 0.09 0.18 0.02 0.   3.94] Loss_P: [2.34 1.89 1.77 0.86 0.57 0.07 0.2  0.02 7.73]\n",
      "Loss_Q: [1.07 1.22 0.82 0.58 0.06 0.18 0.01 0.   3.93] Loss_P: [2.32 1.93 1.76 0.9  0.54 0.07 0.2  0.01 7.73]\n",
      "Loss_Q: [1.07 1.31 0.81 0.54 0.07 0.19 0.02 0.   4.03] Loss_P: [2.35 1.87 1.83 0.87 0.54 0.07 0.16 0.01 7.71]\n",
      "Loss_Q: [1.05 1.26 0.86 0.64 0.06 0.21 0.01 0.   4.09] Loss_P: [2.32 1.91 1.77 0.9  0.59 0.07 0.2  0.02 7.78]\n",
      "Loss_Q: [1.02 1.24 0.79 0.55 0.06 0.24 0.02 0.   3.91] Loss_P: [2.33 1.9  1.84 0.9  0.58 0.07 0.18 0.01 7.83]\n",
      "Loss_Q: [1.04 1.21 0.82 0.54 0.07 0.21 0.01 0.   3.91] Loss_P: [2.25 1.91 1.76 0.98 0.59 0.08 0.21 0.01 7.79]\n",
      "Loss_Q: [1.08 1.22 0.85 0.57 0.06 0.2  0.01 0.   3.99] Loss_P: [2.32 1.87 1.8  0.93 0.52 0.08 0.21 0.02 7.76]\n",
      "Loss_Q: [1.07 1.25 0.86 0.55 0.05 0.2  0.01 0.   4.  ] Loss_P: [2.35 1.89 1.72 0.93 0.56 0.08 0.18 0.02 7.74]\n",
      "Loss_Q: [0.98 1.25 0.84 0.51 0.08 0.16 0.01 0.   3.84] Loss_P: [2.3  1.88 1.74 0.93 0.49 0.08 0.19 0.02 7.63]\n",
      "Loss_Q: [1.08 1.17 0.84 0.54 0.09 0.18 0.01 0.   3.91] Loss_P: [2.28 1.94 1.79 0.86 0.52 0.07 0.21 0.01 7.69]\n",
      "Loss_Q: [1.09 1.28 0.83 0.55 0.06 0.2  0.02 0.   4.02] Loss_P: [2.38 1.96 1.75 0.89 0.56 0.08 0.22 0.01 7.86]\n",
      "Loss_Q: [1.07 1.27 0.84 0.51 0.08 0.22 0.02 0.   4.01] Loss_P: [2.31 1.93 1.75 0.9  0.55 0.08 0.21 0.02 7.74]\n",
      "Loss_Q: [1.01 1.2  0.81 0.52 0.06 0.21 0.01 0.   3.82] Loss_P: [2.33 1.91 1.76 0.94 0.58 0.08 0.21 0.01 7.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.03 1.18 0.81 0.53 0.07 0.19 0.01 0.   3.81] Loss_P: [2.33 1.93 1.7  0.88 0.53 0.08 0.16 0.01 7.62]\n",
      "Loss_Q: [1.08 1.24 0.81 0.54 0.08 0.21 0.02 0.   3.97] Loss_P: [2.29 1.92 1.74 0.91 0.52 0.09 0.2  0.01 7.68]\n",
      "Loss_Q: [1.06 1.23 0.77 0.54 0.06 0.2  0.02 0.   3.88] Loss_P: [2.29 1.92 1.75 0.89 0.53 0.07 0.19 0.01 7.66]\n",
      "Loss_Q: [1.06 1.22 0.83 0.49 0.05 0.2  0.01 0.   3.87] Loss_P: [2.28 2.01 1.71 0.89 0.52 0.09 0.21 0.02 7.72]\n",
      "Loss_Q: [1.09 1.18 0.84 0.59 0.06 0.18 0.01 0.   3.94] Loss_P: [2.31 1.94 1.67 0.93 0.54 0.08 0.23 0.01 7.7 ]\n",
      "Loss_Q: [1.01 1.18 0.81 0.55 0.07 0.2  0.01 0.   3.83] Loss_P: [2.29 2.07 1.68 0.89 0.6  0.08 0.19 0.01 7.81]\n",
      "Loss_Q: [1.04 1.19 0.79 0.53 0.08 0.2  0.01 0.   3.84] Loss_P: [2.3  1.98 1.64 0.88 0.6  0.06 0.21 0.02 7.7 ]\n",
      "Loss_Q: [1.14 1.16 0.83 0.6  0.07 0.2  0.01 0.   4.02] Loss_P: [2.33 1.96 1.7  0.88 0.54 0.08 0.21 0.02 7.73]\n",
      "Loss_Q: [1.09 1.18 0.8  0.56 0.06 0.21 0.03 0.   3.92] Loss_P: [2.25 2.02 1.69 0.89 0.6  0.08 0.22 0.02 7.76]\n",
      "Loss_Q: [1.16 1.13 0.79 0.55 0.07 0.23 0.02 0.   3.96] Loss_P: [2.24 2.03 1.68 0.91 0.57 0.08 0.21 0.01 7.72]\n",
      "Loss_Q: [1.11 1.19 0.86 0.56 0.08 0.2  0.02 0.   4.01] Loss_P: [2.3  1.99 1.67 0.9  0.55 0.05 0.22 0.03 7.7 ]\n",
      "Loss_Q: [1.04 1.21 0.81 0.55 0.06 0.21 0.01 0.   3.89] Loss_P: [2.34 1.96 1.7  0.9  0.57 0.06 0.23 0.02 7.77]\n",
      "Loss_Q: [1.08 1.23 0.82 0.6  0.08 0.23 0.01 0.   4.03] Loss_P: [2.28 2.01 1.72 0.89 0.59 0.05 0.2  0.02 7.76]\n",
      "Loss_Q: [0.98 1.25 0.86 0.61 0.08 0.19 0.01 0.   3.99] Loss_P: [2.38 1.88 1.75 0.91 0.59 0.07 0.21 0.01 7.79]\n",
      "Loss_Q: [1.14 1.19 0.85 0.62 0.07 0.25 0.01 0.   4.12] Loss_P: [2.32 1.98 1.67 0.94 0.58 0.07 0.23 0.01 7.8 ]\n",
      "Loss_Q: [1.12 1.17 0.84 0.58 0.08 0.22 0.02 0.   4.03] Loss_P: [2.27 1.99 1.71 0.9  0.64 0.06 0.22 0.02 7.81]\n",
      "Loss_Q: [1.07 1.23 0.85 0.58 0.08 0.2  0.02 0.   4.03] Loss_P: [2.29 1.97 1.66 0.9  0.59 0.09 0.21 0.02 7.72]\n",
      "Loss_Q: [1.12 1.25 0.81 0.58 0.07 0.21 0.02 0.   4.04] Loss_P: [2.29 2.02 1.68 0.9  0.56 0.05 0.21 0.02 7.73]\n",
      "Loss_Q: [1.07 1.24 0.87 0.57 0.09 0.22 0.01 0.   4.06] Loss_P: [2.31 1.96 1.73 0.93 0.58 0.08 0.22 0.02 7.83]\n",
      "Loss_Q: [0.98 1.24 0.86 0.58 0.07 0.24 0.02 0.   4.  ] Loss_P: [2.39 1.98 1.67 0.9  0.61 0.06 0.23 0.01 7.85]\n",
      "Loss_Q: [1.06 1.29 0.83 0.52 0.06 0.21 0.02 0.   3.99] Loss_P: [2.29 1.98 1.69 0.94 0.57 0.08 0.21 0.02 7.78]\n",
      "Loss_Q: [1.   1.21 0.84 0.56 0.08 0.22 0.01 0.   3.91] Loss_P: [2.43 1.93 1.67 0.9  0.65 0.08 0.2  0.01 7.87]\n",
      "Loss_Q: [1.08 1.2  0.84 0.6  0.07 0.23 0.01 0.   4.01] Loss_P: [2.34 1.95 1.68 0.92 0.58 0.06 0.18 0.02 7.74]\n",
      "Loss_Q: [1.06 1.2  0.85 0.61 0.07 0.22 0.01 0.   4.02] Loss_P: [2.33 2.01 1.66 0.94 0.58 0.06 0.25 0.01 7.85]\n",
      "Loss_Q: [1.05 1.26 0.84 0.61 0.07 0.21 0.01 0.   4.06] Loss_P: [2.28 2.02 1.69 0.99 0.61 0.08 0.22 0.01 7.9 ]\n",
      "Loss_Q: [1.02 1.18 0.81 0.55 0.07 0.2  0.01 0.   3.84] Loss_P: [2.34 1.97 1.65 0.9  0.63 0.07 0.19 0.01 7.75]\n",
      "Loss_Q: [1.03 1.23 0.89 0.6  0.07 0.2  0.01 0.   4.01] Loss_P: [2.33 1.99 1.67 0.93 0.6  0.1  0.2  0.01 7.84]\n",
      "Loss_Q: [1.02 1.22 0.81 0.6  0.08 0.2  0.01 0.   3.94] Loss_P: [2.35 1.95 1.73 0.89 0.61 0.07 0.22 0.01 7.82]\n",
      "Loss_Q: [1.08 1.25 0.88 0.64 0.07 0.24 0.02 0.   4.18] Loss_P: [2.37 1.91 1.68 0.91 0.65 0.08 0.24 0.01 7.85]\n",
      "Loss_Q: [0.99 1.16 0.85 0.66 0.06 0.24 0.02 0.   3.98] Loss_P: [2.34 1.94 1.71 0.94 0.63 0.08 0.23 0.03 7.9 ]\n",
      "Loss_Q: [1.04 1.21 0.87 0.67 0.08 0.25 0.01 0.   4.12] Loss_P: [2.35 1.88 1.65 0.97 0.67 0.05 0.25 0.02 7.84]\n",
      "Loss_Q: [0.99 1.23 0.84 0.59 0.07 0.24 0.03 0.   3.99] Loss_P: [2.38 1.9  1.61 0.95 0.64 0.06 0.24 0.01 7.78]\n",
      "Loss_Q: [0.99 1.21 0.83 0.69 0.07 0.25 0.01 0.   4.05] Loss_P: [2.41 1.92 1.52 0.97 0.65 0.07 0.23 0.01 7.78]\n",
      "Loss_Q: [0.99 1.14 0.88 0.67 0.07 0.25 0.02 0.   4.02] Loss_P: [2.37 1.92 1.63 1.   0.67 0.07 0.25 0.02 7.92]\n",
      "Loss_Q: [1.07 1.23 0.93 0.65 0.05 0.26 0.02 0.   4.21] Loss_P: [2.34 1.92 1.61 0.97 0.69 0.05 0.24 0.02 7.84]\n",
      "Loss_Q: [0.99 1.12 0.89 0.67 0.07 0.2  0.02 0.   3.95] Loss_P: [2.37 1.85 1.64 0.94 0.68 0.05 0.26 0.02 7.8 ]\n",
      "Loss_Q: [0.99 1.17 0.87 0.66 0.05 0.25 0.02 0.   4.01] Loss_P: [2.35 1.95 1.67 0.93 0.68 0.07 0.22 0.01 7.89]\n",
      "Loss_Q: [1.05 1.23 0.89 0.61 0.07 0.23 0.02 0.   4.1 ] Loss_P: [2.31 1.94 1.63 0.96 0.63 0.08 0.27 0.03 7.85]\n",
      "Loss_Q: [1.04 1.21 0.85 0.64 0.06 0.27 0.03 0.   4.1 ] Loss_P: [2.38 1.91 1.66 0.94 0.62 0.05 0.24 0.01 7.82]\n",
      "Loss_Q: [1.01 1.2  0.9  0.64 0.07 0.25 0.01 0.   4.08] Loss_P: [2.34 1.93 1.66 1.01 0.68 0.07 0.29 0.02 8.01]\n",
      "Loss_Q: [1.05 1.24 0.94 0.63 0.04 0.26 0.02 0.   4.17] Loss_P: [2.39 1.93 1.67 1.02 0.67 0.06 0.23 0.02 7.99]\n",
      "Loss_Q: [1.02 1.15 0.92 0.66 0.05 0.25 0.02 0.   4.06] Loss_P: [2.36 1.97 1.68 1.04 0.63 0.07 0.24 0.03 8.01]\n",
      "Loss_Q: [0.99 1.13 0.9  0.58 0.06 0.23 0.02 0.   3.9 ] Loss_P: [2.35 2.01 1.63 1.02 0.58 0.06 0.23 0.01 7.89]\n",
      "Loss_Q: [0.95 1.22 0.91 0.58 0.06 0.21 0.02 0.   3.95] Loss_P: [2.29 1.98 1.62 0.98 0.63 0.05 0.22 0.02 7.81]\n",
      "Loss_Q: [1.07 1.22 0.92 0.61 0.06 0.19 0.01 0.   4.09] Loss_P: [2.31 1.97 1.67 0.99 0.58 0.05 0.19 0.01 7.78]\n",
      "Loss_Q: [1.1  1.24 0.95 0.6  0.07 0.22 0.02 0.   4.2 ] Loss_P: [2.32 1.96 1.67 1.02 0.62 0.06 0.23 0.02 7.91]\n",
      "Loss_Q: [1.01 1.18 0.93 0.68 0.09 0.25 0.02 0.   4.16] Loss_P: [2.36 2.01 1.69 1.05 0.64 0.09 0.22 0.01 8.06]\n",
      "Loss_Q: [1.08 1.25 0.93 0.66 0.04 0.25 0.01 0.   4.23] Loss_P: [2.29 1.97 1.64 1.02 0.64 0.06 0.23 0.02 7.86]\n",
      "Loss_Q: [1.03 1.31 0.9  0.65 0.06 0.24 0.02 0.   4.2 ] Loss_P: [2.3  2.01 1.7  1.04 0.64 0.06 0.25 0.02 8.01]\n",
      "Loss_Q: [1.1  1.29 0.93 0.65 0.07 0.27 0.02 0.   4.32] Loss_P: [2.34 1.96 1.74 0.99 0.62 0.06 0.24 0.01 7.96]\n",
      "Loss_Q: [1.05 1.31 0.9  0.64 0.05 0.28 0.01 0.   4.24] Loss_P: [2.36 1.87 1.75 1.01 0.64 0.07 0.25 0.03 7.98]\n",
      "Loss_Q: [1.12 1.29 0.9  0.62 0.07 0.25 0.01 0.   4.26] Loss_P: [2.32 1.93 1.75 0.98 0.67 0.07 0.27 0.02 8.  ]\n",
      "Loss_Q: [1.04 1.28 0.85 0.62 0.07 0.27 0.01 0.   4.14] Loss_P: [2.35 1.99 1.71 1.02 0.71 0.06 0.26 0.03 8.12]\n",
      "Loss_Q: [1.08 1.21 0.92 0.65 0.07 0.23 0.01 0.   4.18] Loss_P: [2.33 2.   1.7  0.98 0.66 0.05 0.25 0.02 7.99]\n",
      "Loss_Q: [1.09 1.27 0.93 0.69 0.05 0.29 0.02 0.   4.32] Loss_P: [2.37 1.89 1.69 1.01 0.7  0.06 0.31 0.01 8.04]\n",
      "Loss_Q: [1.08 1.27 0.87 0.66 0.07 0.28 0.02 0.   4.24] Loss_P: [2.3  1.94 1.68 0.95 0.68 0.05 0.27 0.01 7.89]\n",
      "Loss_Q: [1.11 1.16 0.89 0.65 0.06 0.27 0.02 0.   4.16] Loss_P: [2.31 1.89 1.64 0.98 0.68 0.06 0.25 0.01 7.81]\n",
      "Loss_Q: [1.17 1.17 0.85 0.62 0.05 0.26 0.02 0.   4.13] Loss_P: [2.31 1.98 1.6  0.96 0.66 0.05 0.32 0.02 7.9 ]\n",
      "Loss_Q: [1.03 1.24 0.86 0.65 0.08 0.28 0.02 0.   4.17] Loss_P: [2.33 1.97 1.62 0.94 0.69 0.06 0.27 0.02 7.9 ]\n",
      "Loss_Q: [1.06 1.18 0.83 0.64 0.05 0.25 0.01 0.   4.02] Loss_P: [2.3  2.   1.74 1.   0.67 0.06 0.28 0.02 8.06]\n",
      "Loss_Q: [1.04 1.19 0.81 0.67 0.06 0.31 0.02 0.   4.11] Loss_P: [2.35 1.89 1.61 0.91 0.65 0.04 0.26 0.02 7.74]\n",
      "Loss_Q: [1.1  1.21 0.79 0.64 0.05 0.29 0.02 0.   4.1 ] Loss_P: [2.32 1.9  1.65 0.9  0.67 0.06 0.29 0.01 7.81]\n",
      "Loss_Q: [1.   1.22 0.83 0.66 0.05 0.26 0.01 0.   4.02] Loss_P: [2.36 1.97 1.72 0.91 0.66 0.04 0.27 0.02 7.95]\n",
      "Loss_Q: [1.06 1.24 0.8  0.64 0.04 0.25 0.01 0.   4.03] Loss_P: [2.3  1.92 1.69 0.92 0.68 0.05 0.26 0.02 7.83]\n",
      "Loss_Q: [1.09 1.19 0.81 0.64 0.07 0.3  0.02 0.   4.12] Loss_P: [2.31 1.91 1.66 0.89 0.67 0.04 0.24 0.01 7.74]\n",
      "Loss_Q: [1.1  1.21 0.82 0.6  0.06 0.26 0.01 0.   4.06] Loss_P: [2.36 1.97 1.68 0.94 0.64 0.05 0.28 0.02 7.94]\n",
      "Loss_Q: [1.09 1.16 0.86 0.63 0.07 0.32 0.01 0.   4.15] Loss_P: [2.32 1.97 1.64 0.99 0.63 0.06 0.29 0.02 7.93]\n",
      "Loss_Q: [1.06 1.17 0.85 0.7  0.05 0.27 0.01 0.   4.1 ] Loss_P: [2.32 1.94 1.7  0.94 0.65 0.06 0.29 0.01 7.9 ]\n",
      "Loss_Q: [1.09 1.22 0.86 0.64 0.06 0.29 0.01 0.   4.17] Loss_P: [2.34 1.96 1.7  0.95 0.65 0.05 0.29 0.03 7.95]\n",
      "Loss_Q: [1.07 1.19 0.82 0.63 0.06 0.28 0.01 0.   4.05] Loss_P: [2.37 1.95 1.68 0.92 0.64 0.07 0.3  0.02 7.95]\n",
      "Loss_Q: [1.05 1.22 0.89 0.62 0.06 0.26 0.01 0.   4.12] Loss_P: [2.33 1.97 1.69 0.97 0.66 0.06 0.27 0.02 7.97]\n",
      "Loss_Q: [1.06 1.2  0.86 0.62 0.08 0.31 0.01 0.   4.14] Loss_P: [2.38 1.93 1.65 1.   0.73 0.05 0.3  0.01 8.05]\n",
      "Loss_Q: [1.02 1.24 0.86 0.67 0.08 0.3  0.02 0.   4.18] Loss_P: [2.38 1.84 1.77 0.97 0.63 0.07 0.3  0.01 7.96]\n",
      "Loss_Q: [1.02 1.17 0.87 0.62 0.07 0.3  0.02 0.   4.08] Loss_P: [2.37 1.89 1.71 0.96 0.66 0.07 0.32 0.01 8.  ]\n",
      "Loss_Q: [1.07 1.17 0.86 0.64 0.06 0.29 0.01 0.   4.1 ] Loss_P: [2.32 1.92 1.74 1.02 0.66 0.08 0.3  0.01 8.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.02 1.2  0.87 0.75 0.05 0.31 0.02 0.   4.22] Loss_P: [2.33 1.92 1.7  1.01 0.64 0.07 0.29 0.02 7.97]\n",
      "Loss_Q: [1.06 1.23 0.85 0.66 0.07 0.28 0.04 0.   4.18] Loss_P: [2.31 1.88 1.68 0.96 0.65 0.05 0.31 0.01 7.87]\n",
      "Loss_Q: [1.   1.24 0.87 0.65 0.07 0.27 0.01 0.   4.11] Loss_P: [2.41 1.88 1.71 1.03 0.65 0.06 0.27 0.01 8.03]\n",
      "Loss_Q: [1.04 1.14 0.85 0.65 0.06 0.3  0.02 0.   4.06] Loss_P: [2.35 1.86 1.68 0.94 0.69 0.06 0.29 0.01 7.88]\n",
      "Loss_Q: [1.1  1.17 0.88 0.64 0.06 0.28 0.02 0.   4.15] Loss_P: [2.33 1.89 1.72 0.98 0.63 0.06 0.28 0.01 7.9 ]\n",
      "Loss_Q: [1.1  1.18 0.91 0.64 0.08 0.25 0.01 0.   4.17] Loss_P: [2.41 1.85 1.71 1.02 0.67 0.05 0.32 0.02 8.04]\n",
      "Loss_Q: [1.03 1.22 0.86 0.67 0.07 0.3  0.01 0.   4.16] Loss_P: [2.25 1.89 1.74 0.97 0.7  0.06 0.28 0.01 7.89]\n",
      "Loss_Q: [1.06 1.12 0.89 0.64 0.07 0.3  0.02 0.   4.1 ] Loss_P: [2.33 1.86 1.71 0.93 0.65 0.05 0.31 0.02 7.86]\n",
      "Loss_Q: [1.04 1.2  0.85 0.63 0.07 0.32 0.01 0.   4.13] Loss_P: [2.35 1.83 1.75 0.92 0.62 0.06 0.32 0.02 7.86]\n",
      "Loss_Q: [1.1  1.15 0.86 0.64 0.06 0.34 0.02 0.   4.18] Loss_P: [2.31 1.9  1.74 0.94 0.66 0.06 0.35 0.02 7.98]\n",
      "Loss_Q: [1.11 1.17 0.83 0.64 0.05 0.31 0.02 0.   4.13] Loss_P: [2.31 1.91 1.74 0.98 0.63 0.07 0.3  0.01 7.96]\n",
      "Loss_Q: [1.16 1.19 0.87 0.62 0.06 0.32 0.01 0.   4.24] Loss_P: [2.36 1.83 1.7  0.95 0.62 0.07 0.34 0.02 7.88]\n",
      "Loss_Q: [0.97 1.12 0.83 0.63 0.08 0.35 0.01 0.   3.99] Loss_P: [2.34 1.82 1.64 0.93 0.63 0.05 0.36 0.02 7.77]\n",
      "Loss_Q: [1.04 1.16 0.79 0.64 0.05 0.4  0.02 0.   4.11] Loss_P: [2.36 1.87 1.61 0.97 0.66 0.06 0.34 0.01 7.89]\n",
      "Loss_Q: [1.05 1.21 0.9  0.61 0.07 0.38 0.03 0.   4.24] Loss_P: [2.32 1.95 1.68 0.98 0.62 0.08 0.37 0.01 8.01]\n",
      "Loss_Q: [0.96 1.17 0.88 0.61 0.08 0.37 0.01 0.   4.08] Loss_P: [2.24 1.93 1.72 1.01 0.62 0.05 0.36 0.02 7.96]\n",
      "Loss_Q: [1.03 1.17 0.85 0.62 0.07 0.36 0.02 0.   4.11] Loss_P: [2.35 1.9  1.69 1.01 0.67 0.07 0.36 0.04 8.1 ]\n",
      "Loss_Q: [1.07 1.2  0.88 0.67 0.06 0.33 0.01 0.   4.23] Loss_P: [2.33 1.95 1.75 1.02 0.69 0.08 0.36 0.02 8.19]\n",
      "Loss_Q: [1.05 1.22 0.89 0.67 0.07 0.34 0.02 0.   4.26] Loss_P: [2.33 1.95 1.68 1.01 0.67 0.05 0.37 0.01 8.07]\n",
      "Loss_Q: [1.11 1.12 0.84 0.65 0.05 0.38 0.01 0.   4.16] Loss_P: [2.29 1.88 1.7  0.98 0.68 0.09 0.37 0.04 8.03]\n",
      "Loss_Q: [1.1  1.15 0.85 0.69 0.07 0.38 0.01 0.   4.25] Loss_P: [2.34 1.85 1.75 0.93 0.66 0.07 0.37 0.02 7.99]\n",
      "Loss_Q: [1.07 1.18 0.84 0.64 0.08 0.36 0.02 0.   4.19] Loss_P: [2.35 1.89 1.74 0.94 0.67 0.07 0.35 0.02 8.03]\n",
      "Loss_Q: [1.03 1.17 0.82 0.63 0.06 0.36 0.02 0.   4.1 ] Loss_P: [2.3  1.85 1.68 0.97 0.66 0.09 0.37 0.03 7.95]\n",
      "Loss_Q: [1.08 1.14 0.81 0.64 0.08 0.36 0.01 0.   4.12] Loss_P: [2.3  1.89 1.69 0.88 0.71 0.07 0.38 0.02 7.93]\n",
      "Loss_Q: [1.02 1.22 0.81 0.67 0.06 0.37 0.01 0.   4.16] Loss_P: [2.32 1.85 1.72 0.9  0.69 0.08 0.39 0.01 7.96]\n",
      "Loss_Q: [1.06 1.16 0.82 0.65 0.08 0.37 0.01 0.   4.13] Loss_P: [2.34 1.88 1.72 0.9  0.7  0.11 0.41 0.02 8.06]\n",
      "Loss_Q: [0.99 1.18 0.84 0.66 0.09 0.36 0.01 0.   4.14] Loss_P: [2.3  1.95 1.67 0.93 0.71 0.08 0.37 0.02 8.02]\n",
      "Loss_Q: [1.05 1.17 0.83 0.64 0.06 0.38 0.01 0.   4.14] Loss_P: [2.29 1.91 1.67 0.92 0.66 0.07 0.38 0.01 7.92]\n",
      "Loss_Q: [1.08 1.16 0.83 0.66 0.09 0.39 0.02 0.   4.23] Loss_P: [2.29 1.94 1.73 0.9  0.68 0.09 0.4  0.01 8.05]\n",
      "Loss_Q: [1.07 1.14 0.86 0.67 0.09 0.38 0.01 0.   4.23] Loss_P: [2.38 1.9  1.74 0.93 0.69 0.1  0.41 0.01 8.15]\n",
      "Loss_Q: [1.11 1.11 0.87 0.63 0.07 0.37 0.02 0.   4.19] Loss_P: [2.33 1.87 1.73 0.99 0.65 0.08 0.38 0.03 8.04]\n",
      "Loss_Q: [1.1  1.21 0.87 0.61 0.08 0.39 0.01 0.   4.27] Loss_P: [2.27 1.88 1.79 0.95 0.64 0.09 0.39 0.02 8.04]\n",
      "Loss_Q: [1.03 1.19 0.9  0.64 0.09 0.35 0.01 0.   4.21] Loss_P: [2.26 1.97 1.76 0.95 0.65 0.09 0.36 0.02 8.06]\n",
      "Loss_Q: [1.07 1.25 0.87 0.64 0.09 0.37 0.01 0.   4.31] Loss_P: [2.26 1.93 1.78 1.02 0.69 0.09 0.34 0.01 8.14]\n",
      "Loss_Q: [1.11 1.19 0.88 0.68 0.06 0.37 0.02 0.   4.31] Loss_P: [2.23 1.95 1.74 0.98 0.66 0.1  0.37 0.02 8.04]\n",
      "Loss_Q: [1.09 1.21 0.89 0.65 0.07 0.4  0.03 0.   4.34] Loss_P: [2.27 1.92 1.8  0.99 0.65 0.08 0.38 0.02 8.1 ]\n",
      "Loss_Q: [1.1  1.14 0.85 0.66 0.09 0.35 0.01 0.   4.21] Loss_P: [2.29 1.93 1.85 0.99 0.64 0.09 0.36 0.02 8.18]\n",
      "Loss_Q: [1.06 1.22 0.88 0.7  0.08 0.38 0.02 0.   4.34] Loss_P: [2.28 1.84 1.78 1.02 0.69 0.09 0.37 0.02 8.08]\n",
      "Loss_Q: [1.09 1.17 0.92 0.66 0.08 0.38 0.02 0.   4.32] Loss_P: [2.32 1.81 1.83 1.04 0.67 0.08 0.34 0.03 8.1 ]\n",
      "Loss_Q: [1.07 1.21 0.92 0.68 0.1  0.38 0.02 0.   4.37] Loss_P: [2.28 1.87 1.82 1.02 0.7  0.08 0.4  0.02 8.18]\n",
      "Loss_Q: [1.16 1.22 0.97 0.63 0.1  0.4  0.02 0.   4.48] Loss_P: [2.32 1.84 1.88 1.07 0.7  0.09 0.38 0.01 8.29]\n",
      "Loss_Q: [1.16 1.19 0.93 0.69 0.1  0.35 0.01 0.   4.43] Loss_P: [2.27 1.88 1.86 1.08 0.71 0.07 0.38 0.01 8.26]\n",
      "Loss_Q: [1.08 1.26 0.91 0.64 0.08 0.35 0.01 0.   4.32] Loss_P: [2.31 1.8  1.84 1.07 0.7  0.09 0.36 0.02 8.18]\n",
      "Loss_Q: [1.09 1.25 0.92 0.69 0.09 0.33 0.01 0.   4.37] Loss_P: [2.31 1.83 1.85 1.03 0.65 0.07 0.32 0.02 8.09]\n",
      "Loss_Q: [0.97 1.33 0.86 0.72 0.08 0.35 0.02 0.   4.32] Loss_P: [2.31 1.87 1.86 0.99 0.69 0.07 0.35 0.01 8.14]\n",
      "Loss_Q: [0.99 1.26 0.86 0.7  0.11 0.33 0.02 0.   4.27] Loss_P: [2.29 1.87 1.87 1.   0.69 0.1  0.34 0.02 8.17]\n",
      "Loss_Q: [0.99 1.25 0.86 0.66 0.09 0.34 0.02 0.   4.21] Loss_P: [2.38 1.84 1.89 0.99 0.71 0.09 0.36 0.02 8.29]\n",
      "Loss_Q: [0.94 1.26 0.82 0.71 0.07 0.36 0.02 0.   4.18] Loss_P: [2.34 1.81 1.92 0.95 0.73 0.09 0.35 0.01 8.2 ]\n",
      "Loss_Q: [1.06 1.33 0.84 0.71 0.07 0.36 0.02 0.   4.39] Loss_P: [2.31 1.83 1.88 0.99 0.73 0.06 0.35 0.02 8.17]\n",
      "Loss_Q: [1.01 1.27 0.89 0.69 0.07 0.37 0.02 0.   4.31] Loss_P: [2.31 1.82 1.89 1.03 0.7  0.06 0.35 0.02 8.18]\n",
      "Loss_Q: [1.05 1.28 0.82 0.65 0.07 0.35 0.01 0.   4.23] Loss_P: [2.26 1.92 1.88 0.94 0.73 0.09 0.35 0.01 8.18]\n",
      "Loss_Q: [1.04 1.26 0.81 0.68 0.07 0.37 0.02 0.   4.24] Loss_P: [2.32 1.8  1.8  0.94 0.68 0.09 0.33 0.01 7.97]\n",
      "Loss_Q: [1.04 1.18 0.81 0.66 0.1  0.34 0.01 0.   4.14] Loss_P: [2.28 1.83 1.8  0.9  0.71 0.09 0.36 0.02 7.99]\n",
      "Loss_Q: [0.93 1.26 0.85 0.7  0.08 0.32 0.02 0.   4.15] Loss_P: [2.29 1.76 1.8  0.91 0.66 0.09 0.33 0.02 7.86]\n",
      "Loss_Q: [0.95 1.23 0.85 0.68 0.06 0.34 0.02 0.   4.12] Loss_P: [2.31 1.79 1.78 0.97 0.7  0.07 0.35 0.02 7.99]\n",
      "Loss_Q: [1.01 1.25 0.84 0.68 0.05 0.33 0.02 0.   4.17] Loss_P: [2.31 1.83 1.83 0.92 0.68 0.07 0.35 0.01 8.01]\n",
      "Loss_Q: [0.98 1.26 0.85 0.69 0.07 0.31 0.01 0.   4.17] Loss_P: [2.27 1.85 1.85 0.98 0.69 0.07 0.32 0.01 8.03]\n",
      "Loss_Q: [1.05 1.17 0.82 0.66 0.06 0.3  0.01 0.   4.07] Loss_P: [2.33 1.81 1.84 0.92 0.71 0.07 0.35 0.01 8.04]\n",
      "Loss_Q: [0.97 1.27 0.83 0.69 0.05 0.33 0.01 0.   4.15] Loss_P: [2.24 1.83 1.88 0.92 0.7  0.06 0.34 0.02 7.99]\n",
      "Loss_Q: [1.01 1.22 0.8  0.68 0.07 0.35 0.01 0.   4.14] Loss_P: [2.32 1.84 1.79 0.85 0.69 0.07 0.33 0.02 7.91]\n",
      "Loss_Q: [1.   1.23 0.85 0.7  0.06 0.34 0.01 0.   4.2 ] Loss_P: [2.29 1.89 1.82 0.95 0.73 0.07 0.32 0.02 8.07]\n",
      "Loss_Q: [0.92 1.23 0.85 0.68 0.06 0.34 0.02 0.   4.1 ] Loss_P: [2.34 1.88 1.83 0.95 0.71 0.08 0.34 0.01 8.14]\n",
      "Loss_Q: [1.05 1.17 0.82 0.71 0.05 0.35 0.02 0.   4.18] Loss_P: [2.36 1.81 1.79 0.95 0.71 0.08 0.34 0.02 8.05]\n",
      "Loss_Q: [0.9  1.18 0.8  0.67 0.09 0.34 0.02 0.   4.  ] Loss_P: [2.35 1.76 1.86 0.97 0.68 0.07 0.37 0.02 8.08]\n",
      "Loss_Q: [0.97 1.19 0.84 0.69 0.06 0.36 0.01 0.   4.1 ] Loss_P: [2.32 1.83 1.8  0.89 0.66 0.07 0.39 0.01 7.96]\n",
      "Loss_Q: [1.04 1.24 0.8  0.66 0.07 0.37 0.01 0.   4.2 ] Loss_P: [2.25 1.85 1.81 0.92 0.7  0.08 0.33 0.02 7.97]\n",
      "Loss_Q: [0.95 1.29 0.81 0.7  0.07 0.36 0.02 0.   4.2 ] Loss_P: [2.27 1.86 1.77 0.95 0.67 0.07 0.37 0.02 7.98]\n",
      "Loss_Q: [0.98 1.16 0.82 0.68 0.07 0.37 0.02 0.   4.11] Loss_P: [2.3  1.89 1.83 0.93 0.71 0.07 0.35 0.01 8.1 ]\n",
      "Loss_Q: [0.92 1.27 0.84 0.71 0.09 0.39 0.02 0.   4.24] Loss_P: [2.29 1.79 1.81 0.9  0.71 0.07 0.39 0.02 7.97]\n",
      "Loss_Q: [0.92 1.21 0.76 0.69 0.09 0.35 0.01 0.   4.04] Loss_P: [2.21 1.8  1.83 0.91 0.66 0.07 0.36 0.02 7.86]\n",
      "Loss_Q: [1.02 1.18 0.85 0.7  0.07 0.37 0.01 0.   4.22] Loss_P: [2.26 1.84 1.81 0.91 0.7  0.09 0.37 0.01 8.  ]\n",
      "Loss_Q: [0.99 1.21 0.83 0.68 0.08 0.33 0.02 0.   4.15] Loss_P: [2.26 1.82 1.85 0.9  0.71 0.07 0.35 0.02 7.99]\n",
      "Loss_Q: [1.03 1.18 0.87 0.66 0.11 0.38 0.03 0.   4.26] Loss_P: [2.25 1.86 1.81 0.95 0.69 0.08 0.37 0.01 8.02]\n",
      "Loss_Q: [0.98 1.15 0.84 0.72 0.08 0.39 0.02 0.   4.17] Loss_P: [2.23 1.76 1.84 0.96 0.7  0.09 0.37 0.02 7.97]\n",
      "Loss_Q: [1.02 1.22 0.86 0.69 0.07 0.38 0.02 0.   4.25] Loss_P: [2.31 1.81 1.83 0.96 0.71 0.08 0.37 0.01 8.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.02 1.18 0.82 0.68 0.06 0.35 0.02 0.   4.12] Loss_P: [2.24 1.83 1.85 0.95 0.71 0.08 0.36 0.01 8.03]\n",
      "Loss_Q: [0.98 1.16 0.82 0.69 0.07 0.32 0.01 0.   4.04] Loss_P: [2.3  1.77 1.84 0.89 0.7  0.09 0.36 0.03 7.98]\n",
      "Loss_Q: [1.   1.18 0.82 0.7  0.09 0.33 0.03 0.   4.13] Loss_P: [2.25 1.8  1.77 0.94 0.7  0.08 0.36 0.02 7.92]\n",
      "Loss_Q: [0.93 1.11 0.84 0.73 0.06 0.36 0.02 0.   4.05] Loss_P: [2.26 1.71 1.76 0.88 0.69 0.08 0.34 0.01 7.72]\n",
      "Loss_Q: [1.01 1.16 0.77 0.68 0.08 0.39 0.02 0.   4.11] Loss_P: [2.26 1.81 1.78 0.94 0.71 0.06 0.37 0.01 7.94]\n",
      "Loss_Q: [0.98 1.22 0.84 0.71 0.08 0.43 0.04 0.   4.28] Loss_P: [2.23 1.81 1.77 0.89 0.7  0.08 0.38 0.02 7.87]\n",
      "Loss_Q: [0.98 1.14 0.78 0.67 0.06 0.38 0.02 0.   4.03] Loss_P: [2.21 1.88 1.83 0.93 0.68 0.06 0.4  0.01 8.01]\n",
      "Loss_Q: [0.98 1.2  0.83 0.66 0.07 0.39 0.01 0.   4.14] Loss_P: [2.3  1.81 1.8  0.96 0.67 0.06 0.39 0.02 8.01]\n",
      "Loss_Q: [0.96 1.25 0.83 0.67 0.07 0.4  0.01 0.   4.19] Loss_P: [2.3  1.82 1.83 0.94 0.69 0.07 0.4  0.01 8.06]\n",
      "Loss_Q: [0.99 1.2  0.86 0.68 0.08 0.39 0.02 0.   4.21] Loss_P: [2.31 1.85 1.83 0.9  0.69 0.06 0.38 0.02 8.05]\n",
      "Loss_Q: [0.96 1.22 0.92 0.68 0.07 0.36 0.01 0.   4.22] Loss_P: [2.3  1.85 1.87 0.95 0.67 0.08 0.4  0.02 8.14]\n",
      "Loss_Q: [1.   1.23 0.84 0.7  0.08 0.39 0.02 0.   4.26] Loss_P: [2.29 1.89 1.83 0.98 0.71 0.08 0.41 0.01 8.18]\n",
      "Loss_Q: [1.06 1.23 0.86 0.7  0.07 0.38 0.02 0.   4.33] Loss_P: [2.37 1.78 1.9  0.98 0.73 0.09 0.36 0.01 8.22]\n",
      "Loss_Q: [0.98 1.24 0.86 0.71 0.07 0.38 0.02 0.   4.24] Loss_P: [2.31 1.84 1.82 0.97 0.72 0.08 0.37 0.01 8.12]\n",
      "Loss_Q: [1.07 1.26 0.86 0.7  0.06 0.39 0.01 0.   4.35] Loss_P: [2.3  1.91 1.83 0.98 0.69 0.07 0.38 0.01 8.18]\n",
      "Loss_Q: [1.02 1.26 0.89 0.69 0.09 0.42 0.01 0.   4.37] Loss_P: [2.29 1.82 1.81 0.99 0.69 0.07 0.44 0.02 8.12]\n",
      "Loss_Q: [0.96 1.28 0.9  0.71 0.08 0.42 0.02 0.   4.37] Loss_P: [2.23 1.88 1.89 0.98 0.7  0.1  0.41 0.02 8.19]\n",
      "Loss_Q: [0.99 1.31 0.88 0.68 0.07 0.39 0.01 0.   4.33] Loss_P: [2.31 1.77 1.83 1.   0.68 0.08 0.42 0.03 8.12]\n",
      "Loss_Q: [0.96 1.24 0.93 0.72 0.07 0.42 0.01 0.   4.35] Loss_P: [2.24 1.86 1.88 0.98 0.69 0.08 0.41 0.01 8.15]\n",
      "Loss_Q: [1.04 1.18 0.93 0.7  0.09 0.43 0.02 0.   4.4 ] Loss_P: [2.29 1.82 1.89 1.01 0.71 0.07 0.45 0.02 8.25]\n",
      "Loss_Q: [1.08 1.26 0.95 0.65 0.05 0.45 0.01 0.   4.44] Loss_P: [2.32 1.81 1.83 1.03 0.7  0.07 0.43 0.02 8.22]\n",
      "Loss_Q: [1.04 1.22 0.91 0.67 0.07 0.44 0.02 0.   4.38] Loss_P: [2.34 1.78 1.85 1.01 0.72 0.07 0.45 0.02 8.25]\n",
      "Loss_Q: [0.98 1.19 0.88 0.68 0.05 0.42 0.02 0.   4.22] Loss_P: [2.32 1.86 1.86 1.05 0.7  0.08 0.45 0.02 8.33]\n",
      "Loss_Q: [1.04 1.24 0.96 0.71 0.07 0.45 0.01 0.   4.48] Loss_P: [2.36 1.77 1.86 1.07 0.69 0.04 0.43 0.01 8.24]\n",
      "Loss_Q: [1.02 1.24 0.89 0.71 0.08 0.42 0.01 0.   4.37] Loss_P: [2.23 1.78 1.91 1.06 0.71 0.07 0.42 0.02 8.21]\n",
      "Loss_Q: [0.96 1.22 0.88 0.64 0.06 0.44 0.02 0.   4.21] Loss_P: [2.33 1.84 1.87 1.03 0.71 0.06 0.45 0.01 8.29]\n",
      "Loss_Q: [1.02 1.29 0.97 0.67 0.05 0.45 0.01 0.   4.46] Loss_P: [2.35 1.77 1.86 1.07 0.68 0.06 0.45 0.01 8.26]\n",
      "Loss_Q: [1.05 1.28 0.98 0.67 0.09 0.45 0.01 0.   4.53] Loss_P: [2.26 1.85 1.91 1.12 0.69 0.07 0.45 0.02 8.38]\n",
      "Loss_Q: [1.02 1.24 0.97 0.7  0.06 0.43 0.01 0.   4.43] Loss_P: [2.27 1.84 1.87 1.09 0.69 0.06 0.43 0.01 8.27]\n",
      "Loss_Q: [1.06 1.26 0.91 0.64 0.07 0.44 0.01 0.   4.39] Loss_P: [2.28 1.84 1.91 1.05 0.68 0.06 0.44 0.02 8.29]\n",
      "Loss_Q: [0.96 1.23 0.89 0.67 0.07 0.45 0.02 0.   4.29] Loss_P: [2.22 1.77 1.88 1.08 0.63 0.09 0.46 0.02 8.14]\n",
      "Loss_Q: [1.03 1.22 0.92 0.66 0.06 0.43 0.01 0.   4.33] Loss_P: [2.3  1.85 1.89 1.09 0.7  0.11 0.44 0.01 8.4 ]\n",
      "Loss_Q: [1.01 1.27 0.88 0.68 0.05 0.44 0.01 0.   4.34] Loss_P: [2.33 1.83 1.84 1.07 0.65 0.06 0.42 0.01 8.22]\n",
      "Loss_Q: [1.13 1.22 0.94 0.67 0.05 0.47 0.02 0.   4.48] Loss_P: [2.38 1.83 1.85 1.01 0.67 0.06 0.45 0.02 8.26]\n",
      "Loss_Q: [1.11 1.25 0.92 0.64 0.05 0.46 0.02 0.   4.44] Loss_P: [2.37 1.78 1.87 1.08 0.68 0.07 0.47 0.01 8.32]\n",
      "Loss_Q: [1.   1.22 0.95 0.67 0.05 0.47 0.01 0.   4.37] Loss_P: [2.24 1.9  1.87 1.05 0.64 0.05 0.47 0.01 8.24]\n",
      "Loss_Q: [1.09 1.24 0.95 0.71 0.08 0.47 0.01 0.   4.54] Loss_P: [2.35 1.77 1.87 1.06 0.71 0.07 0.48 0.01 8.32]\n",
      "Loss_Q: [0.97 1.24 0.91 0.67 0.07 0.47 0.01 0.   4.34] Loss_P: [2.34 1.81 1.8  1.1  0.72 0.06 0.45 0.02 8.31]\n",
      "Loss_Q: [1.02 1.26 0.88 0.66 0.05 0.47 0.01 0.   4.35] Loss_P: [2.3  1.86 1.83 1.05 0.71 0.06 0.47 0.02 8.3 ]\n",
      "Loss_Q: [1.08 1.2  0.95 0.67 0.06 0.49 0.01 0.   4.47] Loss_P: [2.23 1.89 1.87 1.1  0.69 0.07 0.47 0.02 8.33]\n",
      "Loss_Q: [1.04 1.23 0.87 0.65 0.06 0.46 0.01 0.   4.32] Loss_P: [2.34 1.76 1.88 0.99 0.7  0.05 0.46 0.02 8.2 ]\n",
      "Loss_Q: [1.06 1.2  0.89 0.65 0.07 0.45 0.03 0.   4.36] Loss_P: [2.36 1.82 1.83 1.07 0.69 0.07 0.47 0.02 8.32]\n",
      "Loss_Q: [1.14 1.3  0.91 0.64 0.05 0.46 0.02 0.   4.51] Loss_P: [2.31 1.75 1.89 1.07 0.68 0.08 0.47 0.01 8.26]\n",
      "Loss_Q: [1.12 1.2  0.91 0.65 0.06 0.48 0.02 0.   4.43] Loss_P: [2.36 1.81 1.86 1.02 0.7  0.05 0.47 0.02 8.29]\n",
      "Loss_Q: [1.01 1.26 0.84 0.65 0.06 0.46 0.01 0.   4.29] Loss_P: [2.26 1.86 1.78 1.01 0.66 0.07 0.48 0.02 8.13]\n",
      "Loss_Q: [1.21 1.26 0.9  0.64 0.04 0.48 0.01 0.   4.54] Loss_P: [2.3  1.88 1.88 1.03 0.69 0.05 0.47 0.01 8.31]\n",
      "Loss_Q: [1.05 1.19 0.85 0.64 0.07 0.48 0.02 0.   4.3 ] Loss_P: [2.27 1.82 1.86 1.   0.65 0.07 0.48 0.01 8.15]\n",
      "Loss_Q: [1.16 1.23 0.94 0.65 0.07 0.49 0.01 0.   4.56] Loss_P: [2.33 1.83 1.82 1.05 0.65 0.06 0.49 0.01 8.24]\n",
      "Loss_Q: [1.02 1.22 0.87 0.61 0.07 0.49 0.01 0.   4.28] Loss_P: [2.35 1.8  1.82 1.08 0.63 0.07 0.49 0.02 8.26]\n",
      "Loss_Q: [1.02 1.17 0.92 0.64 0.05 0.49 0.02 0.   4.33] Loss_P: [2.3  1.83 1.8  1.01 0.64 0.04 0.49 0.02 8.12]\n",
      "Loss_Q: [1.04 1.19 0.98 0.66 0.04 0.47 0.01 0.   4.39] Loss_P: [2.37 1.89 1.8  1.05 0.67 0.06 0.48 0.02 8.34]\n",
      "Loss_Q: [1.08 1.24 0.93 0.63 0.05 0.48 0.01 0.   4.43] Loss_P: [2.29 1.78 1.79 1.07 0.66 0.06 0.48 0.01 8.15]\n",
      "Loss_Q: [0.99 1.33 0.94 0.64 0.05 0.48 0.01 0.   4.45] Loss_P: [2.33 1.84 1.8  1.06 0.67 0.05 0.48 0.02 8.25]\n",
      "Loss_Q: [1.01 1.24 0.93 0.65 0.06 0.47 0.02 0.   4.37] Loss_P: [2.34 1.83 1.88 1.12 0.67 0.07 0.48 0.02 8.39]\n",
      "Loss_Q: [1.03 1.24 0.94 0.61 0.07 0.47 0.02 0.   4.38] Loss_P: [2.31 1.86 1.85 1.1  0.68 0.08 0.48 0.01 8.36]\n",
      "Loss_Q: [1.11 1.19 0.94 0.66 0.08 0.48 0.01 0.   4.47] Loss_P: [2.36 1.75 1.83 1.13 0.64 0.05 0.47 0.01 8.23]\n",
      "Loss_Q: [1.09 1.16 0.92 0.65 0.05 0.47 0.01 0.   4.35] Loss_P: [2.31 1.81 1.82 1.12 0.69 0.05 0.48 0.02 8.29]\n",
      "Loss_Q: [1.08 1.22 0.99 0.68 0.05 0.48 0.01 0.   4.51] Loss_P: [2.36 1.79 1.82 1.11 0.68 0.05 0.47 0.02 8.29]\n",
      "Loss_Q: [1.08 1.26 0.98 0.64 0.04 0.49 0.03 0.   4.53] Loss_P: [2.3  1.73 1.8  1.15 0.7  0.06 0.49 0.01 8.24]\n",
      "Loss_Q: [1.1  1.19 0.94 0.66 0.04 0.47 0.02 0.   4.41] Loss_P: [2.36 1.76 1.84 1.13 0.67 0.05 0.47 0.01 8.28]\n",
      "Loss_Q: [1.11 1.24 0.97 0.67 0.05 0.45 0.01 0.   4.49] Loss_P: [2.37 1.74 1.79 1.17 0.67 0.05 0.44 0.02 8.26]\n",
      "Loss_Q: [1.02 1.19 0.99 0.66 0.04 0.47 0.01 0.   4.37] Loss_P: [2.31 1.75 1.79 1.14 0.71 0.05 0.46 0.02 8.22]\n",
      "Loss_Q: [1.1  1.24 0.98 0.66 0.05 0.46 0.02 0.   4.51] Loss_P: [2.35 1.86 1.82 1.17 0.65 0.04 0.47 0.02 8.39]\n",
      "Loss_Q: [1.03 1.24 0.88 0.64 0.04 0.48 0.02 0.   4.32] Loss_P: [2.3  1.82 1.83 1.16 0.68 0.05 0.48 0.02 8.32]\n",
      "Loss_Q: [1.06 1.16 0.95 0.65 0.05 0.48 0.02 0.   4.36] Loss_P: [2.34 1.79 1.8  1.11 0.7  0.07 0.48 0.01 8.29]\n",
      "Loss_Q: [1.06 1.16 0.89 0.65 0.03 0.44 0.01 0.   4.25] Loss_P: [2.31 1.84 1.79 1.11 0.67 0.04 0.46 0.01 8.22]\n",
      "Loss_Q: [1.05 1.2  0.97 0.66 0.05 0.48 0.01 0.   4.41] Loss_P: [2.3  1.79 1.79 1.08 0.68 0.06 0.47 0.01 8.17]\n",
      "Loss_Q: [1.11 1.16 0.9  0.7  0.04 0.49 0.02 0.   4.42] Loss_P: [2.32 1.82 1.75 1.09 0.65 0.04 0.49 0.02 8.18]\n",
      "Loss_Q: [1.11 1.17 0.93 0.66 0.06 0.49 0.01 0.   4.45] Loss_P: [2.38 1.81 1.78 1.04 0.68 0.05 0.5  0.02 8.28]\n",
      "Loss_Q: [1.07 1.21 0.95 0.67 0.04 0.49 0.02 0.   4.44] Loss_P: [2.34 1.85 1.79 1.05 0.69 0.04 0.49 0.03 8.28]\n",
      "Loss_Q: [1.07 1.17 0.9  0.66 0.06 0.48 0.01 0.   4.36] Loss_P: [2.34 1.9  1.79 1.03 0.68 0.03 0.49 0.01 8.28]\n",
      "Loss_Q: [1.02 1.22 0.9  0.65 0.05 0.5  0.03 0.   4.37] Loss_P: [2.32 1.91 1.82 1.   0.67 0.04 0.48 0.01 8.24]\n",
      "Loss_Q: [1.11 1.15 0.9  0.68 0.04 0.48 0.01 0.   4.38] Loss_P: [2.26 1.84 1.83 1.08 0.66 0.05 0.48 0.01 8.22]\n",
      "Loss_Q: [1.09 1.17 0.86 0.59 0.04 0.49 0.02 0.   4.26] Loss_P: [2.23 1.84 1.83 1.06 0.68 0.05 0.47 0.02 8.18]\n",
      "Loss_Q: [1.16 1.16 0.91 0.67 0.04 0.48 0.02 0.   4.45] Loss_P: [2.33 1.77 1.81 1.08 0.65 0.07 0.49 0.02 8.22]\n",
      "Loss_Q: [1.13 1.19 0.92 0.63 0.04 0.5  0.01 0.   4.42] Loss_P: [2.27 1.91 1.87 1.06 0.66 0.05 0.48 0.02 8.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.11 1.25 0.9  0.68 0.06 0.48 0.03 0.   4.51] Loss_P: [2.31 1.87 1.83 1.1  0.65 0.05 0.49 0.02 8.32]\n",
      "Loss_Q: [1.09 1.18 0.9  0.65 0.05 0.49 0.01 0.   4.38] Loss_P: [2.32 1.84 1.81 1.06 0.65 0.06 0.49 0.02 8.24]\n",
      "Loss_Q: [1.06 1.15 0.9  0.63 0.04 0.48 0.02 0.   4.29] Loss_P: [2.34 1.8  1.84 1.05 0.64 0.04 0.49 0.02 8.22]\n",
      "Loss_Q: [1.04 1.23 0.92 0.63 0.06 0.49 0.02 0.   4.38] Loss_P: [2.29 1.87 1.78 1.01 0.63 0.06 0.47 0.01 8.12]\n",
      "Loss_Q: [1.07 1.15 0.87 0.65 0.04 0.47 0.02 0.   4.28] Loss_P: [2.27 1.85 1.78 1.04 0.65 0.05 0.47 0.02 8.13]\n",
      "Loss_Q: [1.08 1.24 0.87 0.65 0.07 0.48 0.02 0.   4.4 ] Loss_P: [2.31 1.84 1.76 1.05 0.63 0.08 0.48 0.01 8.16]\n",
      "Loss_Q: [1.02 1.19 0.89 0.6  0.05 0.49 0.01 0.   4.24] Loss_P: [2.33 1.94 1.75 0.99 0.66 0.04 0.48 0.02 8.22]\n",
      "Loss_Q: [1.02 1.17 0.87 0.66 0.04 0.46 0.03 0.   4.25] Loss_P: [2.32 1.88 1.73 1.01 0.65 0.05 0.47 0.01 8.13]\n",
      "Loss_Q: [1.07 1.23 0.87 0.65 0.05 0.46 0.01 0.   4.35] Loss_P: [2.26 1.89 1.73 0.98 0.62 0.06 0.46 0.01 8.02]\n",
      "Loss_Q: [1.03 1.2  0.89 0.65 0.04 0.45 0.02 0.   4.27] Loss_P: [2.35 1.87 1.73 1.01 0.66 0.04 0.44 0.02 8.13]\n",
      "Loss_Q: [1.08 1.14 0.84 0.66 0.06 0.44 0.02 0.   4.24] Loss_P: [2.33 1.83 1.71 1.03 0.63 0.07 0.44 0.02 8.07]\n",
      "Loss_Q: [1.08 1.18 0.9  0.64 0.03 0.43 0.01 0.   4.26] Loss_P: [2.34 1.8  1.74 1.01 0.67 0.06 0.44 0.01 8.06]\n",
      "Loss_Q: [1.09 1.14 0.87 0.67 0.05 0.45 0.01 0.   4.29] Loss_P: [2.35 1.8  1.76 1.04 0.67 0.05 0.45 0.02 8.15]\n",
      "Loss_Q: [1.05 1.1  0.95 0.66 0.05 0.46 0.01 0.   4.28] Loss_P: [2.27 1.86 1.76 1.05 0.64 0.04 0.48 0.01 8.11]\n",
      "Loss_Q: [1.   1.18 0.93 0.66 0.05 0.46 0.02 0.   4.29] Loss_P: [2.35 1.84 1.73 1.07 0.66 0.05 0.48 0.02 8.2 ]\n",
      "Loss_Q: [1.04 1.2  0.96 0.66 0.04 0.46 0.01 0.   4.36] Loss_P: [2.3  1.81 1.71 1.08 0.64 0.06 0.44 0.02 8.05]\n",
      "Loss_Q: [1.05 1.22 0.91 0.65 0.05 0.44 0.01 0.   4.33] Loss_P: [2.37 1.87 1.73 0.99 0.64 0.03 0.46 0.01 8.09]\n",
      "Loss_Q: [1.07 1.09 0.88 0.64 0.07 0.46 0.02 0.   4.24] Loss_P: [2.33 1.86 1.75 0.97 0.67 0.06 0.46 0.02 8.11]\n",
      "Loss_Q: [1.09 1.17 0.94 0.7  0.06 0.45 0.02 0.   4.44] Loss_P: [2.33 1.83 1.79 1.04 0.7  0.05 0.46 0.02 8.23]\n",
      "Loss_Q: [1.08 1.09 0.87 0.66 0.06 0.46 0.01 0.   4.24] Loss_P: [2.34 1.9  1.71 0.95 0.65 0.06 0.44 0.01 8.05]\n",
      "Loss_Q: [1.05 1.17 0.85 0.64 0.04 0.48 0.01 0.   4.24] Loss_P: [2.33 1.9  1.72 0.94 0.65 0.06 0.43 0.01 8.05]\n",
      "Loss_Q: [1.07 1.14 0.86 0.61 0.06 0.45 0.01 0.   4.21] Loss_P: [2.33 1.86 1.74 0.94 0.63 0.05 0.46 0.01 8.01]\n",
      "Loss_Q: [1.06 1.15 0.83 0.64 0.06 0.46 0.01 0.   4.21] Loss_P: [2.34 1.89 1.7  0.96 0.63 0.05 0.44 0.01 8.03]\n",
      "Loss_Q: [1.05 1.19 0.85 0.63 0.06 0.46 0.01 0.   4.27] Loss_P: [2.34 1.91 1.73 1.01 0.66 0.05 0.46 0.03 8.2 ]\n",
      "Loss_Q: [1.09 1.16 0.86 0.63 0.06 0.44 0.01 0.   4.25] Loss_P: [2.32 1.91 1.69 0.91 0.71 0.04 0.46 0.02 8.06]\n",
      "Loss_Q: [1.1  1.18 0.85 0.67 0.05 0.46 0.01 0.   4.31] Loss_P: [2.36 1.87 1.64 0.94 0.67 0.06 0.46 0.01 8.  ]\n",
      "Loss_Q: [1.02 1.14 0.79 0.66 0.05 0.44 0.01 0.   4.12] Loss_P: [2.37 1.84 1.66 0.95 0.64 0.05 0.43 0.01 7.96]\n",
      "Loss_Q: [1.05 1.16 0.86 0.64 0.06 0.46 0.01 0.   4.22] Loss_P: [2.36 1.87 1.64 1.   0.65 0.05 0.46 0.02 8.06]\n",
      "Loss_Q: [1.09 1.08 0.8  0.66 0.04 0.47 0.02 0.   4.16] Loss_P: [2.39 1.94 1.59 0.96 0.65 0.04 0.48 0.03 8.07]\n",
      "Loss_Q: [1.1  1.16 0.8  0.65 0.06 0.43 0.02 0.   4.22] Loss_P: [2.35 1.89 1.63 0.93 0.64 0.04 0.45 0.01 7.94]\n",
      "Loss_Q: [1.03 1.15 0.86 0.62 0.04 0.45 0.01 0.   4.17] Loss_P: [2.36 1.88 1.69 0.98 0.62 0.06 0.45 0.01 8.05]\n",
      "Loss_Q: [1.13 1.2  0.87 0.64 0.06 0.44 0.02 0.   4.37] Loss_P: [2.34 1.94 1.66 0.96 0.63 0.04 0.45 0.02 8.04]\n",
      "Loss_Q: [1.11 1.11 0.79 0.65 0.04 0.44 0.02 0.   4.17] Loss_P: [2.37 1.92 1.67 0.94 0.64 0.05 0.45 0.02 8.07]\n",
      "Loss_Q: [1.06 1.14 0.83 0.66 0.04 0.42 0.01 0.   4.17] Loss_P: [2.34 1.87 1.68 0.92 0.64 0.05 0.44 0.01 7.95]\n",
      "Loss_Q: [1.11 1.16 0.83 0.64 0.05 0.44 0.01 0.   4.24] Loss_P: [2.39 1.88 1.68 0.94 0.64 0.07 0.45 0.02 8.06]\n",
      "Loss_Q: [1.02 1.12 0.84 0.67 0.05 0.45 0.02 0.   4.18] Loss_P: [2.35 1.87 1.72 0.92 0.64 0.06 0.46 0.02 8.04]\n",
      "Loss_Q: [1.1  1.13 0.88 0.65 0.04 0.44 0.02 0.   4.26] Loss_P: [2.35 1.9  1.69 0.99 0.67 0.07 0.46 0.01 8.14]\n",
      "Loss_Q: [1.09 1.18 0.85 0.67 0.04 0.44 0.01 0.   4.29] Loss_P: [2.35 1.93 1.65 0.97 0.68 0.05 0.45 0.01 8.1 ]\n",
      "Loss_Q: [1.04 1.19 0.88 0.65 0.04 0.45 0.01 0.   4.27] Loss_P: [2.36 1.87 1.66 0.93 0.62 0.06 0.44 0.01 7.95]\n",
      "Loss_Q: [0.99 1.19 0.85 0.66 0.04 0.47 0.02 0.   4.21] Loss_P: [2.39 1.91 1.67 0.9  0.71 0.05 0.45 0.02 8.1 ]\n",
      "Loss_Q: [1.01 1.16 0.79 0.64 0.05 0.45 0.01 0.   4.11] Loss_P: [2.36 1.93 1.68 1.   0.71 0.03 0.45 0.01 8.16]\n",
      "Loss_Q: [1.05 1.18 0.87 0.62 0.06 0.46 0.01 0.   4.24] Loss_P: [2.39 1.82 1.64 1.02 0.63 0.04 0.46 0.01 8.01]\n",
      "Loss_Q: [1.03 1.17 0.9  0.68 0.04 0.48 0.02 0.   4.32] Loss_P: [2.37 1.91 1.58 0.95 0.65 0.04 0.46 0.02 7.99]\n",
      "Loss_Q: [1.05 1.18 0.86 0.64 0.05 0.45 0.02 0.   4.25] Loss_P: [2.34 1.91 1.6  1.   0.69 0.05 0.46 0.02 8.06]\n",
      "Loss_Q: [1.02 1.1  0.9  0.65 0.04 0.47 0.02 0.   4.19] Loss_P: [2.35 1.87 1.59 1.   0.69 0.05 0.48 0.02 8.03]\n",
      "Loss_Q: [1.07 1.11 0.9  0.65 0.05 0.45 0.01 0.   4.25] Loss_P: [2.37 1.89 1.63 1.04 0.67 0.06 0.48 0.01 8.15]\n",
      "Loss_Q: [0.99 1.13 0.84 0.66 0.04 0.42 0.02 0.   4.09] Loss_P: [2.34 1.85 1.59 0.98 0.67 0.03 0.44 0.01 7.91]\n",
      "Loss_Q: [1.01 1.09 0.84 0.68 0.06 0.43 0.03 0.   4.13] Loss_P: [2.24 2.03 1.63 1.05 0.7  0.03 0.43 0.02 8.13]\n",
      "Loss_Q: [1.03 1.07 0.91 0.7  0.04 0.43 0.02 0.   4.2 ] Loss_P: [2.31 1.92 1.63 1.08 0.66 0.04 0.44 0.02 8.09]\n",
      "Loss_Q: [1.01 1.11 0.89 0.65 0.05 0.44 0.01 0.   4.16] Loss_P: [2.33 1.88 1.56 1.03 0.68 0.05 0.42 0.01 7.96]\n",
      "Loss_Q: [1.05 1.14 0.97 0.65 0.04 0.44 0.01 0.   4.32] Loss_P: [2.26 1.94 1.65 1.12 0.65 0.03 0.44 0.02 8.1 ]\n",
      "Loss_Q: [1.04 1.11 0.91 0.69 0.05 0.42 0.02 0.   4.24] Loss_P: [2.33 1.92 1.59 1.08 0.7  0.05 0.41 0.02 8.11]\n",
      "Loss_Q: [1.05 1.14 0.9  0.64 0.05 0.42 0.01 0.   4.21] Loss_P: [2.35 1.95 1.62 1.1  0.67 0.07 0.43 0.02 8.2 ]\n",
      "Loss_Q: [1.05 1.14 0.94 0.64 0.06 0.4  0.01 0.   4.23] Loss_P: [2.33 1.92 1.61 1.13 0.63 0.05 0.42 0.02 8.1 ]\n",
      "Loss_Q: [1.06 1.12 1.   0.66 0.05 0.41 0.01 0.   4.31] Loss_P: [2.28 1.97 1.69 1.11 0.66 0.06 0.4  0.01 8.2 ]\n",
      "Loss_Q: [1.   1.14 0.98 0.68 0.05 0.4  0.01 0.   4.26] Loss_P: [2.32 1.92 1.61 1.13 0.66 0.06 0.41 0.01 8.12]\n",
      "Loss_Q: [1.05 1.19 0.98 0.63 0.05 0.4  0.01 0.   4.3 ] Loss_P: [2.34 1.88 1.62 1.15 0.7  0.07 0.42 0.02 8.19]\n",
      "Loss_Q: [1.13 1.14 0.98 0.6  0.04 0.39 0.01 0.   4.3 ] Loss_P: [2.29 1.93 1.62 1.17 0.64 0.04 0.4  0.02 8.12]\n",
      "Loss_Q: [1.06 1.14 0.94 0.64 0.06 0.39 0.01 0.   4.25] Loss_P: [2.38 1.88 1.68 1.16 0.66 0.04 0.36 0.01 8.17]\n",
      "Loss_Q: [1.06 1.16 0.96 0.65 0.06 0.41 0.01 0.   4.32] Loss_P: [2.33 1.93 1.6  1.14 0.64 0.06 0.37 0.02 8.08]\n",
      "Loss_Q: [1.08 1.22 0.96 0.65 0.06 0.39 0.01 0.   4.37] Loss_P: [2.34 1.93 1.65 1.15 0.65 0.06 0.38 0.02 8.17]\n",
      "Loss_Q: [1.05 1.14 0.94 0.66 0.06 0.37 0.02 0.   4.24] Loss_P: [2.36 1.9  1.69 1.15 0.64 0.05 0.39 0.02 8.2 ]\n",
      "Loss_Q: [1.11 1.16 0.91 0.71 0.06 0.38 0.02 0.   4.36] Loss_P: [2.33 1.95 1.64 1.13 0.69 0.04 0.39 0.01 8.17]\n",
      "Loss_Q: [1.03 1.15 0.98 0.67 0.05 0.37 0.02 0.   4.27] Loss_P: [2.3  1.94 1.63 1.15 0.69 0.06 0.38 0.02 8.15]\n",
      "Loss_Q: [1.1  1.13 0.98 0.67 0.05 0.39 0.02 0.   4.35] Loss_P: [2.31 1.94 1.73 1.17 0.7  0.05 0.36 0.02 8.27]\n",
      "Loss_Q: [1.06 1.17 0.96 0.67 0.06 0.37 0.02 0.   4.31] Loss_P: [2.37 1.85 1.65 1.15 0.66 0.05 0.38 0.02 8.13]\n",
      "Loss_Q: [1.   1.11 0.96 0.71 0.05 0.4  0.02 0.   4.25] Loss_P: [2.3  1.92 1.58 1.14 0.72 0.07 0.36 0.02 8.11]\n",
      "Loss_Q: [1.03 1.11 0.92 0.73 0.05 0.37 0.01 0.   4.22] Loss_P: [2.29 1.95 1.6  1.1  0.73 0.05 0.36 0.01 8.1 ]\n",
      "Loss_Q: [1.07 1.16 0.98 0.71 0.04 0.38 0.01 0.   4.36] Loss_P: [2.34 1.89 1.58 1.15 0.67 0.05 0.37 0.02 8.07]\n",
      "Loss_Q: [1.07 1.21 0.99 0.68 0.05 0.37 0.01 0.   4.39] Loss_P: [2.32 1.95 1.65 1.12 0.72 0.06 0.37 0.02 8.2 ]\n",
      "Loss_Q: [1.07 1.11 1.02 0.65 0.04 0.38 0.02 0.   4.29] Loss_P: [2.36 1.92 1.63 1.16 0.74 0.06 0.39 0.02 8.28]\n",
      "Loss_Q: [1.01 1.13 1.   0.72 0.06 0.39 0.02 0.   4.34] Loss_P: [2.36 1.93 1.58 1.11 0.71 0.05 0.39 0.01 8.14]\n",
      "Loss_Q: [1.   1.14 0.96 0.69 0.09 0.38 0.01 0.   4.28] Loss_P: [2.37 1.99 1.58 1.1  0.71 0.06 0.39 0.01 8.21]\n",
      "Loss_Q: [1.07 1.17 0.9  0.72 0.05 0.38 0.01 0.   4.3 ] Loss_P: [2.32 1.96 1.6  1.11 0.71 0.05 0.37 0.02 8.12]\n",
      "Loss_Q: [1.1  1.15 0.92 0.7  0.07 0.4  0.01 0.   4.34] Loss_P: [2.33 1.96 1.63 1.04 0.7  0.06 0.4  0.01 8.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.05 1.15 0.93 0.71 0.06 0.37 0.03 0.   4.31] Loss_P: [2.28 2.   1.58 1.12 0.7  0.06 0.38 0.02 8.16]\n",
      "Loss_Q: [1.01 1.18 0.94 0.73 0.06 0.41 0.01 0.   4.34] Loss_P: [2.32 1.97 1.64 1.12 0.72 0.05 0.36 0.02 8.19]\n",
      "Loss_Q: [1.14 1.2  0.98 0.74 0.07 0.37 0.01 0.   4.51] Loss_P: [2.33 1.94 1.67 1.09 0.73 0.06 0.38 0.02 8.22]\n",
      "Loss_Q: [1.03 1.19 0.96 0.7  0.06 0.38 0.02 0.   4.34] Loss_P: [2.33 1.96 1.6  1.09 0.73 0.05 0.38 0.03 8.18]\n",
      "Loss_Q: [1.04 1.11 0.91 0.71 0.05 0.36 0.02 0.   4.21] Loss_P: [2.33 1.98 1.6  1.09 0.76 0.06 0.36 0.01 8.18]\n",
      "Loss_Q: [1.08 1.14 0.9  0.66 0.05 0.35 0.01 0.   4.21] Loss_P: [2.29 2.03 1.64 1.08 0.7  0.06 0.33 0.02 8.13]\n",
      "Loss_Q: [1.03 1.2  0.94 0.72 0.06 0.36 0.02 0.   4.32] Loss_P: [2.35 1.89 1.57 1.07 0.71 0.03 0.35 0.02 8.  ]\n",
      "Loss_Q: [1.04 1.12 0.89 0.7  0.05 0.36 0.02 0.   4.2 ] Loss_P: [2.33 1.94 1.63 1.09 0.72 0.07 0.34 0.01 8.14]\n",
      "Loss_Q: [1.06 1.18 0.89 0.7  0.05 0.33 0.01 0.   4.22] Loss_P: [2.33 2.01 1.62 1.1  0.77 0.04 0.36 0.01 8.25]\n",
      "Loss_Q: [1.03 1.12 0.88 0.7  0.05 0.37 0.02 0.   4.17] Loss_P: [2.33 1.94 1.63 1.06 0.73 0.06 0.34 0.01 8.1 ]\n",
      "Loss_Q: [1.07 1.19 0.9  0.74 0.06 0.37 0.02 0.   4.34] Loss_P: [2.31 1.94 1.6  0.99 0.71 0.05 0.37 0.02 7.99]\n",
      "Loss_Q: [1.06 1.15 0.83 0.72 0.08 0.34 0.01 0.   4.19] Loss_P: [2.34 2.   1.57 1.02 0.73 0.09 0.34 0.02 8.12]\n",
      "Loss_Q: [1.09 1.2  0.93 0.72 0.05 0.35 0.01 0.   4.35] Loss_P: [2.37 1.95 1.59 1.02 0.7  0.07 0.32 0.02 8.03]\n",
      "Loss_Q: [1.06 1.16 0.86 0.74 0.06 0.33 0.01 0.   4.21] Loss_P: [2.36 2.   1.59 0.99 0.75 0.08 0.34 0.02 8.14]\n",
      "Loss_Q: [1.07 1.16 0.88 0.69 0.08 0.34 0.02 0.   4.25] Loss_P: [2.37 2.   1.58 0.95 0.69 0.07 0.32 0.02 7.99]\n",
      "Loss_Q: [1.1  1.17 0.84 0.71 0.07 0.33 0.01 0.   4.23] Loss_P: [2.35 1.98 1.57 0.93 0.7  0.05 0.35 0.03 7.97]\n",
      "Loss_Q: [1.04 1.16 0.84 0.69 0.05 0.31 0.02 0.   4.12] Loss_P: [2.35 1.97 1.58 0.98 0.72 0.06 0.33 0.02 8.02]\n",
      "Loss_Q: [1.09 1.22 0.89 0.66 0.06 0.33 0.01 0.   4.26] Loss_P: [2.39 2.04 1.63 1.01 0.69 0.06 0.33 0.02 8.16]\n",
      "Loss_Q: [1.07 1.19 0.89 0.72 0.06 0.34 0.01 0.   4.29] Loss_P: [2.34 2.01 1.57 1.03 0.66 0.06 0.34 0.01 8.03]\n",
      "Loss_Q: [1.08 1.23 0.81 0.67 0.07 0.32 0.01 0.   4.19] Loss_P: [2.34 1.95 1.62 0.98 0.7  0.07 0.33 0.02 8.  ]\n",
      "Loss_Q: [1.05 1.13 0.88 0.75 0.09 0.32 0.01 0.   4.23] Loss_P: [2.34 2.   1.59 1.04 0.73 0.06 0.36 0.02 8.13]\n",
      "Loss_Q: [1.06 1.12 0.84 0.68 0.07 0.34 0.02 0.   4.13] Loss_P: [2.34 1.99 1.6  0.99 0.72 0.06 0.33 0.01 8.05]\n",
      "Loss_Q: [1.1  1.15 0.82 0.67 0.06 0.3  0.03 0.   4.12] Loss_P: [2.35 1.98 1.58 0.95 0.71 0.07 0.31 0.02 7.97]\n",
      "Loss_Q: [1.11 1.16 0.73 0.68 0.07 0.29 0.01 0.   4.05] Loss_P: [2.39 2.   1.54 0.92 0.68 0.07 0.32 0.01 7.94]\n",
      "Loss_Q: [1.19 1.14 0.87 0.7  0.06 0.31 0.03 0.   4.3 ] Loss_P: [2.36 1.94 1.65 0.97 0.67 0.06 0.31 0.02 7.98]\n",
      "Loss_Q: [1.12 1.16 0.81 0.72 0.05 0.31 0.02 0.   4.19] Loss_P: [2.34 2.   1.59 0.92 0.69 0.06 0.3  0.02 7.92]\n",
      "Loss_Q: [1.08 1.17 0.81 0.69 0.05 0.27 0.02 0.   4.08] Loss_P: [2.32 1.98 1.54 0.96 0.7  0.06 0.31 0.01 7.88]\n",
      "Loss_Q: [1.16 1.16 0.77 0.67 0.06 0.28 0.01 0.   4.12] Loss_P: [2.31 1.95 1.59 0.92 0.71 0.07 0.27 0.02 7.83]\n",
      "Loss_Q: [1.1  1.16 0.79 0.71 0.06 0.28 0.01 0.   4.1 ] Loss_P: [2.36 1.92 1.5  0.94 0.72 0.07 0.26 0.01 7.77]\n",
      "Loss_Q: [1.07 1.14 0.83 0.72 0.08 0.29 0.01 0.   4.15] Loss_P: [2.33 1.92 1.61 0.94 0.75 0.07 0.27 0.01 7.9 ]\n",
      "Loss_Q: [1.02 1.11 0.81 0.73 0.06 0.29 0.01 0.   4.04] Loss_P: [2.29 2.01 1.59 1.01 0.76 0.07 0.24 0.02 7.98]\n",
      "Loss_Q: [1.03 1.16 0.79 0.77 0.07 0.28 0.02 0.   4.11] Loss_P: [2.39 1.92 1.55 0.98 0.77 0.06 0.31 0.01 7.99]\n",
      "Loss_Q: [1.08 1.1  0.81 0.74 0.09 0.31 0.01 0.   4.14] Loss_P: [2.29 1.98 1.55 0.95 0.74 0.06 0.28 0.02 7.85]\n",
      "Loss_Q: [1.04 1.06 0.86 0.75 0.06 0.31 0.02 0.   4.1 ] Loss_P: [2.36 1.99 1.55 1.   0.8  0.07 0.3  0.02 8.09]\n",
      "Loss_Q: [1.12 1.08 0.84 0.76 0.07 0.27 0.01 0.   4.14] Loss_P: [2.3  1.96 1.54 1.05 0.77 0.08 0.28 0.02 8.  ]\n",
      "Loss_Q: [1.   1.11 0.87 0.75 0.07 0.26 0.01 0.   4.07] Loss_P: [2.37 1.94 1.52 1.02 0.77 0.05 0.28 0.01 7.96]\n",
      "Loss_Q: [1.07 1.04 0.88 0.72 0.06 0.29 0.01 0.   4.07] Loss_P: [2.3  2.   1.56 0.96 0.75 0.06 0.27 0.02 7.92]\n",
      "Loss_Q: [1.13 1.09 0.83 0.75 0.07 0.26 0.01 0.   4.13] Loss_P: [2.34 1.99 1.49 0.9  0.73 0.05 0.29 0.03 7.83]\n",
      "Loss_Q: [1.09 1.12 0.81 0.76 0.08 0.27 0.03 0.   4.16] Loss_P: [2.33 1.96 1.54 0.92 0.75 0.07 0.26 0.01 7.86]\n",
      "Loss_Q: [1.13 1.09 0.85 0.77 0.07 0.27 0.02 0.   4.21] Loss_P: [2.36 1.93 1.46 0.9  0.71 0.05 0.25 0.01 7.68]\n",
      "Loss_Q: [1.08 1.08 0.8  0.74 0.05 0.25 0.01 0.   4.02] Loss_P: [2.39 2.03 1.5  0.98 0.76 0.06 0.25 0.02 7.99]\n",
      "Loss_Q: [1.05 1.08 0.84 0.75 0.07 0.27 0.01 0.   4.07] Loss_P: [2.37 2.01 1.51 0.97 0.76 0.07 0.26 0.02 7.96]\n",
      "Loss_Q: [1.08 1.12 0.85 0.74 0.05 0.28 0.03 0.   4.15] Loss_P: [2.33 2.01 1.46 0.98 0.77 0.07 0.3  0.02 7.92]\n",
      "Loss_Q: [1.1  1.18 0.86 0.75 0.07 0.3  0.02 0.   4.27] Loss_P: [2.33 2.04 1.49 1.01 0.75 0.06 0.28 0.03 7.99]\n",
      "Loss_Q: [1.06 1.16 0.83 0.77 0.07 0.29 0.02 0.   4.19] Loss_P: [2.35 1.96 1.53 1.02 0.79 0.06 0.29 0.02 8.02]\n",
      "Loss_Q: [1.05 1.1  0.85 0.75 0.06 0.28 0.03 0.   4.12] Loss_P: [2.39 1.97 1.47 1.03 0.76 0.07 0.33 0.01 8.03]\n",
      "Loss_Q: [1.   1.12 0.82 0.74 0.06 0.29 0.01 0.   4.04] Loss_P: [2.38 1.96 1.46 0.98 0.76 0.06 0.29 0.01 7.91]\n",
      "Loss_Q: [1.02 1.06 0.82 0.75 0.07 0.29 0.02 0.   4.04] Loss_P: [2.42 1.95 1.42 0.93 0.74 0.06 0.28 0.02 7.81]\n",
      "Loss_Q: [0.99 1.11 0.78 0.73 0.07 0.3  0.01 0.   3.99] Loss_P: [2.35 1.97 1.44 0.93 0.76 0.06 0.31 0.02 7.84]\n",
      "Loss_Q: [1.03 1.12 0.77 0.68 0.04 0.27 0.01 0.   3.93] Loss_P: [2.38 2.08 1.4  0.98 0.74 0.07 0.26 0.01 7.92]\n",
      "Loss_Q: [1.01 1.04 0.79 0.74 0.06 0.27 0.01 0.   3.91] Loss_P: [2.3  2.03 1.39 0.93 0.73 0.05 0.27 0.01 7.73]\n",
      "Loss_Q: [1.03 1.05 0.8  0.74 0.07 0.24 0.01 0.   3.96] Loss_P: [2.43 1.98 1.37 0.94 0.71 0.06 0.28 0.01 7.78]\n",
      "Loss_Q: [1.04 1.13 0.83 0.75 0.05 0.27 0.01 0.   4.08] Loss_P: [2.36 1.95 1.45 0.93 0.75 0.05 0.26 0.01 7.77]\n",
      "Loss_Q: [1.05 1.11 0.77 0.71 0.05 0.25 0.01 0.   3.95] Loss_P: [2.37 1.97 1.51 0.97 0.78 0.06 0.29 0.02 7.96]\n",
      "Loss_Q: [0.98 1.1  0.86 0.78 0.06 0.24 0.01 0.   4.03] Loss_P: [2.41 1.88 1.43 0.96 0.75 0.04 0.29 0.02 7.78]\n",
      "Loss_Q: [0.96 1.08 0.82 0.76 0.07 0.31 0.02 0.   4.02] Loss_P: [2.38 1.91 1.5  0.98 0.79 0.07 0.31 0.01 7.94]\n",
      "Loss_Q: [0.95 1.1  0.85 0.75 0.06 0.28 0.01 0.   4.  ] Loss_P: [2.31 1.93 1.51 0.95 0.76 0.05 0.28 0.02 7.82]\n",
      "Loss_Q: [0.95 1.1  0.82 0.76 0.05 0.24 0.01 0.   3.95] Loss_P: [2.37 1.88 1.54 1.   0.72 0.04 0.27 0.02 7.85]\n",
      "Loss_Q: [1.03 1.16 0.93 0.77 0.05 0.28 0.02 0.   4.23] Loss_P: [2.37 1.91 1.53 1.03 0.76 0.07 0.31 0.01 7.99]\n",
      "Loss_Q: [1.02 1.16 0.92 0.74 0.06 0.31 0.03 0.   4.24] Loss_P: [2.37 2.   1.49 1.08 0.75 0.06 0.28 0.01 8.04]\n",
      "Loss_Q: [1.02 1.18 0.86 0.68 0.05 0.28 0.01 0.   4.09] Loss_P: [2.39 1.94 1.47 1.02 0.75 0.07 0.26 0.02 7.92]\n",
      "Loss_Q: [1.01 1.15 0.89 0.72 0.07 0.27 0.02 0.   4.13] Loss_P: [2.36 1.9  1.47 1.07 0.72 0.08 0.27 0.01 7.87]\n",
      "Loss_Q: [1.01 1.15 0.87 0.7  0.03 0.29 0.01 0.   4.06] Loss_P: [2.35 1.95 1.56 1.05 0.71 0.07 0.31 0.01 8.01]\n",
      "Loss_Q: [1.   1.18 0.81 0.68 0.05 0.26 0.01 0.   3.99] Loss_P: [2.39 1.92 1.56 1.   0.72 0.06 0.29 0.02 7.96]\n",
      "Loss_Q: [0.99 1.13 0.83 0.66 0.05 0.27 0.02 0.   3.95] Loss_P: [2.44 1.93 1.48 0.98 0.7  0.06 0.29 0.01 7.9 ]\n",
      "Loss_Q: [0.95 1.15 0.83 0.72 0.05 0.29 0.01 0.   4.  ] Loss_P: [2.37 1.95 1.51 1.04 0.75 0.05 0.28 0.02 7.98]\n",
      "Loss_Q: [1.02 1.15 0.86 0.71 0.05 0.28 0.02 0.   4.09] Loss_P: [2.35 1.96 1.47 1.   0.77 0.06 0.28 0.02 7.89]\n",
      "Loss_Q: [0.94 1.2  0.85 0.72 0.05 0.3  0.01 0.   4.07] Loss_P: [2.37 1.92 1.54 1.03 0.72 0.06 0.29 0.02 7.95]\n",
      "Loss_Q: [0.95 1.21 0.82 0.71 0.05 0.29 0.01 0.   4.04] Loss_P: [2.39 1.95 1.54 1.   0.73 0.06 0.28 0.02 7.97]\n",
      "Loss_Q: [1.06 1.16 0.84 0.74 0.06 0.25 0.01 0.   4.13] Loss_P: [2.36 1.97 1.53 1.02 0.68 0.06 0.27 0.02 7.89]\n",
      "Loss_Q: [1.02 1.12 0.84 0.69 0.04 0.28 0.01 0.   3.99] Loss_P: [2.31 1.98 1.55 1.   0.75 0.05 0.29 0.01 7.95]\n",
      "Loss_Q: [1.06 1.2  0.86 0.72 0.06 0.28 0.01 0.   4.2 ] Loss_P: [2.33 1.94 1.63 0.99 0.75 0.05 0.29 0.01 7.99]\n",
      "Loss_Q: [1.08 1.15 0.84 0.69 0.04 0.28 0.01 0.   4.09] Loss_P: [2.34 1.99 1.57 1.05 0.77 0.06 0.29 0.01 8.09]\n",
      "Loss_Q: [1.   1.13 0.86 0.77 0.07 0.28 0.01 0.   4.12] Loss_P: [2.38 1.89 1.59 1.06 0.74 0.06 0.28 0.02 8.02]\n",
      "Loss_Q: [1.03 1.15 0.91 0.76 0.05 0.28 0.02 0.   4.2 ] Loss_P: [2.38 1.93 1.53 1.06 0.74 0.08 0.28 0.02 8.01]\n",
      "Loss_Q: [1.08 1.2  0.88 0.73 0.05 0.28 0.01 0.   4.22] Loss_P: [2.35 2.04 1.5  0.96 0.73 0.06 0.25 0.01 7.9 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.02 1.17 0.8  0.73 0.04 0.26 0.02 0.   4.04] Loss_P: [2.37 1.94 1.51 1.03 0.74 0.05 0.26 0.02 7.91]\n",
      "Loss_Q: [1.07 1.17 0.88 0.77 0.06 0.26 0.01 0.   4.23] Loss_P: [2.33 1.91 1.5  1.05 0.73 0.06 0.27 0.02 7.86]\n",
      "Loss_Q: [1.08 1.11 0.87 0.74 0.04 0.29 0.02 0.   4.15] Loss_P: [2.36 1.97 1.47 0.96 0.73 0.04 0.29 0.02 7.83]\n",
      "Loss_Q: [1.1  1.21 0.85 0.76 0.05 0.26 0.01 0.   4.23] Loss_P: [2.38 1.99 1.53 0.98 0.78 0.07 0.23 0.01 7.98]\n",
      "Loss_Q: [1.01 1.21 0.77 0.78 0.05 0.29 0.02 0.   4.13] Loss_P: [2.33 1.9  1.58 0.98 0.85 0.05 0.26 0.01 7.96]\n",
      "Loss_Q: [1.03 1.17 0.8  0.75 0.08 0.26 0.01 0.   4.11] Loss_P: [2.34 1.98 1.55 0.94 0.76 0.04 0.27 0.01 7.89]\n",
      "Loss_Q: [1.05 1.1  0.79 0.78 0.06 0.29 0.01 0.   4.08] Loss_P: [2.37 1.91 1.52 1.01 0.79 0.05 0.27 0.02 7.94]\n",
      "Loss_Q: [1.04 1.16 0.82 0.8  0.05 0.28 0.02 0.   4.17] Loss_P: [2.41 1.97 1.51 0.99 0.74 0.05 0.27 0.02 7.97]\n",
      "Loss_Q: [1.08 1.15 0.84 0.74 0.05 0.26 0.01 0.   4.13] Loss_P: [2.38 1.95 1.56 0.98 0.83 0.06 0.27 0.02 8.04]\n",
      "Loss_Q: [1.03 1.09 0.8  0.8  0.07 0.29 0.02 0.   4.1 ] Loss_P: [2.37 1.92 1.53 0.98 0.78 0.06 0.29 0.02 7.94]\n",
      "Loss_Q: [1.04 1.17 0.84 0.76 0.06 0.26 0.01 0.   4.15] Loss_P: [2.39 1.91 1.47 1.03 0.83 0.06 0.28 0.01 7.99]\n",
      "Loss_Q: [0.96 1.18 0.86 0.78 0.07 0.27 0.03 0.   4.16] Loss_P: [2.41 1.9  1.54 1.01 0.77 0.08 0.31 0.02 8.02]\n",
      "Loss_Q: [1.01 1.11 0.88 0.78 0.07 0.32 0.02 0.   4.17] Loss_P: [2.38 1.88 1.52 1.07 0.78 0.05 0.3  0.02 8.  ]\n",
      "Loss_Q: [0.98 1.15 0.89 0.79 0.05 0.28 0.01 0.   4.16] Loss_P: [2.35 1.86 1.5  1.05 0.79 0.06 0.31 0.02 7.94]\n",
      "Loss_Q: [0.92 1.19 0.83 0.76 0.05 0.3  0.02 0.   4.05] Loss_P: [2.36 1.85 1.56 1.05 0.77 0.07 0.31 0.01 7.99]\n",
      "Loss_Q: [0.93 1.18 0.89 0.78 0.06 0.32 0.01 0.   4.17] Loss_P: [2.39 1.89 1.55 1.09 0.78 0.07 0.32 0.02 8.1 ]\n",
      "Loss_Q: [0.93 1.16 0.86 0.75 0.05 0.32 0.02 0.   4.11] Loss_P: [2.37 1.85 1.53 1.07 0.78 0.06 0.33 0.02 8.02]\n",
      "Loss_Q: [0.92 1.2  0.91 0.75 0.05 0.33 0.02 0.   4.18] Loss_P: [2.39 1.87 1.42 1.01 0.77 0.06 0.3  0.01 7.84]\n",
      "Loss_Q: [0.93 1.11 0.83 0.75 0.06 0.33 0.02 0.   4.03] Loss_P: [2.35 1.92 1.58 1.04 0.76 0.07 0.29 0.02 8.03]\n",
      "Loss_Q: [1.   1.2  0.95 0.75 0.07 0.31 0.01 0.   4.3 ] Loss_P: [2.33 1.9  1.6  1.04 0.77 0.06 0.34 0.02 8.05]\n",
      "Loss_Q: [1.   1.19 0.89 0.75 0.06 0.32 0.01 0.   4.23] Loss_P: [2.31 1.97 1.55 1.08 0.72 0.07 0.31 0.02 8.02]\n",
      "Loss_Q: [0.97 1.21 0.89 0.71 0.07 0.34 0.02 0.   4.21] Loss_P: [2.34 2.01 1.6  1.05 0.73 0.06 0.3  0.02 8.11]\n",
      "Loss_Q: [1.02 1.13 0.87 0.73 0.04 0.36 0.01 0.   4.16] Loss_P: [2.35 1.95 1.57 1.02 0.77 0.05 0.33 0.02 8.05]\n",
      "Loss_Q: [1.04 1.18 0.82 0.76 0.04 0.34 0.01 0.   4.19] Loss_P: [2.31 2.08 1.53 1.04 0.74 0.06 0.33 0.02 8.1 ]\n",
      "Loss_Q: [1.03 1.17 0.82 0.69 0.04 0.34 0.02 0.   4.11] Loss_P: [2.31 2.07 1.52 0.98 0.76 0.06 0.3  0.01 8.01]\n",
      "Loss_Q: [1.01 1.15 0.84 0.72 0.05 0.35 0.02 0.   4.13] Loss_P: [2.33 2.06 1.51 0.96 0.77 0.06 0.35 0.02 8.04]\n",
      "Loss_Q: [1.01 1.15 0.82 0.74 0.06 0.36 0.01 0.   4.15] Loss_P: [2.32 1.98 1.55 0.93 0.74 0.07 0.36 0.01 7.97]\n",
      "Loss_Q: [1.04 1.14 0.79 0.76 0.06 0.35 0.01 0.   4.15] Loss_P: [2.33 1.99 1.49 0.96 0.75 0.06 0.36 0.01 7.95]\n",
      "Loss_Q: [1.02 1.16 0.78 0.71 0.06 0.35 0.01 0.   4.09] Loss_P: [2.31 2.07 1.57 0.91 0.73 0.05 0.35 0.01 8.  ]\n",
      "Loss_Q: [1.06 1.2  0.8  0.76 0.08 0.36 0.02 0.   4.27] Loss_P: [2.35 2.   1.54 0.9  0.7  0.07 0.36 0.02 7.94]\n",
      "Loss_Q: [1.1  1.13 0.77 0.71 0.05 0.35 0.01 0.   4.11] Loss_P: [2.36 1.99 1.61 0.97 0.7  0.08 0.36 0.02 8.08]\n",
      "Loss_Q: [1.04 1.22 0.81 0.74 0.05 0.34 0.01 0.   4.21] Loss_P: [2.32 1.97 1.57 0.94 0.73 0.06 0.35 0.01 7.95]\n",
      "Loss_Q: [1.06 1.11 0.78 0.69 0.03 0.37 0.02 0.   4.05] Loss_P: [2.34 2.   1.56 0.94 0.74 0.09 0.37 0.02 8.07]\n",
      "Loss_Q: [1.09 1.2  0.81 0.78 0.05 0.38 0.02 0.   4.32] Loss_P: [2.33 2.03 1.57 0.95 0.75 0.06 0.37 0.03 8.08]\n",
      "Loss_Q: [1.04 1.15 0.78 0.74 0.05 0.34 0.01 0.   4.11] Loss_P: [2.29 2.06 1.56 0.93 0.72 0.07 0.32 0.02 7.97]\n",
      "Loss_Q: [1.04 1.17 0.77 0.78 0.07 0.36 0.02 0.   4.21] Loss_P: [2.35 2.   1.53 0.91 0.79 0.07 0.35 0.01 8.  ]\n",
      "Loss_Q: [1.04 1.19 0.78 0.74 0.04 0.35 0.01 0.   4.16] Loss_P: [2.31 2.02 1.52 0.94 0.73 0.06 0.36 0.02 7.96]\n",
      "Loss_Q: [1.06 1.15 0.7  0.74 0.06 0.37 0.02 0.   4.09] Loss_P: [2.36 2.   1.51 0.87 0.74 0.07 0.35 0.02 7.91]\n",
      "Loss_Q: [1.03 1.16 0.74 0.72 0.06 0.37 0.02 0.   4.1 ] Loss_P: [2.35 1.97 1.55 0.94 0.73 0.05 0.38 0.02 7.99]\n",
      "Loss_Q: [1.05 1.13 0.73 0.68 0.05 0.39 0.02 0.   4.04] Loss_P: [2.31 2.03 1.57 0.9  0.7  0.05 0.38 0.01 7.95]\n",
      "Loss_Q: [1.02 1.15 0.71 0.67 0.04 0.41 0.01 0.   4.01] Loss_P: [2.33 1.96 1.5  0.85 0.72 0.07 0.39 0.03 7.86]\n",
      "Loss_Q: [1.07 1.13 0.8  0.65 0.06 0.4  0.01 0.   4.12] Loss_P: [2.37 1.95 1.48 0.87 0.69 0.06 0.36 0.02 7.79]\n",
      "Loss_Q: [1.   1.19 0.71 0.71 0.05 0.4  0.02 0.   4.07] Loss_P: [2.35 1.98 1.55 0.85 0.74 0.05 0.37 0.02 7.91]\n",
      "Loss_Q: [1.09 1.15 0.75 0.71 0.05 0.4  0.02 0.   4.17] Loss_P: [2.34 2.02 1.56 0.89 0.78 0.06 0.42 0.02 8.09]\n",
      "Loss_Q: [1.04 1.13 0.74 0.74 0.05 0.41 0.02 0.   4.13] Loss_P: [2.42 1.95 1.58 0.88 0.72 0.03 0.39 0.01 7.98]\n",
      "Loss_Q: [0.99 1.17 0.71 0.69 0.06 0.41 0.02 0.   4.05] Loss_P: [2.33 1.97 1.49 0.83 0.75 0.06 0.4  0.03 7.87]\n",
      "Loss_Q: [1.03 1.2  0.69 0.69 0.06 0.43 0.01 0.   4.12] Loss_P: [2.34 1.96 1.54 0.87 0.73 0.05 0.44 0.01 7.93]\n",
      "Loss_Q: [0.98 1.16 0.71 0.7  0.07 0.42 0.03 0.   4.06] Loss_P: [2.41 1.89 1.56 0.84 0.72 0.05 0.45 0.02 7.94]\n",
      "Loss_Q: [0.99 1.18 0.79 0.75 0.06 0.41 0.01 0.   4.19] Loss_P: [2.37 1.96 1.55 0.94 0.81 0.08 0.43 0.02 8.17]\n",
      "Loss_Q: [0.98 1.17 0.73 0.71 0.05 0.43 0.02 0.   4.09] Loss_P: [2.37 1.99 1.52 0.86 0.74 0.06 0.41 0.02 7.98]\n",
      "Loss_Q: [0.98 1.21 0.83 0.77 0.05 0.43 0.01 0.   4.28] Loss_P: [2.34 1.99 1.49 0.92 0.74 0.06 0.43 0.01 7.99]\n",
      "Loss_Q: [1.04 1.19 0.8  0.74 0.05 0.44 0.01 0.   4.27] Loss_P: [2.37 1.93 1.52 0.91 0.73 0.04 0.43 0.02 7.96]\n",
      "Loss_Q: [0.98 1.16 0.8  0.75 0.03 0.43 0.01 0.   4.15] Loss_P: [2.34 2.05 1.55 0.94 0.77 0.05 0.41 0.01 8.13]\n",
      "Loss_Q: [1.03 1.19 0.8  0.69 0.06 0.42 0.01 0.   4.21] Loss_P: [2.39 1.96 1.54 0.88 0.77 0.04 0.41 0.02 8.02]\n",
      "Loss_Q: [1.08 1.19 0.79 0.7  0.08 0.38 0.01 0.   4.22] Loss_P: [2.34 1.99 1.54 0.94 0.74 0.07 0.38 0.01 8.  ]\n",
      "Loss_Q: [0.98 1.21 0.77 0.77 0.09 0.4  0.01 0.   4.24] Loss_P: [2.35 1.96 1.54 0.9  0.77 0.07 0.4  0.02 8.02]\n",
      "Loss_Q: [1.06 1.25 0.83 0.71 0.06 0.37 0.01 0.   4.29] Loss_P: [2.36 2.03 1.55 0.95 0.76 0.06 0.34 0.01 8.04]\n",
      "Loss_Q: [1.03 1.24 0.79 0.74 0.06 0.38 0.01 0.   4.25] Loss_P: [2.41 1.99 1.58 1.   0.75 0.07 0.4  0.01 8.21]\n",
      "Loss_Q: [1.02 1.16 0.83 0.7  0.06 0.39 0.01 0.   4.17] Loss_P: [2.43 1.94 1.55 0.94 0.76 0.08 0.37 0.02 8.09]\n",
      "Loss_Q: [1.09 1.18 0.83 0.64 0.05 0.37 0.01 0.   4.16] Loss_P: [2.39 1.89 1.59 0.9  0.72 0.07 0.35 0.01 7.91]\n",
      "Loss_Q: [0.99 1.2  0.82 0.69 0.05 0.38 0.01 0.   4.14] Loss_P: [2.32 1.96 1.62 0.95 0.75 0.06 0.39 0.01 8.06]\n",
      "Loss_Q: [0.98 1.26 0.88 0.72 0.06 0.37 0.01 0.   4.28] Loss_P: [2.37 1.98 1.57 0.98 0.72 0.05 0.41 0.01 8.08]\n",
      "Loss_Q: [1.04 1.26 0.82 0.66 0.07 0.37 0.02 0.   4.25] Loss_P: [2.37 1.93 1.58 0.95 0.64 0.08 0.38 0.02 7.95]\n",
      "Loss_Q: [1.01 1.22 0.78 0.66 0.04 0.37 0.01 0.   4.1 ] Loss_P: [2.36 1.97 1.59 0.94 0.69 0.06 0.38 0.02 8.  ]\n",
      "Loss_Q: [0.99 1.16 0.81 0.71 0.06 0.36 0.01 0.   4.1 ] Loss_P: [2.35 2.   1.63 0.93 0.69 0.07 0.39 0.01 8.08]\n",
      "Loss_Q: [0.98 1.17 0.75 0.67 0.04 0.39 0.01 0.   4.01] Loss_P: [2.32 1.94 1.61 0.92 0.73 0.05 0.39 0.01 7.97]\n",
      "Loss_Q: [0.95 1.19 0.75 0.72 0.05 0.38 0.01 0.   4.05] Loss_P: [2.33 1.96 1.62 0.87 0.7  0.07 0.39 0.01 7.94]\n",
      "Loss_Q: [1.04 1.17 0.78 0.68 0.07 0.38 0.01 0.   4.13] Loss_P: [2.34 2.01 1.59 0.84 0.74 0.06 0.4  0.01 8.  ]\n",
      "Loss_Q: [1.03 1.22 0.78 0.7  0.06 0.38 0.01 0.   4.2 ] Loss_P: [2.33 2.01 1.57 0.86 0.69 0.06 0.39 0.01 7.93]\n",
      "Loss_Q: [1.05 1.23 0.76 0.71 0.05 0.39 0.01 0.   4.21] Loss_P: [2.34 1.98 1.62 0.89 0.71 0.06 0.4  0.02 8.01]\n",
      "Loss_Q: [0.91 1.26 0.77 0.69 0.04 0.39 0.01 0.   4.08] Loss_P: [2.34 1.95 1.64 0.96 0.65 0.05 0.41 0.02 8.02]\n",
      "Loss_Q: [0.92 1.27 0.7  0.65 0.05 0.38 0.01 0.   3.99] Loss_P: [2.32 1.9  1.67 0.85 0.67 0.05 0.38 0.02 7.86]\n",
      "Loss_Q: [0.98 1.25 0.75 0.66 0.07 0.39 0.02 0.   4.12] Loss_P: [2.3  1.98 1.69 0.81 0.7  0.08 0.39 0.01 7.95]\n",
      "Loss_Q: [1.05 1.3  0.71 0.64 0.05 0.38 0.01 0.   4.14] Loss_P: [2.35 1.95 1.68 0.73 0.65 0.06 0.39 0.01 7.82]\n",
      "Loss_Q: [0.94 1.19 0.73 0.7  0.07 0.4  0.01 0.   4.04] Loss_P: [2.3  1.98 1.68 0.8  0.67 0.06 0.39 0.02 7.9 ]\n",
      "Loss_Q: [0.99 1.22 0.7  0.71 0.06 0.4  0.01 0.   4.08] Loss_P: [2.36 1.9  1.67 0.84 0.69 0.07 0.39 0.01 7.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.99 1.18 0.7  0.71 0.06 0.42 0.01 0.   4.08] Loss_P: [2.39 1.88 1.67 0.71 0.65 0.07 0.37 0.02 7.76]\n",
      "Loss_Q: [0.95 1.22 0.67 0.68 0.07 0.39 0.02 0.   4.  ] Loss_P: [2.28 1.98 1.66 0.73 0.62 0.08 0.39 0.02 7.76]\n",
      "Loss_Q: [0.97 1.26 0.68 0.65 0.08 0.39 0.01 0.   4.05] Loss_P: [2.32 1.91 1.68 0.82 0.72 0.06 0.37 0.01 7.89]\n",
      "Loss_Q: [0.99 1.26 0.63 0.65 0.06 0.38 0.02 0.   3.98] Loss_P: [2.35 1.85 1.67 0.76 0.74 0.07 0.37 0.01 7.81]\n",
      "Loss_Q: [1.   1.24 0.62 0.65 0.07 0.36 0.02 0.   3.95] Loss_P: [2.32 2.   1.64 0.71 0.66 0.05 0.4  0.02 7.79]\n",
      "Loss_Q: [0.99 1.22 0.58 0.63 0.07 0.39 0.01 0.   3.89] Loss_P: [2.34 1.97 1.6  0.71 0.64 0.08 0.38 0.02 7.73]\n",
      "Loss_Q: [0.95 1.14 0.54 0.64 0.07 0.33 0.01 0.   3.69] Loss_P: [2.39 1.92 1.65 0.6  0.61 0.06 0.36 0.02 7.6 ]\n",
      "Loss_Q: [1.   1.25 0.55 0.58 0.06 0.37 0.03 0.   3.85] Loss_P: [2.36 1.9  1.63 0.59 0.63 0.06 0.32 0.02 7.53]\n",
      "Loss_Q: [1.03 1.19 0.56 0.68 0.05 0.36 0.01 0.   3.88] Loss_P: [2.39 1.9  1.63 0.67 0.63 0.07 0.34 0.01 7.65]\n",
      "Loss_Q: [0.89 1.19 0.57 0.66 0.07 0.34 0.01 0.   3.71] Loss_P: [2.42 1.88 1.63 0.68 0.64 0.06 0.36 0.01 7.69]\n",
      "Loss_Q: [0.99 1.2  0.62 0.62 0.06 0.33 0.01 0.   3.84] Loss_P: [2.34 1.9  1.65 0.7  0.63 0.08 0.31 0.01 7.62]\n",
      "Loss_Q: [0.99 1.18 0.6  0.58 0.05 0.32 0.01 0.   3.72] Loss_P: [2.39 1.95 1.69 0.69 0.62 0.07 0.31 0.01 7.74]\n",
      "Loss_Q: [0.96 1.15 0.65 0.55 0.07 0.33 0.01 0.   3.74] Loss_P: [2.38 1.86 1.7  0.72 0.59 0.08 0.31 0.01 7.66]\n",
      "Loss_Q: [0.94 1.23 0.63 0.57 0.09 0.35 0.01 0.   3.83] Loss_P: [2.42 1.92 1.68 0.7  0.56 0.05 0.33 0.02 7.68]\n",
      "Loss_Q: [0.95 1.21 0.63 0.57 0.06 0.35 0.01 0.   3.79] Loss_P: [2.39 1.93 1.69 0.7  0.57 0.04 0.37 0.01 7.69]\n",
      "Loss_Q: [0.99 1.24 0.59 0.56 0.09 0.36 0.01 0.   3.85] Loss_P: [2.34 1.95 1.7  0.7  0.58 0.06 0.37 0.01 7.71]\n",
      "Loss_Q: [0.99 1.17 0.62 0.52 0.07 0.38 0.01 0.   3.76] Loss_P: [2.36 1.88 1.62 0.67 0.61 0.06 0.37 0.03 7.6 ]\n",
      "Loss_Q: [1.03 1.16 0.6  0.53 0.07 0.4  0.02 0.   3.81] Loss_P: [2.37 1.98 1.65 0.65 0.55 0.04 0.4  0.02 7.65]\n",
      "Loss_Q: [0.95 1.25 0.6  0.5  0.05 0.4  0.01 0.   3.76] Loss_P: [2.37 1.96 1.63 0.66 0.54 0.07 0.39 0.02 7.63]\n",
      "Loss_Q: [0.96 1.25 0.53 0.46 0.08 0.4  0.01 0.   3.71] Loss_P: [2.39 1.94 1.66 0.62 0.55 0.08 0.42 0.01 7.66]\n",
      "Loss_Q: [1.02 1.19 0.54 0.49 0.05 0.4  0.01 0.   3.71] Loss_P: [2.4  2.02 1.59 0.63 0.55 0.07 0.39 0.02 7.67]\n",
      "Loss_Q: [1.01 1.26 0.54 0.52 0.06 0.41 0.01 0.   3.81] Loss_P: [2.41 1.95 1.69 0.66 0.56 0.06 0.43 0.02 7.78]\n",
      "Loss_Q: [0.96 1.23 0.56 0.49 0.05 0.4  0.01 0.   3.7 ] Loss_P: [2.38 1.91 1.66 0.63 0.58 0.07 0.43 0.01 7.66]\n",
      "Loss_Q: [0.96 1.27 0.53 0.53 0.06 0.4  0.01 0.   3.77] Loss_P: [2.39 1.92 1.63 0.63 0.55 0.06 0.4  0.01 7.6 ]\n",
      "Loss_Q: [0.92 1.23 0.52 0.55 0.05 0.42 0.01 0.   3.7 ] Loss_P: [2.43 1.88 1.63 0.58 0.59 0.08 0.43 0.01 7.63]\n",
      "Loss_Q: [0.9  1.24 0.52 0.49 0.05 0.4  0.01 0.   3.61] Loss_P: [2.36 1.94 1.68 0.59 0.59 0.05 0.4  0.02 7.63]\n",
      "Loss_Q: [0.96 1.24 0.56 0.6  0.08 0.41 0.02 0.   3.87] Loss_P: [2.43 1.9  1.65 0.61 0.59 0.09 0.42 0.02 7.7 ]\n",
      "Loss_Q: [0.96 1.27 0.58 0.63 0.08 0.44 0.02 0.   3.98] Loss_P: [2.38 1.94 1.72 0.64 0.64 0.07 0.44 0.01 7.83]\n",
      "Loss_Q: [0.97 1.27 0.61 0.65 0.08 0.43 0.02 0.   4.02] Loss_P: [2.4  1.91 1.64 0.63 0.65 0.05 0.42 0.01 7.72]\n",
      "Loss_Q: [0.92 1.23 0.59 0.64 0.07 0.4  0.02 0.   3.87] Loss_P: [2.38 1.84 1.68 0.7  0.67 0.09 0.41 0.01 7.79]\n",
      "Loss_Q: [0.98 1.19 0.61 0.64 0.08 0.39 0.02 0.   3.91] Loss_P: [2.43 1.89 1.65 0.74 0.7  0.07 0.38 0.01 7.88]\n",
      "Loss_Q: [0.87 1.22 0.64 0.63 0.07 0.4  0.01 0.   3.84] Loss_P: [2.4  1.85 1.65 0.75 0.65 0.08 0.39 0.01 7.77]\n",
      "Loss_Q: [0.97 1.24 0.62 0.61 0.06 0.4  0.01 0.   3.9 ] Loss_P: [2.39 1.89 1.67 0.7  0.64 0.06 0.37 0.02 7.74]\n",
      "Loss_Q: [0.88 1.25 0.67 0.64 0.05 0.4  0.01 0.   3.9 ] Loss_P: [2.44 1.89 1.66 0.71 0.63 0.07 0.4  0.02 7.83]\n",
      "Loss_Q: [1.03 1.17 0.63 0.61 0.07 0.42 0.01 0.   3.93] Loss_P: [2.38 1.92 1.72 0.74 0.65 0.08 0.42 0.02 7.92]\n",
      "Loss_Q: [0.95 1.3  0.61 0.61 0.06 0.43 0.02 0.   3.99] Loss_P: [2.38 1.93 1.68 0.68 0.65 0.08 0.44 0.01 7.86]\n",
      "Loss_Q: [1.02 1.28 0.66 0.66 0.08 0.44 0.02 0.   4.15] Loss_P: [2.39 1.86 1.72 0.67 0.65 0.08 0.44 0.02 7.82]\n",
      "Loss_Q: [1.03 1.23 0.61 0.65 0.05 0.45 0.01 0.   4.04] Loss_P: [2.39 1.92 1.75 0.7  0.65 0.06 0.44 0.01 7.92]\n",
      "Loss_Q: [0.89 1.32 0.66 0.64 0.06 0.45 0.02 0.   4.03] Loss_P: [2.39 1.88 1.74 0.71 0.66 0.06 0.45 0.02 7.9 ]\n",
      "Loss_Q: [0.94 1.26 0.68 0.62 0.06 0.41 0.01 0.   3.98] Loss_P: [2.42 1.82 1.73 0.73 0.65 0.05 0.43 0.01 7.84]\n",
      "Loss_Q: [0.96 1.31 0.74 0.67 0.08 0.43 0.01 0.   4.21] Loss_P: [2.38 1.9  1.75 0.73 0.66 0.07 0.42 0.01 7.93]\n",
      "Loss_Q: [0.98 1.28 0.65 0.64 0.07 0.41 0.01 0.   4.05] Loss_P: [2.36 1.93 1.72 0.68 0.65 0.07 0.43 0.02 7.85]\n",
      "Loss_Q: [0.92 1.38 0.66 0.64 0.06 0.41 0.01 0.   4.1 ] Loss_P: [2.37 1.89 1.74 0.81 0.66 0.05 0.41 0.02 7.95]\n",
      "Loss_Q: [0.89 1.34 0.68 0.62 0.06 0.4  0.01 0.   4.01] Loss_P: [2.41 1.93 1.7  0.75 0.66 0.07 0.44 0.01 7.96]\n",
      "Loss_Q: [0.96 1.27 0.72 0.65 0.05 0.44 0.01 0.   4.09] Loss_P: [2.42 1.91 1.72 0.8  0.65 0.07 0.42 0.02 8.01]\n",
      "Loss_Q: [0.89 1.26 0.7  0.57 0.07 0.41 0.02 0.   3.91] Loss_P: [2.36 1.88 1.77 0.78 0.62 0.08 0.4  0.01 7.89]\n",
      "Loss_Q: [0.89 1.34 0.7  0.62 0.06 0.42 0.01 0.   4.05] Loss_P: [2.42 1.88 1.79 0.83 0.65 0.07 0.42 0.01 8.06]\n",
      "Loss_Q: [0.95 1.33 0.74 0.65 0.06 0.42 0.01 0.   4.16] Loss_P: [2.4  1.91 1.72 0.78 0.61 0.07 0.38 0.02 7.88]\n",
      "Loss_Q: [0.97 1.29 0.73 0.6  0.04 0.4  0.01 0.   4.05] Loss_P: [2.4  1.93 1.64 0.81 0.61 0.06 0.38 0.01 7.84]\n",
      "Loss_Q: [0.91 1.35 0.72 0.61 0.07 0.42 0.02 0.   4.09] Loss_P: [2.39 1.89 1.76 0.74 0.61 0.06 0.42 0.01 7.88]\n",
      "Loss_Q: [0.91 1.42 0.73 0.59 0.05 0.41 0.01 0.   4.12] Loss_P: [2.41 1.91 1.72 0.74 0.58 0.06 0.43 0.01 7.87]\n",
      "Loss_Q: [0.95 1.32 0.73 0.57 0.05 0.4  0.01 0.   4.04] Loss_P: [2.32 1.91 1.77 0.87 0.57 0.06 0.41 0.01 7.92]\n",
      "Loss_Q: [0.93 1.35 0.77 0.58 0.05 0.4  0.01 0.   4.1 ] Loss_P: [2.41 1.91 1.75 0.86 0.61 0.07 0.4  0.01 8.02]\n",
      "Loss_Q: [1.04 1.33 0.79 0.63 0.09 0.37 0.02 0.   4.27] Loss_P: [2.4  1.87 1.76 0.82 0.64 0.05 0.4  0.01 7.96]\n",
      "Loss_Q: [0.91 1.27 0.75 0.63 0.06 0.4  0.01 0.   4.02] Loss_P: [2.38 1.89 1.72 0.8  0.63 0.07 0.39 0.01 7.91]\n",
      "Loss_Q: [0.92 1.29 0.7  0.62 0.05 0.43 0.01 0.   4.03] Loss_P: [2.42 1.9  1.72 0.75 0.6  0.07 0.43 0.01 7.89]\n",
      "Loss_Q: [0.92 1.34 0.69 0.65 0.04 0.4  0.01 0.   4.06] Loss_P: [2.36 1.94 1.7  0.74 0.66 0.07 0.38 0.01 7.86]\n",
      "Loss_Q: [0.97 1.27 0.67 0.62 0.06 0.41 0.01 0.   4.02] Loss_P: [2.35 1.98 1.68 0.77 0.63 0.06 0.4  0.01 7.86]\n",
      "Loss_Q: [0.95 1.28 0.75 0.6  0.07 0.4  0.02 0.   4.07] Loss_P: [2.34 1.92 1.65 0.82 0.62 0.08 0.42 0.01 7.86]\n",
      "Loss_Q: [0.9  1.37 0.7  0.58 0.07 0.41 0.02 0.   4.06] Loss_P: [2.39 1.91 1.65 0.75 0.61 0.05 0.38 0.01 7.76]\n",
      "Loss_Q: [0.95 1.36 0.77 0.63 0.08 0.38 0.02 0.   4.19] Loss_P: [2.37 1.92 1.69 0.8  0.58 0.07 0.35 0.01 7.78]\n",
      "Loss_Q: [0.96 1.31 0.76 0.58 0.06 0.39 0.01 0.   4.07] Loss_P: [2.32 1.94 1.7  0.86 0.62 0.07 0.35 0.01 7.87]\n",
      "Loss_Q: [0.97 1.31 0.76 0.61 0.07 0.39 0.01 0.   4.11] Loss_P: [2.34 1.93 1.7  0.88 0.64 0.07 0.39 0.01 7.95]\n",
      "Loss_Q: [0.97 1.32 0.77 0.59 0.07 0.41 0.01 0.   4.13] Loss_P: [2.38 1.94 1.66 0.87 0.64 0.08 0.38 0.01 7.95]\n",
      "Loss_Q: [0.95 1.25 0.78 0.59 0.09 0.41 0.01 0.   4.08] Loss_P: [2.45 1.94 1.61 0.85 0.56 0.09 0.4  0.01 7.91]\n",
      "Loss_Q: [0.98 1.32 0.71 0.56 0.05 0.41 0.01 0.   4.04] Loss_P: [2.34 1.94 1.75 0.86 0.57 0.06 0.39 0.03 7.93]\n",
      "Loss_Q: [0.97 1.32 0.76 0.56 0.07 0.4  0.01 0.   4.09] Loss_P: [2.35 1.93 1.66 0.85 0.61 0.06 0.4  0.02 7.88]\n",
      "Loss_Q: [1.01 1.32 0.78 0.62 0.06 0.39 0.01 0.   4.19] Loss_P: [2.35 1.93 1.69 0.85 0.62 0.06 0.4  0.01 7.91]\n",
      "Loss_Q: [0.91 1.31 0.76 0.59 0.07 0.39 0.02 0.   4.04] Loss_P: [2.36 1.94 1.68 0.81 0.59 0.07 0.36 0.01 7.82]\n",
      "Loss_Q: [0.95 1.25 0.74 0.58 0.07 0.38 0.03 0.   3.99] Loss_P: [2.35 2.   1.71 0.82 0.61 0.06 0.37 0.01 7.92]\n",
      "Loss_Q: [0.97 1.26 0.74 0.59 0.04 0.37 0.01 0.   3.97] Loss_P: [2.34 2.01 1.71 0.81 0.62 0.05 0.36 0.01 7.9 ]\n",
      "Loss_Q: [0.99 1.25 0.71 0.64 0.07 0.38 0.01 0.   4.04] Loss_P: [2.34 1.97 1.68 0.84 0.62 0.07 0.34 0.01 7.88]\n",
      "Loss_Q: [0.95 1.26 0.76 0.57 0.06 0.39 0.02 0.   4.02] Loss_P: [2.42 1.91 1.71 0.86 0.65 0.06 0.37 0.01 7.99]\n",
      "Loss_Q: [1.03 1.22 0.67 0.55 0.07 0.38 0.02 0.   3.94] Loss_P: [2.32 2.06 1.65 0.79 0.61 0.06 0.37 0.01 7.87]\n",
      "Loss_Q: [0.98 1.23 0.71 0.57 0.05 0.39 0.01 0.   3.95] Loss_P: [2.28 2.04 1.69 0.78 0.61 0.05 0.37 0.01 7.83]\n",
      "Loss_Q: [1.02 1.24 0.67 0.59 0.06 0.36 0.01 0.   3.96] Loss_P: [2.36 1.96 1.7  0.78 0.63 0.06 0.37 0.01 7.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.01 1.33 0.71 0.61 0.07 0.36 0.02 0.   4.12] Loss_P: [2.36 2.01 1.67 0.8  0.64 0.04 0.38 0.01 7.93]\n",
      "Loss_Q: [1.01 1.29 0.7  0.59 0.04 0.35 0.01 0.   4.  ] Loss_P: [2.37 1.93 1.7  0.74 0.61 0.06 0.36 0.01 7.79]\n",
      "Loss_Q: [0.99 1.26 0.72 0.62 0.05 0.36 0.01 0.   4.02] Loss_P: [2.36 1.94 1.67 0.74 0.63 0.07 0.35 0.01 7.77]\n",
      "Loss_Q: [0.99 1.27 0.71 0.55 0.05 0.36 0.01 0.   3.94] Loss_P: [2.33 1.99 1.7  0.78 0.63 0.05 0.37 0.01 7.85]\n",
      "Loss_Q: [0.92 1.24 0.7  0.57 0.05 0.37 0.02 0.   3.86] Loss_P: [2.28 1.97 1.71 0.75 0.58 0.04 0.36 0.01 7.7 ]\n",
      "Loss_Q: [0.96 1.24 0.73 0.59 0.06 0.34 0.01 0.   3.92] Loss_P: [2.36 1.93 1.73 0.72 0.62 0.05 0.34 0.01 7.76]\n",
      "Loss_Q: [0.95 1.18 0.73 0.62 0.06 0.37 0.01 0.   3.92] Loss_P: [2.35 1.88 1.69 0.78 0.62 0.07 0.35 0.01 7.76]\n",
      "Loss_Q: [0.9  1.28 0.74 0.62 0.05 0.32 0.01 0.   3.91] Loss_P: [2.28 1.91 1.76 0.78 0.61 0.05 0.35 0.02 7.76]\n",
      "Loss_Q: [0.93 1.27 0.71 0.56 0.06 0.33 0.01 0.   3.87] Loss_P: [2.38 1.85 1.73 0.82 0.65 0.05 0.35 0.01 7.85]\n",
      "Loss_Q: [0.95 1.32 0.72 0.58 0.05 0.32 0.02 0.   3.97] Loss_P: [2.35 1.92 1.73 0.73 0.57 0.05 0.34 0.01 7.71]\n",
      "Loss_Q: [0.95 1.27 0.65 0.57 0.05 0.34 0.01 0.   3.85] Loss_P: [2.41 1.9  1.69 0.77 0.63 0.09 0.33 0.02 7.84]\n",
      "Loss_Q: [0.89 1.26 0.68 0.53 0.06 0.4  0.01 0.   3.84] Loss_P: [2.31 1.95 1.71 0.66 0.6  0.08 0.36 0.01 7.68]\n",
      "Loss_Q: [0.89 1.31 0.75 0.61 0.06 0.4  0.01 0.   4.03] Loss_P: [2.35 1.95 1.69 0.74 0.54 0.06 0.4  0.01 7.74]\n",
      "Loss_Q: [0.94 1.35 0.69 0.6  0.09 0.39 0.01 0.   4.08] Loss_P: [2.38 1.98 1.76 0.76 0.52 0.05 0.4  0.02 7.87]\n",
      "Loss_Q: [1.   1.31 0.72 0.56 0.08 0.35 0.02 0.   4.03] Loss_P: [2.3  1.98 1.71 0.76 0.58 0.05 0.36 0.01 7.74]\n",
      "Loss_Q: [0.94 1.35 0.7  0.57 0.06 0.38 0.01 0.   4.01] Loss_P: [2.36 2.   1.77 0.7  0.57 0.06 0.37 0.01 7.83]\n",
      "Loss_Q: [1.06 1.37 0.75 0.57 0.07 0.39 0.01 0.   4.23] Loss_P: [2.32 1.99 1.79 0.74 0.55 0.06 0.42 0.01 7.88]\n",
      "Loss_Q: [0.97 1.23 0.71 0.54 0.05 0.38 0.01 0.   3.89] Loss_P: [2.29 2.   1.77 0.74 0.59 0.06 0.37 0.01 7.83]\n",
      "Loss_Q: [1.01 1.34 0.68 0.47 0.05 0.4  0.02 0.   3.97] Loss_P: [2.28 2.06 1.72 0.77 0.56 0.07 0.39 0.01 7.87]\n",
      "Loss_Q: [1.02 1.31 0.71 0.51 0.05 0.36 0.01 0.   3.96] Loss_P: [2.34 2.01 1.76 0.73 0.54 0.07 0.39 0.01 7.84]\n",
      "Loss_Q: [1.08 1.32 0.7  0.53 0.09 0.37 0.03 0.   4.11] Loss_P: [2.29 2.13 1.73 0.73 0.53 0.06 0.37 0.01 7.86]\n",
      "Loss_Q: [1.03 1.27 0.73 0.52 0.05 0.36 0.02 0.   3.98] Loss_P: [2.33 2.08 1.77 0.8  0.51 0.04 0.36 0.01 7.91]\n",
      "Loss_Q: [1.12 1.3  0.65 0.5  0.07 0.36 0.01 0.   4.  ] Loss_P: [2.32 2.03 1.69 0.79 0.52 0.06 0.37 0.01 7.79]\n",
      "Loss_Q: [1.09 1.31 0.74 0.57 0.07 0.36 0.01 0.   4.14] Loss_P: [2.28 2.12 1.75 0.74 0.53 0.05 0.35 0.01 7.83]\n",
      "Loss_Q: [1.11 1.34 0.74 0.54 0.04 0.33 0.02 0.   4.12] Loss_P: [2.39 2.09 1.71 0.77 0.51 0.06 0.33 0.01 7.87]\n",
      "Loss_Q: [1.14 1.29 0.71 0.53 0.08 0.31 0.01 0.   4.07] Loss_P: [2.34 2.06 1.73 0.75 0.54 0.08 0.32 0.01 7.83]\n",
      "Loss_Q: [1.08 1.28 0.71 0.51 0.07 0.32 0.01 0.   3.98] Loss_P: [2.32 2.12 1.74 0.71 0.56 0.05 0.31 0.02 7.82]\n",
      "Loss_Q: [1.13 1.26 0.68 0.55 0.06 0.32 0.01 0.   4.01] Loss_P: [2.36 2.07 1.81 0.69 0.57 0.06 0.33 0.01 7.9 ]\n",
      "Loss_Q: [1.11 1.29 0.66 0.5  0.07 0.33 0.01 0.   3.98] Loss_P: [2.31 2.15 1.79 0.72 0.52 0.07 0.32 0.01 7.88]\n",
      "Loss_Q: [0.97 1.33 0.67 0.53 0.05 0.33 0.01 0.   3.9 ] Loss_P: [2.26 2.12 1.64 0.74 0.57 0.07 0.35 0.01 7.76]\n",
      "Loss_Q: [1.1  1.26 0.67 0.51 0.07 0.34 0.01 0.   3.96] Loss_P: [2.27 2.12 1.66 0.68 0.5  0.07 0.33 0.01 7.63]\n",
      "Loss_Q: [1.1  1.25 0.67 0.52 0.07 0.34 0.01 0.   3.95] Loss_P: [2.33 2.15 1.68 0.74 0.49 0.08 0.31 0.01 7.79]\n",
      "Loss_Q: [1.03 1.31 0.68 0.53 0.05 0.36 0.01 0.   3.97] Loss_P: [2.36 2.1  1.67 0.7  0.54 0.07 0.34 0.01 7.8 ]\n",
      "Loss_Q: [1.11 1.27 0.69 0.5  0.06 0.31 0.01 0.   3.95] Loss_P: [2.29 2.16 1.69 0.77 0.53 0.07 0.35 0.01 7.86]\n",
      "Loss_Q: [1.02 1.29 0.68 0.53 0.06 0.36 0.02 0.   3.96] Loss_P: [2.33 2.06 1.69 0.7  0.54 0.07 0.39 0.01 7.79]\n",
      "Loss_Q: [1.06 1.33 0.67 0.53 0.06 0.37 0.01 0.   4.04] Loss_P: [2.31 2.11 1.69 0.7  0.53 0.07 0.38 0.01 7.81]\n",
      "Loss_Q: [1.   1.28 0.68 0.57 0.08 0.37 0.02 0.   4.01] Loss_P: [2.37 2.07 1.66 0.72 0.49 0.07 0.37 0.01 7.77]\n",
      "Loss_Q: [1.05 1.33 0.69 0.51 0.05 0.4  0.03 0.   4.04] Loss_P: [2.32 2.1  1.71 0.74 0.52 0.06 0.38 0.02 7.85]\n",
      "Loss_Q: [1.17 1.28 0.65 0.5  0.06 0.39 0.01 0.   4.07] Loss_P: [2.37 2.03 1.63 0.69 0.55 0.06 0.38 0.01 7.72]\n",
      "Loss_Q: [1.   1.25 0.64 0.52 0.06 0.38 0.01 0.   3.86] Loss_P: [2.35 2.06 1.7  0.7  0.5  0.05 0.38 0.02 7.75]\n",
      "Loss_Q: [1.05 1.24 0.68 0.47 0.07 0.34 0.02 0.   3.86] Loss_P: [2.44 2.07 1.7  0.68 0.47 0.05 0.37 0.01 7.8 ]\n",
      "Loss_Q: [0.98 1.31 0.63 0.45 0.08 0.36 0.01 0.   3.82] Loss_P: [2.34 2.06 1.73 0.73 0.49 0.06 0.38 0.02 7.81]\n",
      "Loss_Q: [1.05 1.28 0.65 0.47 0.06 0.39 0.01 0.   3.91] Loss_P: [2.34 2.06 1.72 0.72 0.5  0.07 0.37 0.01 7.8 ]\n",
      "Loss_Q: [1.06 1.27 0.66 0.48 0.06 0.42 0.01 0.   3.96] Loss_P: [2.37 2.   1.72 0.69 0.55 0.05 0.42 0.01 7.81]\n",
      "Loss_Q: [1.07 1.26 0.67 0.5  0.07 0.39 0.01 0.   3.97] Loss_P: [2.37 1.95 1.67 0.68 0.48 0.1  0.39 0.01 7.65]\n",
      "Loss_Q: [1.   1.28 0.61 0.47 0.05 0.4  0.01 0.   3.83] Loss_P: [2.35 1.99 1.67 0.68 0.41 0.06 0.38 0.03 7.56]\n",
      "Loss_Q: [0.96 1.28 0.65 0.49 0.07 0.41 0.01 0.   3.87] Loss_P: [2.38 2.   1.71 0.78 0.54 0.06 0.41 0.01 7.89]\n",
      "Loss_Q: [0.95 1.26 0.64 0.47 0.07 0.39 0.01 0.   3.79] Loss_P: [2.38 1.99 1.67 0.81 0.54 0.11 0.4  0.01 7.91]\n",
      "Loss_Q: [0.93 1.34 0.62 0.46 0.05 0.39 0.01 0.   3.79] Loss_P: [2.38 2.02 1.69 0.69 0.48 0.06 0.35 0.01 7.68]\n",
      "Loss_Q: [0.91 1.27 0.66 0.46 0.05 0.41 0.01 0.   3.78] Loss_P: [2.35 2.02 1.68 0.69 0.52 0.06 0.42 0.01 7.76]\n",
      "Loss_Q: [1.   1.32 0.67 0.45 0.11 0.36 0.01 0.   3.92] Loss_P: [2.32 2.   1.74 0.63 0.47 0.07 0.39 0.02 7.64]\n",
      "Loss_Q: [0.98 1.27 0.65 0.48 0.05 0.38 0.01 0.   3.81] Loss_P: [2.37 2.02 1.73 0.68 0.48 0.07 0.39 0.02 7.75]\n",
      "Loss_Q: [1.01 1.28 0.61 0.48 0.06 0.42 0.01 0.   3.86] Loss_P: [2.33 1.96 1.75 0.65 0.47 0.07 0.4  0.01 7.65]\n",
      "Loss_Q: [1.07 1.28 0.64 0.49 0.06 0.43 0.02 0.   3.99] Loss_P: [2.39 2.   1.76 0.71 0.48 0.05 0.44 0.01 7.83]\n",
      "Loss_Q: [0.99 1.36 0.62 0.49 0.06 0.44 0.01 0.   3.97] Loss_P: [2.32 2.02 1.68 0.68 0.46 0.06 0.44 0.01 7.69]\n",
      "Loss_Q: [0.92 1.33 0.59 0.42 0.05 0.42 0.03 0.   3.76] Loss_P: [2.3  2.   1.68 0.72 0.48 0.05 0.43 0.02 7.69]\n",
      "Loss_Q: [0.98 1.34 0.73 0.45 0.07 0.42 0.01 0.   3.98] Loss_P: [2.36 2.   1.68 0.75 0.5  0.06 0.43 0.01 7.8 ]\n",
      "Loss_Q: [0.9  1.35 0.64 0.45 0.07 0.41 0.01 0.   3.83] Loss_P: [2.34 2.03 1.72 0.7  0.42 0.06 0.39 0.01 7.65]\n",
      "Loss_Q: [0.97 1.33 0.68 0.46 0.07 0.39 0.01 0.   3.9 ] Loss_P: [2.37 1.93 1.74 0.75 0.47 0.06 0.38 0.01 7.71]\n",
      "Loss_Q: [0.97 1.27 0.65 0.46 0.07 0.39 0.01 0.   3.84] Loss_P: [2.39 1.96 1.73 0.73 0.44 0.05 0.4  0.01 7.71]\n",
      "Loss_Q: [1.01 1.31 0.71 0.46 0.06 0.38 0.01 0.   3.94] Loss_P: [2.36 1.94 1.74 0.73 0.49 0.07 0.39 0.03 7.75]\n",
      "Loss_Q: [0.91 1.34 0.71 0.53 0.06 0.4  0.01 0.   3.97] Loss_P: [2.36 2.   1.68 0.78 0.42 0.05 0.41 0.02 7.71]\n",
      "Loss_Q: [0.92 1.32 0.68 0.51 0.05 0.37 0.02 0.   3.87] Loss_P: [2.29 1.98 1.65 0.77 0.46 0.05 0.39 0.01 7.61]\n",
      "Loss_Q: [0.96 1.32 0.71 0.47 0.05 0.38 0.01 0.   3.89] Loss_P: [2.41 1.93 1.68 0.75 0.45 0.05 0.41 0.02 7.7 ]\n",
      "Loss_Q: [0.97 1.38 0.7  0.46 0.06 0.41 0.01 0.   3.99] Loss_P: [2.4  1.97 1.68 0.78 0.47 0.06 0.41 0.02 7.79]\n",
      "Loss_Q: [0.87 1.26 0.65 0.44 0.05 0.39 0.02 0.   3.67] Loss_P: [2.33 1.95 1.64 0.74 0.49 0.06 0.4  0.01 7.6 ]\n",
      "Loss_Q: [0.94 1.3  0.65 0.46 0.06 0.37 0.01 0.   3.8 ] Loss_P: [2.39 1.93 1.67 0.75 0.48 0.07 0.39 0.01 7.67]\n",
      "Loss_Q: [0.92 1.23 0.63 0.53 0.08 0.4  0.01 0.   3.79] Loss_P: [2.32 2.   1.63 0.76 0.5  0.05 0.4  0.01 7.67]\n",
      "Loss_Q: [0.9  1.28 0.67 0.48 0.05 0.4  0.01 0.   3.79] Loss_P: [2.32 1.98 1.67 0.7  0.51 0.06 0.41 0.01 7.66]\n",
      "Loss_Q: [0.92 1.32 0.64 0.54 0.06 0.4  0.01 0.   3.89] Loss_P: [2.37 1.99 1.67 0.7  0.52 0.06 0.4  0.01 7.73]\n",
      "Loss_Q: [0.94 1.27 0.6  0.49 0.05 0.39 0.01 0.   3.74] Loss_P: [2.36 1.97 1.72 0.68 0.51 0.07 0.41 0.01 7.72]\n",
      "Loss_Q: [0.95 1.35 0.66 0.54 0.06 0.41 0.01 0.   3.98] Loss_P: [2.37 1.96 1.7  0.7  0.54 0.07 0.42 0.01 7.76]\n",
      "Loss_Q: [0.99 1.31 0.56 0.47 0.05 0.41 0.01 0.   3.8 ] Loss_P: [2.28 2.06 1.68 0.66 0.55 0.06 0.4  0.01 7.7 ]\n",
      "Loss_Q: [0.97 1.39 0.63 0.51 0.05 0.42 0.01 0.   3.97] Loss_P: [2.37 2.   1.68 0.65 0.45 0.04 0.41 0.02 7.63]\n",
      "Loss_Q: [0.95 1.37 0.64 0.56 0.07 0.43 0.01 0.   4.03] Loss_P: [2.29 1.93 1.74 0.66 0.53 0.05 0.43 0.02 7.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.9  1.37 0.65 0.52 0.08 0.44 0.01 0.   3.97] Loss_P: [2.31 1.97 1.72 0.74 0.51 0.08 0.42 0.01 7.75]\n",
      "Loss_Q: [0.94 1.36 0.71 0.55 0.05 0.44 0.01 0.   4.07] Loss_P: [2.35 2.03 1.67 0.72 0.5  0.07 0.43 0.01 7.78]\n",
      "Loss_Q: [0.96 1.4  0.67 0.5  0.08 0.42 0.02 0.   4.05] Loss_P: [2.34 1.96 1.72 0.7  0.5  0.06 0.44 0.01 7.74]\n",
      "Loss_Q: [0.91 1.31 0.65 0.5  0.07 0.43 0.01 0.   3.89] Loss_P: [2.33 1.99 1.74 0.72 0.49 0.06 0.44 0.01 7.78]\n",
      "Loss_Q: [0.96 1.35 0.71 0.53 0.08 0.42 0.02 0.   4.07] Loss_P: [2.34 1.92 1.73 0.74 0.55 0.05 0.42 0.01 7.75]\n",
      "Loss_Q: [0.94 1.35 0.69 0.47 0.09 0.42 0.01 0.   3.97] Loss_P: [2.37 1.94 1.65 0.74 0.53 0.07 0.42 0.01 7.74]\n",
      "Loss_Q: [0.84 1.32 0.68 0.5  0.06 0.44 0.01 0.   3.85] Loss_P: [2.31 2.01 1.68 0.77 0.47 0.07 0.43 0.01 7.74]\n",
      "Loss_Q: [0.86 1.32 0.71 0.47 0.08 0.43 0.01 0.   3.88] Loss_P: [2.34 1.98 1.67 0.73 0.48 0.06 0.43 0.01 7.71]\n",
      "Loss_Q: [0.98 1.35 0.65 0.45 0.07 0.45 0.01 0.   3.96] Loss_P: [2.33 2.02 1.72 0.71 0.44 0.06 0.42 0.01 7.71]\n",
      "Loss_Q: [0.99 1.3  0.71 0.49 0.06 0.43 0.01 0.   3.98] Loss_P: [2.31 1.99 1.7  0.83 0.46 0.07 0.43 0.01 7.82]\n",
      "Loss_Q: [1.02 1.32 0.82 0.54 0.07 0.42 0.02 0.   4.21] Loss_P: [2.34 2.01 1.72 0.79 0.54 0.05 0.42 0.01 7.88]\n",
      "Loss_Q: [1.02 1.28 0.74 0.46 0.06 0.44 0.01 0.   4.02] Loss_P: [2.26 2.02 1.68 0.87 0.48 0.09 0.43 0.01 7.86]\n",
      "Loss_Q: [0.97 1.33 0.78 0.5  0.08 0.43 0.01 0.   4.1 ] Loss_P: [2.38 1.97 1.72 0.89 0.47 0.08 0.42 0.01 7.94]\n",
      "Loss_Q: [0.9  1.3  0.76 0.5  0.07 0.42 0.01 0.   3.95] Loss_P: [2.32 1.97 1.62 0.86 0.49 0.06 0.42 0.01 7.76]\n",
      "Loss_Q: [0.93 1.29 0.76 0.5  0.09 0.4  0.01 0.   3.98] Loss_P: [2.37 1.98 1.67 0.85 0.49 0.05 0.42 0.01 7.85]\n",
      "Loss_Q: [0.99 1.31 0.79 0.49 0.07 0.4  0.01 0.   4.06] Loss_P: [2.35 2.04 1.63 0.91 0.47 0.07 0.4  0.01 7.88]\n",
      "Loss_Q: [0.99 1.22 0.78 0.49 0.06 0.43 0.01 0.   3.97] Loss_P: [2.35 1.99 1.7  0.9  0.5  0.06 0.44 0.01 7.95]\n",
      "Loss_Q: [0.95 1.29 0.8  0.46 0.05 0.41 0.02 0.   3.98] Loss_P: [2.3  2.02 1.66 0.89 0.5  0.06 0.41 0.01 7.87]\n",
      "Loss_Q: [0.92 1.19 0.79 0.49 0.09 0.37 0.01 0.   3.86] Loss_P: [2.3  2.02 1.64 0.88 0.47 0.09 0.37 0.01 7.77]\n",
      "Loss_Q: [0.97 1.37 0.78 0.44 0.06 0.38 0.01 0.   4.01] Loss_P: [2.32 1.93 1.67 0.91 0.47 0.1  0.39 0.02 7.81]\n",
      "Loss_Q: [1.   1.28 0.81 0.47 0.09 0.39 0.01 0.   4.04] Loss_P: [2.33 1.93 1.61 0.91 0.45 0.06 0.4  0.01 7.69]\n",
      "Loss_Q: [0.93 1.25 0.81 0.47 0.08 0.39 0.01 0.   3.94] Loss_P: [2.34 2.01 1.66 0.9  0.49 0.07 0.36 0.01 7.84]\n",
      "Loss_Q: [0.98 1.24 0.79 0.47 0.09 0.39 0.01 0.   3.96] Loss_P: [2.28 1.95 1.64 0.91 0.54 0.1  0.39 0.01 7.82]\n",
      "Loss_Q: [1.   1.21 0.81 0.48 0.07 0.38 0.01 0.   3.95] Loss_P: [2.31 2.01 1.58 0.91 0.47 0.08 0.39 0.01 7.76]\n",
      "Loss_Q: [0.96 1.2  0.82 0.45 0.08 0.39 0.01 0.   3.91] Loss_P: [2.33 1.98 1.58 0.86 0.41 0.08 0.4  0.01 7.66]\n",
      "Loss_Q: [0.94 1.2  0.75 0.46 0.06 0.42 0.01 0.   3.84] Loss_P: [2.29 1.97 1.61 0.9  0.44 0.08 0.43 0.01 7.74]\n",
      "Loss_Q: [0.88 1.23 0.76 0.48 0.08 0.43 0.01 0.   3.86] Loss_P: [2.35 1.99 1.61 0.89 0.47 0.09 0.4  0.01 7.79]\n",
      "Loss_Q: [0.99 1.19 0.75 0.47 0.1  0.44 0.02 0.   3.96] Loss_P: [2.31 2.06 1.58 0.78 0.49 0.1  0.45 0.01 7.79]\n",
      "Loss_Q: [0.93 1.27 0.68 0.47 0.07 0.46 0.01 0.   3.9 ] Loss_P: [2.31 2.   1.55 0.79 0.51 0.08 0.44 0.02 7.7 ]\n",
      "Loss_Q: [0.95 1.29 0.74 0.51 0.07 0.43 0.01 0.   4.  ] Loss_P: [2.37 2.06 1.55 0.84 0.53 0.08 0.43 0.01 7.89]\n",
      "Loss_Q: [1.03 1.25 0.71 0.45 0.06 0.44 0.01 0.   3.95] Loss_P: [2.32 1.98 1.61 0.77 0.54 0.09 0.42 0.01 7.73]\n",
      "Loss_Q: [0.88 1.25 0.69 0.47 0.06 0.44 0.01 0.   3.79] Loss_P: [2.3  2.01 1.63 0.76 0.48 0.07 0.43 0.01 7.68]\n",
      "Loss_Q: [0.88 1.26 0.71 0.48 0.07 0.42 0.01 0.   3.83] Loss_P: [2.23 2.08 1.66 0.78 0.51 0.09 0.45 0.01 7.8 ]\n",
      "Loss_Q: [0.98 1.17 0.69 0.46 0.08 0.42 0.01 0.   3.8 ] Loss_P: [2.27 2.03 1.62 0.82 0.51 0.08 0.43 0.01 7.77]\n",
      "Loss_Q: [0.89 1.26 0.68 0.5  0.06 0.42 0.01 0.   3.82] Loss_P: [2.31 2.   1.62 0.81 0.54 0.07 0.42 0.01 7.78]\n",
      "Loss_Q: [0.93 1.27 0.72 0.49 0.07 0.44 0.02 0.   3.95] Loss_P: [2.26 2.02 1.57 0.79 0.53 0.08 0.41 0.03 7.69]\n",
      "Loss_Q: [1.01 1.27 0.79 0.55 0.07 0.45 0.01 0.   4.14] Loss_P: [2.33 1.96 1.66 0.81 0.55 0.08 0.42 0.01 7.83]\n",
      "Loss_Q: [0.97 1.2  0.72 0.52 0.06 0.44 0.01 0.   3.93] Loss_P: [2.26 2.03 1.64 0.82 0.55 0.08 0.43 0.01 7.81]\n",
      "Loss_Q: [1.04 1.26 0.72 0.49 0.07 0.45 0.01 0.   4.04] Loss_P: [2.22 2.01 1.66 0.82 0.57 0.09 0.44 0.02 7.84]\n",
      "Loss_Q: [0.91 1.29 0.74 0.55 0.08 0.42 0.02 0.   4.  ] Loss_P: [2.27 2.   1.71 0.81 0.54 0.06 0.44 0.01 7.85]\n",
      "Loss_Q: [0.98 1.31 0.73 0.55 0.08 0.42 0.02 0.   4.1 ] Loss_P: [2.31 1.99 1.7  0.79 0.51 0.06 0.44 0.01 7.81]\n",
      "Loss_Q: [1.06 1.28 0.71 0.54 0.06 0.43 0.01 0.   4.08] Loss_P: [2.34 1.97 1.72 0.78 0.53 0.08 0.42 0.02 7.87]\n",
      "Loss_Q: [1.   1.29 0.73 0.5  0.09 0.43 0.01 0.   4.04] Loss_P: [2.34 1.97 1.77 0.79 0.56 0.07 0.44 0.01 7.94]\n",
      "Loss_Q: [0.98 1.33 0.72 0.55 0.09 0.47 0.01 0.   4.14] Loss_P: [2.33 2.01 1.67 0.81 0.59 0.07 0.43 0.02 7.92]\n",
      "Loss_Q: [0.93 1.3  0.73 0.59 0.07 0.46 0.01 0.   4.09] Loss_P: [2.28 2.02 1.7  0.85 0.59 0.07 0.45 0.01 7.97]\n",
      "Loss_Q: [1.   1.28 0.71 0.57 0.09 0.47 0.02 0.   4.13] Loss_P: [2.27 2.03 1.69 0.8  0.63 0.1  0.44 0.01 7.97]\n",
      "Loss_Q: [0.99 1.29 0.72 0.54 0.09 0.45 0.01 0.   4.09] Loss_P: [2.33 1.99 1.71 0.81 0.57 0.09 0.46 0.01 7.98]\n",
      "Loss_Q: [0.96 1.33 0.77 0.56 0.07 0.46 0.01 0.   4.15] Loss_P: [2.32 2.04 1.74 0.87 0.56 0.09 0.45 0.01 8.08]\n",
      "Loss_Q: [1.   1.34 0.7  0.57 0.12 0.45 0.01 0.   4.19] Loss_P: [2.32 2.   1.76 0.81 0.56 0.08 0.44 0.01 7.98]\n",
      "Loss_Q: [1.01 1.31 0.71 0.53 0.09 0.44 0.01 0.   4.1 ] Loss_P: [2.36 1.97 1.74 0.76 0.53 0.07 0.45 0.01 7.89]\n",
      "Loss_Q: [0.98 1.32 0.69 0.51 0.06 0.46 0.01 0.   4.03] Loss_P: [2.29 2.02 1.7  0.81 0.56 0.07 0.46 0.01 7.91]\n",
      "Loss_Q: [1.02 1.37 0.73 0.57 0.07 0.44 0.01 0.   4.21] Loss_P: [2.34 2.01 1.82 0.83 0.57 0.09 0.44 0.02 8.12]\n",
      "Loss_Q: [0.99 1.29 0.74 0.57 0.07 0.45 0.01 0.   4.14] Loss_P: [2.3  2.02 1.77 0.85 0.58 0.07 0.46 0.01 8.06]\n",
      "Loss_Q: [1.03 1.28 0.71 0.55 0.07 0.45 0.01 0.   4.11] Loss_P: [2.35 2.01 1.72 0.87 0.57 0.07 0.44 0.01 8.05]\n",
      "Loss_Q: [1.06 1.27 0.71 0.62 0.08 0.44 0.01 0.   4.17] Loss_P: [2.29 1.94 1.75 0.8  0.6  0.07 0.45 0.01 7.91]\n",
      "Loss_Q: [1.02 1.25 0.69 0.59 0.07 0.45 0.02 0.   4.1 ] Loss_P: [2.33 1.96 1.74 0.84 0.59 0.06 0.44 0.01 7.98]\n",
      "Loss_Q: [0.96 1.32 0.72 0.56 0.07 0.45 0.01 0.   4.08] Loss_P: [2.23 1.99 1.74 0.81 0.6  0.06 0.43 0.01 7.86]\n",
      "Loss_Q: [1.07 1.27 0.77 0.59 0.06 0.46 0.01 0.   4.24] Loss_P: [2.33 1.99 1.84 0.77 0.54 0.06 0.48 0.01 8.02]\n",
      "Loss_Q: [1.04 1.33 0.76 0.57 0.07 0.44 0.01 0.   4.21] Loss_P: [2.35 1.95 1.78 0.85 0.59 0.04 0.47 0.01 8.05]\n",
      "Loss_Q: [1.03 1.32 0.76 0.55 0.06 0.44 0.01 0.   4.16] Loss_P: [2.3  1.89 1.77 0.87 0.58 0.08 0.47 0.01 7.96]\n",
      "Loss_Q: [1.04 1.33 0.78 0.57 0.07 0.45 0.01 0.   4.25] Loss_P: [2.32 1.98 1.83 0.81 0.59 0.08 0.45 0.01 8.07]\n",
      "Loss_Q: [1.   1.31 0.76 0.52 0.05 0.47 0.01 0.   4.13] Loss_P: [2.3  2.   1.83 0.83 0.58 0.06 0.44 0.01 8.04]\n",
      "Loss_Q: [1.03 1.32 0.82 0.56 0.06 0.44 0.01 0.   4.24] Loss_P: [2.36 2.03 1.75 0.9  0.58 0.06 0.46 0.01 8.15]\n",
      "Loss_Q: [1.02 1.34 0.78 0.54 0.05 0.46 0.01 0.   4.2 ] Loss_P: [2.28 1.99 1.76 0.91 0.61 0.07 0.47 0.01 8.09]\n",
      "Loss_Q: [0.94 1.34 0.73 0.57 0.06 0.47 0.01 0.   4.12] Loss_P: [2.32 1.96 1.77 0.84 0.57 0.06 0.48 0.01 8.  ]\n",
      "Loss_Q: [1.02 1.39 0.75 0.57 0.07 0.48 0.02 0.   4.29] Loss_P: [2.32 1.94 1.82 0.87 0.64 0.07 0.47 0.01 8.14]\n",
      "Loss_Q: [0.93 1.31 0.76 0.63 0.07 0.49 0.01 0.   4.21] Loss_P: [2.36 1.87 1.81 0.84 0.65 0.06 0.48 0.02 8.08]\n",
      "Loss_Q: [1.   1.24 0.77 0.63 0.08 0.49 0.02 0.   4.22] Loss_P: [2.29 1.98 1.81 0.84 0.61 0.04 0.5  0.01 8.08]\n",
      "Loss_Q: [0.94 1.27 0.8  0.64 0.05 0.5  0.02 0.   4.21] Loss_P: [2.29 1.88 1.81 0.81 0.62 0.07 0.5  0.01 7.99]\n",
      "Loss_Q: [1.04 1.31 0.76 0.58 0.05 0.5  0.01 0.   4.25] Loss_P: [2.28 1.96 1.84 0.84 0.59 0.05 0.5  0.01 8.07]\n",
      "Loss_Q: [0.93 1.3  0.85 0.61 0.06 0.49 0.01 0.   4.25] Loss_P: [2.28 1.94 1.86 0.88 0.59 0.05 0.49 0.01 8.1 ]\n",
      "Loss_Q: [0.97 1.28 0.71 0.62 0.06 0.49 0.01 0.   4.13] Loss_P: [2.35 1.91 1.82 0.85 0.63 0.05 0.49 0.01 8.1 ]\n",
      "Loss_Q: [0.94 1.32 0.67 0.6  0.06 0.5  0.02 0.   4.1 ] Loss_P: [2.28 1.89 1.86 0.75 0.57 0.06 0.5  0.01 7.92]\n",
      "Loss_Q: [1.03 1.38 0.78 0.61 0.05 0.49 0.01 0.   4.35] Loss_P: [2.32 1.91 1.83 0.86 0.67 0.06 0.48 0.01 8.15]\n",
      "Loss_Q: [0.88 1.39 0.7  0.67 0.07 0.5  0.01 0.   4.23] Loss_P: [2.3  1.87 1.83 0.85 0.65 0.06 0.5  0.01 8.06]\n",
      "Loss_Q: [0.96 1.22 0.77 0.64 0.07 0.49 0.01 0.   4.16] Loss_P: [2.32 1.85 1.86 0.79 0.7  0.07 0.48 0.01 8.1 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.   1.29 0.66 0.6  0.09 0.47 0.01 0.   4.11] Loss_P: [2.26 1.91 1.87 0.78 0.67 0.06 0.47 0.01 8.03]\n",
      "Loss_Q: [0.96 1.3  0.71 0.6  0.06 0.47 0.01 0.   4.11] Loss_P: [2.31 1.88 1.81 0.76 0.68 0.07 0.47 0.01 8.  ]\n",
      "Loss_Q: [0.94 1.28 0.76 0.63 0.08 0.49 0.02 0.   4.19] Loss_P: [2.3  1.86 1.85 0.8  0.66 0.07 0.48 0.01 8.03]\n",
      "Loss_Q: [1.04 1.29 0.75 0.63 0.07 0.48 0.01 0.   4.27] Loss_P: [2.3  1.9  1.86 0.76 0.64 0.06 0.48 0.02 8.01]\n",
      "Loss_Q: [0.99 1.4  0.71 0.61 0.06 0.49 0.01 0.   4.27] Loss_P: [2.3  1.87 1.9  0.8  0.68 0.06 0.49 0.01 8.1 ]\n",
      "Loss_Q: [0.95 1.36 0.7  0.57 0.07 0.5  0.01 0.   4.17] Loss_P: [2.33 1.87 1.95 0.73 0.64 0.05 0.5  0.01 8.08]\n",
      "Loss_Q: [0.98 1.37 0.68 0.56 0.08 0.5  0.01 0.   4.17] Loss_P: [2.27 1.84 1.95 0.82 0.66 0.08 0.48 0.01 8.11]\n",
      "Loss_Q: [1.06 1.37 0.75 0.66 0.06 0.48 0.01 0.   4.39] Loss_P: [2.32 1.89 1.93 0.78 0.61 0.07 0.49 0.01 8.09]\n",
      "Loss_Q: [1.08 1.39 0.72 0.63 0.07 0.5  0.01 0.   4.39] Loss_P: [2.35 1.89 1.94 0.72 0.63 0.05 0.48 0.01 8.06]\n",
      "Loss_Q: [1.02 1.41 0.7  0.61 0.08 0.48 0.01 0.   4.3 ] Loss_P: [2.24 1.95 1.94 0.72 0.64 0.07 0.48 0.02 8.06]\n",
      "Loss_Q: [0.99 1.36 0.66 0.65 0.08 0.47 0.02 0.   4.24] Loss_P: [2.28 1.89 1.9  0.73 0.62 0.05 0.47 0.01 7.95]\n",
      "Loss_Q: [0.94 1.33 0.68 0.62 0.07 0.49 0.01 0.   4.14] Loss_P: [2.32 1.9  1.91 0.77 0.63 0.08 0.46 0.01 8.08]\n",
      "Loss_Q: [1.04 1.44 0.66 0.62 0.06 0.45 0.01 0.   4.29] Loss_P: [2.24 1.9  1.93 0.73 0.66 0.06 0.47 0.01 8.01]\n",
      "Loss_Q: [1.01 1.45 0.66 0.66 0.05 0.47 0.01 0.   4.31] Loss_P: [2.31 1.96 1.99 0.76 0.65 0.07 0.47 0.02 8.22]\n",
      "Loss_Q: [0.97 1.46 0.68 0.67 0.06 0.47 0.01 0.   4.32] Loss_P: [2.28 1.95 2.   0.71 0.69 0.07 0.46 0.01 8.16]\n",
      "Loss_Q: [0.94 1.42 0.66 0.61 0.07 0.43 0.01 0.   4.14] Loss_P: [2.29 1.92 1.87 0.72 0.65 0.04 0.46 0.01 7.96]\n",
      "Loss_Q: [0.97 1.38 0.69 0.63 0.06 0.46 0.01 0.   4.19] Loss_P: [2.28 1.88 1.93 0.79 0.63 0.08 0.47 0.01 8.08]\n",
      "Loss_Q: [1.   1.44 0.7  0.64 0.08 0.49 0.01 0.   4.36] Loss_P: [2.23 1.96 1.94 0.77 0.68 0.05 0.48 0.01 8.12]\n",
      "Loss_Q: [0.97 1.48 0.73 0.63 0.05 0.5  0.01 0.   4.38] Loss_P: [2.26 1.91 1.99 0.77 0.63 0.06 0.51 0.02 8.15]\n",
      "Loss_Q: [1.01 1.43 0.63 0.59 0.06 0.49 0.01 0.   4.21] Loss_P: [2.24 1.97 1.89 0.66 0.61 0.07 0.5  0.01 7.96]\n",
      "Loss_Q: [0.89 1.39 0.63 0.56 0.06 0.52 0.02 0.   4.06] Loss_P: [2.27 1.86 1.86 0.71 0.64 0.06 0.51 0.01 7.92]\n",
      "Loss_Q: [0.94 1.41 0.68 0.62 0.05 0.5  0.02 0.   4.21] Loss_P: [2.28 1.92 1.9  0.74 0.62 0.06 0.5  0.01 8.04]\n",
      "Loss_Q: [0.97 1.43 0.7  0.61 0.05 0.5  0.02 0.   4.28] Loss_P: [2.25 1.95 1.91 0.68 0.65 0.09 0.51 0.01 8.04]\n",
      "Loss_Q: [0.93 1.44 0.6  0.58 0.06 0.5  0.01 0.   4.14] Loss_P: [2.3  1.9  1.92 0.66 0.64 0.07 0.51 0.01 8.01]\n",
      "Loss_Q: [0.95 1.44 0.67 0.6  0.05 0.51 0.02 0.   4.23] Loss_P: [2.29 1.97 1.96 0.73 0.62 0.07 0.52 0.01 8.15]\n",
      "Loss_Q: [0.92 1.42 0.64 0.63 0.04 0.51 0.02 0.   4.18] Loss_P: [2.3  1.89 1.91 0.74 0.61 0.06 0.51 0.01 8.04]\n",
      "Loss_Q: [1.   1.39 0.64 0.6  0.06 0.51 0.02 0.   4.23] Loss_P: [2.34 1.92 1.93 0.7  0.65 0.06 0.5  0.01 8.11]\n",
      "Loss_Q: [1.01 1.39 0.63 0.57 0.07 0.5  0.02 0.   4.19] Loss_P: [2.3  1.94 1.95 0.68 0.61 0.07 0.51 0.01 8.06]\n",
      "Loss_Q: [0.98 1.38 0.64 0.54 0.07 0.51 0.01 0.   4.12] Loss_P: [2.33 1.93 1.94 0.71 0.6  0.06 0.5  0.01 8.08]\n",
      "Loss_Q: [0.94 1.42 0.61 0.64 0.1  0.52 0.02 0.   4.24] Loss_P: [2.34 1.88 1.86 0.72 0.64 0.07 0.5  0.01 8.01]\n",
      "Loss_Q: [0.94 1.34 0.63 0.67 0.07 0.48 0.01 0.   4.13] Loss_P: [2.26 1.95 1.9  0.72 0.65 0.08 0.49 0.01 8.06]\n",
      "Loss_Q: [0.9  1.39 0.64 0.67 0.07 0.5  0.01 0.   4.17] Loss_P: [2.31 1.94 1.83 0.77 0.72 0.06 0.48 0.01 8.11]\n",
      "Loss_Q: [0.96 1.38 0.61 0.63 0.06 0.49 0.01 0.   4.15] Loss_P: [2.3  1.92 1.83 0.66 0.62 0.06 0.49 0.01 7.9 ]\n",
      "Loss_Q: [0.89 1.33 0.62 0.64 0.08 0.49 0.01 0.   4.07] Loss_P: [2.27 2.   1.8  0.73 0.6  0.08 0.5  0.01 7.99]\n",
      "Loss_Q: [0.96 1.34 0.67 0.62 0.08 0.49 0.02 0.   4.18] Loss_P: [2.3  1.91 1.84 0.75 0.68 0.08 0.48 0.01 8.06]\n",
      "Loss_Q: [0.94 1.39 0.64 0.66 0.07 0.5  0.01 0.   4.22] Loss_P: [2.28 1.9  1.84 0.71 0.71 0.08 0.49 0.01 8.02]\n",
      "Loss_Q: [0.95 1.37 0.64 0.67 0.06 0.48 0.02 0.   4.18] Loss_P: [2.28 1.92 1.82 0.71 0.66 0.08 0.49 0.01 7.97]\n",
      "Loss_Q: [0.93 1.41 0.65 0.65 0.06 0.49 0.01 0.   4.19] Loss_P: [2.33 1.93 1.84 0.72 0.68 0.08 0.49 0.01 8.08]\n",
      "Loss_Q: [0.89 1.37 0.64 0.64 0.07 0.49 0.01 0.   4.1 ] Loss_P: [2.29 2.02 1.85 0.73 0.71 0.1  0.47 0.01 8.17]\n",
      "Loss_Q: [1.02 1.35 0.69 0.66 0.06 0.48 0.01 0.   4.26] Loss_P: [2.27 1.94 1.86 0.7  0.63 0.06 0.46 0.02 7.95]\n",
      "Loss_Q: [0.95 1.42 0.67 0.64 0.05 0.49 0.02 0.   4.24] Loss_P: [2.29 1.95 1.82 0.73 0.67 0.08 0.47 0.01 8.02]\n",
      "Loss_Q: [0.89 1.36 0.65 0.66 0.06 0.48 0.01 0.   4.1 ] Loss_P: [2.3  2.   1.8  0.76 0.66 0.06 0.47 0.01 8.06]\n",
      "Loss_Q: [0.91 1.43 0.66 0.59 0.06 0.46 0.01 0.   4.13] Loss_P: [2.35 1.94 1.83 0.73 0.6  0.07 0.47 0.01 8.  ]\n",
      "Loss_Q: [1.02 1.38 0.71 0.61 0.07 0.47 0.01 0.   4.26] Loss_P: [2.3  1.97 1.88 0.74 0.68 0.09 0.44 0.01 8.1 ]\n",
      "Loss_Q: [0.94 1.44 0.66 0.58 0.08 0.46 0.02 0.   4.18] Loss_P: [2.26 1.91 1.87 0.74 0.63 0.07 0.46 0.01 7.95]\n",
      "Loss_Q: [1.04 1.47 0.71 0.59 0.06 0.46 0.01 0.   4.34] Loss_P: [2.24 1.98 1.9  0.72 0.6  0.07 0.48 0.01 8.  ]\n",
      "Loss_Q: [0.94 1.37 0.7  0.62 0.06 0.47 0.01 0.   4.17] Loss_P: [2.31 1.89 1.9  0.76 0.59 0.06 0.47 0.01 7.99]\n",
      "Loss_Q: [0.84 1.43 0.68 0.62 0.07 0.47 0.01 0.   4.11] Loss_P: [2.31 1.98 1.92 0.78 0.6  0.07 0.46 0.01 8.12]\n",
      "Loss_Q: [0.97 1.46 0.69 0.6  0.08 0.46 0.02 0.   4.26] Loss_P: [2.28 1.96 1.87 0.72 0.63 0.06 0.46 0.01 7.99]\n",
      "Loss_Q: [0.94 1.39 0.74 0.64 0.09 0.46 0.02 0.   4.27] Loss_P: [2.29 1.97 1.82 0.76 0.6  0.08 0.43 0.01 7.96]\n",
      "Loss_Q: [0.89 1.41 0.68 0.63 0.07 0.43 0.01 0.   4.13] Loss_P: [2.31 1.95 1.82 0.76 0.61 0.07 0.47 0.02 8.01]\n",
      "Loss_Q: [0.92 1.49 0.64 0.57 0.09 0.45 0.02 0.   4.17] Loss_P: [2.32 1.99 1.8  0.73 0.61 0.08 0.46 0.01 8.  ]\n",
      "Loss_Q: [0.92 1.45 0.67 0.59 0.08 0.47 0.01 0.   4.19] Loss_P: [2.27 1.89 1.87 0.75 0.6  0.08 0.47 0.01 7.93]\n",
      "Loss_Q: [0.85 1.41 0.7  0.62 0.07 0.47 0.01 0.   4.13] Loss_P: [2.29 1.92 1.84 0.75 0.57 0.07 0.46 0.01 7.9 ]\n",
      "Loss_Q: [0.89 1.5  0.65 0.54 0.06 0.46 0.02 0.   4.11] Loss_P: [2.31 1.92 1.89 0.71 0.63 0.07 0.44 0.01 7.97]\n",
      "Loss_Q: [0.95 1.42 0.66 0.65 0.08 0.44 0.01 0.   4.2 ] Loss_P: [2.26 1.91 1.93 0.63 0.58 0.07 0.47 0.01 7.85]\n",
      "Loss_Q: [0.91 1.44 0.65 0.7  0.1  0.47 0.01 0.   4.28] Loss_P: [2.29 1.96 1.94 0.7  0.7  0.09 0.47 0.01 8.15]\n",
      "Loss_Q: [0.84 1.49 0.61 0.65 0.07 0.47 0.02 0.   4.15] Loss_P: [2.29 1.92 1.88 0.67 0.65 0.07 0.46 0.01 7.94]\n",
      "Loss_Q: [0.86 1.47 0.62 0.6  0.08 0.46 0.02 0.   4.1 ] Loss_P: [2.27 1.95 1.88 0.62 0.64 0.08 0.49 0.02 7.96]\n",
      "Loss_Q: [0.87 1.45 0.6  0.55 0.07 0.49 0.01 0.   4.04] Loss_P: [2.28 1.93 1.92 0.7  0.61 0.06 0.47 0.01 7.98]\n",
      "Loss_Q: [0.88 1.45 0.62 0.6  0.08 0.5  0.01 0.   4.14] Loss_P: [2.31 1.91 1.9  0.62 0.6  0.07 0.5  0.01 7.92]\n",
      "Loss_Q: [0.84 1.45 0.67 0.61 0.07 0.51 0.03 0.   4.18] Loss_P: [2.25 1.94 1.84 0.73 0.62 0.09 0.49 0.02 7.97]\n",
      "Loss_Q: [0.93 1.48 0.65 0.61 0.08 0.51 0.01 0.   4.27] Loss_P: [2.26 1.92 1.92 0.73 0.63 0.07 0.48 0.01 8.02]\n",
      "Loss_Q: [0.89 1.47 0.62 0.65 0.07 0.5  0.01 0.   4.2 ] Loss_P: [2.28 1.89 1.89 0.64 0.59 0.07 0.5  0.01 7.87]\n",
      "Loss_Q: [0.89 1.43 0.7  0.64 0.09 0.5  0.01 0.   4.26] Loss_P: [2.29 1.89 1.85 0.76 0.63 0.07 0.49 0.01 7.99]\n",
      "Loss_Q: [0.91 1.45 0.63 0.68 0.06 0.5  0.01 0.   4.26] Loss_P: [2.32 1.87 1.86 0.74 0.64 0.08 0.51 0.01 8.02]\n",
      "Loss_Q: [0.8  1.52 0.67 0.62 0.07 0.5  0.01 0.   4.2 ] Loss_P: [2.36 1.89 1.89 0.75 0.59 0.07 0.5  0.01 8.06]\n",
      "Loss_Q: [0.88 1.5  0.73 0.65 0.08 0.51 0.01 0.   4.35] Loss_P: [2.28 1.9  1.94 0.75 0.72 0.07 0.51 0.01 8.19]\n",
      "Loss_Q: [0.88 1.48 0.67 0.63 0.07 0.5  0.02 0.   4.25] Loss_P: [2.33 1.88 1.8  0.76 0.65 0.07 0.5  0.01 8.  ]\n",
      "Loss_Q: [0.82 1.48 0.66 0.7  0.05 0.5  0.01 0.   4.22] Loss_P: [2.32 1.98 1.82 0.74 0.69 0.06 0.5  0.01 8.12]\n",
      "Loss_Q: [0.87 1.47 0.67 0.7  0.08 0.49 0.01 0.   4.28] Loss_P: [2.29 1.99 1.79 0.74 0.7  0.08 0.49 0.01 8.09]\n",
      "Loss_Q: [0.86 1.4  0.67 0.69 0.07 0.49 0.02 0.   4.2 ] Loss_P: [2.37 1.93 1.82 0.7  0.65 0.06 0.5  0.01 8.04]\n",
      "Loss_Q: [1.02 1.49 0.65 0.67 0.07 0.48 0.02 0.   4.4 ] Loss_P: [2.26 1.95 1.92 0.7  0.67 0.09 0.49 0.01 8.09]\n",
      "Loss_Q: [0.79 1.46 0.6  0.71 0.05 0.49 0.01 0.   4.11] Loss_P: [2.36 1.85 1.89 0.7  0.67 0.07 0.48 0.02 8.04]\n",
      "Loss_Q: [0.86 1.38 0.59 0.68 0.08 0.48 0.01 0.   4.1 ] Loss_P: [2.35 1.87 1.83 0.68 0.65 0.04 0.49 0.02 7.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.86 1.32 0.62 0.65 0.05 0.48 0.01 0.   3.99] Loss_P: [2.33 1.87 1.71 0.69 0.64 0.06 0.49 0.01 7.81]\n",
      "Loss_Q: [0.9  1.34 0.61 0.63 0.06 0.47 0.01 0.   4.01] Loss_P: [2.41 1.85 1.73 0.72 0.64 0.08 0.48 0.01 7.92]\n",
      "Loss_Q: [0.86 1.41 0.66 0.66 0.08 0.47 0.01 0.   4.15] Loss_P: [2.32 1.87 1.79 0.7  0.68 0.08 0.5  0.01 7.94]\n",
      "Loss_Q: [0.86 1.42 0.65 0.68 0.05 0.5  0.01 0.   4.18] Loss_P: [2.34 1.86 1.76 0.77 0.68 0.07 0.5  0.01 7.99]\n",
      "Loss_Q: [0.83 1.33 0.6  0.63 0.05 0.5  0.01 0.   3.93] Loss_P: [2.35 1.83 1.78 0.68 0.64 0.05 0.51 0.02 7.86]\n",
      "Loss_Q: [0.86 1.41 0.57 0.63 0.05 0.51 0.01 0.   4.03] Loss_P: [2.32 1.81 1.8  0.7  0.6  0.07 0.51 0.01 7.82]\n",
      "Loss_Q: [0.84 1.41 0.64 0.7  0.07 0.5  0.02 0.   4.18] Loss_P: [2.28 1.84 1.8  0.7  0.65 0.07 0.5  0.01 7.85]\n",
      "Loss_Q: [0.94 1.44 0.65 0.69 0.05 0.51 0.01 0.   4.28] Loss_P: [2.27 1.87 1.88 0.71 0.66 0.06 0.51 0.02 7.96]\n",
      "Loss_Q: [0.94 1.45 0.68 0.68 0.07 0.5  0.01 0.   4.33] Loss_P: [2.33 1.9  1.87 0.66 0.65 0.07 0.5  0.01 7.98]\n",
      "Loss_Q: [0.88 1.38 0.63 0.65 0.06 0.5  0.01 0.   4.11] Loss_P: [2.36 1.84 1.92 0.73 0.7  0.07 0.5  0.03 8.15]\n",
      "Loss_Q: [0.85 1.4  0.68 0.67 0.05 0.5  0.02 0.   4.16] Loss_P: [2.34 1.87 1.89 0.74 0.7  0.07 0.5  0.01 8.13]\n",
      "Loss_Q: [0.94 1.46 0.76 0.69 0.08 0.5  0.01 0.   4.44] Loss_P: [2.29 1.95 1.88 0.72 0.7  0.05 0.49 0.01 8.09]\n",
      "Loss_Q: [0.94 1.45 0.67 0.66 0.07 0.5  0.01 0.   4.31] Loss_P: [2.3  1.93 1.92 0.75 0.71 0.05 0.49 0.01 8.15]\n",
      "Loss_Q: [0.86 1.42 0.69 0.69 0.07 0.48 0.03 0.   4.24] Loss_P: [2.33 1.87 1.8  0.73 0.69 0.07 0.5  0.01 7.99]\n",
      "Loss_Q: [0.87 1.39 0.66 0.69 0.05 0.5  0.01 0.   4.17] Loss_P: [2.3  1.86 1.89 0.8  0.68 0.07 0.5  0.01 8.11]\n",
      "Loss_Q: [0.9  1.41 0.77 0.72 0.06 0.5  0.02 0.   4.39] Loss_P: [2.29 1.89 1.9  0.84 0.73 0.07 0.51 0.01 8.25]\n",
      "Loss_Q: [0.88 1.35 0.76 0.72 0.06 0.5  0.01 0.   4.28] Loss_P: [2.31 1.89 1.84 0.81 0.72 0.05 0.51 0.01 8.14]\n",
      "Loss_Q: [0.92 1.39 0.78 0.72 0.07 0.52 0.01 0.   4.41] Loss_P: [2.35 1.79 1.83 0.87 0.7  0.06 0.51 0.01 8.13]\n",
      "Loss_Q: [0.87 1.38 0.71 0.71 0.08 0.52 0.01 0.   4.28] Loss_P: [2.29 1.9  1.76 0.85 0.74 0.06 0.51 0.01 8.12]\n",
      "Loss_Q: [0.88 1.43 0.71 0.75 0.08 0.5  0.02 0.   4.38] Loss_P: [2.27 1.84 1.81 0.84 0.73 0.05 0.5  0.01 8.05]\n",
      "Loss_Q: [0.94 1.4  0.76 0.72 0.07 0.5  0.01 0.   4.4 ] Loss_P: [2.36 1.82 1.81 0.88 0.69 0.07 0.49 0.01 8.13]\n",
      "Loss_Q: [0.88 1.42 0.76 0.76 0.06 0.5  0.01 0.   4.38] Loss_P: [2.32 1.83 1.82 0.85 0.76 0.06 0.5  0.02 8.17]\n",
      "Loss_Q: [0.93 1.4  0.78 0.79 0.09 0.49 0.02 0.   4.5 ] Loss_P: [2.26 1.86 1.86 0.9  0.78 0.07 0.51 0.01 8.25]\n",
      "Loss_Q: [0.91 1.4  0.82 0.76 0.1  0.51 0.02 0.   4.51] Loss_P: [2.32 1.8  1.84 0.86 0.75 0.06 0.5  0.01 8.15]\n",
      "Loss_Q: [0.93 1.36 0.72 0.72 0.07 0.51 0.01 0.   4.33] Loss_P: [2.27 1.86 1.77 0.87 0.75 0.09 0.5  0.02 8.13]\n",
      "Loss_Q: [0.93 1.31 0.85 0.77 0.05 0.49 0.01 0.   4.42] Loss_P: [2.33 1.83 1.78 0.88 0.73 0.08 0.49 0.01 8.12]\n",
      "Loss_Q: [0.96 1.35 0.77 0.73 0.07 0.48 0.01 0.   4.37] Loss_P: [2.32 1.79 1.77 0.89 0.78 0.07 0.49 0.01 8.11]\n",
      "Loss_Q: [0.82 1.36 0.78 0.74 0.08 0.48 0.01 0.   4.25] Loss_P: [2.24 1.88 1.81 0.86 0.74 0.08 0.49 0.01 8.11]\n",
      "Loss_Q: [0.9  1.31 0.72 0.71 0.06 0.5  0.01 0.   4.22] Loss_P: [2.38 1.79 1.85 0.8  0.78 0.08 0.49 0.02 8.18]\n",
      "Loss_Q: [0.88 1.36 0.73 0.69 0.05 0.49 0.01 0.   4.21] Loss_P: [2.34 1.81 1.84 0.81 0.75 0.07 0.47 0.01 8.1 ]\n",
      "Loss_Q: [0.87 1.35 0.68 0.68 0.05 0.48 0.01 0.   4.12] Loss_P: [2.36 1.76 1.81 0.77 0.69 0.07 0.49 0.01 7.96]\n",
      "Loss_Q: [0.8  1.33 0.71 0.67 0.06 0.47 0.01 0.   4.05] Loss_P: [2.3  1.88 1.84 0.84 0.76 0.08 0.47 0.03 8.2 ]\n",
      "Loss_Q: [0.84 1.29 0.7  0.73 0.06 0.49 0.01 0.   4.12] Loss_P: [2.23 1.85 1.82 0.84 0.73 0.05 0.48 0.01 8.  ]\n",
      "Loss_Q: [0.9  1.34 0.75 0.71 0.07 0.47 0.01 0.   4.26] Loss_P: [2.29 1.81 1.79 0.82 0.69 0.07 0.49 0.01 7.97]\n",
      "Loss_Q: [0.86 1.36 0.71 0.73 0.06 0.48 0.01 0.   4.2 ] Loss_P: [2.26 1.86 1.83 0.78 0.73 0.06 0.49 0.01 8.02]\n",
      "Loss_Q: [0.89 1.32 0.67 0.64 0.06 0.47 0.01 0.   4.06] Loss_P: [2.24 1.82 1.78 0.78 0.7  0.06 0.47 0.01 7.85]\n",
      "Loss_Q: [0.86 1.28 0.69 0.68 0.07 0.47 0.01 0.   4.07] Loss_P: [2.31 1.89 1.73 0.78 0.7  0.07 0.46 0.02 7.95]\n",
      "Loss_Q: [0.86 1.29 0.7  0.67 0.06 0.46 0.01 0.   4.05] Loss_P: [2.32 1.81 1.78 0.76 0.66 0.07 0.48 0.01 7.89]\n",
      "Loss_Q: [0.88 1.32 0.69 0.59 0.07 0.46 0.01 0.   4.03] Loss_P: [2.28 1.87 1.83 0.82 0.67 0.07 0.46 0.01 8.  ]\n",
      "Loss_Q: [0.84 1.33 0.65 0.59 0.07 0.46 0.02 0.   3.96] Loss_P: [2.36 1.79 1.76 0.8  0.7  0.06 0.48 0.01 7.97]\n",
      "Loss_Q: [0.82 1.29 0.71 0.61 0.05 0.45 0.01 0.   3.95] Loss_P: [2.31 1.79 1.79 0.81 0.67 0.07 0.46 0.01 7.91]\n",
      "Loss_Q: [0.85 1.33 0.74 0.66 0.05 0.46 0.01 0.   4.1 ] Loss_P: [2.28 1.85 1.73 0.81 0.71 0.05 0.47 0.01 7.9 ]\n",
      "Loss_Q: [0.88 1.3  0.69 0.67 0.06 0.46 0.01 0.   4.06] Loss_P: [2.32 1.87 1.73 0.81 0.71 0.07 0.46 0.01 7.99]\n",
      "Loss_Q: [0.79 1.34 0.74 0.66 0.05 0.47 0.02 0.   4.08] Loss_P: [2.33 1.76 1.76 0.85 0.68 0.06 0.47 0.01 7.91]\n",
      "Loss_Q: [0.86 1.38 0.73 0.67 0.07 0.45 0.02 0.   4.18] Loss_P: [2.36 1.79 1.78 0.88 0.7  0.1  0.48 0.01 8.11]\n",
      "Loss_Q: [0.81 1.35 0.69 0.67 0.06 0.46 0.01 0.   4.05] Loss_P: [2.3  1.85 1.76 0.84 0.75 0.07 0.47 0.01 8.03]\n",
      "Loss_Q: [0.85 1.31 0.7  0.67 0.06 0.45 0.02 0.   4.06] Loss_P: [2.3  1.77 1.73 0.8  0.64 0.04 0.47 0.01 7.77]\n",
      "Loss_Q: [0.76 1.33 0.68 0.65 0.05 0.46 0.01 0.   3.94] Loss_P: [2.35 1.68 1.76 0.73 0.67 0.07 0.48 0.01 7.74]\n",
      "Loss_Q: [0.82 1.32 0.63 0.68 0.06 0.47 0.01 0.   4.  ] Loss_P: [2.36 1.8  1.74 0.79 0.71 0.06 0.48 0.01 7.96]\n",
      "Loss_Q: [0.77 1.36 0.74 0.72 0.06 0.46 0.01 0.   4.13] Loss_P: [2.38 1.77 1.8  0.86 0.71 0.09 0.48 0.02 8.12]\n",
      "Loss_Q: [0.82 1.34 0.66 0.71 0.07 0.48 0.01 0.   4.09] Loss_P: [2.38 1.72 1.7  0.79 0.71 0.08 0.48 0.01 7.86]\n",
      "Loss_Q: [0.76 1.3  0.66 0.76 0.06 0.48 0.01 0.   4.04] Loss_P: [2.31 1.76 1.72 0.85 0.79 0.06 0.47 0.02 7.96]\n",
      "Loss_Q: [0.81 1.26 0.72 0.71 0.06 0.49 0.01 0.   4.05] Loss_P: [2.33 1.69 1.7  0.83 0.74 0.06 0.46 0.01 7.83]\n",
      "Loss_Q: [0.87 1.23 0.71 0.73 0.06 0.46 0.01 0.   4.07] Loss_P: [2.32 1.76 1.73 0.77 0.71 0.06 0.43 0.01 7.79]\n",
      "Loss_Q: [0.87 1.3  0.71 0.68 0.07 0.45 0.01 0.   4.08] Loss_P: [2.33 1.76 1.64 0.8  0.72 0.06 0.44 0.01 7.75]\n",
      "Loss_Q: [0.82 1.27 0.67 0.7  0.05 0.43 0.01 0.   3.95] Loss_P: [2.39 1.73 1.72 0.8  0.73 0.05 0.41 0.01 7.84]\n",
      "Loss_Q: [0.79 1.3  0.65 0.69 0.06 0.4  0.02 0.   3.91] Loss_P: [2.29 1.78 1.69 0.81 0.7  0.06 0.37 0.02 7.71]\n",
      "Loss_Q: [0.81 1.33 0.66 0.71 0.06 0.42 0.01 0.   4.01] Loss_P: [2.35 1.74 1.77 0.74 0.78 0.05 0.44 0.01 7.88]\n",
      "Loss_Q: [0.72 1.31 0.65 0.78 0.08 0.42 0.01 0.   3.97] Loss_P: [2.36 1.75 1.75 0.78 0.76 0.07 0.43 0.01 7.91]\n",
      "Loss_Q: [0.89 1.27 0.65 0.72 0.05 0.43 0.02 0.   4.03] Loss_P: [2.37 1.7  1.73 0.72 0.77 0.05 0.41 0.01 7.77]\n",
      "Loss_Q: [0.78 1.27 0.62 0.68 0.07 0.42 0.01 0.   3.84] Loss_P: [2.36 1.74 1.69 0.78 0.76 0.06 0.41 0.01 7.8 ]\n",
      "Loss_Q: [0.79 1.24 0.64 0.69 0.06 0.4  0.01 0.   3.82] Loss_P: [2.38 1.75 1.72 0.8  0.77 0.07 0.4  0.02 7.9 ]\n",
      "Loss_Q: [0.83 1.23 0.61 0.73 0.05 0.41 0.01 0.   3.87] Loss_P: [2.36 1.76 1.74 0.72 0.78 0.07 0.4  0.01 7.83]\n",
      "Loss_Q: [0.83 1.24 0.7  0.74 0.06 0.35 0.01 0.   3.94] Loss_P: [2.38 1.7  1.83 0.76 0.8  0.06 0.4  0.02 7.95]\n",
      "Loss_Q: [0.8  1.29 0.65 0.78 0.07 0.38 0.02 0.   3.99] Loss_P: [2.35 1.77 1.78 0.74 0.75 0.05 0.39 0.01 7.85]\n",
      "Loss_Q: [0.81 1.34 0.65 0.73 0.06 0.41 0.01 0.   4.01] Loss_P: [2.35 1.69 1.8  0.68 0.75 0.06 0.41 0.01 7.74]\n",
      "Loss_Q: [0.83 1.25 0.64 0.73 0.07 0.41 0.01 0.   3.94] Loss_P: [2.38 1.73 1.81 0.67 0.69 0.06 0.39 0.01 7.73]\n",
      "Loss_Q: [0.8  1.34 0.66 0.68 0.05 0.39 0.01 0.   3.94] Loss_P: [2.37 1.73 1.78 0.72 0.72 0.05 0.42 0.01 7.8 ]\n",
      "Loss_Q: [0.83 1.29 0.64 0.62 0.06 0.4  0.02 0.   3.86] Loss_P: [2.34 1.72 1.75 0.74 0.69 0.06 0.42 0.02 7.75]\n",
      "Loss_Q: [0.8  1.32 0.63 0.64 0.07 0.42 0.01 0.   3.89] Loss_P: [2.38 1.68 1.81 0.75 0.68 0.06 0.43 0.01 7.8 ]\n",
      "Loss_Q: [0.78 1.38 0.62 0.68 0.07 0.39 0.02 0.   3.94] Loss_P: [2.33 1.72 1.79 0.72 0.66 0.05 0.41 0.01 7.69]\n",
      "Loss_Q: [0.83 1.33 0.67 0.71 0.07 0.43 0.01 0.   4.05] Loss_P: [2.36 1.75 1.78 0.7  0.69 0.05 0.39 0.01 7.72]\n",
      "Loss_Q: [0.79 1.28 0.64 0.63 0.05 0.4  0.01 0.   3.81] Loss_P: [2.36 1.73 1.73 0.78 0.7  0.04 0.42 0.01 7.76]\n",
      "Loss_Q: [0.87 1.32 0.7  0.66 0.06 0.4  0.01 0.   4.02] Loss_P: [2.38 1.74 1.69 0.71 0.64 0.09 0.44 0.02 7.7 ]\n",
      "Loss_Q: [0.73 1.32 0.67 0.66 0.06 0.46 0.01 0.   3.91] Loss_P: [2.37 1.71 1.71 0.77 0.73 0.09 0.45 0.01 7.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.78 1.33 0.64 0.6  0.05 0.45 0.01 0.   3.87] Loss_P: [2.36 1.73 1.69 0.72 0.62 0.06 0.44 0.01 7.63]\n",
      "Loss_Q: [0.79 1.32 0.69 0.67 0.06 0.44 0.02 0.   3.99] Loss_P: [2.39 1.73 1.77 0.79 0.74 0.08 0.42 0.01 7.93]\n",
      "Loss_Q: [0.81 1.29 0.66 0.74 0.06 0.42 0.01 0.   4.  ] Loss_P: [2.35 1.69 1.72 0.74 0.7  0.06 0.44 0.02 7.73]\n",
      "Loss_Q: [0.82 1.31 0.69 0.66 0.07 0.43 0.01 0.   3.99] Loss_P: [2.36 1.69 1.69 0.78 0.75 0.07 0.44 0.02 7.79]\n",
      "Loss_Q: [0.74 1.28 0.67 0.66 0.06 0.45 0.01 0.   3.86] Loss_P: [2.34 1.71 1.73 0.77 0.7  0.08 0.44 0.01 7.76]\n",
      "Loss_Q: [0.74 1.24 0.63 0.64 0.04 0.43 0.01 0.   3.74] Loss_P: [2.36 1.73 1.71 0.72 0.69 0.05 0.45 0.01 7.73]\n",
      "Loss_Q: [0.8  1.28 0.72 0.65 0.06 0.46 0.01 0.   3.98] Loss_P: [2.34 1.74 1.74 0.79 0.69 0.06 0.45 0.01 7.82]\n",
      "Loss_Q: [0.85 1.24 0.68 0.66 0.06 0.44 0.02 0.   3.94] Loss_P: [2.37 1.7  1.77 0.74 0.62 0.06 0.45 0.02 7.73]\n",
      "Loss_Q: [0.82 1.32 0.69 0.63 0.04 0.46 0.01 0.   3.97] Loss_P: [2.3  1.77 1.79 0.76 0.66 0.05 0.44 0.01 7.79]\n",
      "Loss_Q: [0.77 1.28 0.73 0.66 0.07 0.44 0.01 0.   3.96] Loss_P: [2.36 1.64 1.73 0.79 0.65 0.06 0.43 0.01 7.68]\n",
      "Loss_Q: [0.83 1.33 0.68 0.7  0.06 0.47 0.01 0.   4.09] Loss_P: [2.36 1.71 1.76 0.73 0.63 0.07 0.45 0.02 7.72]\n",
      "Loss_Q: [0.86 1.24 0.74 0.66 0.08 0.43 0.01 0.   4.  ] Loss_P: [2.34 1.72 1.75 0.82 0.68 0.08 0.45 0.01 7.85]\n",
      "Loss_Q: [0.8  1.26 0.67 0.64 0.05 0.42 0.01 0.   3.85] Loss_P: [2.34 1.71 1.74 0.77 0.68 0.05 0.42 0.01 7.72]\n",
      "Loss_Q: [0.83 1.27 0.72 0.7  0.06 0.43 0.01 0.   4.02] Loss_P: [2.41 1.74 1.73 0.8  0.75 0.05 0.42 0.01 7.9 ]\n",
      "Loss_Q: [0.83 1.26 0.74 0.64 0.05 0.43 0.01 0.   3.95] Loss_P: [2.38 1.75 1.74 0.85 0.74 0.04 0.45 0.01 7.97]\n",
      "Loss_Q: [0.81 1.23 0.76 0.73 0.05 0.44 0.01 0.   4.04] Loss_P: [2.32 1.72 1.76 0.88 0.7  0.06 0.43 0.01 7.88]\n",
      "Loss_Q: [0.82 1.21 0.77 0.68 0.05 0.41 0.01 0.   3.96] Loss_P: [2.33 1.76 1.78 0.86 0.73 0.04 0.43 0.01 7.94]\n",
      "Loss_Q: [0.77 1.31 0.73 0.68 0.05 0.43 0.01 0.   3.98] Loss_P: [2.37 1.69 1.8  0.82 0.74 0.06 0.42 0.01 7.9 ]\n",
      "Loss_Q: [0.88 1.24 0.73 0.67 0.05 0.43 0.02 0.   4.02] Loss_P: [2.35 1.67 1.78 0.8  0.71 0.05 0.46 0.01 7.84]\n",
      "Loss_Q: [0.81 1.29 0.68 0.68 0.07 0.43 0.01 0.   3.98] Loss_P: [2.34 1.66 1.77 0.81 0.67 0.06 0.45 0.01 7.77]\n",
      "Loss_Q: [0.87 1.28 0.7  0.69 0.08 0.43 0.01 0.   4.04] Loss_P: [2.35 1.71 1.81 0.83 0.71 0.06 0.45 0.01 7.93]\n",
      "Loss_Q: [0.86 1.21 0.75 0.72 0.06 0.4  0.03 0.   4.03] Loss_P: [2.37 1.69 1.78 0.74 0.71 0.08 0.4  0.01 7.79]\n",
      "Loss_Q: [0.9  1.21 0.72 0.66 0.08 0.43 0.02 0.   4.02] Loss_P: [2.33 1.77 1.81 0.76 0.75 0.07 0.43 0.01 7.93]\n",
      "Loss_Q: [0.88 1.21 0.71 0.69 0.08 0.42 0.01 0.   4.  ] Loss_P: [2.38 1.75 1.83 0.75 0.73 0.05 0.41 0.02 7.93]\n",
      "Loss_Q: [0.86 1.28 0.68 0.69 0.08 0.39 0.02 0.   4.  ] Loss_P: [2.34 1.76 1.8  0.76 0.71 0.08 0.43 0.01 7.89]\n",
      "Loss_Q: [0.82 1.31 0.71 0.74 0.06 0.45 0.02 0.   4.1 ] Loss_P: [2.39 1.77 1.75 0.77 0.71 0.07 0.43 0.02 7.91]\n",
      "Loss_Q: [0.83 1.22 0.7  0.73 0.07 0.41 0.01 0.   3.97] Loss_P: [2.37 1.77 1.76 0.76 0.7  0.07 0.42 0.01 7.86]\n",
      "Loss_Q: [0.92 1.23 0.75 0.73 0.09 0.41 0.01 0.   4.13] Loss_P: [2.37 1.71 1.77 0.79 0.65 0.05 0.42 0.01 7.78]\n",
      "Loss_Q: [0.77 1.2  0.7  0.66 0.06 0.42 0.02 0.   3.82] Loss_P: [2.39 1.71 1.72 0.79 0.66 0.05 0.43 0.01 7.76]\n",
      "Loss_Q: [0.89 1.19 0.7  0.69 0.06 0.4  0.01 0.   3.93] Loss_P: [2.39 1.71 1.72 0.77 0.69 0.11 0.42 0.01 7.83]\n",
      "Loss_Q: [0.82 1.2  0.73 0.66 0.06 0.43 0.02 0.   3.92] Loss_P: [2.41 1.7  1.71 0.82 0.67 0.07 0.42 0.01 7.82]\n",
      "Loss_Q: [0.88 1.21 0.68 0.68 0.09 0.4  0.01 0.   3.96] Loss_P: [2.36 1.67 1.73 0.83 0.66 0.08 0.39 0.02 7.75]\n",
      "Loss_Q: [0.83 1.17 0.78 0.71 0.08 0.37 0.02 0.   3.96] Loss_P: [2.39 1.69 1.71 0.81 0.73 0.1  0.39 0.01 7.84]\n",
      "Loss_Q: [0.91 1.15 0.75 0.67 0.08 0.4  0.01 0.   3.97] Loss_P: [2.36 1.71 1.74 0.83 0.71 0.1  0.41 0.01 7.88]\n",
      "Loss_Q: [0.81 1.18 0.72 0.65 0.07 0.41 0.01 0.   3.85] Loss_P: [2.42 1.68 1.74 0.82 0.69 0.06 0.43 0.01 7.86]\n",
      "Loss_Q: [0.82 1.18 0.73 0.7  0.08 0.41 0.01 0.   3.93] Loss_P: [2.36 1.73 1.68 0.81 0.7  0.08 0.42 0.01 7.8 ]\n",
      "Loss_Q: [0.77 1.24 0.69 0.67 0.08 0.42 0.01 0.   3.87] Loss_P: [2.41 1.68 1.75 0.87 0.71 0.08 0.42 0.02 7.93]\n",
      "Loss_Q: [0.83 1.17 0.77 0.68 0.06 0.42 0.01 0.   3.94] Loss_P: [2.43 1.65 1.71 0.87 0.68 0.08 0.4  0.01 7.83]\n",
      "Loss_Q: [0.77 1.2  0.72 0.72 0.09 0.41 0.02 0.   3.92] Loss_P: [2.36 1.67 1.7  0.78 0.72 0.08 0.4  0.01 7.73]\n",
      "Loss_Q: [0.84 1.14 0.69 0.69 0.08 0.4  0.01 0.   3.85] Loss_P: [2.38 1.65 1.65 0.82 0.75 0.08 0.38 0.01 7.71]\n",
      "Loss_Q: [0.77 1.23 0.74 0.67 0.08 0.41 0.01 0.   3.9 ] Loss_P: [2.36 1.64 1.7  0.81 0.7  0.06 0.4  0.01 7.68]\n",
      "Loss_Q: [0.76 1.28 0.74 0.7  0.06 0.39 0.01 0.   3.93] Loss_P: [2.42 1.7  1.69 0.8  0.7  0.07 0.43 0.01 7.82]\n",
      "Loss_Q: [0.76 1.18 0.75 0.68 0.08 0.4  0.01 0.   3.85] Loss_P: [2.35 1.71 1.72 0.88 0.73 0.06 0.37 0.01 7.82]\n",
      "Loss_Q: [0.74 1.22 0.69 0.73 0.08 0.38 0.02 0.   3.86] Loss_P: [2.38 1.68 1.76 0.83 0.74 0.06 0.4  0.01 7.85]\n",
      "Loss_Q: [0.83 1.24 0.78 0.72 0.06 0.42 0.02 0.   4.07] Loss_P: [2.35 1.73 1.73 0.86 0.7  0.08 0.39 0.02 7.85]\n",
      "Loss_Q: [0.85 1.18 0.79 0.68 0.06 0.4  0.01 0.   3.97] Loss_P: [2.28 1.7  1.76 0.87 0.66 0.06 0.38 0.01 7.73]\n",
      "Loss_Q: [0.78 1.27 0.78 0.69 0.06 0.39 0.01 0.   3.98] Loss_P: [2.4  1.67 1.72 0.9  0.71 0.07 0.37 0.01 7.85]\n",
      "Loss_Q: [0.8  1.23 0.79 0.69 0.08 0.36 0.02 0.   3.98] Loss_P: [2.33 1.66 1.72 0.86 0.74 0.08 0.36 0.02 7.76]\n",
      "Loss_Q: [0.84 1.2  0.78 0.66 0.1  0.37 0.01 0.   3.96] Loss_P: [2.38 1.72 1.71 0.83 0.72 0.08 0.35 0.02 7.81]\n",
      "Loss_Q: [0.81 1.22 0.77 0.71 0.08 0.39 0.01 0.   4.  ] Loss_P: [2.36 1.68 1.74 0.87 0.72 0.09 0.37 0.01 7.83]\n",
      "Loss_Q: [0.83 1.21 0.75 0.74 0.09 0.37 0.01 0.   4.01] Loss_P: [2.3  1.75 1.73 0.88 0.78 0.07 0.37 0.02 7.9 ]\n",
      "Loss_Q: [0.83 1.18 0.77 0.73 0.09 0.41 0.01 0.   4.03] Loss_P: [2.32 1.75 1.75 0.87 0.78 0.1  0.38 0.02 7.96]\n",
      "Loss_Q: [0.83 1.19 0.77 0.75 0.08 0.36 0.01 0.   3.98] Loss_P: [2.37 1.61 1.7  0.83 0.75 0.08 0.37 0.01 7.72]\n",
      "Loss_Q: [0.83 1.17 0.75 0.76 0.07 0.39 0.01 0.   3.98] Loss_P: [2.32 1.71 1.68 0.88 0.82 0.07 0.4  0.01 7.89]\n",
      "Loss_Q: [0.8  1.19 0.76 0.75 0.08 0.36 0.01 0.   3.96] Loss_P: [2.29 1.74 1.73 0.85 0.72 0.07 0.4  0.01 7.81]\n",
      "Loss_Q: [0.81 1.22 0.82 0.74 0.07 0.39 0.01 0.   4.04] Loss_P: [2.32 1.71 1.69 0.86 0.8  0.07 0.38 0.01 7.85]\n",
      "Loss_Q: [0.87 1.24 0.77 0.75 0.07 0.39 0.01 0.   4.1 ] Loss_P: [2.29 1.76 1.74 0.85 0.77 0.08 0.37 0.01 7.86]\n",
      "Loss_Q: [0.84 1.25 0.74 0.75 0.06 0.41 0.01 0.   4.05] Loss_P: [2.29 1.78 1.77 0.85 0.78 0.07 0.41 0.01 7.95]\n",
      "Loss_Q: [0.78 1.27 0.77 0.76 0.06 0.39 0.01 0.   4.04] Loss_P: [2.39 1.66 1.81 0.82 0.73 0.06 0.41 0.01 7.9 ]\n",
      "Loss_Q: [0.79 1.24 0.73 0.75 0.07 0.4  0.01 0.   4.  ] Loss_P: [2.34 1.66 1.8  0.78 0.71 0.05 0.38 0.01 7.73]\n",
      "Loss_Q: [0.75 1.23 0.75 0.73 0.07 0.4  0.01 0.   3.93] Loss_P: [2.41 1.7  1.77 0.8  0.75 0.06 0.38 0.01 7.87]\n",
      "Loss_Q: [0.72 1.23 0.69 0.73 0.07 0.39 0.01 0.   3.84] Loss_P: [2.34 1.73 1.8  0.84 0.75 0.06 0.38 0.01 7.9 ]\n",
      "Loss_Q: [0.68 1.27 0.7  0.75 0.07 0.41 0.01 0.   3.88] Loss_P: [2.36 1.67 1.74 0.84 0.79 0.08 0.4  0.01 7.89]\n",
      "Loss_Q: [0.75 1.23 0.76 0.78 0.06 0.4  0.01 0.   3.99] Loss_P: [2.35 1.75 1.74 0.8  0.8  0.06 0.39 0.01 7.92]\n",
      "Loss_Q: [0.76 1.26 0.79 0.79 0.09 0.44 0.01 0.   4.13] Loss_P: [2.36 1.63 1.71 0.8  0.82 0.08 0.42 0.01 7.83]\n",
      "Loss_Q: [0.79 1.2  0.79 0.79 0.09 0.41 0.01 0.   4.08] Loss_P: [2.37 1.64 1.75 0.91 0.78 0.06 0.39 0.01 7.92]\n",
      "Loss_Q: [0.75 1.23 0.77 0.79 0.07 0.43 0.02 0.   4.06] Loss_P: [2.29 1.69 1.75 0.88 0.83 0.06 0.42 0.01 7.92]\n",
      "Loss_Q: [0.85 1.3  0.81 0.77 0.07 0.41 0.01 0.   4.22] Loss_P: [2.32 1.69 1.74 0.88 0.78 0.06 0.42 0.03 7.92]\n",
      "Loss_Q: [0.82 1.28 0.81 0.79 0.06 0.41 0.01 0.   4.18] Loss_P: [2.32 1.68 1.79 0.87 0.8  0.1  0.42 0.01 7.98]\n",
      "Loss_Q: [0.87 1.24 0.81 0.79 0.04 0.44 0.01 0.   4.2 ] Loss_P: [2.28 1.72 1.81 0.91 0.83 0.05 0.44 0.01 8.05]\n",
      "Loss_Q: [0.82 1.22 0.77 0.76 0.06 0.45 0.01 0.   4.09] Loss_P: [2.36 1.63 1.8  0.86 0.78 0.07 0.44 0.02 7.97]\n",
      "Loss_Q: [0.82 1.22 0.77 0.77 0.07 0.44 0.02 0.   4.11] Loss_P: [2.31 1.71 1.81 0.86 0.82 0.09 0.44 0.01 8.06]\n",
      "Loss_Q: [0.8  1.28 0.78 0.75 0.06 0.43 0.01 0.   4.11] Loss_P: [2.33 1.68 1.79 0.9  0.79 0.05 0.41 0.01 7.97]\n",
      "Loss_Q: [0.79 1.16 0.78 0.77 0.06 0.42 0.01 0.   3.99] Loss_P: [2.36 1.66 1.74 0.96 0.82 0.06 0.39 0.01 7.99]\n",
      "Loss_Q: [0.8  1.21 0.82 0.78 0.07 0.4  0.01 0.   4.09] Loss_P: [2.3  1.67 1.7  0.89 0.83 0.07 0.4  0.01 7.88]\n",
      "Loss_Q: [0.79 1.17 0.78 0.79 0.07 0.43 0.01 0.   4.04] Loss_P: [2.3  1.65 1.75 0.93 0.81 0.07 0.43 0.01 7.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.77 1.25 0.84 0.79 0.05 0.43 0.02 0.   4.16] Loss_P: [2.31 1.64 1.77 0.91 0.8  0.08 0.45 0.01 7.97]\n",
      "Loss_Q: [0.85 1.27 0.82 0.8  0.08 0.43 0.01 0.   4.27] Loss_P: [2.36 1.64 1.81 0.98 0.76 0.06 0.44 0.01 8.05]\n",
      "Loss_Q: [0.77 1.26 0.87 0.83 0.1  0.44 0.01 0.   4.28] Loss_P: [2.34 1.67 1.81 1.02 0.81 0.08 0.42 0.01 8.16]\n",
      "Loss_Q: [0.8  1.29 0.89 0.82 0.09 0.43 0.01 0.   4.33] Loss_P: [2.36 1.65 1.76 0.96 0.84 0.06 0.44 0.01 8.09]\n",
      "Loss_Q: [0.81 1.29 0.87 0.79 0.08 0.43 0.01 0.   4.3 ] Loss_P: [2.36 1.71 1.82 0.99 0.8  0.05 0.45 0.01 8.18]\n",
      "Loss_Q: [0.86 1.29 0.87 0.8  0.08 0.47 0.01 0.   4.37] Loss_P: [2.35 1.72 1.78 1.04 0.78 0.08 0.48 0.01 8.24]\n",
      "Loss_Q: [0.88 1.26 0.86 0.82 0.08 0.44 0.01 0.   4.35] Loss_P: [2.33 1.7  1.8  1.02 0.81 0.09 0.45 0.01 8.2 ]\n",
      "Loss_Q: [0.83 1.3  0.85 0.77 0.06 0.46 0.01 0.   4.29] Loss_P: [2.33 1.72 1.81 1.04 0.82 0.07 0.45 0.01 8.25]\n",
      "Loss_Q: [0.86 1.27 0.86 0.84 0.06 0.47 0.01 0.   4.38] Loss_P: [2.31 1.71 1.78 1.03 0.81 0.08 0.48 0.01 8.23]\n",
      "Loss_Q: [0.92 1.25 0.89 0.78 0.07 0.48 0.01 0.   4.4 ] Loss_P: [2.37 1.7  1.76 1.02 0.81 0.07 0.5  0.01 8.23]\n",
      "Loss_Q: [0.92 1.29 0.84 0.74 0.07 0.5  0.01 0.   4.37] Loss_P: [2.32 1.69 1.8  0.94 0.81 0.06 0.5  0.02 8.15]\n",
      "Loss_Q: [0.89 1.2  0.82 0.79 0.07 0.5  0.01 0.   4.28] Loss_P: [2.36 1.75 1.8  1.   0.8  0.06 0.51 0.01 8.29]\n",
      "Loss_Q: [0.86 1.26 0.81 0.81 0.06 0.52 0.01 0.   4.32] Loss_P: [2.33 1.74 1.77 0.96 0.81 0.06 0.5  0.02 8.18]\n",
      "Loss_Q: [0.84 1.25 0.85 0.81 0.06 0.5  0.01 0.   4.33] Loss_P: [2.34 1.72 1.81 0.98 0.84 0.05 0.51 0.01 8.25]\n",
      "Loss_Q: [0.83 1.26 0.81 0.75 0.06 0.51 0.01 0.   4.24] Loss_P: [2.35 1.71 1.75 0.97 0.78 0.05 0.51 0.01 8.12]\n",
      "Loss_Q: [0.83 1.2  0.82 0.78 0.06 0.52 0.01 0.   4.21] Loss_P: [2.36 1.74 1.82 0.97 0.8  0.07 0.51 0.01 8.29]\n",
      "Loss_Q: [0.9  1.3  0.88 0.81 0.06 0.51 0.01 0.   4.47] Loss_P: [2.33 1.68 1.74 1.01 0.81 0.07 0.5  0.01 8.15]\n",
      "Loss_Q: [0.83 1.24 0.81 0.75 0.05 0.5  0.01 0.   4.19] Loss_P: [2.39 1.69 1.8  1.02 0.81 0.06 0.5  0.02 8.28]\n",
      "Loss_Q: [0.88 1.24 0.86 0.8  0.06 0.5  0.01 0.   4.36] Loss_P: [2.36 1.71 1.79 0.98 0.84 0.05 0.51 0.01 8.26]\n",
      "Loss_Q: [0.79 1.31 0.8  0.76 0.06 0.5  0.03 0.   4.25] Loss_P: [2.37 1.73 1.72 0.98 0.85 0.06 0.51 0.01 8.23]\n",
      "Loss_Q: [0.84 1.31 0.83 0.8  0.06 0.52 0.01 0.   4.38] Loss_P: [2.34 1.72 1.76 1.01 0.82 0.08 0.51 0.01 8.24]\n",
      "Loss_Q: [0.84 1.24 0.88 0.76 0.06 0.51 0.02 0.   4.3 ] Loss_P: [2.31 1.74 1.77 0.98 0.79 0.05 0.51 0.01 8.15]\n",
      "Loss_Q: [0.86 1.31 0.85 0.76 0.09 0.51 0.01 0.   4.37] Loss_P: [2.32 1.71 1.8  1.07 0.8  0.07 0.51 0.02 8.3 ]\n",
      "Loss_Q: [0.87 1.27 0.81 0.79 0.07 0.5  0.01 0.   4.31] Loss_P: [2.32 1.74 1.76 0.96 0.82 0.06 0.5  0.01 8.16]\n",
      "Loss_Q: [0.85 1.27 0.86 0.8  0.07 0.5  0.02 0.   4.38] Loss_P: [2.22 1.72 1.82 0.97 0.85 0.06 0.5  0.03 8.17]\n",
      "Loss_Q: [0.9  1.23 0.84 0.79 0.06 0.5  0.01 0.   4.34] Loss_P: [2.31 1.74 1.81 0.95 0.81 0.05 0.5  0.02 8.19]\n",
      "Loss_Q: [0.87 1.25 0.85 0.76 0.05 0.5  0.02 0.   4.3 ] Loss_P: [2.34 1.74 1.79 0.98 0.82 0.08 0.5  0.02 8.25]\n",
      "Loss_Q: [0.85 1.25 0.84 0.74 0.06 0.49 0.01 0.   4.24] Loss_P: [2.35 1.7  1.74 0.98 0.83 0.07 0.5  0.01 8.18]\n",
      "Loss_Q: [0.89 1.29 0.84 0.72 0.05 0.5  0.01 0.   4.3 ] Loss_P: [2.31 1.74 1.77 1.02 0.82 0.08 0.5  0.01 8.24]\n",
      "Loss_Q: [0.89 1.21 0.89 0.79 0.06 0.5  0.01 0.   4.35] Loss_P: [2.33 1.73 1.76 1.01 0.76 0.06 0.5  0.01 8.16]\n",
      "Loss_Q: [0.88 1.21 0.83 0.78 0.07 0.5  0.02 0.   4.29] Loss_P: [2.31 1.73 1.74 1.05 0.84 0.07 0.51 0.01 8.25]\n",
      "Loss_Q: [0.93 1.2  0.9  0.77 0.05 0.49 0.01 0.   4.35] Loss_P: [2.29 1.76 1.81 1.03 0.86 0.07 0.5  0.02 8.33]\n",
      "Loss_Q: [0.86 1.27 0.88 0.83 0.05 0.51 0.01 0.   4.42] Loss_P: [2.39 1.76 1.8  1.02 0.81 0.08 0.5  0.01 8.37]\n",
      "Loss_Q: [0.85 1.27 0.82 0.78 0.08 0.5  0.02 0.   4.32] Loss_P: [2.32 1.67 1.74 1.02 0.82 0.09 0.5  0.01 8.17]\n",
      "Loss_Q: [0.88 1.27 0.81 0.76 0.06 0.5  0.01 0.   4.29] Loss_P: [2.27 1.69 1.76 0.99 0.79 0.06 0.5  0.01 8.06]\n",
      "Loss_Q: [0.86 1.24 0.82 0.77 0.07 0.51 0.01 0.   4.27] Loss_P: [2.34 1.71 1.75 0.98 0.78 0.07 0.51 0.01 8.14]\n",
      "Loss_Q: [0.86 1.3  0.78 0.77 0.05 0.51 0.01 0.   4.28] Loss_P: [2.34 1.67 1.78 0.96 0.77 0.07 0.51 0.01 8.11]\n",
      "Loss_Q: [0.94 1.27 0.82 0.76 0.04 0.51 0.01 0.   4.35] Loss_P: [2.3  1.76 1.82 0.94 0.83 0.06 0.51 0.01 8.23]\n",
      "Loss_Q: [0.87 1.32 0.79 0.78 0.09 0.51 0.03 0.   4.38] Loss_P: [2.34 1.68 1.79 0.97 0.83 0.09 0.5  0.01 8.21]\n",
      "Loss_Q: [0.87 1.29 0.81 0.8  0.05 0.51 0.01 0.   4.35] Loss_P: [2.33 1.69 1.78 0.93 0.78 0.08 0.51 0.01 8.12]\n",
      "Loss_Q: [0.92 1.28 0.81 0.8  0.07 0.5  0.01 0.   4.39] Loss_P: [2.33 1.67 1.77 0.97 0.84 0.07 0.5  0.02 8.16]\n",
      "Loss_Q: [0.81 1.25 0.82 0.83 0.07 0.5  0.01 0.   4.3 ] Loss_P: [2.37 1.63 1.73 0.96 0.79 0.06 0.51 0.01 8.06]\n",
      "Loss_Q: [0.89 1.25 0.81 0.79 0.07 0.51 0.02 0.   4.33] Loss_P: [2.38 1.7  1.78 0.97 0.82 0.08 0.51 0.03 8.27]\n",
      "Loss_Q: [0.85 1.29 0.77 0.78 0.08 0.5  0.01 0.   4.29] Loss_P: [2.32 1.7  1.8  0.99 0.83 0.07 0.5  0.02 8.23]\n",
      "Loss_Q: [0.86 1.33 0.79 0.75 0.06 0.5  0.01 0.   4.29] Loss_P: [2.35 1.69 1.85 0.99 0.84 0.06 0.51 0.01 8.29]\n",
      "Loss_Q: [0.86 1.27 0.84 0.83 0.06 0.51 0.01 0.   4.38] Loss_P: [2.36 1.71 1.8  0.99 0.8  0.07 0.5  0.01 8.25]\n",
      "Loss_Q: [0.84 1.32 0.76 0.8  0.06 0.51 0.01 0.   4.3 ] Loss_P: [2.34 1.69 1.81 0.93 0.81 0.06 0.51 0.01 8.15]\n",
      "Loss_Q: [0.91 1.32 0.8  0.78 0.07 0.51 0.02 0.   4.41] Loss_P: [2.36 1.73 1.82 0.94 0.81 0.07 0.51 0.01 8.25]\n",
      "Loss_Q: [0.83 1.33 0.78 0.75 0.05 0.5  0.01 0.   4.27] Loss_P: [2.39 1.68 1.8  0.93 0.81 0.05 0.51 0.01 8.18]\n",
      "Loss_Q: [0.9  1.33 0.78 0.8  0.06 0.51 0.01 0.   4.4 ] Loss_P: [2.36 1.7  1.82 0.9  0.78 0.06 0.51 0.01 8.14]\n",
      "Loss_Q: [0.88 1.34 0.76 0.83 0.07 0.5  0.01 0.   4.39] Loss_P: [2.32 1.71 1.8  0.9  0.83 0.07 0.5  0.01 8.15]\n",
      "Loss_Q: [0.89 1.28 0.76 0.8  0.06 0.51 0.01 0.   4.3 ] Loss_P: [2.37 1.71 1.79 0.94 0.85 0.06 0.5  0.01 8.23]\n",
      "Loss_Q: [0.83 1.33 0.71 0.79 0.08 0.5  0.01 0.   4.24] Loss_P: [2.35 1.66 1.78 0.9  0.82 0.06 0.51 0.01 8.1 ]\n",
      "Loss_Q: [0.86 1.31 0.73 0.8  0.06 0.51 0.01 0.   4.29] Loss_P: [2.3  1.74 1.81 0.9  0.78 0.06 0.51 0.01 8.11]\n",
      "Loss_Q: [0.91 1.28 0.74 0.76 0.06 0.5  0.01 0.   4.27] Loss_P: [2.31 1.7  1.75 0.9  0.8  0.05 0.52 0.01 8.05]\n",
      "Loss_Q: [0.79 1.24 0.7  0.87 0.08 0.51 0.01 0.   4.2 ] Loss_P: [2.37 1.71 1.75 0.92 0.8  0.05 0.5  0.03 8.12]\n",
      "Loss_Q: [0.82 1.3  0.76 0.78 0.07 0.51 0.01 0.   4.25] Loss_P: [2.36 1.66 1.79 0.91 0.87 0.07 0.51 0.01 8.18]\n",
      "Loss_Q: [0.87 1.36 0.74 0.77 0.07 0.51 0.01 0.   4.34] Loss_P: [2.34 1.69 1.84 0.92 0.85 0.07 0.51 0.01 8.24]\n",
      "Loss_Q: [0.79 1.3  0.7  0.75 0.06 0.5  0.01 0.   4.12] Loss_P: [2.35 1.69 1.72 0.87 0.85 0.07 0.51 0.02 8.08]\n",
      "Loss_Q: [0.92 1.31 0.73 0.83 0.08 0.5  0.01 0.   4.38] Loss_P: [2.37 1.65 1.76 0.86 0.79 0.09 0.5  0.01 8.04]\n",
      "Loss_Q: [0.77 1.3  0.66 0.79 0.08 0.49 0.02 0.   4.12] Loss_P: [2.37 1.65 1.74 0.86 0.87 0.07 0.5  0.01 8.06]\n",
      "Loss_Q: [0.89 1.27 0.66 0.81 0.08 0.49 0.01 0.   4.21] Loss_P: [2.4  1.61 1.72 0.77 0.87 0.07 0.5  0.01 7.96]\n",
      "Loss_Q: [0.77 1.22 0.69 0.84 0.07 0.5  0.02 0.   4.1 ] Loss_P: [2.36 1.65 1.73 0.86 0.84 0.06 0.49 0.01 8.01]\n",
      "Loss_Q: [0.79 1.24 0.68 0.87 0.06 0.49 0.01 0.   4.14] Loss_P: [2.38 1.66 1.68 0.9  0.85 0.08 0.51 0.01 8.07]\n",
      "Loss_Q: [0.87 1.33 0.76 0.89 0.08 0.48 0.02 0.   4.43] Loss_P: [2.33 1.78 1.72 0.88 0.9  0.09 0.49 0.01 8.19]\n",
      "Loss_Q: [0.83 1.25 0.71 0.85 0.09 0.48 0.01 0.   4.23] Loss_P: [2.34 1.73 1.72 0.83 0.89 0.11 0.49 0.01 8.12]\n",
      "Loss_Q: [0.86 1.3  0.68 0.82 0.09 0.47 0.01 0.   4.24] Loss_P: [2.38 1.73 1.78 0.91 0.81 0.09 0.48 0.01 8.19]\n",
      "Loss_Q: [0.84 1.26 0.74 0.88 0.08 0.49 0.01 0.   4.3 ] Loss_P: [2.34 1.7  1.76 0.94 0.86 0.09 0.48 0.01 8.18]\n",
      "Loss_Q: [0.83 1.3  0.75 0.85 0.08 0.47 0.01 0.   4.29] Loss_P: [2.36 1.68 1.72 0.89 0.9  0.11 0.45 0.01 8.1 ]\n",
      "Loss_Q: [0.88 1.3  0.74 0.9  0.07 0.46 0.01 0.   4.37] Loss_P: [2.41 1.69 1.73 0.91 0.9  0.09 0.46 0.01 8.19]\n",
      "Loss_Q: [0.9  1.33 0.74 0.86 0.08 0.46 0.01 0.   4.38] Loss_P: [2.34 1.74 1.74 0.91 0.92 0.13 0.47 0.01 8.25]\n",
      "Loss_Q: [0.82 1.26 0.73 0.83 0.07 0.45 0.01 0.   4.17] Loss_P: [2.38 1.73 1.76 0.92 0.94 0.07 0.46 0.03 8.29]\n",
      "Loss_Q: [0.88 1.25 0.75 0.85 0.07 0.47 0.01 0.   4.29] Loss_P: [2.37 1.72 1.73 0.88 0.85 0.11 0.49 0.01 8.16]\n",
      "Loss_Q: [0.88 1.33 0.81 0.86 0.09 0.44 0.01 0.   4.42] Loss_P: [2.28 1.77 1.75 0.94 0.86 0.07 0.47 0.02 8.16]\n",
      "Loss_Q: [0.92 1.24 0.75 0.84 0.1  0.46 0.02 0.   4.33] Loss_P: [2.34 1.82 1.82 0.94 0.87 0.09 0.45 0.01 8.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.88 1.3  0.74 0.87 0.1  0.46 0.01 0.   4.35] Loss_P: [2.38 1.74 1.82 0.96 0.85 0.09 0.47 0.01 8.32]\n",
      "Loss_Q: [0.89 1.3  0.79 0.85 0.11 0.49 0.02 0.   4.44] Loss_P: [2.34 1.8  1.75 0.91 0.9  0.12 0.47 0.02 8.3 ]\n",
      "Loss_Q: [0.88 1.33 0.79 0.84 0.1  0.48 0.02 0.   4.44] Loss_P: [2.36 1.79 1.76 0.93 0.83 0.09 0.5  0.01 8.27]\n",
      "Loss_Q: [0.92 1.27 0.76 0.84 0.08 0.48 0.02 0.   4.37] Loss_P: [2.41 1.77 1.76 0.91 0.86 0.09 0.48 0.01 8.29]\n",
      "Loss_Q: [0.86 1.25 0.74 0.87 0.1  0.49 0.01 0.   4.32] Loss_P: [2.34 1.77 1.81 0.91 0.89 0.09 0.48 0.01 8.3 ]\n",
      "Loss_Q: [0.9  1.25 0.71 0.85 0.1  0.48 0.04 0.   4.34] Loss_P: [2.34 1.76 1.73 0.89 0.88 0.09 0.48 0.02 8.18]\n",
      "Loss_Q: [0.84 1.31 0.72 0.86 0.1  0.5  0.01 0.   4.34] Loss_P: [2.41 1.76 1.71 0.92 0.87 0.11 0.5  0.01 8.29]\n",
      "Loss_Q: [0.92 1.26 0.74 0.86 0.09 0.48 0.02 0.   4.37] Loss_P: [2.37 1.72 1.71 0.88 0.84 0.1  0.5  0.01 8.13]\n",
      "Loss_Q: [0.88 1.3  0.78 0.86 0.08 0.49 0.01 0.   4.4 ] Loss_P: [2.38 1.68 1.71 0.91 0.87 0.09 0.48 0.01 8.12]\n",
      "Loss_Q: [0.86 1.27 0.76 0.84 0.11 0.47 0.01 0.   4.32] Loss_P: [2.4  1.75 1.75 0.93 0.87 0.09 0.5  0.01 8.3 ]\n",
      "Loss_Q: [0.88 1.23 0.73 0.79 0.11 0.46 0.02 0.   4.22] Loss_P: [2.34 1.72 1.79 0.94 0.83 0.1  0.47 0.01 8.2 ]\n",
      "Loss_Q: [0.86 1.26 0.75 0.81 0.11 0.44 0.01 0.   4.24] Loss_P: [2.37 1.68 1.75 0.86 0.84 0.13 0.45 0.01 8.09]\n",
      "Loss_Q: [0.84 1.28 0.78 0.83 0.11 0.45 0.01 0.   4.29] Loss_P: [2.35 1.76 1.7  0.9  0.82 0.1  0.45 0.01 8.08]\n",
      "Loss_Q: [0.83 1.17 0.71 0.84 0.12 0.44 0.02 0.   4.14] Loss_P: [2.37 1.72 1.74 0.95 0.85 0.13 0.43 0.01 8.2 ]\n",
      "Loss_Q: [0.88 1.27 0.78 0.76 0.09 0.43 0.01 0.   4.21] Loss_P: [2.31 1.76 1.82 0.94 0.79 0.12 0.45 0.01 8.2 ]\n",
      "Loss_Q: [0.91 1.25 0.74 0.78 0.11 0.43 0.01 0.   4.23] Loss_P: [2.35 1.71 1.8  0.98 0.84 0.1  0.45 0.01 8.24]\n",
      "Loss_Q: [0.89 1.24 0.79 0.77 0.12 0.43 0.01 0.   4.26] Loss_P: [2.37 1.71 1.68 0.91 0.81 0.13 0.42 0.01 8.04]\n",
      "Loss_Q: [0.85 1.22 0.74 0.75 0.12 0.44 0.01 0.   4.14] Loss_P: [2.41 1.67 1.73 0.91 0.76 0.1  0.44 0.01 8.02]\n",
      "Loss_Q: [0.87 1.21 0.74 0.76 0.1  0.42 0.01 0.   4.11] Loss_P: [2.39 1.68 1.77 0.89 0.78 0.1  0.44 0.01 8.05]\n",
      "Loss_Q: [0.9  1.27 0.79 0.79 0.1  0.44 0.01 0.   4.31] Loss_P: [2.4  1.71 1.74 0.86 0.8  0.1  0.43 0.02 8.05]\n",
      "Loss_Q: [0.78 1.27 0.72 0.83 0.08 0.42 0.02 0.   4.12] Loss_P: [2.43 1.68 1.74 0.89 0.82 0.1  0.42 0.01 8.09]\n",
      "Loss_Q: [0.81 1.29 0.73 0.83 0.1  0.42 0.01 0.   4.19] Loss_P: [2.36 1.7  1.79 0.87 0.81 0.12 0.41 0.01 8.06]\n",
      "Loss_Q: [0.84 1.29 0.78 0.85 0.1  0.38 0.   0.   4.26] Loss_P: [2.4  1.71 1.7  0.89 0.85 0.1  0.41 0.02 8.08]\n",
      "Loss_Q: [0.87 1.24 0.71 0.79 0.09 0.39 0.02 0.   4.11] Loss_P: [2.36 1.72 1.78 0.95 0.84 0.1  0.41 0.02 8.17]\n",
      "Loss_Q: [0.77 1.37 0.78 0.77 0.08 0.38 0.01 0.   4.16] Loss_P: [2.34 1.81 1.79 0.9  0.83 0.08 0.39 0.01 8.14]\n",
      "Loss_Q: [0.88 1.25 0.74 0.79 0.1  0.38 0.01 0.   4.15] Loss_P: [2.34 1.76 1.81 0.85 0.82 0.08 0.38 0.02 8.06]\n",
      "Loss_Q: [0.85 1.29 0.77 0.84 0.11 0.38 0.01 0.   4.25] Loss_P: [2.35 1.78 1.77 0.89 0.86 0.09 0.37 0.01 8.13]\n",
      "Loss_Q: [0.89 1.19 0.76 0.84 0.11 0.39 0.02 0.   4.2 ] Loss_P: [2.33 1.72 1.73 0.92 0.91 0.1  0.38 0.01 8.1 ]\n",
      "Loss_Q: [0.89 1.26 0.72 0.83 0.11 0.41 0.02 0.   4.22] Loss_P: [2.37 1.7  1.73 0.88 0.85 0.15 0.4  0.02 8.1 ]\n",
      "Loss_Q: [0.91 1.26 0.73 0.89 0.09 0.41 0.01 0.   4.3 ] Loss_P: [2.33 1.78 1.79 0.92 0.87 0.1  0.38 0.01 8.18]\n",
      "Loss_Q: [0.95 1.33 0.75 0.86 0.09 0.42 0.02 0.   4.41] Loss_P: [2.37 1.7  1.76 0.88 0.89 0.11 0.45 0.01 8.17]\n",
      "Loss_Q: [0.88 1.29 0.69 0.88 0.08 0.43 0.01 0.   4.27] Loss_P: [2.34 1.75 1.77 0.88 0.89 0.1  0.43 0.01 8.18]\n",
      "Loss_Q: [0.83 1.26 0.73 0.9  0.12 0.45 0.01 0.   4.3 ] Loss_P: [2.35 1.72 1.78 0.88 0.86 0.13 0.44 0.01 8.18]\n",
      "Loss_Q: [0.82 1.28 0.77 0.85 0.09 0.42 0.01 0.   4.23] Loss_P: [2.41 1.7  1.74 0.86 0.82 0.09 0.41 0.02 8.05]\n",
      "Loss_Q: [0.9  1.31 0.75 0.83 0.1  0.43 0.01 0.   4.31] Loss_P: [2.34 1.77 1.74 0.92 0.81 0.09 0.42 0.01 8.1 ]\n",
      "Loss_Q: [0.85 1.24 0.72 0.84 0.1  0.41 0.01 0.   4.15] Loss_P: [2.29 1.83 1.75 0.88 0.85 0.1  0.4  0.01 8.11]\n",
      "Loss_Q: [0.87 1.29 0.76 0.85 0.08 0.41 0.01 0.   4.26] Loss_P: [2.32 1.81 1.71 0.88 0.84 0.07 0.39 0.01 8.04]\n",
      "Loss_Q: [0.88 1.32 0.73 0.82 0.09 0.38 0.01 0.   4.24] Loss_P: [2.28 1.84 1.79 0.86 0.87 0.1  0.38 0.01 8.13]\n",
      "Loss_Q: [0.78 1.21 0.73 0.84 0.08 0.38 0.02 0.   4.05] Loss_P: [2.35 1.78 1.74 0.87 0.84 0.07 0.38 0.02 8.06]\n",
      "Loss_Q: [0.91 1.25 0.74 0.87 0.1  0.37 0.01 0.   4.26] Loss_P: [2.3  1.79 1.78 0.92 0.89 0.1  0.4  0.01 8.18]\n",
      "Loss_Q: [0.9  1.33 0.78 0.9  0.1  0.41 0.01 0.   4.42] Loss_P: [2.3  1.75 1.74 0.92 0.9  0.11 0.39 0.03 8.14]\n",
      "Loss_Q: [0.84 1.32 0.73 0.88 0.1  0.42 0.01 0.   4.3 ] Loss_P: [2.31 1.8  1.77 0.93 0.9  0.08 0.4  0.01 8.2 ]\n",
      "Loss_Q: [0.87 1.33 0.82 0.9  0.13 0.43 0.01 0.   4.48] Loss_P: [2.35 1.78 1.75 0.92 0.89 0.11 0.4  0.01 8.2 ]\n",
      "Loss_Q: [0.89 1.31 0.78 0.92 0.08 0.43 0.01 0.   4.41] Loss_P: [2.33 1.74 1.85 0.91 0.95 0.1  0.45 0.01 8.34]\n",
      "Loss_Q: [0.83 1.38 0.75 0.89 0.1  0.45 0.01 0.   4.41] Loss_P: [2.35 1.75 1.76 0.95 0.91 0.11 0.44 0.01 8.28]\n",
      "Loss_Q: [0.87 1.3  0.76 0.88 0.11 0.44 0.01 0.   4.37] Loss_P: [2.33 1.83 1.67 0.94 0.9  0.09 0.44 0.01 8.21]\n",
      "Loss_Q: [0.82 1.28 0.79 0.94 0.08 0.46 0.01 0.   4.37] Loss_P: [2.33 1.73 1.8  0.97 0.93 0.08 0.46 0.01 8.31]\n",
      "Loss_Q: [0.83 1.27 0.78 0.9  0.11 0.46 0.02 0.   4.38] Loss_P: [2.37 1.75 1.75 0.92 0.95 0.11 0.47 0.01 8.33]\n",
      "Loss_Q: [0.89 1.3  0.75 0.92 0.1  0.43 0.02 0.   4.41] Loss_P: [2.35 1.7  1.68 0.94 0.95 0.08 0.43 0.01 8.15]\n",
      "Loss_Q: [0.91 1.2  0.77 0.91 0.11 0.45 0.01 0.   4.36] Loss_P: [2.32 1.66 1.71 0.91 0.92 0.11 0.46 0.01 8.1 ]\n",
      "Loss_Q: [0.83 1.17 0.75 0.96 0.1  0.47 0.01 0.   4.3 ] Loss_P: [2.32 1.77 1.66 0.92 0.92 0.08 0.47 0.01 8.15]\n",
      "Loss_Q: [0.88 1.27 0.74 0.93 0.11 0.46 0.01 0.   4.39] Loss_P: [2.33 1.74 1.68 0.94 0.93 0.1  0.45 0.01 8.18]\n",
      "Loss_Q: [0.82 1.2  0.75 0.94 0.13 0.49 0.01 0.   4.34] Loss_P: [2.32 1.72 1.62 0.89 0.95 0.1  0.48 0.01 8.08]\n",
      "Loss_Q: [0.91 1.19 0.76 0.92 0.08 0.48 0.01 0.   4.35] Loss_P: [2.3  1.8  1.7  0.9  0.94 0.08 0.47 0.01 8.2 ]\n",
      "Loss_Q: [0.74 1.19 0.8  0.98 0.09 0.48 0.01 0.   4.28] Loss_P: [2.37 1.74 1.65 0.98 1.03 0.11 0.47 0.01 8.36]\n",
      "Loss_Q: [0.83 1.17 0.77 0.98 0.09 0.47 0.03 0.   4.33] Loss_P: [2.33 1.68 1.69 0.97 0.97 0.08 0.46 0.01 8.19]\n",
      "Loss_Q: [0.9  1.24 0.76 0.95 0.1  0.45 0.01 0.   4.4 ] Loss_P: [2.35 1.76 1.7  0.94 0.98 0.09 0.46 0.01 8.3 ]\n",
      "Loss_Q: [0.81 1.17 0.8  0.95 0.07 0.49 0.01 0.   4.3 ] Loss_P: [2.32 1.74 1.73 0.95 0.99 0.09 0.46 0.01 8.29]\n",
      "Loss_Q: [0.82 1.24 0.78 0.96 0.1  0.48 0.01 0.   4.37] Loss_P: [2.34 1.72 1.74 0.94 0.98 0.08 0.47 0.01 8.28]\n",
      "Loss_Q: [0.82 1.22 0.79 0.93 0.09 0.47 0.01 0.   4.34] Loss_P: [2.34 1.77 1.69 0.94 0.99 0.09 0.46 0.01 8.28]\n",
      "Loss_Q: [0.83 1.18 0.82 0.97 0.09 0.46 0.01 0.   4.36] Loss_P: [2.36 1.75 1.71 0.92 1.   0.12 0.46 0.01 8.34]\n",
      "Loss_Q: [0.86 1.27 0.79 0.94 0.08 0.45 0.02 0.   4.41] Loss_P: [2.31 1.79 1.67 0.94 1.   0.09 0.47 0.01 8.26]\n",
      "Loss_Q: [0.85 1.24 0.8  1.   0.09 0.47 0.01 0.   4.46] Loss_P: [2.35 1.81 1.7  0.88 0.99 0.07 0.45 0.01 8.26]\n",
      "Loss_Q: [0.84 1.23 0.8  0.98 0.09 0.45 0.01 0.   4.4 ] Loss_P: [2.39 1.7  1.71 0.98 1.   0.07 0.46 0.01 8.32]\n",
      "Loss_Q: [0.87 1.21 0.79 1.   0.09 0.46 0.01 0.   4.44] Loss_P: [2.35 1.75 1.67 1.01 0.99 0.11 0.47 0.01 8.36]\n",
      "Loss_Q: [0.86 1.17 0.8  0.92 0.09 0.46 0.01 0.   4.31] Loss_P: [2.37 1.75 1.61 0.95 0.99 0.1  0.45 0.01 8.24]\n",
      "Loss_Q: [0.85 1.29 0.86 0.96 0.07 0.45 0.02 0.   4.51] Loss_P: [2.33 1.8  1.66 0.99 0.99 0.08 0.46 0.01 8.32]\n",
      "Loss_Q: [0.88 1.25 0.8  0.91 0.09 0.46 0.01 0.   4.41] Loss_P: [2.39 1.77 1.7  0.98 0.99 0.08 0.46 0.01 8.39]\n",
      "Loss_Q: [0.86 1.19 0.85 0.97 0.08 0.47 0.01 0.   4.44] Loss_P: [2.35 1.72 1.67 0.94 0.98 0.06 0.47 0.01 8.2 ]\n",
      "Loss_Q: [0.87 1.28 0.83 0.93 0.12 0.47 0.01 0.   4.51] Loss_P: [2.32 1.8  1.74 1.01 0.96 0.08 0.48 0.01 8.39]\n",
      "Loss_Q: [0.9  1.2  0.81 0.92 0.1  0.45 0.01 0.   4.4 ] Loss_P: [2.31 1.82 1.59 0.95 0.95 0.08 0.47 0.01 8.17]\n",
      "Loss_Q: [0.9  1.22 0.84 0.89 0.07 0.48 0.01 0.   4.42] Loss_P: [2.25 1.84 1.65 0.99 0.98 0.09 0.46 0.02 8.28]\n",
      "Loss_Q: [0.86 1.25 0.79 0.91 0.09 0.48 0.01 0.   4.39] Loss_P: [2.33 1.78 1.7  0.94 0.97 0.08 0.47 0.01 8.28]\n",
      "Loss_Q: [0.89 1.19 0.83 0.92 0.09 0.47 0.02 0.   4.41] Loss_P: [2.33 1.73 1.63 0.96 0.97 0.1  0.47 0.01 8.21]\n",
      "Loss_Q: [0.87 1.27 0.83 0.92 0.1  0.47 0.01 0.   4.48] Loss_P: [2.32 1.76 1.68 0.94 0.98 0.08 0.47 0.01 8.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.85 1.2  0.78 0.96 0.09 0.5  0.02 0.   4.4 ] Loss_P: [2.35 1.77 1.64 0.94 0.96 0.09 0.49 0.01 8.23]\n",
      "Loss_Q: [0.83 1.26 0.85 0.98 0.1  0.51 0.01 0.   4.53] Loss_P: [2.34 1.76 1.67 0.95 0.93 0.1  0.49 0.01 8.24]\n",
      "Loss_Q: [0.88 1.21 0.8  0.89 0.09 0.49 0.02 0.   4.36] Loss_P: [2.31 1.77 1.68 0.93 0.96 0.07 0.49 0.01 8.21]\n",
      "Loss_Q: [0.86 1.23 0.8  0.91 0.08 0.49 0.01 0.   4.37] Loss_P: [2.36 1.77 1.66 0.92 0.9  0.09 0.51 0.03 8.24]\n",
      "Loss_Q: [0.9  1.16 0.78 0.9  0.08 0.48 0.01 0.   4.32] Loss_P: [2.33 1.79 1.71 0.93 0.95 0.1  0.5  0.01 8.33]\n",
      "Loss_Q: [0.81 1.25 0.8  0.91 0.1  0.48 0.01 0.   4.36] Loss_P: [2.39 1.75 1.67 0.91 0.97 0.09 0.49 0.01 8.3 ]\n",
      "Loss_Q: [0.89 1.26 0.85 0.94 0.1  0.49 0.02 0.   4.53] Loss_P: [2.31 1.71 1.74 0.89 0.95 0.07 0.48 0.01 8.16]\n",
      "Loss_Q: [0.84 1.19 0.8  0.91 0.07 0.5  0.02 0.   4.34] Loss_P: [2.37 1.76 1.65 0.91 0.95 0.09 0.49 0.02 8.24]\n",
      "Loss_Q: [0.9  1.17 0.77 0.88 0.08 0.48 0.01 0.   4.29] Loss_P: [2.38 1.79 1.67 0.9  0.98 0.09 0.48 0.02 8.31]\n",
      "Loss_Q: [0.94 1.23 0.84 0.96 0.09 0.47 0.01 0.   4.54] Loss_P: [2.38 1.79 1.73 0.97 0.97 0.11 0.46 0.01 8.41]\n",
      "Loss_Q: [0.86 1.25 0.87 0.97 0.09 0.47 0.01 0.   4.52] Loss_P: [2.35 1.83 1.65 0.92 0.97 0.08 0.47 0.01 8.28]\n",
      "Loss_Q: [0.88 1.15 0.82 1.   0.09 0.48 0.01 0.   4.44] Loss_P: [2.29 1.76 1.65 0.99 1.   0.08 0.48 0.01 8.26]\n",
      "Loss_Q: [0.87 1.21 0.84 0.91 0.07 0.48 0.01 0.   4.39] Loss_P: [2.34 1.74 1.67 0.97 0.91 0.07 0.48 0.01 8.19]\n",
      "Loss_Q: [0.78 1.24 0.84 0.93 0.07 0.47 0.01 0.   4.33] Loss_P: [2.35 1.81 1.67 0.9  0.95 0.07 0.48 0.01 8.24]\n",
      "Loss_Q: [0.89 1.32 0.83 0.9  0.07 0.47 0.02 0.   4.49] Loss_P: [2.36 1.8  1.75 0.95 0.93 0.09 0.48 0.01 8.36]\n",
      "Loss_Q: [0.88 1.25 0.81 0.91 0.08 0.47 0.02 0.   4.41] Loss_P: [2.38 1.76 1.72 0.95 0.99 0.08 0.47 0.03 8.36]\n",
      "Loss_Q: [0.84 1.23 0.78 0.92 0.08 0.46 0.01 0.   4.34] Loss_P: [2.36 1.83 1.67 0.93 0.95 0.06 0.46 0.01 8.28]\n",
      "Loss_Q: [0.82 1.29 0.81 0.92 0.09 0.43 0.01 0.   4.37] Loss_P: [2.37 1.78 1.69 0.96 0.97 0.07 0.45 0.01 8.31]\n",
      "Loss_Q: [0.84 1.31 0.81 0.89 0.06 0.46 0.01 0.   4.38] Loss_P: [2.36 1.79 1.65 0.92 0.92 0.07 0.45 0.01 8.18]\n",
      "Loss_Q: [0.83 1.3  0.79 0.87 0.08 0.45 0.01 0.   4.33] Loss_P: [2.33 1.82 1.72 0.97 0.97 0.06 0.45 0.01 8.32]\n",
      "Loss_Q: [0.89 1.22 0.81 0.91 0.07 0.45 0.01 0.   4.35] Loss_P: [2.38 1.77 1.75 0.96 0.92 0.09 0.45 0.01 8.33]\n",
      "Loss_Q: [0.88 1.32 0.83 0.91 0.07 0.43 0.01 0.   4.45] Loss_P: [2.34 1.71 1.67 0.98 0.95 0.09 0.42 0.01 8.17]\n",
      "Loss_Q: [0.85 1.27 0.77 0.94 0.09 0.44 0.01 0.   4.38] Loss_P: [2.37 1.73 1.66 0.94 0.91 0.08 0.45 0.01 8.14]\n",
      "Loss_Q: [0.75 1.22 0.8  0.92 0.08 0.41 0.01 0.   4.2 ] Loss_P: [2.32 1.76 1.71 0.96 0.91 0.1  0.44 0.01 8.21]\n",
      "Loss_Q: [0.81 1.27 0.79 0.93 0.09 0.45 0.01 0.   4.35] Loss_P: [2.41 1.75 1.71 0.93 0.94 0.09 0.45 0.01 8.27]\n",
      "Loss_Q: [0.78 1.24 0.76 0.89 0.06 0.45 0.01 0.   4.19] Loss_P: [2.37 1.77 1.71 0.91 0.96 0.06 0.45 0.01 8.24]\n",
      "Loss_Q: [0.84 1.22 0.73 0.9  0.07 0.43 0.02 0.   4.2 ] Loss_P: [2.4  1.69 1.65 0.87 0.93 0.07 0.46 0.01 8.08]\n",
      "Loss_Q: [0.91 1.27 0.84 0.94 0.09 0.47 0.01 0.   4.52] Loss_P: [2.31 1.76 1.66 0.91 0.92 0.07 0.46 0.02 8.11]\n",
      "Loss_Q: [0.84 1.22 0.77 0.87 0.06 0.45 0.01 0.   4.22] Loss_P: [2.41 1.69 1.68 0.92 0.89 0.07 0.47 0.02 8.15]\n",
      "Loss_Q: [0.82 1.17 0.8  0.85 0.07 0.43 0.01 0.   4.14] Loss_P: [2.35 1.75 1.62 0.91 0.94 0.06 0.44 0.02 8.08]\n",
      "Loss_Q: [0.81 1.23 0.76 0.86 0.07 0.45 0.01 0.   4.19] Loss_P: [2.39 1.78 1.64 0.95 0.87 0.05 0.46 0.01 8.15]\n",
      "Loss_Q: [0.83 1.15 0.79 0.85 0.06 0.44 0.02 0.   4.14] Loss_P: [2.35 1.76 1.65 0.94 0.86 0.07 0.44 0.01 8.08]\n",
      "Loss_Q: [0.88 1.26 0.79 0.83 0.07 0.43 0.01 0.   4.28] Loss_P: [2.35 1.74 1.75 0.93 0.89 0.06 0.44 0.01 8.19]\n",
      "Loss_Q: [0.86 1.17 0.75 0.82 0.08 0.42 0.01 0.   4.09] Loss_P: [2.41 1.74 1.71 0.92 0.86 0.06 0.43 0.01 8.16]\n",
      "Loss_Q: [0.86 1.24 0.82 0.85 0.08 0.46 0.02 0.   4.32] Loss_P: [2.34 1.73 1.67 0.96 0.86 0.06 0.44 0.01 8.08]\n",
      "Loss_Q: [0.93 1.22 0.82 0.86 0.07 0.44 0.01 0.   4.35] Loss_P: [2.36 1.81 1.71 0.95 0.84 0.06 0.45 0.01 8.19]\n",
      "Loss_Q: [0.8  1.27 0.84 0.85 0.06 0.43 0.02 0.   4.28] Loss_P: [2.29 1.8  1.68 0.9  0.84 0.06 0.45 0.02 8.05]\n",
      "Loss_Q: [0.82 1.23 0.79 0.86 0.07 0.46 0.02 0.   4.25] Loss_P: [2.29 1.84 1.69 0.95 0.86 0.04 0.45 0.01 8.14]\n",
      "Loss_Q: [0.87 1.26 0.83 0.83 0.07 0.47 0.01 0.   4.33] Loss_P: [2.38 1.74 1.71 0.93 0.89 0.06 0.47 0.01 8.19]\n",
      "Loss_Q: [0.82 1.25 0.83 0.87 0.05 0.48 0.01 0.   4.31] Loss_P: [2.3  1.83 1.67 0.96 0.89 0.05 0.48 0.01 8.21]\n",
      "Loss_Q: [0.82 1.2  0.81 0.85 0.07 0.47 0.02 0.   4.23] Loss_P: [2.31 1.77 1.74 0.96 0.88 0.07 0.49 0.01 8.23]\n",
      "Loss_Q: [0.9  1.23 0.85 0.85 0.05 0.47 0.01 0.   4.37] Loss_P: [2.27 1.86 1.7  0.99 0.89 0.06 0.48 0.01 8.25]\n",
      "Loss_Q: [0.89 1.18 0.89 0.87 0.04 0.48 0.01 0.   4.37] Loss_P: [2.4  1.83 1.66 0.99 0.9  0.05 0.49 0.02 8.33]\n",
      "Loss_Q: [0.89 1.19 0.86 0.94 0.07 0.47 0.02 0.   4.43] Loss_P: [2.33 1.79 1.66 0.94 0.88 0.05 0.47 0.01 8.12]\n",
      "Loss_Q: [0.92 1.19 0.86 0.89 0.05 0.48 0.01 0.   4.4 ] Loss_P: [2.32 1.81 1.61 1.02 0.91 0.05 0.46 0.01 8.2 ]\n",
      "Loss_Q: [0.93 1.2  0.91 0.89 0.04 0.45 0.01 0.   4.44] Loss_P: [2.3  1.84 1.62 1.05 0.92 0.06 0.45 0.01 8.26]\n",
      "Loss_Q: [0.88 1.26 0.87 0.86 0.05 0.47 0.01 0.   4.41] Loss_P: [2.32 1.83 1.69 1.02 0.87 0.04 0.46 0.01 8.24]\n",
      "Loss_Q: [0.87 1.19 0.84 0.88 0.05 0.47 0.01 0.   4.32] Loss_P: [2.34 1.85 1.64 0.97 0.93 0.06 0.48 0.01 8.28]\n",
      "Loss_Q: [0.84 1.21 0.84 0.88 0.07 0.45 0.01 0.   4.3 ] Loss_P: [2.35 1.84 1.64 1.05 0.88 0.05 0.46 0.01 8.28]\n",
      "Loss_Q: [0.89 1.2  0.87 0.86 0.06 0.48 0.01 0.   4.37] Loss_P: [2.29 1.87 1.68 1.03 0.96 0.06 0.47 0.01 8.37]\n",
      "Loss_Q: [0.92 1.16 0.89 0.9  0.05 0.46 0.02 0.   4.41] Loss_P: [2.32 1.75 1.65 1.01 0.93 0.05 0.47 0.01 8.19]\n",
      "Loss_Q: [0.84 1.27 0.88 0.92 0.07 0.47 0.01 0.   4.44] Loss_P: [2.34 1.8  1.62 1.   0.94 0.07 0.44 0.01 8.23]\n",
      "Loss_Q: [0.88 1.23 0.85 0.89 0.05 0.45 0.02 0.   4.37] Loss_P: [2.29 1.82 1.67 1.01 0.96 0.07 0.46 0.01 8.28]\n",
      "Loss_Q: [0.9  1.19 0.89 0.92 0.06 0.47 0.02 0.   4.45] Loss_P: [2.35 1.8  1.57 0.95 0.91 0.04 0.46 0.01 8.08]\n",
      "Loss_Q: [0.9  1.2  0.87 0.89 0.08 0.46 0.01 0.   4.41] Loss_P: [2.3  1.89 1.59 1.   0.95 0.06 0.47 0.01 8.27]\n",
      "Loss_Q: [0.86 1.11 0.84 0.88 0.06 0.46 0.03 0.   4.24] Loss_P: [2.36 1.85 1.59 0.95 0.99 0.06 0.47 0.01 8.27]\n",
      "Loss_Q: [0.9  1.13 0.89 0.95 0.04 0.47 0.01 0.   4.37] Loss_P: [2.33 1.87 1.63 0.96 0.96 0.05 0.45 0.01 8.26]\n",
      "Loss_Q: [0.89 1.16 0.87 0.92 0.06 0.46 0.01 0.   4.36] Loss_P: [2.36 1.83 1.6  0.99 0.91 0.06 0.46 0.01 8.21]\n",
      "Loss_Q: [0.91 1.18 0.87 0.96 0.04 0.47 0.01 0.   4.44] Loss_P: [2.31 1.85 1.63 0.99 0.96 0.08 0.48 0.02 8.33]\n",
      "Loss_Q: [0.9  1.13 0.86 0.9  0.05 0.47 0.02 0.   4.32] Loss_P: [2.31 1.86 1.52 0.96 0.95 0.04 0.48 0.01 8.12]\n",
      "Loss_Q: [0.85 1.15 0.86 0.94 0.06 0.48 0.01 0.   4.35] Loss_P: [2.37 1.76 1.62 0.97 0.97 0.07 0.48 0.01 8.26]\n",
      "Loss_Q: [0.86 1.22 0.84 0.98 0.05 0.47 0.01 0.   4.44] Loss_P: [2.3  1.77 1.59 1.02 1.02 0.06 0.47 0.02 8.27]\n",
      "Loss_Q: [0.9  1.16 0.87 0.92 0.05 0.48 0.01 0.   4.38] Loss_P: [2.35 1.79 1.64 0.97 1.   0.07 0.48 0.01 8.32]\n",
      "Loss_Q: [0.91 1.17 0.85 0.9  0.07 0.48 0.01 0.   4.4 ] Loss_P: [2.35 1.72 1.63 0.96 0.99 0.06 0.47 0.01 8.19]\n",
      "Loss_Q: [0.83 1.18 0.85 0.94 0.07 0.49 0.02 0.   4.38] Loss_P: [2.39 1.74 1.6  0.95 1.01 0.06 0.49 0.01 8.26]\n",
      "Loss_Q: [0.86 1.2  0.86 0.95 0.05 0.5  0.01 0.   4.42] Loss_P: [2.32 1.75 1.65 1.06 1.01 0.08 0.48 0.01 8.36]\n",
      "Loss_Q: [0.81 1.14 0.83 0.93 0.04 0.48 0.01 0.   4.25] Loss_P: [2.38 1.71 1.62 1.04 0.98 0.09 0.49 0.01 8.32]\n",
      "Loss_Q: [0.8  1.2  0.87 0.92 0.06 0.47 0.02 0.   4.35] Loss_P: [2.41 1.78 1.64 0.91 0.96 0.06 0.48 0.01 8.26]\n",
      "Loss_Q: [0.93 1.15 0.91 0.96 0.05 0.47 0.01 0.   4.48] Loss_P: [2.35 1.82 1.56 0.96 0.97 0.05 0.48 0.01 8.21]\n",
      "Loss_Q: [0.88 1.1  0.89 0.95 0.06 0.48 0.01 0.   4.36] Loss_P: [2.39 1.81 1.6  1.   1.02 0.08 0.49 0.01 8.39]\n",
      "Loss_Q: [0.87 1.17 0.91 0.94 0.07 0.47 0.01 0.   4.43] Loss_P: [2.36 1.82 1.61 0.98 0.98 0.05 0.5  0.01 8.31]\n",
      "Loss_Q: [0.85 1.27 0.85 0.96 0.08 0.5  0.01 0.   4.52] Loss_P: [2.36 1.76 1.62 0.96 1.   0.05 0.48 0.01 8.24]\n",
      "Loss_Q: [0.82 1.1  0.86 0.97 0.05 0.5  0.01 0.   4.33] Loss_P: [2.45 1.79 1.55 0.94 1.   0.05 0.5  0.01 8.3 ]\n",
      "Loss_Q: [0.89 1.19 0.89 0.93 0.09 0.5  0.01 0.   4.51] Loss_P: [2.38 1.79 1.6  0.94 0.99 0.07 0.5  0.01 8.31]\n",
      "Loss_Q: [0.88 1.2  0.87 0.94 0.06 0.5  0.01 0.   4.46] Loss_P: [2.3  1.87 1.63 0.96 0.98 0.07 0.5  0.02 8.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.94 1.16 0.87 0.93 0.06 0.5  0.01 0.   4.48] Loss_P: [2.38 1.82 1.59 0.97 0.96 0.07 0.5  0.02 8.31]\n",
      "Loss_Q: [0.89 1.21 0.87 0.96 0.05 0.5  0.01 0.   4.49] Loss_P: [2.34 1.81 1.62 0.98 1.02 0.05 0.5  0.01 8.32]\n",
      "Loss_Q: [0.87 1.19 0.89 0.97 0.07 0.51 0.01 0.   4.51] Loss_P: [2.36 1.77 1.59 0.98 0.96 0.05 0.49 0.01 8.21]\n",
      "Loss_Q: [0.9  1.22 0.91 0.94 0.05 0.49 0.01 0.   4.51] Loss_P: [2.36 1.82 1.62 0.99 0.96 0.08 0.49 0.01 8.33]\n",
      "Loss_Q: [0.93 1.23 0.88 0.89 0.05 0.5  0.01 0.   4.48] Loss_P: [2.41 1.77 1.6  0.94 0.94 0.05 0.49 0.01 8.23]\n",
      "Loss_Q: [0.9  1.18 0.89 0.95 0.06 0.5  0.01 0.   4.48] Loss_P: [2.35 1.87 1.58 0.93 0.96 0.05 0.5  0.02 8.27]\n",
      "Loss_Q: [0.87 1.16 0.81 0.93 0.07 0.49 0.02 0.   4.36] Loss_P: [2.41 1.81 1.61 0.95 0.98 0.06 0.5  0.01 8.32]\n",
      "Loss_Q: [0.91 1.22 0.85 0.93 0.06 0.49 0.01 0.   4.47] Loss_P: [2.4  1.79 1.63 1.01 0.97 0.06 0.5  0.02 8.38]\n",
      "Loss_Q: [0.88 1.28 0.84 1.02 0.07 0.5  0.01 0.   4.61] Loss_P: [2.4  1.84 1.63 0.96 0.99 0.06 0.5  0.02 8.4 ]\n",
      "Loss_Q: [0.9  1.22 0.81 1.   0.07 0.49 0.01 0.   4.51] Loss_P: [2.38 1.8  1.63 0.99 0.98 0.05 0.48 0.01 8.33]\n",
      "Loss_Q: [0.94 1.24 0.86 0.95 0.06 0.49 0.01 0.   4.55] Loss_P: [2.39 1.81 1.63 0.99 1.03 0.06 0.5  0.01 8.44]\n",
      "Loss_Q: [0.91 1.24 0.86 1.01 0.04 0.49 0.01 0.   4.56] Loss_P: [2.47 1.81 1.57 0.93 1.01 0.07 0.49 0.01 8.36]\n",
      "Loss_Q: [0.92 1.21 0.84 1.   0.06 0.5  0.01 0.   4.54] Loss_P: [2.38 1.82 1.58 0.97 1.05 0.07 0.49 0.02 8.38]\n",
      "Loss_Q: [0.93 1.19 0.84 0.94 0.05 0.51 0.01 0.   4.46] Loss_P: [2.39 1.84 1.63 0.91 1.01 0.08 0.5  0.01 8.36]\n",
      "Loss_Q: [0.95 1.29 0.84 0.98 0.07 0.49 0.01 0.   4.64] Loss_P: [2.41 1.76 1.63 0.91 1.04 0.06 0.5  0.02 8.33]\n",
      "Loss_Q: [0.87 1.21 0.86 0.92 0.03 0.49 0.01 0.   4.41] Loss_P: [2.37 1.8  1.63 0.91 1.   0.07 0.49 0.01 8.28]\n",
      "Loss_Q: [0.89 1.22 0.84 0.93 0.06 0.49 0.02 0.   4.43] Loss_P: [2.38 1.82 1.68 0.94 1.05 0.07 0.5  0.01 8.45]\n",
      "Loss_Q: [0.93 1.25 0.86 0.98 0.08 0.49 0.01 0.   4.59] Loss_P: [2.35 1.81 1.63 0.9  1.   0.06 0.5  0.01 8.25]\n",
      "Loss_Q: [1.01 1.2  0.85 0.99 0.05 0.49 0.01 0.   4.6 ] Loss_P: [2.37 1.81 1.6  0.94 1.   0.07 0.5  0.01 8.31]\n",
      "Loss_Q: [0.92 1.2  0.83 0.94 0.04 0.49 0.01 0.   4.43] Loss_P: [2.4  1.84 1.61 0.94 1.03 0.06 0.5  0.02 8.4 ]\n",
      "Loss_Q: [0.95 1.22 0.83 0.96 0.05 0.5  0.01 0.   4.51] Loss_P: [2.36 1.86 1.61 0.95 1.02 0.07 0.49 0.01 8.37]\n",
      "Loss_Q: [0.88 1.21 0.82 0.96 0.08 0.5  0.01 0.   4.46] Loss_P: [2.36 1.89 1.59 0.87 0.97 0.06 0.51 0.01 8.24]\n",
      "Loss_Q: [1.04 1.23 0.79 0.98 0.06 0.51 0.02 0.   4.63] Loss_P: [2.35 1.89 1.65 0.93 1.02 0.07 0.5  0.01 8.42]\n",
      "Loss_Q: [0.93 1.27 0.77 0.98 0.06 0.5  0.01 0.   4.53] Loss_P: [2.35 1.85 1.6  0.95 1.04 0.06 0.5  0.01 8.36]\n",
      "Loss_Q: [0.87 1.19 0.78 0.96 0.07 0.48 0.02 0.   4.37] Loss_P: [2.36 1.85 1.61 0.9  0.99 0.06 0.5  0.01 8.27]\n",
      "Loss_Q: [0.84 1.2  0.84 0.92 0.05 0.49 0.01 0.   4.36] Loss_P: [2.42 1.83 1.69 0.89 0.99 0.06 0.5  0.01 8.4 ]\n",
      "Loss_Q: [0.97 1.17 0.82 0.97 0.07 0.48 0.01 0.   4.48] Loss_P: [2.34 1.89 1.57 0.92 0.99 0.08 0.48 0.01 8.27]\n",
      "Loss_Q: [0.93 1.22 0.78 0.92 0.06 0.48 0.01 0.   4.41] Loss_P: [2.35 1.81 1.6  0.85 1.01 0.06 0.48 0.01 8.17]\n",
      "Loss_Q: [0.83 1.18 0.79 0.97 0.07 0.49 0.02 0.   4.34] Loss_P: [2.35 1.85 1.59 0.87 1.   0.06 0.5  0.01 8.23]\n",
      "Loss_Q: [0.93 1.13 0.86 0.98 0.09 0.5  0.01 0.   4.5 ] Loss_P: [2.33 1.78 1.57 0.89 0.99 0.04 0.5  0.02 8.13]\n",
      "Loss_Q: [0.88 1.21 0.83 0.94 0.07 0.49 0.01 0.   4.43] Loss_P: [2.34 1.81 1.62 0.88 1.   0.07 0.49 0.01 8.21]\n",
      "Loss_Q: [0.79 1.14 0.82 0.91 0.07 0.48 0.02 0.   4.22] Loss_P: [2.32 1.84 1.57 0.83 0.96 0.06 0.47 0.01 8.06]\n",
      "Loss_Q: [0.84 1.12 0.78 0.93 0.06 0.46 0.02 0.   4.22] Loss_P: [2.35 1.81 1.55 0.89 0.98 0.07 0.48 0.01 8.12]\n",
      "Loss_Q: [0.85 1.09 0.83 0.9  0.09 0.48 0.01 0.   4.24] Loss_P: [2.31 1.84 1.53 0.91 0.99 0.08 0.49 0.01 8.15]\n",
      "Loss_Q: [0.77 1.1  0.83 0.92 0.08 0.48 0.02 0.   4.19] Loss_P: [2.35 1.84 1.5  0.88 0.97 0.07 0.48 0.02 8.12]\n",
      "Loss_Q: [0.86 1.15 0.82 0.94 0.08 0.46 0.01 0.   4.33] Loss_P: [2.32 1.86 1.6  0.92 1.01 0.06 0.46 0.01 8.25]\n",
      "Loss_Q: [0.88 1.16 0.85 0.95 0.07 0.46 0.01 0.   4.38] Loss_P: [2.32 1.83 1.55 0.91 0.99 0.06 0.47 0.01 8.15]\n",
      "Loss_Q: [0.84 1.15 0.83 1.   0.07 0.49 0.01 0.   4.39] Loss_P: [2.32 1.83 1.61 0.91 0.99 0.08 0.46 0.01 8.21]\n",
      "Loss_Q: [0.89 1.19 0.88 0.99 0.09 0.46 0.01 0.   4.51] Loss_P: [2.33 1.84 1.56 0.94 0.97 0.09 0.49 0.01 8.22]\n",
      "Loss_Q: [0.86 1.13 0.84 0.93 0.07 0.49 0.02 0.   4.34] Loss_P: [2.31 1.88 1.59 0.94 0.97 0.06 0.49 0.01 8.24]\n",
      "Loss_Q: [0.87 1.14 0.84 1.02 0.08 0.49 0.02 0.   4.45] Loss_P: [2.35 1.86 1.6  0.95 0.98 0.06 0.5  0.01 8.31]\n",
      "Loss_Q: [0.85 1.17 0.84 0.93 0.07 0.49 0.01 0.   4.37] Loss_P: [2.3  1.85 1.58 0.94 1.01 0.07 0.51 0.01 8.26]\n",
      "Loss_Q: [0.85 1.09 0.87 0.99 0.07 0.49 0.01 0.   4.35] Loss_P: [2.32 1.84 1.6  0.94 1.   0.07 0.5  0.01 8.27]\n",
      "Loss_Q: [0.83 1.11 0.87 0.99 0.07 0.5  0.01 0.   4.37] Loss_P: [2.29 1.83 1.59 0.95 0.97 0.09 0.48 0.01 8.21]\n",
      "Loss_Q: [0.86 1.14 0.84 0.96 0.08 0.5  0.01 0.   4.39] Loss_P: [2.35 1.81 1.59 0.95 0.99 0.08 0.49 0.01 8.27]\n",
      "Loss_Q: [0.85 1.16 0.88 0.95 0.06 0.48 0.01 0.   4.39] Loss_P: [2.24 1.86 1.62 0.94 0.97 0.06 0.49 0.01 8.19]\n",
      "Loss_Q: [0.83 1.15 0.9  0.94 0.08 0.47 0.01 0.   4.37] Loss_P: [2.3  1.82 1.63 0.96 0.97 0.07 0.48 0.01 8.24]\n",
      "Loss_Q: [0.84 1.12 0.87 0.97 0.05 0.5  0.01 0.   4.36] Loss_P: [2.33 1.85 1.57 0.95 1.01 0.05 0.5  0.01 8.28]\n",
      "Loss_Q: [0.8  1.14 0.82 0.96 0.04 0.49 0.02 0.   4.27] Loss_P: [2.33 1.82 1.58 0.94 0.97 0.07 0.5  0.01 8.23]\n",
      "Loss_Q: [0.75 1.18 0.84 0.92 0.08 0.5  0.01 0.   4.28] Loss_P: [2.34 1.79 1.56 0.92 0.96 0.07 0.52 0.02 8.17]\n",
      "Loss_Q: [0.76 1.13 0.82 0.89 0.06 0.5  0.01 0.   4.16] Loss_P: [2.36 1.76 1.56 0.91 0.98 0.08 0.5  0.02 8.17]\n",
      "Loss_Q: [0.75 1.14 0.8  0.94 0.06 0.48 0.01 0.   4.19] Loss_P: [2.37 1.71 1.61 0.89 0.91 0.07 0.49 0.01 8.04]\n",
      "Loss_Q: [0.8  1.11 0.85 0.88 0.06 0.49 0.01 0.   4.19] Loss_P: [2.3  1.75 1.55 0.9  0.96 0.08 0.49 0.01 8.03]\n",
      "Loss_Q: [0.85 1.13 0.86 0.96 0.05 0.49 0.01 0.   4.34] Loss_P: [2.31 1.79 1.56 0.88 0.89 0.06 0.47 0.01 7.97]\n",
      "Loss_Q: [0.81 1.12 0.88 0.86 0.05 0.49 0.01 0.   4.21] Loss_P: [2.37 1.75 1.6  0.95 0.97 0.06 0.5  0.01 8.22]\n",
      "Loss_Q: [0.76 1.11 0.85 0.87 0.07 0.48 0.01 0.   4.15] Loss_P: [2.33 1.8  1.58 0.92 0.97 0.06 0.47 0.01 8.13]\n",
      "Loss_Q: [0.84 1.12 0.85 0.93 0.05 0.46 0.02 0.   4.26] Loss_P: [2.37 1.83 1.57 0.97 1.   0.07 0.47 0.03 8.31]\n",
      "Loss_Q: [0.8  1.17 0.87 0.92 0.06 0.49 0.02 0.   4.34] Loss_P: [2.36 1.77 1.57 0.94 0.97 0.06 0.48 0.01 8.16]\n",
      "Loss_Q: [0.83 1.19 0.83 0.87 0.05 0.5  0.01 0.   4.27] Loss_P: [2.37 1.77 1.61 0.95 1.02 0.08 0.51 0.01 8.32]\n",
      "Loss_Q: [0.88 1.15 0.81 0.89 0.06 0.49 0.01 0.   4.3 ] Loss_P: [2.35 1.76 1.63 0.87 0.91 0.09 0.49 0.01 8.11]\n",
      "Loss_Q: [0.86 1.15 0.91 0.95 0.09 0.49 0.01 0.   4.47] Loss_P: [2.35 1.77 1.59 0.91 0.95 0.06 0.5  0.01 8.13]\n",
      "Loss_Q: [0.8  1.19 0.89 0.9  0.08 0.51 0.02 0.   4.39] Loss_P: [2.35 1.74 1.59 0.98 1.01 0.06 0.49 0.01 8.23]\n",
      "Loss_Q: [0.86 1.18 0.86 0.94 0.09 0.5  0.01 0.   4.44] Loss_P: [2.37 1.72 1.65 0.9  0.94 0.07 0.5  0.01 8.17]\n",
      "Loss_Q: [0.83 1.14 0.84 0.91 0.09 0.49 0.01 0.   4.32] Loss_P: [2.37 1.73 1.6  0.84 0.96 0.07 0.49 0.02 8.1 ]\n",
      "Loss_Q: [0.92 1.2  0.83 0.93 0.06 0.5  0.01 0.   4.45] Loss_P: [2.33 1.79 1.64 0.94 1.02 0.08 0.51 0.01 8.31]\n",
      "Loss_Q: [0.83 1.19 0.84 0.94 0.06 0.51 0.01 0.   4.38] Loss_P: [2.4  1.81 1.65 0.92 0.96 0.09 0.51 0.01 8.34]\n",
      "Loss_Q: [0.89 1.19 0.81 0.93 0.06 0.5  0.02 0.   4.4 ] Loss_P: [2.34 1.77 1.66 0.88 0.95 0.07 0.5  0.01 8.19]\n",
      "Loss_Q: [0.86 1.21 0.82 0.86 0.05 0.51 0.01 0.   4.32] Loss_P: [2.4  1.78 1.64 0.83 0.89 0.05 0.5  0.01 8.11]\n",
      "Loss_Q: [0.9  1.21 0.8  0.88 0.06 0.52 0.01 0.   4.38] Loss_P: [2.35 1.83 1.71 0.91 0.99 0.08 0.51 0.01 8.39]\n",
      "Loss_Q: [0.87 1.25 0.8  0.9  0.06 0.5  0.01 0.   4.39] Loss_P: [2.41 1.77 1.64 0.87 0.88 0.06 0.5  0.01 8.15]\n",
      "Loss_Q: [0.9  1.2  0.78 0.84 0.05 0.51 0.01 0.   4.3 ] Loss_P: [2.35 1.76 1.63 0.88 0.89 0.06 0.51 0.01 8.08]\n",
      "Loss_Q: [0.82 1.18 0.76 0.82 0.07 0.5  0.01 0.   4.17] Loss_P: [2.37 1.81 1.69 0.84 0.96 0.07 0.51 0.01 8.25]\n",
      "Loss_Q: [0.9  1.22 0.78 0.91 0.08 0.5  0.01 0.   4.4 ] Loss_P: [2.32 1.81 1.64 0.86 0.97 0.07 0.51 0.01 8.19]\n",
      "Loss_Q: [0.87 1.22 0.83 0.88 0.06 0.5  0.01 0.   4.35] Loss_P: [2.36 1.75 1.64 0.86 0.95 0.06 0.5  0.02 8.13]\n",
      "Loss_Q: [0.89 1.2  0.81 0.93 0.05 0.51 0.02 0.   4.4 ] Loss_P: [2.4  1.74 1.64 0.86 1.03 0.07 0.51 0.01 8.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.86 1.13 0.76 0.85 0.07 0.52 0.01 0.   4.21] Loss_P: [2.4  1.79 1.55 0.91 0.97 0.07 0.51 0.01 8.21]\n",
      "Loss_Q: [0.8  1.19 0.84 0.88 0.05 0.51 0.01 0.   4.28] Loss_P: [2.37 1.8  1.53 0.87 0.99 0.08 0.5  0.01 8.16]\n",
      "Loss_Q: [0.85 1.13 0.8  0.86 0.07 0.51 0.02 0.   4.22] Loss_P: [2.33 1.86 1.54 0.83 0.92 0.06 0.5  0.01 8.04]\n",
      "Loss_Q: [0.86 1.2  0.8  0.86 0.04 0.5  0.01 0.   4.28] Loss_P: [2.39 1.82 1.6  0.82 0.91 0.05 0.51 0.02 8.11]\n",
      "Loss_Q: [0.84 1.2  0.8  0.84 0.07 0.48 0.01 0.   4.25] Loss_P: [2.43 1.79 1.59 0.84 0.91 0.1  0.51 0.01 8.16]\n",
      "Loss_Q: [0.87 1.19 0.85 0.85 0.06 0.5  0.01 0.   4.33] Loss_P: [2.35 1.85 1.56 0.9  0.9  0.06 0.5  0.01 8.13]\n",
      "Loss_Q: [0.9  1.12 0.82 0.83 0.05 0.5  0.01 0.   4.23] Loss_P: [2.4  1.83 1.57 0.88 0.85 0.06 0.51 0.02 8.13]\n",
      "Loss_Q: [0.83 1.11 0.82 0.77 0.05 0.5  0.01 0.   4.11] Loss_P: [2.38 1.8  1.52 0.93 0.82 0.06 0.5  0.01 8.02]\n",
      "Loss_Q: [0.91 1.11 0.9  0.8  0.05 0.52 0.01 0.   4.28] Loss_P: [2.41 1.75 1.58 0.9  0.86 0.07 0.5  0.01 8.09]\n",
      "Loss_Q: [0.87 1.15 0.79 0.79 0.07 0.49 0.02 0.   4.18] Loss_P: [2.35 1.85 1.6  0.87 0.88 0.05 0.49 0.01 8.09]\n",
      "Loss_Q: [0.93 1.16 0.86 0.87 0.08 0.51 0.01 0.   4.43] Loss_P: [2.37 1.83 1.64 0.79 0.85 0.06 0.5  0.01 8.06]\n",
      "Loss_Q: [0.9  1.2  0.8  0.88 0.05 0.5  0.02 0.   4.34] Loss_P: [2.41 1.76 1.58 0.84 0.85 0.06 0.5  0.02 8.02]\n",
      "Loss_Q: [0.88 1.21 0.79 0.78 0.05 0.51 0.01 0.   4.23] Loss_P: [2.36 1.78 1.64 0.82 0.86 0.06 0.5  0.01 8.02]\n",
      "Loss_Q: [0.87 1.2  0.84 0.88 0.06 0.5  0.01 0.   4.37] Loss_P: [2.36 1.8  1.66 0.9  0.85 0.05 0.5  0.01 8.13]\n",
      "Loss_Q: [0.97 1.15 0.84 0.8  0.07 0.49 0.01 0.   4.34] Loss_P: [2.38 1.8  1.62 0.88 0.82 0.06 0.5  0.01 8.07]\n",
      "Loss_Q: [0.91 1.17 0.79 0.79 0.07 0.5  0.01 0.   4.23] Loss_P: [2.4  1.85 1.6  0.86 0.83 0.06 0.51 0.01 8.11]\n",
      "Loss_Q: [0.98 1.17 0.77 0.79 0.06 0.49 0.01 0.   4.28] Loss_P: [2.38 1.85 1.62 0.8  0.8  0.04 0.5  0.01 7.99]\n",
      "Loss_Q: [0.96 1.13 0.77 0.72 0.04 0.49 0.01 0.   4.12] Loss_P: [2.4  1.86 1.61 0.84 0.78 0.07 0.5  0.01 8.07]\n",
      "Loss_Q: [0.89 1.18 0.8  0.77 0.07 0.5  0.02 0.   4.23] Loss_P: [2.4  1.84 1.61 0.83 0.74 0.06 0.5  0.02 7.98]\n",
      "Loss_Q: [0.94 1.16 0.87 0.78 0.07 0.51 0.01 0.   4.35] Loss_P: [2.45 1.84 1.62 0.89 0.77 0.05 0.51 0.02 8.14]\n",
      "Loss_Q: [0.96 1.15 0.8  0.72 0.09 0.51 0.01 0.   4.24] Loss_P: [2.42 1.83 1.6  0.88 0.8  0.05 0.5  0.01 8.1 ]\n",
      "Loss_Q: [0.98 1.19 0.78 0.73 0.08 0.52 0.01 0.   4.28] Loss_P: [2.32 1.84 1.66 0.81 0.76 0.07 0.5  0.01 7.98]\n",
      "Loss_Q: [0.93 1.12 0.77 0.75 0.09 0.5  0.01 0.   4.17] Loss_P: [2.42 1.81 1.67 0.73 0.78 0.07 0.51 0.01 7.99]\n",
      "Loss_Q: [0.88 1.18 0.77 0.75 0.09 0.52 0.01 0.   4.2 ] Loss_P: [2.31 1.76 1.71 0.81 0.77 0.06 0.51 0.01 7.94]\n",
      "Loss_Q: [1.   1.22 0.78 0.82 0.07 0.51 0.01 0.   4.4 ] Loss_P: [2.36 1.77 1.69 0.79 0.79 0.05 0.5  0.01 7.95]\n",
      "Loss_Q: [0.99 1.21 0.8  0.78 0.06 0.51 0.01 0.   4.36] Loss_P: [2.4  1.78 1.69 0.87 0.87 0.08 0.51 0.01 8.22]\n",
      "Loss_Q: [1.   1.2  0.8  0.82 0.06 0.51 0.02 0.   4.41] Loss_P: [2.36 1.79 1.76 0.89 0.84 0.06 0.51 0.01 8.23]\n",
      "Loss_Q: [0.95 1.24 0.81 0.77 0.08 0.51 0.01 0.   4.36] Loss_P: [2.34 1.82 1.77 0.88 0.81 0.08 0.51 0.01 8.22]\n",
      "Loss_Q: [0.9  1.25 0.82 0.8  0.09 0.51 0.01 0.   4.38] Loss_P: [2.36 1.79 1.78 0.86 0.84 0.08 0.5  0.01 8.2 ]\n",
      "Loss_Q: [0.92 1.19 0.85 0.84 0.06 0.5  0.01 0.   4.38] Loss_P: [2.39 1.77 1.74 0.88 0.81 0.06 0.5  0.01 8.17]\n",
      "Loss_Q: [0.92 1.24 0.76 0.73 0.07 0.51 0.02 0.   4.25] Loss_P: [2.36 1.78 1.74 0.87 0.88 0.07 0.51 0.01 8.22]\n",
      "Loss_Q: [0.94 1.22 0.78 0.78 0.09 0.5  0.01 0.   4.31] Loss_P: [2.41 1.78 1.76 0.84 0.8  0.09 0.52 0.01 8.19]\n",
      "Loss_Q: [0.94 1.29 0.8  0.81 0.06 0.51 0.03 0.   4.43] Loss_P: [2.37 1.77 1.81 0.92 0.87 0.08 0.51 0.01 8.34]\n",
      "Loss_Q: [0.92 1.28 0.78 0.78 0.08 0.5  0.01 0.   4.35] Loss_P: [2.32 1.83 1.81 0.87 0.82 0.1  0.51 0.01 8.27]\n",
      "Loss_Q: [1.   1.29 0.82 0.79 0.08 0.5  0.01 0.   4.49] Loss_P: [2.39 1.82 1.77 0.92 0.83 0.08 0.51 0.01 8.34]\n",
      "Loss_Q: [0.95 1.26 0.85 0.82 0.09 0.51 0.02 0.   4.49] Loss_P: [2.42 1.82 1.72 0.98 0.86 0.08 0.51 0.01 8.4 ]\n",
      "Loss_Q: [1.01 1.26 0.88 0.9  0.07 0.5  0.01 0.   4.64] Loss_P: [2.37 1.82 1.76 0.98 0.93 0.09 0.52 0.01 8.47]\n",
      "Loss_Q: [0.91 1.34 0.88 0.85 0.08 0.51 0.01 0.   4.59] Loss_P: [2.34 1.83 1.78 0.98 0.91 0.09 0.51 0.01 8.44]\n",
      "Loss_Q: [1.02 1.25 0.91 0.91 0.07 0.51 0.01 0.   4.68] Loss_P: [2.36 1.82 1.75 0.97 0.91 0.1  0.51 0.01 8.43]\n",
      "Loss_Q: [0.96 1.29 0.87 0.83 0.06 0.5  0.01 0.   4.53] Loss_P: [2.37 1.91 1.73 1.03 0.92 0.07 0.51 0.02 8.55]\n",
      "Loss_Q: [0.94 1.32 0.86 0.86 0.06 0.5  0.01 0.   4.55] Loss_P: [2.36 1.82 1.77 0.96 0.91 0.08 0.51 0.02 8.43]\n",
      "Loss_Q: [1.02 1.27 0.86 0.85 0.06 0.51 0.01 0.   4.58] Loss_P: [2.36 1.86 1.77 0.99 0.87 0.08 0.52 0.01 8.45]\n",
      "Loss_Q: [0.97 1.31 0.89 0.87 0.05 0.5  0.01 0.   4.62] Loss_P: [2.34 1.9  1.81 0.96 0.88 0.05 0.51 0.01 8.46]\n",
      "Loss_Q: [1.02 1.22 0.9  0.89 0.07 0.5  0.01 0.   4.61] Loss_P: [2.36 1.87 1.78 0.97 0.85 0.06 0.5  0.01 8.4 ]\n",
      "Loss_Q: [1.01 1.3  0.84 0.88 0.06 0.49 0.01 0.   4.59] Loss_P: [2.32 1.9  1.78 1.04 0.92 0.07 0.51 0.01 8.54]\n",
      "Loss_Q: [0.98 1.16 0.89 0.86 0.05 0.5  0.01 0.   4.45] Loss_P: [2.32 1.82 1.79 1.02 0.87 0.08 0.5  0.01 8.41]\n",
      "Loss_Q: [0.97 1.21 0.84 0.85 0.05 0.5  0.01 0.   4.44] Loss_P: [2.33 1.8  1.76 1.01 0.9  0.06 0.5  0.01 8.37]\n",
      "Loss_Q: [0.99 1.27 0.91 0.81 0.08 0.49 0.01 0.   4.54] Loss_P: [2.37 1.85 1.78 1.01 0.86 0.05 0.49 0.02 8.44]\n",
      "Loss_Q: [0.97 1.27 0.9  0.82 0.04 0.5  0.02 0.   4.52] Loss_P: [2.36 1.86 1.81 0.95 0.87 0.06 0.49 0.01 8.4 ]\n",
      "Loss_Q: [0.92 1.24 0.89 0.76 0.08 0.5  0.01 0.   4.4 ] Loss_P: [2.33 1.85 1.76 1.01 0.83 0.05 0.5  0.01 8.35]\n",
      "Loss_Q: [0.98 1.32 0.94 0.77 0.07 0.51 0.01 0.   4.61] Loss_P: [2.35 1.81 1.75 1.01 0.83 0.05 0.5  0.01 8.3 ]\n",
      "Loss_Q: [1.01 1.27 0.96 0.77 0.06 0.5  0.01 0.   4.57] Loss_P: [2.31 1.89 1.75 1.   0.81 0.07 0.51 0.01 8.36]\n",
      "Loss_Q: [0.94 1.31 0.94 0.76 0.05 0.51 0.01 0.   4.51] Loss_P: [2.42 1.8  1.79 1.03 0.8  0.06 0.51 0.01 8.42]\n",
      "Loss_Q: [1.05 1.26 0.93 0.75 0.05 0.5  0.01 0.   4.54] Loss_P: [2.39 1.82 1.81 1.05 0.75 0.05 0.51 0.01 8.4 ]\n",
      "Loss_Q: [1.02 1.33 0.98 0.82 0.05 0.5  0.01 0.   4.71] Loss_P: [2.32 1.91 1.83 1.08 0.79 0.07 0.5  0.01 8.51]\n",
      "Loss_Q: [1.   1.28 0.95 0.78 0.04 0.49 0.01 0.   4.57] Loss_P: [2.36 1.82 1.78 1.05 0.81 0.04 0.49 0.01 8.35]\n",
      "Loss_Q: [0.98 1.31 1.01 0.78 0.07 0.5  0.02 0.   4.65] Loss_P: [2.33 1.85 1.77 1.08 0.77 0.05 0.5  0.01 8.36]\n",
      "Loss_Q: [1.02 1.28 0.98 0.77 0.04 0.5  0.01 0.   4.6 ] Loss_P: [2.36 1.76 1.82 1.11 0.77 0.05 0.5  0.01 8.38]\n",
      "Loss_Q: [0.97 1.24 0.93 0.77 0.05 0.5  0.01 0.   4.47] Loss_P: [2.29 1.82 1.84 1.07 0.8  0.05 0.5  0.02 8.4 ]\n",
      "Loss_Q: [0.99 1.27 0.96 0.74 0.05 0.52 0.01 0.   4.55] Loss_P: [2.34 1.79 1.76 1.09 0.75 0.05 0.51 0.01 8.29]\n",
      "Loss_Q: [1.01 1.32 0.97 0.69 0.05 0.51 0.01 0.   4.55] Loss_P: [2.32 1.88 1.77 1.09 0.79 0.06 0.5  0.01 8.42]\n",
      "Loss_Q: [0.99 1.22 0.94 0.74 0.05 0.51 0.01 0.   4.45] Loss_P: [2.29 1.81 1.78 1.07 0.79 0.05 0.5  0.02 8.31]\n",
      "Loss_Q: [1.04 1.3  0.94 0.8  0.08 0.51 0.01 0.   4.68] Loss_P: [2.4  1.73 1.85 1.1  0.83 0.05 0.51 0.01 8.47]\n",
      "Loss_Q: [0.88 1.32 0.92 0.76 0.05 0.5  0.01 0.   4.44] Loss_P: [2.38 1.82 1.83 1.05 0.77 0.06 0.5  0.01 8.42]\n",
      "Loss_Q: [0.97 1.29 0.92 0.79 0.06 0.5  0.01 0.   4.53] Loss_P: [2.34 1.81 1.8  1.01 0.74 0.06 0.5  0.01 8.27]\n",
      "Loss_Q: [0.98 1.29 0.87 0.79 0.08 0.51 0.01 0.   4.51] Loss_P: [2.34 1.86 1.77 1.06 0.83 0.07 0.51 0.01 8.44]\n",
      "Loss_Q: [0.96 1.22 0.91 0.77 0.07 0.5  0.01 0.   4.44] Loss_P: [2.35 1.78 1.76 0.99 0.74 0.08 0.51 0.01 8.23]\n",
      "Loss_Q: [0.96 1.28 0.9  0.8  0.07 0.5  0.01 0.   4.53] Loss_P: [2.34 1.81 1.77 0.98 0.78 0.07 0.51 0.01 8.26]\n",
      "Loss_Q: [0.99 1.27 0.88 0.7  0.06 0.51 0.01 0.   4.42] Loss_P: [2.37 1.82 1.71 0.99 0.76 0.06 0.5  0.01 8.22]\n",
      "Loss_Q: [0.94 1.32 0.89 0.74 0.07 0.5  0.01 0.   4.46] Loss_P: [2.4  1.76 1.79 1.03 0.79 0.08 0.5  0.01 8.37]\n",
      "Loss_Q: [0.98 1.23 0.93 0.74 0.07 0.49 0.01 0.   4.45] Loss_P: [2.3  1.83 1.75 1.05 0.81 0.06 0.5  0.01 8.31]\n",
      "Loss_Q: [0.98 1.32 0.88 0.72 0.06 0.49 0.01 0.   4.45] Loss_P: [2.31 1.86 1.77 0.98 0.77 0.07 0.49 0.01 8.25]\n",
      "Loss_Q: [0.91 1.26 0.9  0.73 0.07 0.49 0.02 0.   4.39] Loss_P: [2.34 1.9  1.75 1.01 0.79 0.03 0.48 0.01 8.31]\n",
      "Loss_Q: [0.89 1.21 0.89 0.75 0.05 0.49 0.01 0.   4.29] Loss_P: [2.35 1.81 1.74 0.97 0.77 0.07 0.48 0.01 8.19]\n",
      "Loss_Q: [1.11 1.28 0.91 0.76 0.04 0.48 0.02 0.   4.59] Loss_P: [2.38 1.87 1.71 1.02 0.81 0.06 0.48 0.01 8.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.93 1.19 0.93 0.79 0.05 0.5  0.01 0.   4.41] Loss_P: [2.32 1.89 1.69 1.03 0.79 0.04 0.48 0.01 8.25]\n",
      "Loss_Q: [0.99 1.29 0.95 0.78 0.06 0.47 0.01 0.   4.55] Loss_P: [2.36 1.87 1.69 1.04 0.8  0.06 0.49 0.01 8.32]\n",
      "Loss_Q: [0.94 1.19 0.95 0.81 0.06 0.49 0.01 0.   4.45] Loss_P: [2.29 1.92 1.7  1.07 0.89 0.06 0.48 0.01 8.42]\n",
      "Loss_Q: [1.06 1.24 0.95 0.74 0.06 0.51 0.02 0.   4.57] Loss_P: [2.31 1.87 1.7  1.1  0.79 0.06 0.49 0.01 8.32]\n",
      "Loss_Q: [1.02 1.2  0.98 0.78 0.07 0.5  0.01 0.   4.56] Loss_P: [2.4  1.89 1.68 1.09 0.82 0.07 0.51 0.01 8.48]\n",
      "Loss_Q: [0.93 1.21 1.01 0.74 0.05 0.5  0.02 0.   4.45] Loss_P: [2.32 1.88 1.64 1.08 0.79 0.06 0.5  0.01 8.29]\n",
      "Loss_Q: [0.98 1.2  0.97 0.79 0.07 0.5  0.01 0.   4.51] Loss_P: [2.38 1.87 1.65 1.03 0.73 0.05 0.51 0.01 8.24]\n",
      "Loss_Q: [0.92 1.18 0.96 0.78 0.05 0.51 0.01 0.   4.4 ] Loss_P: [2.36 1.87 1.62 1.07 0.8  0.06 0.52 0.01 8.3 ]\n",
      "Loss_Q: [1.   1.17 1.01 0.77 0.05 0.5  0.02 0.   4.53] Loss_P: [2.34 1.88 1.6  1.07 0.77 0.06 0.51 0.02 8.25]\n",
      "Loss_Q: [0.98 1.18 0.97 0.78 0.06 0.5  0.01 0.   4.48] Loss_P: [2.31 1.96 1.63 1.03 0.8  0.04 0.51 0.01 8.29]\n",
      "Loss_Q: [1.   1.17 1.   0.79 0.04 0.5  0.01 0.   4.51] Loss_P: [2.35 1.92 1.56 1.07 0.77 0.07 0.51 0.01 8.27]\n",
      "Loss_Q: [0.98 1.14 0.98 0.84 0.06 0.5  0.01 0.   4.5 ] Loss_P: [2.26 1.91 1.65 1.12 0.86 0.05 0.51 0.01 8.36]\n",
      "Loss_Q: [1.   1.18 1.01 0.8  0.07 0.5  0.02 0.   4.57] Loss_P: [2.29 1.94 1.6  1.1  0.84 0.05 0.51 0.02 8.36]\n",
      "Loss_Q: [0.95 1.18 1.02 0.81 0.05 0.49 0.01 0.   4.51] Loss_P: [2.38 1.87 1.61 1.09 0.82 0.07 0.5  0.01 8.36]\n",
      "Loss_Q: [1.06 1.18 1.03 0.77 0.06 0.51 0.01 0.   4.62] Loss_P: [2.32 1.97 1.61 1.12 0.78 0.08 0.5  0.01 8.39]\n",
      "Loss_Q: [0.98 1.19 0.99 0.8  0.06 0.48 0.02 0.   4.53] Loss_P: [2.32 1.96 1.64 1.1  0.82 0.06 0.48 0.01 8.38]\n",
      "Loss_Q: [1.05 1.19 0.98 0.82 0.04 0.47 0.01 0.   4.57] Loss_P: [2.32 1.86 1.74 1.13 0.84 0.06 0.49 0.01 8.44]\n",
      "Loss_Q: [1.07 1.21 0.97 0.83 0.05 0.47 0.01 0.   4.61] Loss_P: [2.35 1.92 1.71 1.13 0.8  0.04 0.51 0.02 8.48]\n",
      "Loss_Q: [1.05 1.21 1.04 0.82 0.06 0.48 0.01 0.   4.67] Loss_P: [2.32 1.96 1.61 1.17 0.89 0.05 0.49 0.01 8.49]\n",
      "Loss_Q: [0.95 1.2  1.04 0.74 0.05 0.49 0.01 0.   4.48] Loss_P: [2.32 1.89 1.7  1.17 0.85 0.05 0.49 0.01 8.5 ]\n",
      "Loss_Q: [1.04 1.2  0.99 0.81 0.04 0.48 0.02 0.   4.57] Loss_P: [2.35 1.88 1.66 1.13 0.8  0.06 0.47 0.01 8.37]\n",
      "Loss_Q: [1.03 1.14 1.03 0.79 0.05 0.46 0.01 0.   4.52] Loss_P: [2.34 1.97 1.62 1.16 0.85 0.06 0.47 0.01 8.49]\n",
      "Loss_Q: [1.09 1.18 1.03 0.77 0.05 0.47 0.01 0.   4.61] Loss_P: [2.29 1.95 1.6  1.15 0.78 0.05 0.48 0.01 8.29]\n",
      "Loss_Q: [1.01 1.17 0.99 0.75 0.05 0.49 0.02 0.   4.47] Loss_P: [2.32 1.95 1.61 1.15 0.82 0.05 0.48 0.01 8.4 ]\n",
      "Loss_Q: [1.01 1.18 1.04 0.76 0.06 0.48 0.02 0.   4.55] Loss_P: [2.3  1.91 1.59 1.19 0.84 0.08 0.48 0.01 8.4 ]\n",
      "Loss_Q: [1.01 1.19 1.04 0.78 0.05 0.47 0.01 0.   4.55] Loss_P: [2.29 1.92 1.56 1.18 0.81 0.04 0.48 0.01 8.28]\n",
      "Loss_Q: [1.02 1.12 1.05 0.75 0.04 0.47 0.01 0.   4.47] Loss_P: [2.29 1.95 1.55 1.21 0.8  0.06 0.48 0.01 8.34]\n",
      "Loss_Q: [1.03 1.18 1.05 0.8  0.07 0.47 0.01 0.   4.61] Loss_P: [2.34 1.89 1.58 1.19 0.75 0.05 0.48 0.01 8.3 ]\n",
      "Loss_Q: [1.01 1.16 1.06 0.76 0.05 0.46 0.01 0.   4.52] Loss_P: [2.33 1.86 1.53 1.19 0.81 0.04 0.45 0.01 8.22]\n",
      "Loss_Q: [1.   1.15 1.   0.73 0.04 0.46 0.01 0.   4.39] Loss_P: [2.35 1.95 1.62 1.17 0.88 0.05 0.46 0.03 8.5 ]\n",
      "Loss_Q: [0.99 1.13 1.07 0.74 0.05 0.44 0.01 0.   4.43] Loss_P: [2.42 1.89 1.55 1.17 0.79 0.06 0.44 0.02 8.34]\n",
      "Loss_Q: [1.   1.13 1.04 0.78 0.07 0.46 0.02 0.   4.49] Loss_P: [2.37 1.9  1.56 1.16 0.77 0.06 0.44 0.01 8.28]\n",
      "Loss_Q: [0.95 1.14 1.02 0.78 0.06 0.44 0.01 0.   4.41] Loss_P: [2.37 1.92 1.5  1.17 0.81 0.06 0.49 0.01 8.32]\n",
      "Loss_Q: [0.92 1.16 1.01 0.75 0.05 0.48 0.01 0.   4.37] Loss_P: [2.35 1.9  1.54 1.13 0.79 0.07 0.46 0.01 8.25]\n",
      "Loss_Q: [0.91 1.08 1.03 0.74 0.05 0.47 0.01 0.   4.28] Loss_P: [2.33 1.89 1.51 1.13 0.76 0.05 0.47 0.01 8.14]\n",
      "Loss_Q: [0.97 1.19 1.04 0.78 0.05 0.46 0.02 0.   4.51] Loss_P: [2.37 1.91 1.49 1.13 0.83 0.05 0.47 0.01 8.26]\n",
      "Loss_Q: [0.91 1.12 1.03 0.77 0.05 0.47 0.01 0.   4.37] Loss_P: [2.4  1.86 1.57 1.16 0.81 0.04 0.46 0.02 8.31]\n",
      "Loss_Q: [0.99 1.15 1.02 0.78 0.04 0.45 0.01 0.   4.43] Loss_P: [2.42 1.81 1.61 1.12 0.78 0.05 0.45 0.01 8.25]\n",
      "Loss_Q: [0.96 1.22 1.   0.77 0.05 0.45 0.01 0.   4.45] Loss_P: [2.36 1.89 1.56 1.14 0.77 0.06 0.44 0.01 8.23]\n",
      "Loss_Q: [1.01 1.16 0.98 0.81 0.05 0.42 0.01 0.   4.43] Loss_P: [2.36 1.92 1.57 1.14 0.84 0.05 0.42 0.02 8.31]\n",
      "Loss_Q: [0.99 1.18 0.99 0.81 0.06 0.42 0.01 0.   4.46] Loss_P: [2.34 1.91 1.62 1.15 0.8  0.07 0.43 0.01 8.34]\n",
      "Loss_Q: [0.99 1.19 1.   0.82 0.05 0.44 0.01 0.   4.5 ] Loss_P: [2.35 1.95 1.56 1.18 0.86 0.06 0.48 0.01 8.44]\n",
      "Loss_Q: [0.97 1.17 1.04 0.84 0.05 0.45 0.01 0.   4.52] Loss_P: [2.34 1.94 1.59 1.13 0.85 0.05 0.46 0.01 8.37]\n",
      "Loss_Q: [0.95 1.21 0.98 0.8  0.05 0.48 0.02 0.   4.48] Loss_P: [2.3  1.99 1.54 1.11 0.88 0.06 0.46 0.01 8.34]\n",
      "Loss_Q: [1.05 1.16 1.   0.88 0.06 0.45 0.02 0.   4.62] Loss_P: [2.43 1.93 1.61 1.13 0.83 0.07 0.47 0.01 8.47]\n",
      "Loss_Q: [0.95 1.17 1.02 0.8  0.04 0.45 0.01 0.   4.43] Loss_P: [2.38 1.9  1.54 1.16 0.86 0.06 0.45 0.01 8.36]\n",
      "Loss_Q: [0.99 1.17 1.03 0.84 0.05 0.43 0.01 0.   4.51] Loss_P: [2.36 1.89 1.54 1.19 0.88 0.06 0.43 0.01 8.35]\n",
      "Loss_Q: [0.95 1.11 1.01 0.77 0.05 0.43 0.01 0.   4.32] Loss_P: [2.38 1.83 1.61 1.15 0.83 0.06 0.42 0.01 8.29]\n",
      "Loss_Q: [0.93 1.18 1.02 0.81 0.06 0.44 0.01 0.   4.46] Loss_P: [2.34 1.87 1.6  1.19 0.86 0.06 0.42 0.01 8.35]\n",
      "Loss_Q: [0.96 1.21 1.05 0.86 0.05 0.39 0.01 0.   4.52] Loss_P: [2.31 1.86 1.62 1.19 0.87 0.05 0.4  0.02 8.32]\n",
      "Loss_Q: [0.97 1.21 1.01 0.86 0.05 0.38 0.02 0.   4.5 ] Loss_P: [2.35 1.87 1.63 1.15 0.86 0.05 0.42 0.01 8.34]\n",
      "Loss_Q: [1.   1.21 1.03 0.87 0.06 0.38 0.01 0.   4.56] Loss_P: [2.33 1.92 1.6  1.17 0.84 0.06 0.38 0.01 8.3 ]\n",
      "Loss_Q: [1.02 1.19 0.99 0.89 0.06 0.38 0.01 0.   4.55] Loss_P: [2.3  1.94 1.59 1.17 0.88 0.05 0.39 0.01 8.33]\n",
      "Loss_Q: [1.06 1.29 1.06 0.86 0.05 0.42 0.01 0.   4.75] Loss_P: [2.33 1.92 1.63 1.17 0.89 0.06 0.41 0.01 8.41]\n",
      "Loss_Q: [1.05 1.24 1.02 0.84 0.04 0.41 0.01 0.   4.61] Loss_P: [2.43 1.79 1.71 1.16 0.88 0.05 0.44 0.01 8.47]\n",
      "Loss_Q: [1.01 1.2  1.03 0.84 0.06 0.41 0.01 0.   4.55] Loss_P: [2.34 1.94 1.6  1.18 0.85 0.05 0.41 0.01 8.37]\n",
      "Loss_Q: [0.96 1.2  1.05 0.85 0.06 0.38 0.01 0.   4.52] Loss_P: [2.3  1.99 1.65 1.19 0.85 0.05 0.4  0.01 8.44]\n",
      "Loss_Q: [0.96 1.16 1.03 0.84 0.05 0.39 0.01 0.   4.44] Loss_P: [2.32 1.92 1.66 1.15 0.86 0.06 0.38 0.01 8.36]\n",
      "Loss_Q: [1.   1.26 1.   0.84 0.06 0.37 0.01 0.   4.54] Loss_P: [2.34 1.94 1.63 1.22 0.86 0.05 0.4  0.01 8.45]\n",
      "Loss_Q: [0.99 1.2  1.01 0.83 0.05 0.36 0.01 0.   4.46] Loss_P: [2.34 1.94 1.62 1.2  0.89 0.07 0.36 0.01 8.43]\n",
      "Loss_Q: [1.03 1.25 1.01 0.83 0.05 0.4  0.02 0.   4.58] Loss_P: [2.34 1.91 1.66 1.19 0.87 0.07 0.38 0.01 8.42]\n",
      "Loss_Q: [1.03 1.17 1.02 0.84 0.05 0.38 0.01 0.   4.5 ] Loss_P: [2.31 1.95 1.59 1.2  0.83 0.04 0.38 0.01 8.32]\n",
      "Loss_Q: [1.08 1.15 0.98 0.82 0.06 0.38 0.01 0.   4.48] Loss_P: [2.32 1.96 1.59 1.16 0.87 0.05 0.38 0.01 8.35]\n",
      "Loss_Q: [1.03 1.18 1.01 0.82 0.04 0.4  0.01 0.   4.51] Loss_P: [2.31 1.92 1.61 1.19 0.84 0.07 0.41 0.02 8.36]\n",
      "Loss_Q: [1.09 1.17 1.05 0.83 0.05 0.44 0.01 0.   4.65] Loss_P: [2.33 1.97 1.63 1.2  0.87 0.05 0.42 0.01 8.47]\n",
      "Loss_Q: [1.04 1.24 1.06 0.81 0.07 0.42 0.01 0.   4.64] Loss_P: [2.31 1.92 1.7  1.2  0.82 0.05 0.4  0.02 8.42]\n",
      "Loss_Q: [1.01 1.22 1.01 0.8  0.04 0.4  0.03 0.   4.5 ] Loss_P: [2.31 1.96 1.64 1.2  0.83 0.06 0.42 0.01 8.43]\n",
      "Loss_Q: [1.   1.2  1.04 0.84 0.08 0.44 0.01 0.   4.62] Loss_P: [2.36 1.88 1.69 1.2  0.82 0.06 0.41 0.02 8.43]\n",
      "Loss_Q: [0.95 1.25 1.06 0.82 0.05 0.41 0.01 0.   4.55] Loss_P: [2.32 1.94 1.67 1.19 0.84 0.06 0.42 0.01 8.45]\n",
      "Loss_Q: [1.02 1.14 1.01 0.83 0.05 0.4  0.01 0.   4.47] Loss_P: [2.36 1.93 1.67 1.22 0.86 0.06 0.41 0.01 8.53]\n",
      "Loss_Q: [0.99 1.19 1.09 0.78 0.06 0.44 0.01 0.   4.57] Loss_P: [2.31 1.85 1.66 1.21 0.84 0.08 0.43 0.01 8.4 ]\n",
      "Loss_Q: [0.98 1.27 1.03 0.8  0.07 0.41 0.01 0.   4.55] Loss_P: [2.3  1.8  1.6  1.19 0.85 0.05 0.43 0.01 8.23]\n",
      "Loss_Q: [1.06 1.2  1.03 0.77 0.07 0.43 0.01 0.   4.58] Loss_P: [2.31 1.91 1.65 1.15 0.8  0.05 0.44 0.01 8.34]\n",
      "Loss_Q: [0.99 1.21 1.02 0.81 0.05 0.4  0.01 0.   4.48] Loss_P: [2.31 1.88 1.67 1.21 0.79 0.07 0.37 0.01 8.3 ]\n",
      "Loss_Q: [1.02 1.18 1.08 0.81 0.06 0.39 0.02 0.   4.58] Loss_P: [2.34 1.9  1.73 1.18 0.81 0.07 0.41 0.02 8.47]\n",
      "Loss_Q: [1.02 1.16 1.02 0.78 0.05 0.39 0.02 0.   4.44] Loss_P: [2.32 1.86 1.66 1.17 0.82 0.08 0.41 0.01 8.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.   1.22 1.04 0.81 0.07 0.38 0.01 0.   4.53] Loss_P: [2.29 1.9  1.6  1.18 0.81 0.05 0.38 0.01 8.24]\n",
      "Loss_Q: [1.   1.22 0.98 0.8  0.06 0.37 0.01 0.   4.44] Loss_P: [2.32 1.87 1.59 1.19 0.83 0.06 0.37 0.01 8.24]\n",
      "Loss_Q: [1.06 1.23 0.99 0.81 0.07 0.41 0.01 0.   4.57] Loss_P: [2.35 1.84 1.65 1.15 0.86 0.04 0.39 0.01 8.29]\n",
      "Loss_Q: [1.   1.16 1.   0.78 0.05 0.35 0.01 0.   4.35] Loss_P: [2.34 1.94 1.66 1.14 0.83 0.05 0.38 0.02 8.35]\n",
      "Loss_Q: [0.95 1.2  1.03 0.8  0.06 0.38 0.01 0.   4.45] Loss_P: [2.33 1.89 1.68 1.11 0.82 0.05 0.38 0.01 8.26]\n",
      "Loss_Q: [0.98 1.18 0.96 0.73 0.04 0.38 0.02 0.   4.29] Loss_P: [2.27 1.85 1.65 1.16 0.77 0.04 0.37 0.01 8.12]\n",
      "Loss_Q: [1.02 1.19 1.02 0.76 0.05 0.37 0.02 0.   4.42] Loss_P: [2.29 1.89 1.71 1.14 0.75 0.05 0.36 0.02 8.21]\n",
      "Loss_Q: [1.09 1.16 1.02 0.72 0.06 0.35 0.01 0.   4.4 ] Loss_P: [2.3  1.93 1.7  1.15 0.74 0.06 0.34 0.01 8.22]\n",
      "Loss_Q: [0.97 1.17 1.01 0.77 0.05 0.31 0.01 0.   4.29] Loss_P: [2.33 1.9  1.69 1.15 0.77 0.04 0.31 0.02 8.21]\n",
      "Loss_Q: [0.95 1.21 1.01 0.74 0.05 0.36 0.01 0.   4.34] Loss_P: [2.3  1.89 1.68 1.16 0.82 0.05 0.36 0.01 8.28]\n",
      "Loss_Q: [1.05 1.17 0.98 0.73 0.04 0.32 0.01 0.   4.3 ] Loss_P: [2.33 1.89 1.67 1.16 0.78 0.05 0.32 0.01 8.22]\n",
      "Loss_Q: [1.02 1.13 1.02 0.74 0.04 0.33 0.01 0.   4.31] Loss_P: [2.32 1.88 1.71 1.09 0.79 0.06 0.34 0.02 8.21]\n",
      "Loss_Q: [1.02 1.13 0.99 0.81 0.05 0.29 0.01 0.   4.31] Loss_P: [2.32 1.9  1.67 1.2  0.8  0.06 0.33 0.01 8.3 ]\n",
      "Loss_Q: [1.01 1.18 1.01 0.83 0.05 0.35 0.01 0.   4.45] Loss_P: [2.29 1.85 1.76 1.17 0.89 0.05 0.32 0.02 8.35]\n",
      "Loss_Q: [1.01 1.2  1.   0.85 0.05 0.3  0.01 0.   4.43] Loss_P: [2.3  1.97 1.77 1.11 0.84 0.06 0.32 0.01 8.38]\n",
      "Loss_Q: [1.01 1.26 0.97 0.83 0.07 0.35 0.01 0.   4.49] Loss_P: [2.31 1.89 1.73 1.13 0.88 0.05 0.33 0.01 8.34]\n",
      "Loss_Q: [1.03 1.14 0.96 0.81 0.07 0.33 0.01 0.   4.35] Loss_P: [2.28 1.95 1.77 1.16 0.88 0.05 0.32 0.02 8.43]\n",
      "Loss_Q: [1.05 1.25 1.01 0.85 0.07 0.32 0.01 0.   4.56] Loss_P: [2.28 1.92 1.74 1.16 0.87 0.08 0.34 0.02 8.4 ]\n",
      "Loss_Q: [1.05 1.22 0.98 0.86 0.06 0.29 0.01 0.   4.47] Loss_P: [2.3  1.9  1.72 1.17 0.83 0.04 0.34 0.01 8.31]\n",
      "Loss_Q: [0.94 1.24 1.03 0.81 0.05 0.36 0.01 0.   4.46] Loss_P: [2.37 1.88 1.68 1.17 0.85 0.07 0.33 0.02 8.37]\n",
      "Loss_Q: [0.97 1.23 0.98 0.86 0.05 0.34 0.01 0.   4.45] Loss_P: [2.25 1.9  1.67 1.14 0.96 0.09 0.32 0.01 8.34]\n",
      "Loss_Q: [0.99 1.19 0.96 0.88 0.05 0.35 0.02 0.   4.43] Loss_P: [2.35 1.91 1.65 1.1  0.83 0.06 0.36 0.02 8.28]\n",
      "Loss_Q: [0.93 1.18 0.97 0.82 0.06 0.36 0.01 0.   4.33] Loss_P: [2.32 1.93 1.71 1.06 0.91 0.07 0.36 0.01 8.37]\n",
      "Loss_Q: [1.04 1.17 0.97 0.81 0.06 0.33 0.01 0.   4.39] Loss_P: [2.38 1.89 1.63 1.07 0.91 0.06 0.34 0.01 8.28]\n",
      "Loss_Q: [0.95 1.19 0.99 0.86 0.05 0.31 0.02 0.   4.37] Loss_P: [2.27 1.89 1.66 1.12 0.86 0.06 0.35 0.03 8.23]\n",
      "Loss_Q: [1.05 1.2  1.01 0.87 0.06 0.32 0.01 0.   4.51] Loss_P: [2.33 1.9  1.66 1.13 0.84 0.06 0.34 0.01 8.28]\n",
      "Loss_Q: [0.95 1.15 1.03 0.8  0.05 0.32 0.01 0.   4.31] Loss_P: [2.28 1.96 1.6  1.14 0.88 0.07 0.32 0.01 8.27]\n",
      "Loss_Q: [1.03 1.15 1.   0.79 0.06 0.36 0.01 0.   4.4 ] Loss_P: [2.31 1.91 1.62 1.14 0.85 0.05 0.32 0.01 8.22]\n",
      "Loss_Q: [1.03 1.21 0.99 0.82 0.05 0.35 0.01 0.   4.44] Loss_P: [2.33 1.89 1.6  1.11 0.82 0.07 0.35 0.01 8.18]\n",
      "Loss_Q: [1.05 1.17 1.01 0.87 0.06 0.35 0.02 0.   4.53] Loss_P: [2.35 1.85 1.65 1.1  0.92 0.06 0.36 0.01 8.28]\n",
      "Loss_Q: [0.98 1.15 0.99 0.88 0.07 0.35 0.01 0.   4.44] Loss_P: [2.33 1.89 1.64 1.14 0.94 0.06 0.36 0.01 8.38]\n",
      "Loss_Q: [0.99 1.22 0.99 0.93 0.05 0.39 0.01 0.   4.57] Loss_P: [2.42 1.85 1.65 1.13 0.93 0.07 0.34 0.01 8.41]\n",
      "Loss_Q: [0.94 1.19 0.99 0.92 0.08 0.38 0.01 0.   4.52] Loss_P: [2.28 1.91 1.62 1.13 0.89 0.06 0.34 0.01 8.24]\n",
      "Loss_Q: [0.98 1.19 0.97 0.86 0.05 0.33 0.02 0.   4.41] Loss_P: [2.32 1.87 1.6  1.1  0.92 0.08 0.33 0.01 8.24]\n",
      "Loss_Q: [0.95 1.09 1.04 0.88 0.07 0.36 0.02 0.   4.4 ] Loss_P: [2.27 1.86 1.64 1.15 0.88 0.05 0.33 0.01 8.2 ]\n",
      "Loss_Q: [0.91 1.12 1.02 0.8  0.08 0.39 0.01 0.   4.34] Loss_P: [2.31 1.91 1.54 1.11 0.84 0.09 0.37 0.01 8.18]\n",
      "Loss_Q: [0.95 1.11 0.99 0.89 0.07 0.33 0.01 0.   4.36] Loss_P: [2.33 1.83 1.6  1.11 0.87 0.08 0.35 0.01 8.17]\n",
      "Loss_Q: [0.96 1.12 0.99 0.85 0.07 0.4  0.01 0.   4.4 ] Loss_P: [2.3  1.96 1.56 1.14 0.84 0.07 0.37 0.01 8.25]\n",
      "Loss_Q: [0.98 1.12 1.01 0.85 0.06 0.36 0.02 0.   4.4 ] Loss_P: [2.29 1.95 1.62 1.12 0.91 0.08 0.37 0.01 8.36]\n",
      "Loss_Q: [0.93 1.17 1.   0.83 0.09 0.36 0.01 0.   4.38] Loss_P: [2.28 1.94 1.56 1.1  0.93 0.08 0.36 0.01 8.26]\n",
      "Loss_Q: [0.99 1.18 1.   0.88 0.07 0.37 0.01 0.   4.49] Loss_P: [2.36 1.87 1.59 1.07 0.87 0.07 0.33 0.01 8.17]\n",
      "Loss_Q: [0.94 1.18 1.   0.88 0.08 0.39 0.02 0.   4.49] Loss_P: [2.32 1.94 1.56 1.09 0.91 0.06 0.36 0.01 8.25]\n",
      "Loss_Q: [0.96 1.16 0.99 0.83 0.07 0.39 0.01 0.   4.42] Loss_P: [2.32 1.91 1.62 1.14 0.9  0.1  0.35 0.01 8.34]\n",
      "Loss_Q: [0.96 1.22 1.   0.87 0.07 0.37 0.01 0.   4.5 ] Loss_P: [2.3  1.94 1.63 1.08 0.91 0.08 0.37 0.02 8.33]\n",
      "Loss_Q: [0.92 1.19 0.94 0.9  0.07 0.37 0.01 0.   4.39] Loss_P: [2.3  1.96 1.58 1.07 0.89 0.06 0.33 0.01 8.2 ]\n",
      "Loss_Q: [0.98 1.17 0.96 0.86 0.07 0.35 0.01 0.   4.39] Loss_P: [2.36 1.87 1.64 1.08 0.91 0.08 0.38 0.01 8.32]\n",
      "Loss_Q: [0.92 1.18 0.98 0.84 0.07 0.4  0.01 0.   4.41] Loss_P: [2.32 1.87 1.58 1.09 0.91 0.09 0.39 0.01 8.26]\n",
      "Loss_Q: [0.86 1.28 1.   0.88 0.08 0.43 0.01 0.   4.54] Loss_P: [2.33 1.87 1.58 1.11 0.91 0.07 0.4  0.01 8.28]\n",
      "Loss_Q: [0.92 1.22 0.95 0.92 0.07 0.41 0.01 0.   4.49] Loss_P: [2.32 1.96 1.58 1.1  0.92 0.08 0.41 0.01 8.39]\n",
      "Loss_Q: [0.86 1.21 0.96 0.87 0.07 0.41 0.01 0.   4.4 ] Loss_P: [2.33 1.85 1.6  1.1  0.97 0.1  0.41 0.01 8.37]\n",
      "Loss_Q: [0.99 1.27 0.96 0.92 0.09 0.42 0.01 0.   4.67] Loss_P: [2.33 1.85 1.63 1.06 0.92 0.11 0.44 0.01 8.36]\n",
      "Loss_Q: [0.86 1.19 0.95 0.9  0.08 0.4  0.01 0.   4.39] Loss_P: [2.27 1.93 1.56 1.09 0.92 0.1  0.41 0.01 8.29]\n",
      "Loss_Q: [0.83 1.22 0.99 0.9  0.1  0.42 0.01 0.   4.48] Loss_P: [2.33 1.97 1.61 1.1  0.92 0.1  0.42 0.02 8.47]\n",
      "Loss_Q: [1.   1.23 0.93 0.9  0.09 0.42 0.02 0.   4.6 ] Loss_P: [2.29 1.92 1.6  1.11 0.92 0.08 0.4  0.01 8.31]\n",
      "Loss_Q: [0.9  1.23 1.   0.93 0.09 0.4  0.02 0.   4.58] Loss_P: [2.33 1.9  1.55 1.09 0.89 0.08 0.39 0.02 8.25]\n",
      "Loss_Q: [0.92 1.16 0.95 0.86 0.08 0.41 0.01 0.   4.4 ] Loss_P: [2.3  2.01 1.55 1.06 0.89 0.07 0.42 0.02 8.32]\n",
      "Loss_Q: [0.92 1.24 0.99 0.91 0.1  0.4  0.02 0.   4.59] Loss_P: [2.32 1.88 1.59 1.09 0.87 0.09 0.41 0.01 8.26]\n",
      "Loss_Q: [0.94 1.25 0.97 0.91 0.08 0.46 0.01 0.   4.62] Loss_P: [2.36 1.87 1.56 1.13 0.91 0.06 0.43 0.01 8.34]\n",
      "Loss_Q: [0.89 1.22 0.99 0.91 0.08 0.41 0.01 0.   4.5 ] Loss_P: [2.33 1.85 1.67 1.11 0.92 0.08 0.42 0.01 8.4 ]\n",
      "Loss_Q: [0.91 1.21 0.95 0.89 0.1  0.4  0.01 0.   4.47] Loss_P: [2.36 1.87 1.59 1.08 0.89 0.07 0.43 0.01 8.3 ]\n",
      "Loss_Q: [0.93 1.29 0.96 0.85 0.11 0.46 0.01 0.   4.62] Loss_P: [2.35 1.82 1.68 1.13 0.9  0.1  0.45 0.01 8.45]\n",
      "Loss_Q: [1.   1.17 0.95 0.86 0.1  0.44 0.01 0.   4.52] Loss_P: [2.37 1.82 1.64 1.1  0.88 0.1  0.43 0.02 8.37]\n",
      "Loss_Q: [0.91 1.2  0.98 0.79 0.08 0.43 0.01 0.   4.39] Loss_P: [2.3  1.91 1.63 1.14 0.84 0.1  0.44 0.01 8.37]\n",
      "Loss_Q: [0.89 1.29 0.98 0.83 0.1  0.45 0.01 0.   4.54] Loss_P: [2.33 1.84 1.69 1.12 0.83 0.09 0.44 0.01 8.35]\n",
      "Loss_Q: [0.9  1.29 1.   0.86 0.07 0.42 0.02 0.   4.57] Loss_P: [2.34 1.87 1.58 1.1  0.86 0.09 0.43 0.01 8.28]\n",
      "Loss_Q: [0.98 1.25 0.99 0.87 0.07 0.45 0.01 0.   4.61] Loss_P: [2.33 1.9  1.61 1.18 0.89 0.09 0.43 0.01 8.44]\n",
      "Loss_Q: [0.9  1.27 0.97 0.83 0.06 0.44 0.01 0.   4.48] Loss_P: [2.33 1.84 1.64 1.13 0.86 0.08 0.44 0.01 8.34]\n",
      "Loss_Q: [0.95 1.25 0.99 0.85 0.09 0.42 0.01 0.   4.57] Loss_P: [2.3  1.89 1.71 1.15 0.81 0.08 0.46 0.01 8.41]\n",
      "Loss_Q: [0.9  1.25 1.   0.85 0.09 0.45 0.01 0.   4.54] Loss_P: [2.33 1.85 1.69 1.13 0.91 0.1  0.46 0.02 8.47]\n",
      "Loss_Q: [0.94 1.3  1.   0.88 0.09 0.45 0.02 0.   4.68] Loss_P: [2.33 1.8  1.69 1.16 0.91 0.09 0.45 0.01 8.44]\n",
      "Loss_Q: [0.91 1.24 0.95 0.86 0.09 0.43 0.01 0.   4.49] Loss_P: [2.33 1.85 1.69 1.13 0.85 0.09 0.47 0.02 8.43]\n",
      "Loss_Q: [0.89 1.26 1.   0.86 0.08 0.45 0.01 0.   4.56] Loss_P: [2.33 1.85 1.65 1.14 0.93 0.13 0.47 0.02 8.51]\n",
      "Loss_Q: [0.99 1.28 0.95 0.86 0.1  0.47 0.   0.   4.65] Loss_P: [2.31 1.83 1.73 1.08 0.88 0.07 0.46 0.01 8.36]\n",
      "Loss_Q: [0.96 1.25 1.01 0.9  0.1  0.44 0.   0.   4.69] Loss_P: [2.4  1.88 1.71 1.12 0.88 0.08 0.46 0.01 8.54]\n",
      "Loss_Q: [0.93 1.3  0.95 0.87 0.1  0.44 0.01 0.   4.62] Loss_P: [2.34 1.83 1.71 1.15 0.93 0.12 0.46 0.03 8.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.94 1.26 1.01 0.87 0.1  0.45 0.01 0.   4.64] Loss_P: [2.33 1.82 1.66 1.11 0.95 0.09 0.45 0.02 8.43]\n",
      "Loss_Q: [0.92 1.22 0.97 0.9  0.09 0.41 0.02 0.   4.52] Loss_P: [2.36 1.92 1.67 1.11 0.91 0.08 0.43 0.01 8.48]\n",
      "Loss_Q: [0.92 1.27 0.97 0.86 0.09 0.45 0.02 0.   4.58] Loss_P: [2.35 1.94 1.67 1.11 0.92 0.08 0.42 0.01 8.5 ]\n",
      "Loss_Q: [0.96 1.27 0.95 0.87 0.1  0.43 0.01 0.   4.59] Loss_P: [2.27 1.85 1.63 1.07 0.91 0.09 0.44 0.03 8.28]\n",
      "Loss_Q: [0.93 1.27 0.95 0.87 0.09 0.43 0.01 0.   4.56] Loss_P: [2.35 1.78 1.72 1.11 0.86 0.09 0.42 0.01 8.32]\n",
      "Loss_Q: [1.04 1.27 0.93 0.81 0.08 0.41 0.01 0.   4.56] Loss_P: [2.33 1.83 1.71 1.11 0.92 0.08 0.45 0.01 8.44]\n",
      "Loss_Q: [0.97 1.25 1.01 0.91 0.08 0.43 0.01 0.   4.65] Loss_P: [2.32 1.9  1.73 1.15 0.9  0.07 0.41 0.01 8.49]\n",
      "Loss_Q: [0.92 1.27 1.03 0.9  0.08 0.43 0.01 0.   4.64] Loss_P: [2.31 1.89 1.69 1.15 0.95 0.09 0.41 0.01 8.5 ]\n",
      "Loss_Q: [0.95 1.25 1.03 0.84 0.08 0.41 0.01 0.   4.55] Loss_P: [2.33 1.85 1.7  1.17 0.91 0.1  0.41 0.01 8.48]\n",
      "Loss_Q: [0.94 1.28 1.   0.87 0.08 0.42 0.01 0.   4.6 ] Loss_P: [2.37 1.83 1.68 1.15 0.96 0.1  0.41 0.02 8.52]\n",
      "Loss_Q: [0.93 1.23 1.   0.91 0.06 0.42 0.01 0.   4.56] Loss_P: [2.36 1.82 1.69 1.16 0.95 0.08 0.41 0.01 8.49]\n",
      "Loss_Q: [0.95 1.23 1.01 0.93 0.07 0.39 0.01 0.   4.59] Loss_P: [2.29 1.9  1.74 1.14 0.99 0.09 0.41 0.02 8.58]\n",
      "Loss_Q: [0.94 1.27 1.01 0.9  0.08 0.39 0.01 0.   4.61] Loss_P: [2.3  1.83 1.75 1.2  0.93 0.09 0.4  0.03 8.53]\n",
      "Loss_Q: [1.06 1.23 0.97 0.92 0.08 0.41 0.02 0.   4.69] Loss_P: [2.32 1.86 1.69 1.14 0.96 0.11 0.41 0.01 8.51]\n",
      "Loss_Q: [0.98 1.2  1.04 0.93 0.08 0.41 0.01 0.   4.65] Loss_P: [2.31 1.87 1.68 1.2  0.94 0.07 0.42 0.01 8.5 ]\n",
      "Loss_Q: [1.01 1.28 1.01 0.91 0.08 0.38 0.01 0.   4.69] Loss_P: [2.35 1.84 1.72 1.19 0.93 0.08 0.39 0.01 8.52]\n",
      "Loss_Q: [0.99 1.23 1.05 0.92 0.07 0.38 0.01 0.   4.65] Loss_P: [2.32 1.84 1.72 1.18 0.92 0.07 0.39 0.01 8.45]\n",
      "Loss_Q: [1.   1.25 0.98 0.91 0.07 0.38 0.01 0.   4.6 ] Loss_P: [2.38 1.8  1.75 1.17 0.95 0.09 0.37 0.01 8.52]\n",
      "Loss_Q: [1.   1.28 0.97 0.86 0.08 0.39 0.01 0.   4.58] Loss_P: [2.37 1.89 1.73 1.16 0.95 0.09 0.39 0.02 8.6 ]\n",
      "Loss_Q: [1.01 1.26 0.97 0.92 0.08 0.4  0.01 0.   4.67] Loss_P: [2.41 1.84 1.67 1.13 0.95 0.1  0.38 0.01 8.48]\n",
      "Loss_Q: [0.98 1.25 0.96 0.92 0.07 0.38 0.   0.   4.56] Loss_P: [2.35 1.87 1.63 1.14 0.92 0.07 0.36 0.01 8.35]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[948], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_data):\n\u001b[0;32m      6\u001b[0m     d0 \u001b[38;5;241m=\u001b[39m dataset[:,index[i]:index[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m     Alpha_Q \u001b[38;5;241m=\u001b[39m wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n\u001b[0;32m      8\u001b[0m     Theta,Loss_P \u001b[38;5;241m=\u001b[39m sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n\u001b[0;32m      9\u001b[0m     Alpha_P \u001b[38;5;241m=\u001b[39m sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m, in \u001b[0;36mwake_sample\u001b[1;34m(n_dz, d0, value_set, Phi, activation_type, bias)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         q \u001b[38;5;241m=\u001b[39m sigmoid(np\u001b[38;5;241m.\u001b[39mmatmul(phi[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],S))\n\u001b[1;32m---> 35\u001b[0m     S \u001b[38;5;241m=\u001b[39m ((q \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mlen\u001b[39m(q),\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\u001b[38;5;241m*\u001b[39m(a\u001b[38;5;241m-\u001b[39mb)\u001b[38;5;241m+\u001b[39mb   \u001b[38;5;66;03m# rejection sampling as a or b\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     Alpha_Q[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m S\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla\n",
    "\n",
    "Loss_Q: [3.84 2.99 1.47 0.   8.3 ] Loss_P: [ 4.47  3.87  2.99  1.48 12.8 ]\n",
    "\n",
    "Loss_Q: [1.15 1.04 0.5  0.   2.69] Loss_P: [2.7  2.21 1.27 0.55 6.72]\n",
    "\n",
    "Loss_Q: [0.6  0.2  0.03 0.   0.83] Loss_P: [2.56 1.91 0.26 0.03 4.77]\n",
    "\n",
    "Loss_Q: [1.35 0.45 0.02 0.   1.82] Loss_P: [2.33 2.8  0.56 0.02 5.72]\n",
    "\n",
    "Loss_Q: [0.86 0.86 0.03 0.   1.74] Loss_P: [2.28 2.19 1.08 0.02 5.57]\n",
    "\n",
    "Loss_Q: [1.29 0.62 0.04 0.   1.95] Loss_P: [2.24 2.72 0.81 0.03 5.81]\n",
    "\n",
    "Loss_Q: [1.11 0.73 0.06 0.   1.9 ] Loss_P: [2.31 2.44 0.99 0.03 5.77]\n",
    "\n",
    "Loss_Q: [1.19 0.65 0.03 0.   1.87] Loss_P: [2.22 2.55 0.96 0.04 5.76]\n",
    "\n",
    "Loss_Q: [1.39 0.6  0.02 0.   2.  ] Loss_P: [2.46 2.65 0.76 0.02 5.89]\n",
    "\n",
    "Loss_Q: [1.01 0.69 0.04 0.   1.74] Loss_P: [2.47 2.27 0.93 0.04 5.72]\n",
    "\n",
    "Loss_Q: [1.07 0.57 0.03 0.   1.66] Loss_P: [2.43 2.36 0.74 0.04 5.57]\n",
    "\n",
    "Loss_Q: [1.11 0.51 0.04 0.   1.67] Loss_P: [2.37 2.44 0.64 0.03 5.48]\n",
    "\n",
    "Loss_Q: [1.28 0.62 0.03 0.   1.93] Loss_P: [2.38 2.55 0.87 0.02 5.83]\n",
    "\n",
    "Loss_Q: [1.25 0.7  0.02 0.   1.98] Loss_P: [2.45 2.47 0.95 0.04 5.9 ]\n",
    "\n",
    "Loss_Q: [0.79 0.57 0.35 0.   1.7 ] Loss_P: [1.86 1.25 1.62 0.85 5.58]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- {1,-1}\n",
    "\n",
    "Loss_Q: [3.99 2.99 1.5  0.   8.48] Loss_P: [ 4.98  3.99  3.    1.5  13.48]\n",
    "\n",
    "Loss_Q: [1.59 1.78 0.94 0.   4.31] Loss_P: [2.28 2.65 2.33 1.32 8.58]\n",
    "\n",
    "Loss_Q: [1.01 1.2  0.64 0.   2.85] Loss_P: [1.84 1.93 1.95 1.34 7.06]\n",
    "\n",
    "Loss_Q: [1.15 1.37 0.44 0.   2.96] Loss_P: [1.9  2.46 1.64 1.26 7.26]\n",
    "\n",
    "Loss_Q: [1.03 1.11 0.44 0.   2.59] Loss_P: [1.8  1.88 1.84 1.15 6.67]\n",
    "\n",
    "Loss_Q: [0.84 0.67 0.56 0.   2.07] Loss_P: [1.68 1.6  1.73 1.03 6.03]\n",
    "\n",
    "Loss_Q: [0.78 0.59 0.49 0.   1.86] Loss_P: [1.75 1.42 1.76 0.92 5.84]\n",
    "\n",
    "Loss_Q: [0.72 0.59 0.35 0.   1.67] Loss_P: [1.78 1.36 1.56 0.96 5.66]\n",
    "\n",
    "Loss_Q: [0.94 0.58 0.34 0.   1.86] Loss_P: [1.94 1.42 1.45 0.99 5.79]\n",
    "\n",
    "Loss_Q: [0.9  0.56 0.36 0.   1.81] Loss_P: [1.88 1.36 1.63 0.94 5.81]\n",
    "\n",
    "Loss_Q: [0.84 0.48 0.34 0.   1.66] Loss_P: [1.78 1.27 1.78 0.68 5.52]\n",
    "\n",
    "Loss_Q: [0.75 0.41 0.35 0.   1.51] Loss_P: [1.82 1.31 1.43 0.78 5.35]\n",
    "\n",
    "Loss_Q: [0.83 0.52 0.36 0.   1.72] Loss_P: [1.81 1.51 1.53 0.76 5.61]\n",
    "\n",
    "Loss_Q: [0.81 0.47 0.35 0.   1.63] Loss_P: [1.71 1.56 1.63 0.68 5.58]\n",
    "\n",
    "Loss_Q: [0.78 0.66 0.32 0.   1.76] Loss_P: [1.8  1.6  1.59 0.66 5.66]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- data bias\n",
    "\n",
    "Loss_Q: [3.93 2.94 1.49 0.   8.37] Loss_P: [ 4.4   3.94  2.93  1.49 12.76]\n",
    "\n",
    "Loss_Q: [1.8  2.24 0.84 0.   4.88] Loss_P: [2.44 2.76 2.56 1.09 8.84]\n",
    "\n",
    "Loss_Q: [1.23 2.26 0.9  0.   4.39] Loss_P: [2.19 2.47 2.62 1.05 8.33]\n",
    "\n",
    "Loss_Q: [1.49 2.31 0.89 0.   4.69] Loss_P: [2.39 2.68 2.48 1.15 8.69]\n",
    "\n",
    "Loss_Q: [1.62 2.19 0.83 0.   4.64] Loss_P: [2.31 2.73 2.45 1.08 8.57]\n",
    "\n",
    "Loss_Q: [1.59 2.24 0.76 0.   4.59] Loss_P: [2.23 2.78 2.51 0.98 8.51]\n",
    "\n",
    "Loss_Q: [1.67 2.27 0.78 0.   4.72] Loss_P: [2.23 2.78 2.52 1.03 8.57]\n",
    "\n",
    "Loss_Q: [1.68 2.28 0.75 0.   4.7 ] Loss_P: [2.29 2.87 2.4  1.03 8.59]\n",
    "\n",
    "Loss_Q: [1.65 2.26 0.87 0.   4.79] Loss_P: [2.3  2.84 2.42 1.14 8.71]\n",
    "\n",
    "Loss_Q: [1.75 2.22 0.9  0.   4.88] Loss_P: [2.28 2.91 2.44 1.18 8.81]\n",
    "\n",
    "Loss_Q: [1.69 2.35 0.81 0.   4.85] Loss_P: [2.25 2.82 2.55 1.08 8.7 ]\n",
    "\n",
    "Loss_Q: [1.64 2.28 0.89 0.   4.81] Loss_P: [2.28 2.88 2.4  1.22 8.79]\n",
    "\n",
    "Loss_Q: [1.6  2.36 0.81 0.   4.77] Loss_P: [2.25 2.87 2.47 1.11 8.7 ]\n",
    "\n",
    "Loss_Q: [1.69 2.27 0.81 0.   4.77] Loss_P: [2.21 2.83 2.45 1.15 8.64]\n",
    "\n",
    "Loss_Q: [1.56 2.28 0.83 0.   4.67] Loss_P: [2.25 2.73 2.53 1.1  8.6 ]\n",
    "\n",
    "Loss_Q: [1.61 2.33 0.87 0.   4.81] Loss_P: [2.26 2.79 2.59 1.11 8.74]\n",
    "\n",
    "Loss_Q: [1.6  2.39 0.85 0.   4.84] Loss_P: [2.19 2.8  2.6  1.07 8.67]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- instantiation bias\n",
    "\n",
    "Loss_Q: [3.67 2.92 1.48 0.   8.07] Loss_P: [ 4.44  3.7   2.92  1.45 12.51]\n",
    "\n",
    "Loss_Q: [0.61 0.34 0.1  0.   1.05] Loss_P: [2.88 1.75 0.39 0.08 5.1 ]\n",
    "\n",
    "Loss_Q: [0.58 0.15 0.09 0.   0.83] Loss_P: [3.13 1.49 0.14 0.1  4.86]\n",
    "\n",
    "Loss_Q: [0.45 0.05 0.02 0.   0.52] Loss_P: [3.04 1.41 0.08 0.02 4.54]\n",
    "\n",
    "Loss_Q: [0.86 0.13 0.02 0.   1.01] Loss_P: [2.85 1.92 0.14 0.03 4.94]\n",
    "\n",
    "Loss_Q: [0.86 0.1  0.02 0.   0.98] Loss_P: [2.7  2.1  0.12 0.01 4.92]\n",
    "\n",
    "Loss_Q: [0.68 0.23 0.01 0.   0.92] Loss_P: [2.82 1.73 0.26 0.01 4.83]\n",
    "\n",
    "Loss_Q: [0.78 0.09 0.01 0.   0.88] Loss_P: [2.71 1.98 0.1  0.01 4.81]\n",
    "\n",
    "Loss_Q: [0.69 0.08 0.01 0.   0.79] Loss_P: [2.71 1.95 0.11 0.01 4.79]\n",
    "\n",
    "Loss_Q: [0.8  0.14 0.02 0.   0.96] Loss_P: [2.8  1.96 0.13 0.03 4.92]\n",
    "\n",
    "\n",
    "Loss_Q: [0.69 0.51 0.02 0.   1.22] Loss_P: [2.6  1.94 0.61 0.03 5.19]\n",
    "\n",
    "Loss_Q: [0.56 0.43 0.01 0.   1.  ] Loss_P: [2.49 1.87 0.54 0.01 4.91]\n",
    "\n",
    "Loss_Q: [0.62 0.31 0.01 0.   0.95] Loss_P: [2.63 1.76 0.55 0.   4.94]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- instantiation bias & data bias\n",
    "\n",
    "Loss_Q: [3.86 2.99 1.48 0.   8.32] Loss_P: [ 4.51  3.86  2.98  1.49 12.83]\n",
    "\n",
    "Loss_Q: [1.26 1.19 0.32 0.   2.77] Loss_P: [2.56 2.44 1.33 0.3  6.62]\n",
    "\n",
    "Loss_Q: [1.08 0.77 0.87 0.   2.72] Loss_P: [2.46 2.47 0.8  0.89 6.63]\n",
    "\n",
    "Loss_Q: [0.99 0.62 0.55 0.   2.16] Loss_P: [2.46 2.31 0.73 0.5  6.01]\n",
    "\n",
    "Loss_Q: [0.99 0.37 0.37 0.   1.73] Loss_P: [2.36 2.44 0.39 0.47 5.66]\n",
    "\n",
    "Loss_Q: [1.36 0.53 0.09 0.   1.98] Loss_P: [2.32 2.76 0.63 0.07 5.78]\n",
    "\n",
    "Loss_Q: [1.16 0.85 0.21 0.   2.22] Loss_P: [2.32 2.58 1.01 0.26 6.17]\n",
    "\n",
    "Loss_Q: [1.15 0.57 0.11 0.   1.82] Loss_P: [2.2  2.62 0.73 0.1  5.65]\n",
    "\n",
    "Loss_Q: [1.09 0.55 0.08 0.   1.73] Loss_P: [2.15 2.67 0.75 0.09 5.65]\n",
    "\n",
    "Loss_Q: [1.   0.56 0.12 0.   1.68] Loss_P: [2.28 2.42 0.73 0.13 5.55]\n",
    "\n",
    "Loss_Q: [0.94 0.65 0.11 0.   1.69] Loss_P: [2.14 2.55 0.79 0.13 5.6 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- add one more layer ([10,8,6,3,1] --> [10,8,7,5,3,1])\n",
    "\n",
    "Loss_Q: [ 3.95  3.5   2.49  1.49  0.   11.42] Loss_P: [ 4.52  3.97  3.49  2.49  1.48 15.95]\n",
    "\n",
    "Loss_Q: [0.96 0.93 0.66 0.1  0.   2.65] Loss_P: [2.59 2.17 0.97 0.69 0.1  6.52]\n",
    "\n",
    "Loss_Q: [0.89 0.9  0.59 0.1  0.   2.48] Loss_P: [2.42 2.29 0.93 0.61 0.08 6.34]\n",
    "\n",
    "Loss_Q: [0.82 0.92 0.38 0.07 0.   2.2 ] Loss_P: [2.55 2.12 1.05 0.4  0.08 6.2 ]\n",
    "\n",
    "Loss_Q: [0.73 1.02 0.28 0.05 0.   2.08] Loss_P: [2.43 1.89 1.28 0.27 0.07 5.95]\n",
    "\n",
    "Loss_Q: [0.69 0.92 0.43 0.07 0.   2.11] Loss_P: [2.37 1.91 1.08 0.5  0.08 5.94]\n",
    "\n",
    "Loss_Q: [0.63 0.87 0.32 0.07 0.   1.89] Loss_P: [2.23 1.98 1.12 0.32 0.05 5.7 ]\n",
    "\n",
    "Loss_Q: [0.74 1.05 0.18 0.07 0.   2.03] Loss_P: [2.35 1.85 1.34 0.18 0.05 5.77]\n",
    "\n",
    "Loss_Q: [0.69 0.78 0.53 0.07 0.   2.07] Loss_P: [2.34 1.94 0.99 0.62 0.07 5.96]\n",
    "\n",
    "Loss_Q: [0.59 1.15 0.58 0.1  0.   2.42] Loss_P: [2.37 1.8  1.34 0.63 0.08 6.22]\n",
    "\n",
    "Loss_Q: [0.67 0.78 0.34 0.06 0.   1.85] Loss_P: [2.31 1.9  0.98 0.34 0.09 5.62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- change instantiation layer to hidden layer ([10,8,6,3,1] --> [10,8,(7),5,3,1])\n",
    "\n",
    "Loss_Q: [3.98 2.49 1.49 0.   7.97] Loss_P: [ 4.56  3.98  2.49  1.5  12.53]\n",
    "\n",
    "Loss_Q: [1.48 1.21 0.61 0.   3.3 ] Loss_P: [2.6  2.8  1.25 0.7  7.35]\n",
    "\n",
    "Loss_Q: [1.19 2.47 1.26 0.   4.91] Loss_P: [2.49 2.6  2.44 1.27 8.8 ]\n",
    "\n",
    "Loss_Q: [1.22 2.49 1.33 0.   5.04] Loss_P: [2.33 2.77 2.43 1.37 8.9 ]\n",
    "\n",
    "Loss_Q: [1.07 2.49 1.33 0.   4.89] Loss_P: [2.29 2.69 2.45 1.4  8.82]\n",
    "\n",
    "Loss_Q: [1.14 2.49 1.44 0.   5.08] Loss_P: [2.36 2.66 2.45 1.47 8.94]\n",
    "\n",
    "Loss_Q: [1.22 2.49 1.37 0.   5.09] Loss_P: [2.39 2.76 2.43 1.46 9.04]\n",
    "\n",
    "Loss_Q: [1.2  2.5  1.19 0.   4.88] Loss_P: [2.42 2.67 2.44 1.22 8.75]\n",
    "\n",
    "Loss_Q: [0.8  2.5  1.35 0.   4.65] Loss_P: [2.49 2.12 2.45 1.4  8.46]\n",
    "\n",
    "Loss_Q: [1.09 2.5  1.46 0.   5.05] Loss_P: [2.34 2.67 2.47 1.5  8.99]\n",
    "\n",
    "Loss_Q: [1.   2.5  1.27 0.   4.77] Loss_P: [2.15 2.75 2.43 1.32 8.65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- add activation layer between each layer ([10,8,6,3,1] --> [10,(9),8,(7),6,(5),3,(2),1])\n",
    "\n",
    "Loss_Q: [3.69 2.86 1.4  0.   7.95] Loss_P: [ 4.25  3.68  2.88  1.38 12.18]\n",
    "\n",
    "Loss_Q: [1.11 0.34 0.03 0.   1.48] Loss_P: [3.78 1.55 0.46 0.03 5.82]\n",
    "\n",
    "Loss_Q: [1.04 0.63 0.06 0.   1.74] Loss_P: [3.79 1.43 0.57 0.05 5.83]\n",
    "\n",
    "Loss_Q: [0.37 1.13 0.06 0.   1.56] Loss_P: [3.75 0.74 1.19 0.06 5.74]\n",
    "\n",
    "Loss_Q: [0.44 0.9  0.06 0.   1.4 ] Loss_P: [3.76 0.81 0.87 0.06 5.51]\n",
    "\n",
    "Loss_Q: [0.58 0.66 0.02 0.   1.26] Loss_P: [3.75 0.94 0.66 0.03 5.39]\n",
    "\n",
    "Loss_Q: [0.6  0.57 0.05 0.   1.21] Loss_P: [3.74 0.99 0.55 0.06 5.35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla -- add instantiation layer between each layer ([10,8,6,3,1] --> [10,9,8,7,6,5,3,2,1])\n",
    "\n",
    "Loss_Q: [ 4.45  3.98  3.48  2.99  2.49  1.5   1.    0.   19.89] Loss_P: [ 4.53  4.44  3.97  3.48  2.99  2.49  1.5   1.   24.41]\n",
    "\n",
    "Loss_Q: [1.14 0.25 0.38 0.62 0.62 0.41 0.08 0.   3.49] Loss_P: [2.43 2.49 0.3  0.29 0.62 0.58 0.39 0.07 7.17]\n",
    "\n",
    "Loss_Q: [1.18 0.59 0.22 0.32 0.42 0.2  0.05 0.   3.  ] Loss_P: [2.48 2.42 0.83 0.22 0.27 0.4  0.19 0.05 6.86]\n",
    "\n",
    "Loss_Q: [0.78 0.9  0.42 0.37 0.52 0.87 0.1  0.   3.96] Loss_P: [2.37 2.04 1.18 0.45 0.41 0.58 0.92 0.08 8.03]\n",
    "\n",
    "Loss_Q: [1.04 1.28 0.58 0.32 0.24 0.24 0.04 0.   3.74] Loss_P: [2.43 2.16 1.47 0.68 0.34 0.31 0.21 0.04 7.65]\n",
    "\n",
    "Loss_Q: [0.73 0.89 0.44 0.29 0.12 0.4  0.1  0.   2.98] Loss_P: [2.39 1.83 1.28 0.47 0.31 0.11 0.4  0.09 6.88]\n",
    "\n",
    "Loss_Q: [0.94 0.81 0.34 0.25 0.07 0.18 0.05 0.   2.63] Loss_P: [2.32 2.19 1.08 0.43 0.24 0.09 0.17 0.05 6.56]\n",
    "\n",
    "Loss_Q: [0.83 0.99 0.85 0.6  0.04 0.42 0.03 0.   3.76] Loss_P: [2.45 2.1  1.12 0.85 0.69 0.06 0.44 0.02 7.73]\n",
    "\n",
    "Loss_Q: [0.78 1.34 0.52 0.45 0.08 0.09 0.01 0.   3.26] Loss_P: [2.37 1.79 1.81 0.64 0.49 0.07 0.07 0.02 7.26]\n",
    "\n",
    "Loss_Q: [0.9  1.33 0.6  1.03 0.04 0.06 0.03 0.   3.99] Loss_P: [2.31 1.93 1.74 0.59 1.02 0.03 0.07 0.03 7.74]\n",
    "\n",
    "Loss_Q: [0.88 1.12 0.91 0.65 0.05 0.14 0.03 0.   3.77] Loss_P: [2.38 1.62 1.72 1.04 0.66 0.06 0.14 0.04 7.67]\n",
    "\n",
    "Loss_Q: [0.93 0.98 0.27 0.67 0.09 0.12 0.04 0.   3.11] Loss_P: [2.4  1.7  1.61 0.26 0.62 0.07 0.11 0.05 6.81]\n",
    "\n",
    "Loss_Q: [0.81 1.08 0.52 0.44 0.06 0.2  0.03 0.   3.13] Loss_P: [2.32 1.64 1.62 0.51 0.42 0.04 0.19 0.02 6.77]\n",
    "\n",
    "Loss_Q: [0.98 1.25 0.96 0.92 0.07 0.38 0.   0.   4.56] Loss_P: [2.35 1.87 1.63 1.14 0.92 0.07 0.36 0.01 8.35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 1., 0., ..., 1., 0., 1.],\n",
       "       [1., 0., 1., ..., 1., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 1.]])"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 10000\n",
    "generation = generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution,statistics, MSE,ABS_Error = metrics(generation,reordered_set,dataset)\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(dataset, axis = 1, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.229175"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (np.abs(counts_t[:256]-1)).sum()/256\n",
    "ABS_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e074396050>"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABcEAAAMtCAYAAABXe7m6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABWjElEQVR4nO3df5xVdZ0/8Pfl1wwajIsGDAmIZij+IASTH6KWSuGP1U2TYiVJzVgxITKNtBxtE21NUUldd9VZc0PaBc2SUigBfYg/A9dMDROCcOZLusmIBgic7x8uNy4MA3eEuTOfeT4fj/Nwzud8zjnvz7mfe2d8eTw3l2VZFgAAAAAAkKA2pS4AAAAAAAB2FyE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQrHalLmBX2bRpU7z++uvRqVOnyOVypS4HAAAAAIB6ZFkWb7/9dvTo0SPatNn992knE4K//vrr0bNnz1KXAQAAAADATlixYkXsu+++u/08yYTgnTp1ioj3L1znzp1LXA0AAAAAAPWpq6uLnj175jPd3S2ZEHzzI1A6d+4sBAcAAAAAaOaa6rHWvhgTAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQl80xwAAAAAGD327hxY7z33nulLoNmrH379tG2bdtSl5EnBAcAAAAAdijLsqitrY233nqr1KXQAuy1117RvXv3Jvvyy4YIwQEAAACAHdocgHft2jX22GOPZhFu0vxkWRbvvvturFq1KiIiKisrS1yREBwAAAAA2IGNGzfmA/C999671OXQzHXs2DEiIlatWhVdu3Yt+aNRfDEmAAAAANCgzc8A32OPPUpcCS3F5rnSHJ4fLwQHAAAAAHaKR6Cws5rTXBGCAwAAAACQLCE4AAAAAADJ8sWYAAAAAECj7PfNh5r0fMuuPblJz1dKVVVV8cADD8TixYtLXUqMHTs23nrrrXjggQdKXUqjuBMcAAAAAEhWbW1tTJgwIT760Y9GeXl5dOvWLY4++ui4/fbb49133y11eY1SVVUVuVyuwWXZsmVFH3fZsmWRy+WaRfC+K7kTHAAAAABI0muvvRbDhg2LvfbaK6655po47LDDYsOGDfH73/8+7rrrrujRo0f8/d//fb37vvfee9G+ffsmrnjnXHLJJTFu3Lj8+pFHHhkXXHBBfPnLX863ffjDH87/vH79+ujQoUOT1ticuBMcAAAAAEjShRdeGO3atYtnn302zjrrrDj44IPjsMMOizPOOCMeeuihOPXUU/N9c7lc3H777XHaaafFnnvuGf/8z/8cERG33XZbHHDAAdGhQ4fo27dv/OhHP8rvU9+d02+99VbkcrmYN29eRETMmzcvcrlc/OpXv4pBgwbFHnvsEUOHDo1XXnmloNZrr702unXrFp06dYrzzjsv1q5du91xfehDH4ru3bvnl7Zt20anTp3y69/85jfjjDPOiClTpkSPHj3iYx/7WH6MWz/SZK+99orq6uqIiOjTp09ERAwYMCByuVwcd9xxBX2vv/76qKysjL333jvGjx8f77333g5fg+ZACA4AAAAAJOfNN9+MRx55JMaPHx977rlnvX1yuVzB+pVXXhmnnXZavPDCC3HuuefG/fffHxMmTIivf/3r8dvf/ja+8pWvxJe+9KV49NFHi67n8ssvjx/84Afx7LPPRrt27eLcc8/Nb/vJT34SV155ZXzve9+LZ599NiorK+PWW28t+hxb+tWvfhUvvfRSzJkzJ37+85/v1D5PP/10RETMnTs3ampqYtasWfltjz76aPzhD3+IRx99NP7jP/4jqqur8+F5c+dxKAAAAABAcl599dXIsiz69u1b0L7PPvvk77IeP358XHfddflto0ePLginR48eHWPHjo0LL7wwIiImTZoUTz75ZFx//fXxyU9+sqh6vve978Wxxx4bERHf/OY34+STT461a9dGeXl5TJ06Nc4999w4//zzIyLin//5n2Pu3LkN3g2+I3vuuWf8+7//e1GPQdn8CJW99947unfvXrDt7/7u72LatGnRtm3bOOigg+Lkk0+OX/3qVwWPYGmu3AkOAAAAACRr67u9n3766Vi8eHEccsghsW7duoJtgwYNKlh/6aWXYtiwYQVtw4YNi5deeqnoOg4//PD8z5WVlRERsWrVqvx5hgwZUtB/6/ViHXbYYbv0OeCHHHJItG3bNr9eWVmZr7+5cyc4AAAAAJCcj370o5HL5eLll18uaN9///0jIqJjx47b7FPfY1O2DtGzLMu3tWnTJt+22faek73ll2xu3n/Tpk07HEdjbW8sW9Yasf16t7b1l4TmcrndWv+u5E5wAAAAACA5e++9d5x44okxbdq0eOeddxp1jIMPPjgef/zxgrYnnngiDj744Ij42+NDampq8tu3/JLMYs7z5JNPFrRtvb4rfPjDHy6odcmSJfHuu+/m1zffOb5x48Zdfu5Scic4AAAAAJCkW2+9NYYNGxaDBg2KqqqqOPzww6NNmzbxzDPPxMsvvxwDBw5scP9vfOMbcdZZZ8URRxwRxx9/fPzsZz+LWbNmxdy5cyPi/bvJBw8eHNdee23st99+8cYbb8QVV1xRdJ0TJkyIc845JwYNGhRHH310/Od//me8+OKL+bvWd5VPfepTMW3atBg8eHBs2rQpLrvssoI7vLt27RodO3aMX/7yl7HvvvtGeXl5VFRU7NIaSkEIDgAAAAA0yrJrTy51CQ064IADYtGiRXHNNdfE5MmT409/+lOUlZVFv3794pJLLsl/4eX2nH766XHTTTfFv/zLv8TFF18cffr0ibvvvjuOO+64fJ+77rorzj333Bg0aFD07ds3vv/978eIESOKqnPUqFHxhz/8IS677LJYu3ZtnHHGGfFP//RP8fDDDzdm2Nv1gx/8IL70pS/FMcccEz169Iibbropnnvuufz2du3axc033xxXX311fOc734nhw4fHvHnzdmkNpZDLtn4ITAtVV1cXFRUVsXr16ujcuXOpywEAAACAZKxduzaWLl0affr0ifLy8lKXQwvQ0Jxp6izXM8EBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAAWqDq6urYa6+9Sl1Gs9eu1AUAAAAAAC1UVUUTn291o3arra2NKVOmxEMPPRR/+tOfoqKiIg488MA4++yz44tf/GLsscceu7jQXW+//faLiRMnxsSJE/Nto0aNipNOOql0RbUQQnAAAAAAIFmvvfZaDBs2LPbaa6+45ppr4rDDDosNGzbE73//+7jrrruiR48e8fd///clqS3Lsti4cWO0a9e4mLZjx47RsWPHXVxVejwOBZqrpv4vqQAAAAAJuvDCC6Ndu3bx7LPPxllnnRUHH3xwHHbYYXHGGWfEQw89FKeeempERKxevTouuOCC6Nq1a3Tu3Dk+9alPxfPPP58/TlVVVXz84x+PH/3oR7HffvtFRUVFfP7zn4+333473yfLsvj+978f+++/f3Ts2DH69+8f//3f/53fPm/evMjlcvHwww/HoEGDoqysLB577LH4wx/+EKeddlp069YtPvShD8WRRx4Zc+fOze933HHHxR//+Mf42te+FrlcLnK5XETU/ziU2267LQ444IDo0KFD9O3bN370ox8VbM/lcvHv//7v8Q//8A+xxx57xIEHHhgPPvjgLrvezZEQHAAAAABI0ptvvhmPPPJIjB8/Pvbcc896++RyuciyLE4++eSora2N2bNnx3PPPRdHHHFEHH/88fG///u/+b5/+MMf4oEHHoif//zn8fOf/zzmz58f1157bX77FVdcEXfffXfcdttt8eKLL8bXvva1OPvss2P+/PkF57z00ktjypQp8dJLL8Xhhx8ea9asiZNOOinmzp0bixYtik9/+tNx6qmnxvLlyyMiYtasWbHvvvvG1VdfHTU1NVFTU1PvWO6///6YMGFCfP3rX4/f/va38ZWvfCW+9KUvxaOPPlrQ76qrroqzzjor/ud//idOOumk+Md//MeCcabG41AAAAAAgCS9+uqrkWVZ9O3bt6B9n332ibVr10ZExPjx4+PTn/50vPDCC7Fq1aooKyuLiIjrr78+Hnjggfjv//7vuOCCCyIiYtOmTVFdXR2dOnWKiIgxY8bEr371q/je974X77zzTtxwww3x61//OoYMGRIREfvvv388/vjj8a//+q9x7LHH5s9/9dVXx4knnphf33vvvaN///759X/+53+O+++/Px588MG46KKLokuXLtG2bdvo1KlTdO/efbvjvf7662Ps2LFx4YUXRkTEpEmT4sknn4zrr78+PvnJT+b7jR07Nr7whS9ERMQ111wTt9xySzz99NPxmc98psgr3DIIwQEAAACApG1+fMhmTz/9dGzatCn+8R//MdatWxfPPfdcrFmzJvbee++Cfn/961/jD3/4Q359v/32ywfgERGVlZWxatWqiIj43e9+F2vXri0ItyMi1q9fHwMGDChoGzRoUMH6O++8E1dddVX8/Oc/j9dffz02bNgQf/3rX/N3gu+sl156KR/YbzZs2LC46aabCtoOP/zw/M977rlndOrUKT+OFAnBAQAAAIAkffSjH41cLhcvv/xyQfv+++8fEZH/UslNmzZFZWVlzJs3b5tjbPnM7fbt2xdsy+VysWnTpvwxIiIeeuih+MhHPlLQb/Pd5Ztt/WiWb3zjG/Hwww/H9ddfHx/96EejY8eOceaZZ8b69et3cqSFNW0py7Jt2hoaR4qE4AAAAABAkvbee+848cQTY9q0afHVr351u88FP+KII6K2tjbatWsX++23X6PO1a9fvygrK4vly5cXPPpkZzz22GMxduzY+Id/+IeIiFizZk0sW7asoE+HDh1i48aNDR7n4IMPjscffzy++MUv5tueeOKJOPjgg4uqJzVCcAAAAAAgWbfeemsMGzYsBg0aFFVVVXH44YdHmzZt4plnnomXX345Bg4cGCeccEIMGTIkTj/99Ljuuuuib9++8frrr8fs2bPj9NNP3+bxJfXp1KlTXHLJJfG1r30tNm3aFEcffXTU1dXFE088ER/60IfinHPO2e6+H/3oR2PWrFlx6qmnRi6Xi29/+9vb3Jm93377xYIFC+Lzn/98lJWVxT777LPNcb7xjW/EWWedlf9Sz5/97Gcxa9asmDt3bvEXLiFCcAAAAAAgWQcccEAsWrQorrnmmpg8eXL86U9/irKysujXr19ccsklceGFF0Yul4vZs2fH5ZdfHueee278+c9/ju7du8cxxxwT3bp12+lzffe7342uXbvGlClT4rXXXou99torjjjiiPjWt77V4H433nhjnHvuuTF06NDYZ5994rLLLou6urqCPldffXV85StfiQMOOCDWrVsXWZZtc5zTTz89brrppviXf/mXuPjii6NPnz5x9913x3HHHbfTY0hRLqvvarVAdXV1UVFREatXr47OnTuXuhz44KoqIqpWl7oKAAAAgFi7dm0sXbo0+vTpE+Xl5aUuhxagoTnT1Flum91+BgAAAAAAKBEhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJKioEnzJlShx55JHRqVOn6Nq1a5x++unxyiuv7HC/+fPnx8CBA6O8vDz233//uP3227fpM3PmzOjXr1/+m1nvv//+YkoDAAAAAIBtFBWCz58/P8aPHx9PPvlkzJkzJzZs2BAjRoyId955Z7v7LF26NE466aQYPnx4LFq0KL71rW/FxRdfHDNnzsz3WbhwYYwaNSrGjBkTzz//fIwZMybOOuuseOqppxo/MgAAAABgl9q0aVOpS6CFaE5zJZdlWdbYnf/85z9H165dY/78+XHMMcfU2+eyyy6LBx98MF566aV827hx4+L555+PhQsXRkTEqFGjoq6uLn7xi1/k+3zmM5+Jv/u7v4vp06fvVC11dXVRUVERq1evjs6dOzd2SNB8VFVEVK0udRUAAAAAsWnTpliyZEm0bds2PvzhD0eHDh0il8uVuiyaoSzLYv369fHnP/85Nm7cGAceeGC0aVN4L3ZTZ7ntPsjOq1e/H9B16dJlu30WLlwYI0aMKGj79Kc/HXfeeWe899570b59+1i4cGF87Wtf26bP1KlTt3vcdevWxbp16/LrdXV1jRgBAAAAALAjbdq0iT59+kRNTU28/vrrpS6HFmCPPfaIXr16bROAl0KjQ/Asy2LSpElx9NFHx6GHHrrdfrW1tdGtW7eCtm7dusWGDRvijTfeiMrKyu32qa2t3e5xp0yZEldddVVjywcAAAAAitChQ4fo1atXbNiwITZu3FjqcmjG2rZtG+3atWs2/7dAo0Pwiy66KP7nf/4nHn/88R323Xqwm5/AsmV7fX0aukiTJ0+OSZMm5dfr6uqiZ8+eO1U7AAAAAFC8XC4X7du3j/bt25e6FNhpjQrBv/rVr8aDDz4YCxYsiH333bfBvt27d9/mju5Vq1ZFu3btYu+9926wz9Z3h2+prKwsysrKGlM+AAAAAACtRFEPZMmyLC666KKYNWtW/PrXv44+ffrscJ8hQ4bEnDlzCtoeeeSRGDRoUP6/GG2vz9ChQ4spDwAAAAAAChQVgo8fPz7uvffe+PGPfxydOnWK2traqK2tjb/+9a/5PpMnT44vfvGL+fVx48bFH//4x5g0aVK89NJLcdddd8Wdd94Zl1xySb7PhAkT4pFHHonrrrsuXn755bjuuuti7ty5MXHixA8+QgAAAAAAWq2iQvDbbrstVq9eHccdd1xUVlbmlxkzZuT71NTUxPLly/Prffr0idmzZ8e8efPi4x//eHz3u9+Nm2++Oc4444x8n6FDh8Z9990Xd999dxx++OFRXV0dM2bMiKOOOmoXDBEAAAAAgNYql23+lsoWrq6uLioqKmL16tXRuXPnUpcDH1xVRUTV6lJXAQAAAAC7VFNnuUXdCQ4AAAAAAC2JEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEhW0SH4ggUL4tRTT40ePXpELpeLBx54oMH+Y8eOjVwut81yyCGH5PtUV1fX22ft2rVFDwgAAAAAADYrOgR/5513on///jFt2rSd6n/TTTdFTU1NflmxYkV06dIlPve5zxX069y5c0G/mpqaKC8vL7Y8AAAAAADIa1fsDiNHjoyRI0fudP+KioqoqKjIrz/wwAPxl7/8Jb70pS8V9MvlctG9e/diywEAAAAAgO1q8meC33nnnXHCCSdE7969C9rXrFkTvXv3jn333TdOOeWUWLRoUYPHWbduXdTV1RUsAAAAAACwpSYNwWtqauIXv/hFnH/++QXtBx10UFRXV8eDDz4Y06dPj/Ly8hg2bFgsWbJku8eaMmVK/i7zioqK6Nmz5+4uHwAAAACAFqZJQ/Dq6urYa6+94vTTTy9oHzx4cJx99tnRv3//GD58ePzkJz+Jj33sY3HLLbds91iTJ0+O1atX55cVK1bs5uoBAAAAAGhpin4meGNlWRZ33XVXjBkzJjp06NBg3zZt2sSRRx7Z4J3gZWVlUVZWtqvLBAAAAAAgIU12J/j8+fPj1VdfjfPOO2+HfbMsi8WLF0dlZWUTVAYAAAAAQKqKvhN8zZo18eqrr+bXly5dGosXL44uXbpEr169YvLkybFy5cq45557Cva7884746ijjopDDz10m2NeddVVMXjw4DjwwAOjrq4ubr755li8eHH88Ic/bMSQAAAAAADgfUWH4M8++2x88pOfzK9PmjQpIiLOOeecqK6ujpqamli+fHnBPqtXr46ZM2fGTTfdVO8x33rrrbjggguitrY2KioqYsCAAbFgwYL4xCc+UWx5AAAAAACQl8uyLCt1EbtCXV1dVFRUxOrVq6Nz586lLgc+uKqKiKrVpa4CAAAAAHapps5ym+yZ4AAAAAAA0NSE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAApVBVUeoKAAAAAFoFITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCl1JVRakrAAAAAABImhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBoRhVFaWuoGm1pvFuPdbWNHYAAACAhAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQnF2rqqLUFQAAAAAA5AnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkFR2CL1iwIE499dTo0aNH5HK5eOCBBxrsP2/evMjlctssL7/8ckG/mTNnRr9+/aKsrCz69esX999/f7GlAQAAAABAgaJD8HfeeSf69+8f06ZNK2q/V155JWpqavLLgQcemN+2cOHCGDVqVIwZMyaef/75GDNmTJx11lnx1FNPFVseAAAAAADktSt2h5EjR8bIkSOLPlHXrl1jr732qnfb1KlT48QTT4zJkydHRMTkyZNj/vz5MXXq1Jg+fXrR5wIAAAAAgIgmfCb4gAEDorKyMo4//vh49NFHC7YtXLgwRowYUdD26U9/Op544ontHm/dunVRV1dXsAAAAAAAwJZ2ewheWVkZd9xxR8ycOTNmzZoVffv2jeOPPz4WLFiQ71NbWxvdunUr2K9bt25RW1u73eNOmTIlKioq8kvPnj132xgAAAAAAGiZin4cSrH69u0bffv2za8PGTIkVqxYEddff30cc8wx+fZcLlewX5Zl27RtafLkyTFp0qT8el1dnSAcAAAAAIACTfY4lC0NHjw4lixZkl/v3r37Nnd9r1q1apu7w7dUVlYWnTt3LlgAAAAAAGBLJQnBFy1aFJWVlfn1IUOGxJw5cwr6PPLIIzF06NCmLg0AAAAAgIQU/TiUNWvWxKuvvppfX7p0aSxevDi6dOkSvXr1ismTJ8fKlSvjnnvuiYiIqVOnxn777ReHHHJIrF+/Pu69996YOXNmzJw5M3+MCRMmxDHHHBPXXXddnHbaafHTn/405s6dG48//vguGCIAAAAAAK1V0SH4s88+G5/85Cfz65ufy33OOedEdXV11NTUxPLly/Pb169fH5dcckmsXLkyOnbsGIccckg89NBDcdJJJ+X7DB06NO6777644oor4tvf/nYccMABMWPGjDjqqKM+yNgAAAAAAGjlig7BjzvuuMiybLvbq6urC9YvvfTSuPTSS3d43DPPPDPOPPPMYsuBxqmq+L9/ri5tHTQ/VRXmBQAAAEBCSvJMcAAAAAAAaApCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEb2pVFaWuAAAAAACg1RCCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoLvalUVpa6A3a2qomlfZ3MKAAAAABpNCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsI3lxUVZS6Amha5jwAAAAATUAIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgO0FpUVZS6AgAAAIAmJwQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJLVOkPwqopSV1BarX38lJ45+DeuBQAAAMBu1TpDcAAAAAAAWgUhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCF4qVRVlLoCWpqqCvNmZ+zqa+S6AwAAALRoQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFmtJwSvqmjZx2+OWuOYm0qprm0pX9PWOObdLeWxAQAAAOyk1hOCAwAAAADQ6gjBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIVusLwasqSl0BNE9bvzc2r1dVeN+0Bl5nAAAAIFGtLwQHAAAAAKDVEIIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQrKJD8AULFsSpp54aPXr0iFwuFw888ECD/WfNmhUnnnhifPjDH47OnTvHkCFD4uGHHy7oU11dHblcbptl7dq1xZYHAAAAAAB5RYfg77zzTvTv3z+mTZu2U/0XLFgQJ554YsyePTuee+65+OQnPxmnnnpqLFq0qKBf586do6ampmApLy8vtjwAAAAAAMhrV+wOI0eOjJEjR+50/6lTpxasX3PNNfHTn/40fvazn8WAAQPy7blcLrp3715sObtWVUVE1eqWd2wAAAAAAOrV5M8E37RpU7z99tvRpUuXgvY1a9ZE7969Y999941TTjllmzvFt7Zu3bqoq6srWAAAAAAAYEtNHoL/4Ac/iHfeeSfOOuusfNtBBx0U1dXV8eCDD8b06dOjvLw8hg0bFkuWLNnucaZMmRIVFRX5pWfPnk1RPgAAAAAALUiThuDTp0+PqqqqmDFjRnTt2jXfPnjw4Dj77LOjf//+MXz48PjJT34SH/vYx+KWW27Z7rEmT54cq1evzi8rVqxoiiEAAAAAANCCFP1M8MaaMWNGnHfeefFf//VfccIJJzTYt02bNnHkkUc2eCd4WVlZlJWV7eoyAQAAAABISJPcCT59+vQYO3Zs/PjHP46TTz55h/2zLIvFixdHZWVlE1QHAAAAAECqir4TfM2aNfHqq6/m15cuXRqLFy+OLl26RK9evWLy5MmxcuXKuOeeeyLi/QD8i1/8Ytx0000xePDgqK2tjYiIjh07RkVFRUREXHXVVTF48OA48MADo66uLm6++eZYvHhx/PCHP9wVYwQAAAAAoJUq+k7wZ599NgYMGBADBgyIiIhJkybFgAED4jvf+U5ERNTU1MTy5cvz/f/1X/81NmzYEOPHj4/Kysr8MmHChHyft956Ky644II4+OCDY8SIEbFy5cpYsGBBfOITn/ig4wMAAAAAoBUr+k7w4447LrIs2+726urqgvV58+bt8Jg33nhj3HjjjcWWAgAAAAAADWqSZ4JTpKqKUldAa/JB5pu5CgAAAEAzJwQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBG9OqipKXcHuk/LYYFfyXgEAAADYpYTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCE7rVlVRmn0/iA963l1V9+4a/648bqleIwAAAACaDSE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsIXiqqipKXUHLV8w1bInXuyXWXEpNfb2qKrxGAAAAALuAEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQPCKiqqL+n3fneVLXmsZaSqlc51TG0dK47gAAAEArIAQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBKdQVUWpK9i+qormXR+71wd5/Us1d0px3h2dr5h6tuzr/QcAAAC0UEJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQvD6VFWUuoJCza2e+rSEGneH1jrulmrr1yuV16+hcexojKleEwAAAID/IwQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBG9tqip2rm1XHLuqYtcdm0It8bp+kJp3dt/mel2aa10AAAAArYAQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcEBAAAAAEiWEBwAAAAAgGQJwQEAAAAASJYQHAAAAACAZAnBAQAAAABIlhAcAAAAAIBkCcF3pKqiNPumrqoiveuT2nga0hRjbU3Xc3drzPvN9QcAAAASIQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlFh+ALFiyIU089NXr06BG5XC4eeOCBHe4zf/78GDhwYJSXl8f+++8ft99++zZ9Zs6cGf369YuysrLo169f3H///cWWBgAAAAAABYoOwd95553o379/TJs2baf6L126NE466aQYPnx4LFq0KL71rW/FxRdfHDNnzsz3WbhwYYwaNSrGjBkTzz//fIwZMybOOuuseOqpp4otDwAAAAAA8toVu8PIkSNj5MiRO93/9ttvj169esXUqVMjIuLggw+OZ599Nq6//vo444wzIiJi6tSpceKJJ8bkyZMjImLy5Mkxf/78mDp1akyfPr3YEgEAAAAAICKa4JngCxcujBEjRhS0ffrTn45nn3023nvvvQb7PPHEE9s97rp166Kurq5gAQAAAACALRV9J3ixamtro1u3bgVt3bp1iw0bNsQbb7wRlZWV2+1TW1u73eNOmTIlrrrqqm3aD73y4WhTtkdERCy79uTY75sPvf9zeWG/ze31bduyz7Ly//vnFsfa2rJrTy7oX9++27Oj+rY89tZjqq+OrY9XMM4t6qhv/A2Nt77z7sz46hvTluesr8btrW89ph3VuDPHyo+7nr71nWt7c2VHx9qZ16yh8zR07p29Hvn2rcde37nqeU23npP1bdvRmLZ7/RsY79bbGxpvfWOob/+dvc4N7buj8dY7zga2bTmGrce/s+fZ2c+d+sbWUL/tjQUAAACgJdjtd4JHRORyuYL1LMu2aa+vz9ZtW5o8eXKsXr06v6xYsWIXVsyWlpWPLnUJzYLrsH27+trszmtdzLG319dcAAAAAGg5dvud4N27d9/mju5Vq1ZFu3btYu+9926wz9Z3h2+prKwsysrKdn3BAAAAAAAkY7ffCT5kyJCYM2dOQdsjjzwSgwYNivbt2zfYZ+jQobu7PAAAAAAAElb0neBr1qyJV199Nb++dOnSWLx4cXTp0iV69eoVkydPjpUrV8Y999wTERHjxo2LadOmxaRJk+LLX/5yLFy4MO68886YPn16/hgTJkyIY445Jq677ro47bTT4qc//WnMnTs3Hn/88V0wRAAAAAAAWqui7wR/9tlnY8CAATFgwICIiJg0aVIMGDAgvvOd70RERE1NTSxfvjzfv0+fPjF79uyYN29efPzjH4/vfve7cfPNN8cZZ5yR7zN06NC477774u67747DDz88qqurY8aMGXHUUUd90PEBAAAAANCKFX0n+HHHHZf/Ysv6VFdXb9N27LHHxm9+85sGj3vmmWfGmWeeWWw5AAAAAACwXbv9meAAAAAAAFAqQnAAAAAAAJIlBAcAAAAAIFlCcJqNZeWjC/5J87KsfPRueW1a4+u9K8dciuvXGl8zAAAAoOUSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCN2BZ+ejd2r+pj5ei+q7RzralJPXxFaux1+ODXMfd+RpsPrbXGQAAAKB4QnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlC8EQsKx9d6hKARvL+BQAAANh9hOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4C3UsvLRRbWnbFn56N067qa4pqV63VrjfNmVSnH9dvd8L/X5AAAAAHY1ITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyWr1Ifiy8tGlLqEklpWPTnrsKY+tIaUcd+pzqhiNuQ6uHQAAAMDu0epDcAAAAAAA0iUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWULwnbCsfHSpSyiZphp7Mef5IDW15teSQuYCAAAAQOsgBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUE382WlY/ercfenccvpVTGtXkcxYyn2LHvzmOnIOUxpzw2AAAAgF1FCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsI3gjLykfHsvLRpS6jybXUMbfUurfUUsfQUuvenlKMJ7VrCAAAANDUhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLLalbqA3WFZ+ejYb+2P8z83xXmau/evw+pSl7FbX4/dUUOx9TaX69xcNYfXf3faVa9/6tcJAAAAoCm5ExwAAAAAgGQJwQEAAAAASFajQvBbb701+vTpE+Xl5TFw4MB47LHHttt37NixkcvltlkOOeSQfJ/q6up6+6xdu7Yx5QEAAAAAQEQ0IgSfMWNGTJw4MS6//PJYtGhRDB8+PEaOHBnLly+vt/9NN90UNTU1+WXFihXRpUuX+NznPlfQr3PnzgX9ampqory8vHGjAgAAAACAaEQIfsMNN8R5550X559/fhx88MExderU6NmzZ9x222319q+oqIju3bvnl2effTb+8pe/xJe+9KWCfrlcrqBf9+7dGzciAAAAAAD4P0WF4OvXr4/nnnsuRowYUdA+YsSIeOKJJ3bqGHfeeWeccMIJ0bt374L2NWvWRO/evWPfffeNU045JRYtWtTgcdatWxd1dXUFCwAAAAAAbKmoEPyNN96IjRs3Rrdu3Qrau3XrFrW1tTvcv6amJn7xi1/E+eefX9B+0EEHRXV1dTz44IMxffr0KC8vj2HDhsWSJUu2e6wpU6ZERUVFfunZs2cxQwEAAAAAoBVo1Bdj5nK5gvUsy7Zpq091dXXstddecfrppxe0Dx48OM4+++zo379/DB8+PH7yk5/Exz72sbjlllu2e6zJkyfH6tWr88uKFSsaMxQAAAAAABJWVAi+zz77RNu2bbe563vVqlXb3B2+tSzL4q677ooxY8ZEhw4dGi6qTZs48sgjG7wTvKysLDp37lywFGNZ+ehYVj66qH2KPX5zPl5zOdfuVuxYdvfYt5x3KV3n1q65vZbNrR4AAACAUioqBO/QoUMMHDgw5syZU9A+Z86cGDp0aIP7zp8/P1599dU477zzdnieLMti8eLFUVlZWUx5AAAAAABQoF2xO0yaNCnGjBkTgwYNiiFDhsQdd9wRy5cvj3HjxkXE+48pWblyZdxzzz0F+915551x1FFHxaGHHrrNMa+66qoYPHhwHHjggVFXVxc333xzLF68OH74wx82clgAAAAAANCIEHzUqFHx5ptvxtVXXx01NTVx6KGHxuzZs6N3794R8f6XXy5fvrxgn9WrV8fMmTPjpptuqveYb731VlxwwQVRW1sbFRUVMWDAgFiwYEF84hOfaMSQAAAAAADgfUWH4BERF154YVx44YX1bquurt6mraKiIt59993tHu/GG2+MG2+8sTGlAAAAAADAdhX1THAAAAAAAGhJhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAstqVuoDWbln56GZxjMae6/311U12/t1pV1/HYo+X2rXcb+2PS11Gq1PMnGvKzw0AAACAUnInOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQrHalLmBX+235eRGRa7LzLSsfHRGrm+x8KXr/Gn7wPi1VU4wttevnfQcAAADAznInOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCF4M7WsfPROtTV1Dc1Fc66tpXNtdw3XEQAAAKB5EIIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSgidoWfnoUpdQMq157FtqDtehKWtoDuMFAAAAoHkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKC02wtKx9d6hJapK2vW0u/jqmNBwAAAICmJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBKfJLCsf3aT70TqYHwAAAAA0RAgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4LCVZeWjS10CO8lrBQAAQNGqKkpdAdDEhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4K3csvLR9f7MrleK6+s1BQAAAKC1E4IDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMlqVAh+6623Rp8+faK8vDwGDhwYjz322Hb7zps3L3K53DbLyy+/XNBv5syZ0a9fvygrK4t+/frF/fff35jSdqtl5aNjWfnoUpcBAAAAAMBOKjoEnzFjRkycODEuv/zyWLRoUQwfPjxGjhwZy5cvb3C/V155JWpqavLLgQcemN+2cOHCGDVqVIwZMyaef/75GDNmTJx11lnx1FNPFT8iAAAAAAD4P0WH4DfccEOcd955cf7558fBBx8cU6dOjZ49e8Ztt93W4H5du3aN7t2755e2bdvmt02dOjVOPPHEmDx5chx00EExefLkOP7442Pq1KlFDwgAAAAAADYrKgRfv359PPfcczFixIiC9hEjRsQTTzzR4L4DBgyIysrKOP744+PRRx8t2LZw4cJtjvnpT3+6wWOuW7cu6urqChYAAAAAANhSUSH4G2+8ERs3boxu3boVtHfr1i1qa2vr3aeysjLuuOOOmDlzZsyaNSv69u0bxx9/fCxYsCDfp7a2tqhjRkRMmTIlKioq8kvPnj2LGQoAAAAAAK1Au8bslMvlCtazLNumbbO+fftG37598+tDhgyJFStWxPXXXx/HHHNMo44ZETF58uSYNGlSfr2urk4QDgAAAABAgaLuBN9nn32ibdu229yhvWrVqm3u5G7I4MGDY8mSJfn17t27F33MsrKy6Ny5c8ECAAAAAABbKioE79ChQwwcODDmzJlT0D5nzpwYOnToTh9n0aJFUVlZmV8fMmTINsd85JFHijomAAAAAABsrejHoUyaNCnGjBkTgwYNiiFDhsQdd9wRy5cvj3HjxkXE+48pWblyZdxzzz0RETF16tTYb7/94pBDDon169fHvffeGzNnzoyZM2fmjzlhwoQ45phj4rrrrovTTjstfvrTn8bcuXPj8ccf30XDBAAAAACgNSo6BB81alS8+eabcfXVV0dNTU0ceuihMXv27Ojdu3dERNTU1MTy5cvz/devXx+XXHJJrFy5Mjp27BiHHHJIPPTQQ3HSSSfl+wwdOjTuu+++uOKKK+Lb3/52HHDAATFjxow46qijdsEQW7dl5aNLXQJA81VVEVG1utRVAAAAALtRo74Y88ILL4wLL7yw3m3V1dUF65deemlceumlOzzmmWeeGWeeeWZjygEAAAAAgHoV9UxwAAAAAABoSYTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgsBssKx9d6hJoZswJAAAAgNIQggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyUo2BF9WPrrUJew2KY8NWorGvg+9fwFamaqKUlcAAMD2+Fut1Ug2BAcAAAAAACE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAtARVFaXZFwAAAFo4ITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOACkqKqi1BUAH5T3MbCr+DwBoJUTggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCQwksKx9d6hKaVGsbL0BRqireXwAAANgthOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAABA61BVUeoKKAEhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAANByVVWUugKaOSE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAQHNXVVH4T1ofrz00P96XQFPzuQONJgQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZjQrBb7311ujTp0+Ul5fHwIED47HHHttu31mzZsWJJ54YH/7wh6Nz584xZMiQePjhhwv6VFdXRy6X22ZZu3ZtY8oDAAAAAICIaEQIPmPGjJg4cWJcfvnlsWjRohg+fHiMHDkyli9fXm//BQsWxIknnhizZ8+O5557Lj75yU/GqaeeGosWLSro17lz56ipqSlYysvLGzcqAAAAAACIiHbF7nDDDTfEeeedF+eff35EREydOjUefvjhuO2222LKlCnb9J86dWrB+jXXXBM//elP42c/+1kMGDAg357L5aJ79+7FlgMAAAAANFZVRUTV6lJXAbtVUXeCr1+/Pp577rkYMWJEQfuIESPiiSee2KljbNq0Kd5+++3o0qVLQfuaNWuid+/ese+++8Ypp5yyzZ3iW1u3bl3U1dUVLAAAAAAAsKWiQvA33ngjNm7cGN26dSto79atW9TW1u7UMX7wgx/EO++8E2eddVa+7aCDDorq6up48MEHY/r06VFeXh7Dhg2LJUuWbPc4U6ZMiYqKivzSs2fPYoYCAAAAAEAr0KgvxszlcgXrWZZt01af6dOnR1VVVcyYMSO6du2abx88eHCcffbZ0b9//xg+fHj85Cc/iY997GNxyy23bPdYkydPjtWrV+eXFStWNGYoAAAAAAAkrKhngu+zzz7Rtm3bbe76XrVq1TZ3h29txowZcd5558V//dd/xQknnNBg3zZt2sSRRx7Z4J3gZWVlUVZWtvPFAwAAAADQ6hR1J3iHDh1i4MCBMWfOnIL2OXPmxNChQ7e73/Tp02Ps2LHx4x//OE4++eQdnifLsli8eHFUVlYWUx4AAAAAABQo6k7wiIhJkybFmDFjYtCgQTFkyJC44447Yvny5TFu3LiIeP8xJStXrox77rknIt4PwL/4xS/GTTfdFIMHD87fRd6xY8eoqKiIiIirrroqBg8eHAceeGDU1dXFzTffHIsXL44f/vCHu2qcAAAAAAC0QkWH4KNGjYo333wzrr766qipqYlDDz00Zs+eHb17946IiJqamli+fHm+/7/+67/Ghg0bYvz48TF+/Ph8+znnnBPV1dUREfHWW2/FBRdcELW1tVFRUREDBgyIBQsWxCc+8YkPODwAAAAAAFqzokPwiIgLL7wwLrzwwnq3bQ62N5s3b94Oj3fjjTfGjTfe2JhSAAAAAABgu4p6JjikaFn56FKXAADwN1UVpa4AoPXx2UtTM+eahuvM/xGCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDQGtRVVHqCmDXMZ8Bdr+qCp+3zZ3Xh61tnhNbzo3WNk9a23jZKUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAgOaoqqLUFQAALZW/I1omrxvsNkJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAKIWqiveXHfUBPhAhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMkSggMAAAAAkCwhOAAAAAAAyRKCAwAAAACQLCE4AAAAQOqqKkpdAc1JVYU5QasiBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAA4G+qKkpdQbpKcW29ngBCcAAAAAAA0iUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwACANVRVpnaexmnt9ALRMfr/seq4pH4T5kx6v6W4lBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwBorKqKv/1z888Au5rPFwDY/er7fet3cDKE4AAAAAAAJEsIDgAAAABAsoTgAAAAAAAkSwgOAAAAAECyhOAAAAAAACRLCA4AAAAAQLKE4AAAAAAAJEsIDgAAAABAsoTgAEDzVFVR6gpaNteP7dnR3Chm7phn79t8Hba8HvVdm6qK7bfvTNuu5vWDlqWlvWe3V29LG8eu1JrHnoItf99/0L+nPujfW7tyLjX2WC3sb0YhOAAAAAAAyRKCAwAAAACQLCE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMlqVAh+6623Rp8+faK8vDwGDhwYjz32WIP958+fHwMHDozy8vLYf//94/bbb9+mz8yZM6Nfv35RVlYW/fr1i/vvv78xpQEAAAAAQF7RIfiMGTNi4sSJcfnll8eiRYti+PDhMXLkyFi+fHm9/ZcuXRonnXRSDB8+PBYtWhTf+ta34uKLL46ZM2fm+yxcuDBGjRoVY8aMieeffz7GjBkTZ511Vjz11FONHxkAAAAAAK1eu2J3uOGGG+K8886L888/PyIipk6dGg8//HDcdtttMWXKlG3633777dGrV6+YOnVqREQcfPDB8eyzz8b1118fZ5xxRv4YJ554YkyePDkiIiZPnhzz58+PqVOnxvTp0+utY926dbFu3br8+urVqyMiom5d9rdOdXURDa1vaUd9d+W+zaWO1rZvc6mjte3bXOpobfvu7jrq6urf3tK0pLEUW+uW/Tf/3JLGG/HBxrw7be/aRjTu/Nvbt7GvYUt9vYvVUsbXmDrrew135Xh3dKxiztVSXofdbWdfs515v9fXtruus9fvg2lJ16+xv1O3nLMtabyNUcq/Ixqzb0Nt2ztPU7+GO1NbqZSqhh39Hbmja7Yz86cxv5Oaw2vyQW1vvMX8jt3Zvjvzt39j/t4q5j3TlH8b7or96ulb93/rWbad/GFXy4qwbt26rG3bttmsWbMK2i+++OLsmGOOqXef4cOHZxdffHFB26xZs7J27dpl69evz7Isy3r27JndcMMNBX1uuOGGrFevXtut5corr8wiwmKxWCwWi8VisVgsFovFYrFYLC1w+cMf/lBMPN1oRd0J/sYbb8TGjRujW7duBe3dunWL2traevepra2tt/+GDRvijTfeiMrKyu322d4xI96/W3zSpEn59bfeeit69+4dy5cvj4qKimKGRStQV1cXPXv2jBUrVkTnzp1LXQ7NjPlRHNcLCnlP0BDzg4aYHzTE/GBHzBEaYn7Q3K1evTp69eoVXbp0aZLzFf04lIiIXC5XsJ5l2TZtO+q/dXuxxywrK4uysrJt2isqKry52a7OnTubH2yX+VEc1wsKeU/QEPODhpgfNMT8YEfMERpiftDctWnTpmnOU0znffbZJ9q2bbvNHdqrVq3a5k7uzbp3715v/3bt2sXee+/dYJ/tHRMAAAAAAHZGUSF4hw4dYuDAgTFnzpyC9jlz5sTQoUPr3WfIkCHb9H/kkUdi0KBB0b59+wb7bO+YAAAAAACwM4p+HMqkSZNizJgxMWjQoBgyZEjccccdsXz58hg3blxEvP+s7pUrV8Y999wTERHjxo2LadOmxaRJk+LLX/5yLFy4MO68886YPn16/pgTJkyIY445Jq677ro47bTT4qc//WnMnTs3Hn/88Z2uq6ysLK688sp6H5EC5gcNMT+K43pBIe8JGmJ+0BDzg4aYH+yIOUJDzA+au6aeo7ls8wO6i3DrrbfG97///aipqYlDDz00brzxxjjmmGMiImLs2LGxbNmymDdvXr7//Pnz42tf+1q8+OKL0aNHj7jsssvyoflm//3f/x1XXHFFvPbaa3HAAQfE9773vfjsZz/7wUYHAAAAAECr1qgQHAAAAAAAWoKm+fpNAAAAAAAoASE4AAAAAADJEoIDAAAAAJAsITgAAAAAAMlKIgS/9dZbo0+fPlFeXh4DBw6Mxx57rNQl0QQWLFgQp556avTo0SNyuVw88MADBduzLIuqqqro0aNHdOzYMY477rh48cUXC/qsW7cuvvrVr8Y+++wTe+65Z/z93/99/OlPf2rCUbC7TJkyJY488sjo1KlTdO3aNU4//fR45ZVXCvqYI3+zM9dr7NixkcvlCpbBgwdvc6yFCxfGpz71qdhzzz1jr732iuOOOy7++te/NtVQYJebMmVK5HK5mDhxYr7N50frVlVVtc3nYffu3fPbzQ9WrlwZZ599duy9996xxx57xMc//vF47rnn8tvNkdZrv/322+bzI5fLxfjx4yPC3GjtNmzYEFdccUX06dMnOnbsGPvvv39cffXVsWnTpnwfc6R1e/vtt2PixInRu3fv6NixYwwdOjSeeeaZ/Hbzg6b2QbO5//3f/42vfvWr0bdv39hjjz2iV69ecfHFF8fq1avrPd+6devi4x//eORyuVi8eHFRtbb4EHzGjBkxceLEuPzyy2PRokUxfPjwGDlyZCxfvrzUpbGbvfPOO9G/f/+YNm1avdu///3vxw033BDTpk2LZ555Jrp37x4nnnhivP322/k+EydOjPvvvz/uu+++ePzxx2PNmjVxyimnxMaNG5tqGOwm8+fPj/Hjx8eTTz4Zc+bMiQ0bNsSIESPinXfeyfcxR/5mZ65XRMRnPvOZqKmpyS+zZ88u2L5w4cL4zGc+EyNGjIinn346nnnmmbjooouiTZsW/+uGVuqZZ56JO+64Iw4//PCCdp8fHHLIIQWfhy+88EJ+m/nRuv3lL3+JYcOGRfv27eMXv/hF/O53v4sf/OAHsddee+X7mCOt1zPPPFPw2TFnzpyIiPjc5z4XEeZGa3fdddfF7bffHtOmTYuXXnopvv/978e//Mu/xC233JLvY460bueff37MmTMnfvSjH8ULL7wQI0aMiBNOOCFWrlwZEeYHTe+DZnOvv/56vP7663H99dfHCy+8ENXV1fHLX/4yzjvvvHqPd+mll0aPHj0aV2zWwn3iE5/Ixo0bV9B20EEHZd/85jdLVBGlEBHZ/fffn1/ftGlT1r179+zaa6/Nt61duzarqKjIbr/99izLsuytt97K2rdvn9133335PitXrszatGmT/fKXv2yy2mkaq1atyiIimz9/fpZl5siObH29sizLzjnnnOy0005rcL+jjjoqu+KKK3ZzddA03n777ezAAw/M5syZkx177LHZhAkTsizz+UGWXXnllVn//v3r3WZ+cNlll2VHH330drebI2xpwoQJ2QEHHJBt2rTJ3CA7+eSTs3PPPbeg7bOf/Wx29tlnZ1nm86O1e/fdd7O2bdtmP//5zwva+/fvn11++eXmByXXmGyuPj/5yU+yDh06ZO+9915B++zZs7ODDjooe/HFF7OIyBYtWlRUfS361rz169fHc889FyNGjChoHzFiRDzxxBMlqormYOnSpVFbW1swN8rKyuLYY4/Nz43nnnsu3nvvvYI+PXr0iEMPPdT8SdDm/5WmS5cuEWGO7MjW12uzefPmRdeuXeNjH/tYfPnLX45Vq1blt61atSqeeuqp6Nq1awwdOjS6desWxx57bDz++ONNWjvsKuPHj4+TTz45TjjhhIJ2nx9ERCxZsiR69OgRffr0ic9//vPx2muvRYT5QcSDDz4YgwYNis997nPRtWvXGDBgQPzbv/1bfrs5wmbr16+Pe++9N84999zI5XLmBnH00UfHr371q/j9738fERHPP/98PP7443HSSSdFhM+P1m7Dhg2xcePGKC8vL2jv2LFjPP744+YHzc7OzMn6rF69Ojp37hzt2rXLt/2///f/4stf/nL86Ec/ij322KNR9bToEPyNN96IjRs3Rrdu3Qrau3XrFrW1tSWqiuZg8+vf0Nyora2NDh06xN/93d9ttw9pyLIsJk2aFEcffXQceuihEWGONKS+6xURMXLkyPjP//zP+PWvfx0/+MEP4plnnolPfepTsW7duoiIfABUVVUVX/7yl+OXv/xlHHHEEXH88cfHkiVLSjIWaKz77rsvfvOb38SUKVO22ebzg6OOOiruueeeePjhh+Pf/u3fora2NoYOHRpvvvmm+UG89tprcdttt8WBBx4YDz/8cIwbNy4uvvjiuOeeeyLCZwh/88ADD8Rbb70VY8eOjQhzg4jLLrssvvCFL8RBBx0U7du3jwEDBsTEiRPjC1/4QkSYI61dp06dYsiQIfHd7343Xn/99di4cWPce++98dRTT0VNTY35QbOzM3Nya2+++WZ897vfja985Sv5tizLYuzYsTFu3LgYNGhQo+tpt+MuzV8ulytYz7JsmzZap8bMDfMnPRdddFH8z//8T713JJsj29re9Ro1alT+50MPPTQGDRoUvXv3joceeig++9nP5r+w5ytf+Up86UtfioiIAQMGxK9+9au466676g0ToTlasWJFTJgwIR555JFt7rTZks+P1mvkyJH5nw877LAYMmRIHHDAAfEf//Ef+S8MNj9ar02bNsWgQYPimmuuiYj3fxe++OKLcdttt8UXv/jFfD9zhDvvvDNGjhy5zbNNzY3Wa8aMGXHvvffGj3/84zjkkENi8eLFMXHixOjRo0ecc845+X7mSOv1ox/9KM4999z4yEc+Em3bto0jjjgiRo8eHb/5zW/yfcwPmpudnZN1dXVx8sknR79+/eLKK6/Mt99yyy1RV1cXkydP/kB1tOg7wffZZ59o27btNv/1YNWqVdv8VwZal+7du0dENDg3unfvHuvXr4+//OUv2+1Dy/fVr341HnzwwXj00Udj3333zbebI/Xb3vWqT2VlZfTu3Tt/l3dlZWVERPTr16+g38EHH+zLimlRnnvuuVi1alUMHDgw2rVrF+3atYv58+fHzTffHO3atcu//31+sNmee+4Zhx12WCxZssTvF6KysrLB34XmCBERf/zjH2Pu3Llx/vnn59vMDb7xjW/EN7/5zfj85z8fhx12WIwZMya+9rWv5W8mMUc44IADYv78+bFmzZpYsWJFPP300/Hee+9Fnz59zA+anZ2Zk5u9/fbb8ZnPfCY+9KEPxf333x/t27fPb/v1r38dTz75ZJSVlUW7du3iox/9aEREDBo0qOA/EO5Iiw7BO3ToEAMHDsx/o/Zmc+bMiaFDh5aoKpqDzb8Atpwb69evj/nz5+fnxsCBA6N9+/YFfWpqauK3v/2t+ZOALMvioosuilmzZsWvf/3r6NOnT8F2c6TQjq5Xfd58881YsWJFPvzeb7/9okePHvHKK68U9Pv9738fvXv33i11w+5w/PHHxwsvvBCLFy/OL4MGDYp//Md/jMWLF8f+++/v84MC69ati5deeikqKyv9fiGGDRvW4O9Cc4SIiLvvvju6du0aJ598cr7N3ODdd9+NNm0KY5q2bdvm/49Lc4TN9txzz6isrIy//OUv8fDDD8dpp51mftDs7MycjHj/DvARI0ZEhw4d4sEHH9zm/8a9+eab4/nnn8//u9ns2bMj4v3/e+Z73/vezhdU1NdoNkP33Xdf1r59++zOO+/Mfve732UTJ07M9txzz2zZsmWlLo3d7O23384WLVqULVq0KIuI7IYbbsgWLVqU/fGPf8yyLMuuvfbarKKiIps1a1b2wgsvZF/4wheyysrKrK6uLn+McePGZfvuu282d+7c7De/+U32qU99Kuvfv3+2YcOGUg2LXeSf/umfsoqKimzevHlZTU1Nfnn33XfzfcyRv9nR9Xr77bezr3/969kTTzyRLV26NHv00UezIUOGZB/5yEcKrteNN96Yde7cOfuv//qvbMmSJdkVV1yRlZeXZ6+++mqphga7xLHHHptNmDAhv+7zo3X7+te/ns2bNy977bXXsieffDI75ZRTsk6dOuX//jQ/Wrenn346a9euXfa9730vW7JkSfaf//mf2R577JHde++9+T7mSOu2cePGrFevXtlll122zTZzo3U755xzso985CPZz3/+82zp0qXZrFmzsn322Se79NJL833Mkdbtl7/8ZfaLX/wie+2117JHHnkk69+/f/aJT3wiW79+fZZl5gdN74Nmc3V1ddlRRx2VHXbYYdmrr75akEdsb04uXbo0i4hs0aJFRdXa4kPwLMuyH/7wh1nv3r2zDh06ZEcccUQ2f/78UpdEE3j00UeziNhmOeecc7Isy7JNmzZlV155Zda9e/esrKwsO+aYY7IXXnih4Bh//etfs4suuijr0qVL1rFjx+yUU07Jli9fXoLRsKvVNzciIrv77rvzfcyRv9nR9Xr33XezESNGZB/+8Iez9u3bZ7169crOOeeceq/FlClTsn333TfbY489siFDhmSPPfZYE48Gdr2tQ3CfH63bqFGjssrKyqx9+/ZZjx49ss9+9rPZiy++mN9ufvCzn/0sO/TQQ7OysrLsoIMOyu64446C7eZI6/bwww9nEZG98sor22wzN1q3urq6bMKECVmvXr2y8vLybP/9988uv/zybN26dfk+5kjrNmPGjGz//ffPOnTokHXv3j0bP3589tZbb+W3mx80tQ+azW1v/4jIli5dWu85GxuC57Isy3b+vnEAAAAAAGg5WvQzwQEAAAAAoCFCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZQnAAAAAAAJIlBAcAAAAAIFlCcAAAAAAAkiUEBwAAAAAgWUJwAAAAAACSJQQHAAAAACBZ/x9m1xVZQ3MvHQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - counts_d.size))] = counts_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(np.arange(counts_d.size),counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(18, 10))\n",
    "# ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "# ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "# ax.set(xlim=(0, values_d.size))\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.9533,\n",
       " 'FN': array([], dtype=int32),\n",
       " 'n_fn': 0,\n",
       " 'FP': array([[ 293,  324,  333,  335,  337,  369,  378,  397,  399,  401,  412,\n",
       "          416,  418,  420,  426,  428,  469,  500,  509,  511,  513,  527,\n",
       "          533,  541,  589,  591,  593,  597,  601,  605,  607,  623,  682,\n",
       "          713,  722,  724,  726,  767,  786,  788,  797,  801,  805,  807,\n",
       "          809,  829,  834,  858,  880,  882,  884,  886,  890,  892,  894,\n",
       "          898,  900,  902,  906,  908,  944,  946,  948,  952,  954,  956,\n",
       "          960,  962,  964,  970,  976,  980,  984,  988, 1002, 1012, 1020],\n",
       "        [  19,   16,   15,   16,    8,    1,    3,    3,    2,    2,   14,\n",
       "            7,   11,   11,    3,    1,    8,   17,    9,   15,   11,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    2,    1,   11,\n",
       "           15,    4,    8,    3,    9,    4,    3,    1,    8,    7,    4,\n",
       "            5,    1,    1,   22,    1,    9,    6,    7,    6,   11,    3,\n",
       "            8,    8,    8,    5,   10,   12,    8,    9,   13,   17,    2,\n",
       "            2,    1,    1,    1,    1,    3,    1,    1,    1,    1,    1]],\n",
       "       dtype=int64),\n",
       " 'n_fp': 77}"
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08059488000000001"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla\n",
    "\n",
    "percent: 0.9237,0.929,0.9371,0.9361,0.9324,0.9371,0.9437,0.9403, 0.9495,0.9406,0.9405,0.9431,0.9409\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 102,96,71,64,67,69,66,66,64,63,63,73,65\n",
    "\n",
    "mse: 0.14,0.20,0.15,0.12,0.09,0.1,0.09,0.09,0.10,0.085,0.0856,0.07565,0.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- {1,-1}\n",
    "\n",
    "percent: 0.6061,0.598,0.6876,0.7785,0.8961,0.8628,0.8902,0.9216,0.927,0.9191,0.9305,0.933,0.9292,0.9321\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 473,402,378,352,278,315,289,235,228,240,223,208,205,216\n",
    "\n",
    "mse: 0.2074,0.2306,0.1741,0.1486,0.1041,0.1087,0.0866,0.0957,0.0945.0.0817,0.0928,0.0744,0.0728,0.0867\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- {1,-1} makes the network converge slower but more stable\n",
    "- The distribution difference and precision are similar but {1,-1} presents more outliers\n",
    "- No evidence that {1,-1} is better than {0,1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- data bias\n",
    "\n",
    "percent: 0.9303,0.936,0.9442,0.9562,0.9502,0.9564,0.9489,0.9524,0.9494,0.9495,0.9528,0.9572,0.9558,0.9559,0.9564,0.9553\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 97,90,72,57,62,59,56,50,58,56,58,51,56,55,51,50\n",
    "\n",
    "mse: 0.1061,0.1076,0.0881,0.1080,0.0887,0.0753,0.0871,0.0843,0.0717,0.0691,0.0769,0.0717,0.0688,0.0696,0.0781,0.0727\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- improved performance on all dimensions of measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- instantiation bias\n",
    "\n",
    "percent: 0.9027,0.9007,0.8943,0.8936,0.9159,0.9127,0.9305,0.9241,0.923,0.9281,0.934,0.9297\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 108,94,104,96,101,90,92,91,91,86,87,78\n",
    "\n",
    "mse: 0.2791,0.1675,0.1301,0.1242,0.1163,0.1135,0.1355,0.1054,0.1098,0.1,0.0866,0.0848\n",
    "\n",
    "Conclusion: \n",
    "\n",
    "- worse performance than the vanilla model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- instantiation bias & data bias\n",
    "\n",
    "percent: 0.9288,0.9332,0.9346,0.9452,0.9485,0.9492,0.9515,0.9487,0.954,0.9554\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 106,79,75,68,63,68,56,60,55,58\n",
    "\n",
    "mse: 0.1086,0.0934,0.1075,0.1097,0.0981,0.085,0.091,0.0868,0.0764,0.0856\n",
    "\n",
    "Conclusion:\n",
    "- Doesn't outperform the data bias only case. Seems there's no reason to add instantiation bias in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- one more layer\n",
    "\n",
    "percent: 0.9237,0.9433,0.9378,0.9447,0.953,0.9545,0.9546,0.95,0.9529\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 105,70,70,66,60,66,54,63,59\n",
    "\n",
    "mse: 0.1321,0.1102,0.0821,0.0689,0.0727,0.0779,0.0738,0.0611,0.0647\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- It outperforms the vanilla version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- change the added instantiation layer to activation layer\n",
    "\n",
    "percent: 0.9208,0.949,0.941,0.9402,0.9436,0.9429,0.9409,0.9394,0.9437,0.9452\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 124,104,86,73,71,68,70,74,61,67\n",
    "\n",
    "mse: 0.1134,0.1189,0.0933,0.0867,0.0798,0.0734,0.0738,0.0802,0.0761,0.0696\n",
    "\n",
    "Conclusion:\n",
    "- it improves the vanilla version a little bit but not as much as its instantiation conjugate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- add activation layer between each layer ([10,8,6,3,1] --> [10,(9),8,(7),6,(5),3,(2),1])\n",
    "\n",
    "percent: 0.8129,0.8233,0.8243,0.8137,0.8142\n",
    "\n",
    "n_fn: 0,0,0,0,0\n",
    "\n",
    "n_fp: 154,144,138,135,136\n",
    "\n",
    "mse: 0.1675,0.1721,0.1606,0.1587,0.1670\n",
    "\n",
    "Conclusion:\n",
    "- So bad, shouldn't be. Maybe I wrote something wrong, maybe for another trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla -- add instantiation layer between each layer ([10,8,6,3,1] --> [10,9,8,7,6,5,3,2,1])\n",
    "\n",
    "percent: 0.9261,0.9356,0.9387,0.9563,0.9481,0.9521,0.95,0.9582,0.9535,0.9559,0.9533,0.9541,0.9533\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 118,122,111,102,107,92,95,97,92,91,81,78,77\n",
    "\n",
    "mse: 0.1403,0.1291,0.1104,0.1218,0.0888,0.1033,0.0969,0.1027,0.0789,0.0853,0.0863,0.073,0.0805\n",
    "\n",
    "Conclusion:\n",
    "- It works but doesn't perform as well as expected; doesn't show much improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': array([[ 1.80545922e+00,  2.38136505e+00,  2.02559191e+00,\n",
       "          1.37810896e+00,  2.03346936e+00,  3.14131605e+00,\n",
       "          4.43259714e+00,  2.43791638e+00,  4.01432821e+00,\n",
       "          8.98418612e+00],\n",
       "        [ 8.94748627e-02,  1.01760186e+01, -2.75469195e-02,\n",
       "          9.11117545e-02,  7.48641602e-02, -3.50636061e-02,\n",
       "         -3.47624160e-01,  3.35699066e-02, -2.80273177e-02,\n",
       "         -2.58989891e+01],\n",
       "        [-7.55097093e-02, -7.89645503e+00,  7.34088545e-02,\n",
       "          1.18022399e+00,  2.23089040e-01,  6.05104789e-01,\n",
       "          5.55701045e+00,  2.66752192e-01,  7.63221018e-01,\n",
       "          7.06287767e+01],\n",
       "        [-1.17801719e-01,  3.75016049e-01, -1.58069824e-01,\n",
       "         -1.24091272e+00,  7.07099683e-01, -3.72936088e+00,\n",
       "          2.82434109e+00, -2.58312720e-01,  6.45619853e-01,\n",
       "          2.24281347e+01],\n",
       "        [ 1.55909819e-01,  7.30731919e-02,  1.85687444e-01,\n",
       "         -1.45080648e+00,  1.52039761e-02,  3.51199381e+00,\n",
       "         -1.53269505e+00,  7.82539210e+00, -1.99097771e+00,\n",
       "         -1.28490841e-01],\n",
       "        [-5.09831663e-01,  5.12064700e-01, -1.84468052e+00,\n",
       "         -7.75653632e-01, -5.80545156e+00,  8.87509185e-01,\n",
       "          1.61504522e+00, -2.07682990e+00,  2.27739811e+00,\n",
       "          2.53785834e+01],\n",
       "        [ 3.24252760e+00,  9.26478111e-03,  6.08537209e+00,\n",
       "          3.52494070e-01,  5.60662619e-01,  3.42433543e+00,\n",
       "         -3.11438352e+00, -6.30248869e+00, -7.81448764e-01,\n",
       "         -2.22798350e+01],\n",
       "        [-5.49921987e+00,  1.33508425e-01, -5.00739021e+00,\n",
       "         -3.73342631e-01,  2.03993600e+00,  4.04110471e+00,\n",
       "         -8.51278785e-01, -4.63493630e+00,  9.12745197e-01,\n",
       "          3.03741495e+00],\n",
       "        [ 7.69390849e+00,  1.22993978e-01, -9.30225796e-03,\n",
       "          1.99818679e+00,  1.96738389e-01, -3.59318548e+00,\n",
       "          2.31464536e+00,  6.27097355e+00,  8.63925164e-01,\n",
       "          2.29527584e+01],\n",
       "        [ 5.60617636e+00, -1.48865045e-01, -5.89124841e+00,\n",
       "         -8.43025433e-01,  1.30154540e+00,  1.19020955e+00,\n",
       "         -8.89107705e-01, -1.52663644e+00, -2.40745468e-01,\n",
       "         -5.14998589e+00]]),\n",
       " 'Theta_21': array([[-0.95194098, -0.07919937, -1.03533588, -0.65462787, -0.21734468,\n",
       "          0.301432  , -0.52394175, -1.64496789,  0.26378317],\n",
       "        [ 0.19321582, -0.09771048,  0.58913192, -0.07649933, -1.56967832,\n",
       "          3.66826604, -0.68022064,  0.41214552, -2.26300244],\n",
       "        [-1.76626663, -1.6761582 , -1.28890809,  0.55048628,  1.25729222,\n",
       "         -0.05129295, -0.35145037,  0.41067213,  4.21960483],\n",
       "        [-0.97857672, -1.73570977, -0.76814796, -0.30732388, -0.35712742,\n",
       "         -1.61470173, -1.48098535, -1.68509111, -1.81653777],\n",
       "        [-2.03742558, -0.04161262,  1.92143555, -0.3731282 ,  1.59158392,\n",
       "         -0.49643577, -1.47680667, -0.14013635,  2.93943626],\n",
       "        [-0.35887178,  0.47187326, -0.15226215, -0.80691931,  1.96040417,\n",
       "         -0.78043967, -1.14305733,  4.26610585,  1.67980181],\n",
       "        [ 0.324636  ,  1.2940069 , -3.13104717, -1.51044204,  7.56662723,\n",
       "         -1.39993316, -1.35787382, -2.04666112, 13.42018566],\n",
       "        [-1.05783121,  2.5077801 , -0.71722418, -0.12948105, -2.18297271,\n",
       "         -0.11303704, -0.27612579, -3.42787369, -5.49466002],\n",
       "        [ 5.20312653, -1.33112066, -1.37263654, -0.5147639 ,  1.53665028,\n",
       "         -0.73706805, -1.36270881, -1.60059576,  2.60499662]]),\n",
       " 'Theta_32': array([[-1.64128546, -0.34596642,  2.59601215, -2.03913714,  0.08846787,\n",
       "         -0.11594998, -1.09253131, -7.49198685],\n",
       "        [-0.54427447,  0.91798121, -0.66231293, -0.96261225, -1.56653039,\n",
       "          1.17192456, -0.19979386, -3.05589874],\n",
       "        [-0.35136597, -0.63701965, -2.53942122, -1.73906839, -0.37995586,\n",
       "         -0.44116615, -0.34903946, -0.32600948],\n",
       "        [-0.27747931, -1.08392337, -0.75665991, -1.06739577,  0.37546173,\n",
       "         -1.69853441,  0.12782906,  0.71744366],\n",
       "        [-3.10129711,  3.70884446,  2.2742678 , -2.29655811,  0.38141733,\n",
       "          3.163626  ,  1.91971538,  4.18955926],\n",
       "        [-0.08448123,  0.6408049 , -1.04207996, -0.73742716,  0.34022405,\n",
       "         -0.30901118, -1.37548129, -0.37051862],\n",
       "        [-1.51397147, -1.99441725, -0.3308847 , -1.27818373, -0.66683913,\n",
       "         -1.81402746, -0.56185587, -5.74696406],\n",
       "        [-1.3206139 ,  0.28389218, -1.68282579, -0.71806144,  0.83868935,\n",
       "          0.77460943, -0.10796733, -7.77312373]]),\n",
       " 'Theta_43': array([[-1.1850155 , -1.96677314, -1.59775791, -2.03535771, -0.39841017,\n",
       "         -0.16474561, -6.60381996],\n",
       "        [ 0.18274919,  0.52094073,  1.51588622,  0.85712373,  1.26253618,\n",
       "          1.43736383,  6.74113517],\n",
       "        [ 1.13229479,  0.63362072,  0.92406875,  0.16228047,  0.7968853 ,\n",
       "         -0.74583526,  4.01803667],\n",
       "        [-1.09216817, -0.40926664, -1.60461271, -1.03993069, -1.89675217,\n",
       "          0.02589214, -6.76244934],\n",
       "        [-0.19887015,  0.4829101 , -1.17852838,  0.37112486, -0.07809142,\n",
       "          1.06460771,  1.31312779],\n",
       "        [-1.31717402,  0.98372809,  2.3562282 ,  1.486569  ,  0.80909677,\n",
       "          0.60783321,  4.74529565],\n",
       "        [-0.18827889, -1.20824319,  0.36378744,  0.13855118,  0.04576014,\n",
       "          0.10203677,  5.50110177]]),\n",
       " 'Theta_54': array([[-1.30243093,  0.3367488 , -1.52280914, -0.08733274, -0.72983092,\n",
       "         -3.35892994],\n",
       "        [-0.88739573,  0.99763424, -0.63979895,  1.58736854, -1.15234397,\n",
       "          5.11311166],\n",
       "        [-1.37712541,  2.4300253 , -1.76084054,  3.1759356 , -1.21983416,\n",
       "          8.07468227],\n",
       "        [-1.59890978,  2.1394243 , -1.15816129,  2.29849592, -1.41911718,\n",
       "          9.9837679 ],\n",
       "        [-2.56205111,  0.97183961, -1.0439843 ,  2.59561165, -0.59532141,\n",
       "          3.47990549],\n",
       "        [ 0.18610557,  1.47794075, -0.49764463,  0.49873599,  0.64820626,\n",
       "          1.62713219]]),\n",
       " 'Theta_65': array([[ -3.22176553,  -2.78286938,  -2.16558757, -13.61254659],\n",
       "        [  4.56903506,   3.35305568,  -1.72919355,  10.83473299],\n",
       "        [ -3.22434655,  -2.21416735,  -0.99218529,  -6.96328731],\n",
       "        [  4.50357175,   2.81415506,   0.45361781,  13.4766824 ],\n",
       "        [ -2.08690818,  -0.87883181,  -0.37136934,  -8.79800809]]),\n",
       " 'Theta_76': array([[ 3.53321809e+00,  3.81612759e+00,  1.50423343e+01],\n",
       "        [ 2.31422673e+00,  3.12934333e+00,  1.06978722e+01],\n",
       "        [-6.43614584e-03,  1.22501765e+00, -3.17032710e+00]]),\n",
       " 'Theta_87': array([[5.56497094, 5.56497094],\n",
       "        [6.14087064, 6.14087064]])}"
      ]
     },
     "execution_count": 956,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Phi_01': array([[-1.08124298e+01, -3.08489307e-02, -1.19700342e-01,\n",
       "         -1.48145473e-01,  1.63999934e-01, -5.16467338e-01,\n",
       "          3.27228193e+00, -5.02737767e+00,  6.93071278e+00,\n",
       "          5.21449000e+00, -1.46637002e+01],\n",
       "        [-3.29923050e+00,  9.45941619e+00, -7.43009415e+00,\n",
       "          3.28453225e-01,  7.20373146e-02,  3.88275458e-01,\n",
       "          1.21475465e-01,  2.29195353e-01,  1.41920346e-01,\n",
       "         -8.08890554e-02, -6.23782885e+00],\n",
       "        [-1.70858185e-01, -4.00479959e-02, -9.35877576e-03,\n",
       "         -1.30278954e-01,  3.32906301e-01, -1.97881099e+00,\n",
       "          6.42541565e+00, -4.94431587e+00, -6.96017299e-02,\n",
       "         -5.86726429e+00, -2.06285783e+00],\n",
       "        [-3.07687589e+00,  6.75561347e-02,  1.03788893e+00,\n",
       "         -1.20260452e+00, -1.44346730e+00, -8.73471983e-01,\n",
       "          1.79146223e-01, -3.53006632e-01,  1.96028307e+00,\n",
       "         -7.73804449e-01, -6.24029099e+00],\n",
       "        [ 1.51096982e-01,  1.17935672e-01,  1.02226498e-01,\n",
       "          6.06354432e-01,  1.43344885e-01, -5.73401424e+00,\n",
       "          6.41021443e-01,  2.07749245e+00,  1.83417692e-01,\n",
       "          1.27085696e+00, -2.91894497e+00],\n",
       "        [ 2.58870223e+00,  2.33308463e-03,  4.52168451e-01,\n",
       "         -3.56968239e+00,  3.85030963e+00,  7.94555875e-01,\n",
       "          3.45207645e+00,  3.25245478e+00, -3.38700694e+00,\n",
       "          1.15714828e+00, -4.03075738e-02],\n",
       "        [ 4.94415678e+00,  2.49309227e-01,  2.17242133e+00,\n",
       "          2.65601278e+00, -1.08151712e+00,  1.07117228e+00,\n",
       "         -1.88129061e+00, -3.23050078e-01,  1.84172873e+00,\n",
       "         -5.40713649e-01,  2.50780566e+00],\n",
       "        [-7.85786212e+00,  1.18994805e-01,  2.33900245e-01,\n",
       "         -2.87016508e-01,  7.92084156e+00, -1.96073161e+00,\n",
       "         -6.15125755e+00, -4.26933193e+00,  5.79567657e+00,\n",
       "         -1.58903671e+00, -1.11803797e+01],\n",
       "        [ 1.20505268e+00, -5.23089621e-02,  6.61715373e-01,\n",
       "          5.10904930e-01, -1.90200891e+00,  2.22186811e+00,\n",
       "         -8.07765824e-01,  6.77027208e-01,  7.89163611e-01,\n",
       "         -1.01920771e-01, -1.52549472e+00]]),\n",
       " 'Phi_12': array([[-1.00085572e+00,  1.94494063e-01, -1.70955454e+00,\n",
       "         -8.06598046e-01, -1.97433680e+00, -3.54650324e-01,\n",
       "         -5.17744102e-01, -1.00554457e+00,  5.17910504e+00,\n",
       "         -1.41467540e+01],\n",
       "        [-1.23916968e-01, -1.06508457e-01, -1.70928189e+00,\n",
       "         -1.69192548e+00, -1.72293679e-02,  5.55413503e-01,\n",
       "          1.64352048e+00,  2.50616956e+00, -1.22803732e+00,\n",
       "         -7.79906765e+00],\n",
       "        [-1.10373449e+00,  5.11333731e-01, -1.26920390e+00,\n",
       "         -6.91981806e-01,  1.87499164e+00, -1.75117589e-01,\n",
       "         -3.20468118e+00, -7.73909637e-01, -1.32483911e+00,\n",
       "         -1.82358367e+01],\n",
       "        [-5.89086702e-01, -6.06470395e-02,  5.01562715e-01,\n",
       "         -3.53349605e-01, -2.47891244e-01, -7.57778830e-01,\n",
       "         -1.79514077e+00, -9.84566405e-02, -5.85935044e-01,\n",
       "         -1.31154941e+01],\n",
       "        [ 4.80976035e-02, -3.49873505e-01,  1.18872969e+00,\n",
       "         -9.51552018e-01,  2.03587209e+00,  2.13942425e+00,\n",
       "          5.48517775e+00, -1.16496397e+00,  2.66591579e+00,\n",
       "          6.64869103e+00],\n",
       "        [ 1.75175510e-01,  3.63102651e+00, -6.28607806e-02,\n",
       "         -1.58615250e+00, -4.70403543e-01, -8.61724015e-01,\n",
       "         -1.46041274e+00, -7.04948607e-02, -6.62049338e-01,\n",
       "          1.31412671e+00],\n",
       "        [-4.70122347e-01, -5.41002026e-01, -4.29109856e-01,\n",
       "         -1.48479881e+00, -1.39984765e+00, -1.15142049e+00,\n",
       "         -1.63961866e+00, -2.72445421e-01, -1.28770419e+00,\n",
       "         -8.27984363e+00],\n",
       "        [-1.67287874e+00,  5.04307705e-01,  4.21640303e-01,\n",
       "         -1.67503648e+00, -1.96531728e-01,  4.10187023e+00,\n",
       "         -2.42906475e+00, -3.28156190e+00, -1.41573774e+00,\n",
       "         -6.14293214e+00]]),\n",
       " 'Phi_23': array([[-1.67045387, -0.58312498, -0.26879064, -0.35128201, -4.36811134,\n",
       "         -0.20647304, -1.46727665, -1.40279566, -9.50543568],\n",
       "        [-0.34366471,  0.86587668, -0.53758527, -0.76663809,  5.04526227,\n",
       "          0.62771932, -2.07535781,  0.2332677 ,  5.43643551],\n",
       "        [ 2.5758975 , -0.64140514, -2.43908436, -0.71575609,  2.76880056,\n",
       "         -1.04100669, -0.37354967, -1.69457483,  3.74521496],\n",
       "        [-1.98844646, -0.94415205, -1.71545611, -1.04487932, -3.1541956 ,\n",
       "         -0.65349265, -1.301756  , -0.62592685, -8.65479703],\n",
       "        [ 0.01135638, -1.6214342 , -0.37335385,  0.46115925,  0.91343122,\n",
       "          0.32105276, -0.7682338 ,  0.78027193, -4.67381406],\n",
       "        [-0.08191898,  1.15212255, -0.34351282, -1.58390282,  4.74802097,\n",
       "         -0.36509623, -1.80615863,  0.86909293,  6.74491767],\n",
       "        [-1.22754825, -0.24489234, -0.50584061,  0.11236892,  1.05453429,\n",
       "         -1.41753674, -0.58506698, -0.26882241,  1.63648484]]),\n",
       " 'Phi_34': array([[-1.32025853,  0.31780096,  1.11550328, -1.08430117, -0.14804608,\n",
       "         -1.11482303, -0.17234274, -1.58882205],\n",
       "        [-1.95955251,  0.81746181,  0.73147299, -0.19424746,  0.51379759,\n",
       "          1.39714471, -1.03921655, -2.3739269 ],\n",
       "        [-1.92930082,  2.23241849,  1.15305082, -1.35613573, -0.82637051,\n",
       "          2.85895912,  0.42728536,  1.22726654],\n",
       "        [-2.14215387,  1.7099424 ,  0.29862464, -0.85611831,  0.49427017,\n",
       "          2.08524527,  0.13512892,  1.68704147],\n",
       "        [-0.78608571,  1.65765628,  0.78033309, -1.81283389, -0.02508577,\n",
       "          1.34084308,  0.06971769,  3.09351554],\n",
       "        [-0.38210825,  1.48600345, -0.84633116,  0.14358335,  0.99984892,\n",
       "          0.79039646,  0.07499263, -1.07704732]]),\n",
       " 'Phi_45': array([[-1.34230506, -1.05774583, -1.47450626, -1.7248123 , -2.63128207,\n",
       "          0.09525922, -4.31581143],\n",
       "        [ 0.03877843,  0.80207807,  2.00593625,  1.53022665,  0.51760202,\n",
       "          1.83950923, -2.83423612],\n",
       "        [-1.44746263, -0.74600942, -1.93406224, -1.2685306 , -1.37952617,\n",
       "         -0.60650347, -8.08392349],\n",
       "        [-1.02201584,  1.1852837 ,  2.82637458,  1.48343448,  2.7896734 ,\n",
       "          0.4325179 , -5.22433347],\n",
       "        [-0.73583409, -1.15433621, -1.12196748, -1.247855  , -0.39086039,\n",
       "          0.82241536, -5.19121097]]),\n",
       " 'Phi_56': array([[-2.84215115,  3.79312601, -2.68957597,  3.86553418, -2.52545019,\n",
       "          0.18794158],\n",
       "        [-2.69447886,  2.94078098, -1.78171322,  2.55056667, -1.23781367,\n",
       "         -3.47379699],\n",
       "        [-2.16781871, -0.70224525, -1.12950125,  1.85081915, -0.47031009,\n",
       "         -3.27731063]]),\n",
       " 'Phi_67': array([[ 3.82675693,  2.62545586, -0.84150479,  1.16076326],\n",
       "        [ 3.05873988,  3.13128263,  0.22368386, -2.77142261]]),\n",
       " 'Phi_78': array([[0., 0., 0.]])}"
      ]
     },
     "execution_count": 957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(3, 1,figsize=(18, 10))\n",
    "# ax[0].bar(values_d,counts_d)\n",
    "# ax[0].set_title(\"Ground Truth\")\n",
    "# ax[1].bar(values_t,counts_t)\n",
    "# ax[1].set_title(\"Generation\")\n",
    "# ax[2].bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "# ax[2].bar(values_t,counts_t,label = \"Generation\")\n",
    "# for i in range(3):\n",
    "#     ax[i].set(xlim=(0, x_lim), xticks=xtick)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'Phi': Phi, 'Theta': Theta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('parameters_vanilla.npy',parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = np.load('parameters_vanilla.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Phi_01': array([[ 1.92796655e+00,  2.94443837e-01,  1.20437446e-01,\n",
       "         -2.93494387e+00,  1.14451998e+00,  1.24251114e+00,\n",
       "          1.51921169e+00,  1.59644270e-03,  1.43610993e-01,\n",
       "         -3.27353442e-01, -8.07596068e-01],\n",
       "        [ 9.61126670e-01, -7.64624720e-02,  2.61773104e-02,\n",
       "          1.98567201e-01,  2.36421360e-01,  4.22571126e-01,\n",
       "         -3.11872392e-01,  1.77729163e+00,  2.11794095e+00,\n",
       "         -2.82399176e+00, -1.59537280e+00],\n",
       "        [ 1.43496948e+00,  1.99793054e-01, -3.59512838e-01,\n",
       "          3.76847494e-01, -4.47598269e+00, -5.36343000e+00,\n",
       "          1.76820195e+00,  1.41831424e+00,  8.15743805e-01,\n",
       "          1.51699929e-01, -1.89982373e+00],\n",
       "        [ 3.97379809e+00,  1.39237874e+00,  3.76179067e-01,\n",
       "          1.33692692e+00,  5.76617340e-01,  2.07658835e+00,\n",
       "          7.45678413e-01,  1.77486527e+00,  6.16503991e-01,\n",
       "          1.46255757e+00,  1.52595770e+00],\n",
       "        [-7.86013507e-01,  4.27564694e-02, -1.21812688e-01,\n",
       "          3.76855770e-02,  5.70128655e+00, -4.83495728e+00,\n",
       "         -2.57993425e-01,  5.40138382e-01,  8.96388070e-02,\n",
       "          1.08545607e-01, -3.66327447e+00],\n",
       "        [-1.79449521e+00,  1.32364712e-01, -1.36284325e-02,\n",
       "          2.48939811e-01,  2.09961333e-01, -5.80100341e-01,\n",
       "          7.45198885e-01, -7.70367844e+00,  8.45535356e+00,\n",
       "         -3.19786827e-01, -4.71983004e+00],\n",
       "        [-3.86557401e-01, -1.53415996e-01, -7.87226231e-02,\n",
       "          1.40215275e-01,  3.03937584e-01, -6.17201230e-01,\n",
       "          6.69066554e+00, -5.93395080e+00, -7.86795052e+00,\n",
       "         -7.02526969e+00, -2.37752087e+00],\n",
       "        [ 3.93042541e-01, -8.61563531e+00,  9.50386213e+00,\n",
       "         -3.83625057e-01, -5.30703076e-01, -7.63670985e-01,\n",
       "         -1.22175747e-01, -8.93278558e-02, -5.88033713e-02,\n",
       "         -1.25833963e-01, -2.13416369e+00]]),\n",
       " 'Phi_12': array([[-0.76665441, -1.38910056, -0.01117015, -3.400645  , -0.21756073,\n",
       "         -0.79442313, -0.28321128, -0.90193856, -9.74721335],\n",
       "        [ 0.3761852 ,  0.95577071, -0.69445865,  4.7811265 ,  0.62612523,\n",
       "         -0.48457161, -0.05813446,  1.23400409,  5.79159784],\n",
       "        [-0.68336737,  0.82120976, -1.14342279, -1.98912183, -5.40044249,\n",
       "         -1.31202656, -0.78499655, -0.64536117, -9.8006799 ],\n",
       "        [-1.36554814, -0.42400306,  0.2945373 ,  1.54205698,  0.12245034,\n",
       "          1.40669242, -4.072289  , -0.06099771, -2.22246208],\n",
       "        [ 0.806278  ,  0.20866536,  0.38810242,  6.41197803, -0.20111523,\n",
       "         -0.86845294, -0.15897047, -0.69792946,  5.93584111],\n",
       "        [ 2.12754832, -0.05553575, -1.3315937 , -0.27267397,  0.10908106,\n",
       "          1.17730495, -0.60802973, -0.22129315, -7.02689873]]),\n",
       " 'Phi_23': array([[-3.81069838,  4.72072297, -2.12071678,  0.49087035,  2.56313501,\n",
       "          0.52659333,  3.85025333],\n",
       "        [-1.43385474, -2.7943881 , -1.75680646, -0.5628803 , -2.1419295 ,\n",
       "         -1.06962812, -9.75314965],\n",
       "        [-1.21791352,  1.18044601, -0.5432572 ,  0.53869412,  3.09310344,\n",
       "         -0.04182975,  3.64985011]]),\n",
       " 'Phi_34': array([[0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para['Phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
