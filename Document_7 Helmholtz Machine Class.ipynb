{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af70d803",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Document 7</span> \n",
    "\n",
    "# Helmholtz Machine Class\n",
    "\n",
    "In this version, we made some improvements based on previous development and experiments:\n",
    "\n",
    "- We struggled with adding bias to different types of layers, including instantiation layer, MLP hidden layer, data layer. Based on our first round of experiments, the effect of having bias seems ambiguous, only data bias improved the model performance. In this version, we got rid of the confusing and redundant bias choice, integrating it into a more scientific framework -- batch normalization. By performing batch normalization, we recentered the pre-activation term in each layer which gives better structured layer values and derivatives. By adaptive scaling and bias, we attain some control over the mean and variance of all layers.\n",
    "- In the previous version, we pointed out the effect and variability of binary outcomes of sampling layers. We tried to change the classical binary outcomes {0,1} to {-1,1}, and hoped for better performance. However, the modified values seem down-performed the vanillas version for certain reasons. In this version, we add an option to parameterize the binary outcomes of each sampling layer, which may improve the results comparing to predefined values.\n",
    "- We retained the deep structure of hidden layers, although this feature didn't make much difference in the control experiments. As means to promote the correlation between two adjacent sampling layers, we believe this feature will take effect in following experiments.\n",
    "\n",
    "Something new:\n",
    "\n",
    "- **Adaptive learning rate.** In the previous experiments, we set the learning rate manually, which is quite inconvenient and confusing, since it's difficult to know what learning rate is suitable and when to change it. Due to the nature of wake-sleep algorithm, it's infeasible to train a decreasing loss curve and use it to evaluate the model performance. In this version we take the advantage of Taylor expansion and designed some mechanism to modify the learning rate. This feature is applicable to parameter-specific learning rate design (instead of one learning rate for all parameters), but this feature is still under test.\n",
    "\n",
    "- **Layer normalization learning rate.** In Document 6, we pointed out that the values of derivatives are not necessary for achieving decreasing loss, only its sign (+ or -) matters. In this version we made our first attempt on this feature. For the parameter matrix $W$, its rows are normalized by batch normalization, while the inside row distribution is not modifiable. We designed a layer-norm learning rate, which adjusts the learning rate of each parameter within a row separately to diversify the row distribution, namely increasing its entropy.\n",
    "\n",
    "- **MI (mutual information) diversifying training algorithm.** Experimented on the wake-sleep algorithm in previous experiments, it produces too many outliers due to its training on stochastic sampling. It works in certain cases but not applicable to any data distribution. Therefore, based on my understanding, I proposed a new traning algorithm based on maximizing mutual information of adjacent sampling layers, and hope this algorithm could improve model performance and training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96291179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904cb37",
   "metadata": {},
   "source": [
    "The model structure looks like below:\n",
    "\n",
    "<img src=\"Modern.jpg\" style=\"width:800px\">\n",
    "<caption><center> **Figure 1**: Modern Helmholtz Machine. Blue neurons represent sampling layers, where the neurons take binary values. Orange neurons indicate the inserted activations, where the neurons take value in real numbers in given range.    </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a31b6",
   "metadata": {},
   "source": [
    "The skeleton of the model is given by the sampling layers (blue neurons) where each neuron represents a Bernoulli variable. \n",
    "\n",
    "<img src=\"Helm_global.jpg\" style=\"width:700px\">\n",
    "<caption><center> **Figure 2**: External structure of the Helmholtz Machine, composed of sampling layers. Each adjacent pair may be concatenated by hidden activation layers.    </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d965bf",
   "metadata": {},
   "source": [
    "The internal structure of adjacent sampling layers:\n",
    "\n",
    "<img src=\"muti-bernoulli.jpg\" style=\"width:800px\">\n",
    "<caption><center> **Figure 3**: Multivariate Bernoulli Distribution. In wake phase, we go from input $\\mathbf{x}$ to output $\\mathbf{y}$ by weight $\\Phi$. Blue neurons represent sampling layers, where each neuron takes binary value and is computed as an independent Bernoulli variable. Orange neurons are inserted activations, which transform a shallow neural network with one-step prameter updating to deep neural network with backpropogation.\n",
    "</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255f1cb",
   "metadata": {},
   "source": [
    "The computation and parameter updation of the Helmholtz machine is layer-separated by instance sampling. Therefore, the programming architecture of this model is modular -- we perform all functions on the submodule in Figure 3 then assemble them as the skeleton in Figure 2 to achieve the Helmholtz machine as shown in Figure 1. The `Helmholtz_machine class` describes this skeleton in Figure 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b1e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161be893",
   "metadata": {},
   "source": [
    "#### Define model structure\n",
    "\n",
    "n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "n is the maximum number of inserted layers between adjacent instantiation layers.\n",
    "\n",
    "A neural network in Figure 1 has structure ndarray:\n",
    "$$\n",
    "n_{dz} = \n",
    "\\begin{pmatrix}\n",
    "10 & 8 & 6 & 3 & 1 \\\\\n",
    "9 & 0 & 5 & 0 & 0 \\\\\n",
    "0 & 0 & 4 & 0 & 0  \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The first row gives the number of neurons in each instantiation layer. The second row represents the first inserted layer, and there is a 9-neuron layer inserted between $d_0$ and $z_1$, a 5-neuron layer inserted between $z_2$ and $z_3$. The third row represents the second inserted layer, and there is only one 4-neuron layer inserted between $z_2$ and $z_3$ above the first 5-neuron inserted layer. Obviously, $n_{dz}(i,k) \\ne 0$ if and only if $n_{dz}(i,j) \\ne 0, \\forall j < k$; the last column of  $n_{dz}$ is all zero except for the value in the first row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca19f5",
   "metadata": {},
   "source": [
    "In this class, we have following quantities for computation:\n",
    "\n",
    "- Parameters\n",
    "    - Matrix $\\phi$, of shape $(n_l,n_x+2)$. The first $(n_l,n_x)$ is weight matrix from layer $x$ to $l$, penultimate column $(n_l,1)$ is adaptive bias, last column $(n_l,1)$ is adaptive scale. Matrix $\\phi$'s are for every concatenated layers, dictionary of dictionary.\n",
    "    - Binary outcomes of every sampling layer, matrix $Scalar$ of shape $(n_{layer},2)$, in this example $n_{layer}=5$. This matrix, the sampled binary values, are shared between wake and sleep phases.\n",
    "    \n",
    "- Statistics\n",
    "    - Batch normalization statistics, statistical mean and variance of each layer, matrix $norm$ of shape $(n_l,3)$, same length as parameter set. First column is batch summation $\\sum_i z_i$; second column is batch square sum $\\sum_i z_i^2$; third column is fixed bias placeholder that pulls the pre-activition mean to $0$. Separate statistics for wake and sleep phases, dictionary of dictionary.\n",
    "    - Learning rate update criteria. \n",
    "    - Latent assignment distribution\n",
    "- Control variables\n",
    "    - add_norm: control whether to add the current instance to batch normalization counts.\n",
    "    - \n",
    "    - learning rate: assign different learning rate to each submodule, matrix of shape $(2,n_{layer}-1)$. First row stores lr for wake phase modules (last entry is idle), 3 modules; second row stores lr for sleep phase modules, 4 modules.\n",
    "    \n",
    "- Derivatives\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "e0206308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_para_init(n_dz_slice,init_type):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz_slice -- 2 columns of n_dz\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "\n",
    "    Returns:\n",
    "    Phi, Theta -- -2 column bias, -1 column scale\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row is counts, column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    \"\"\"\n",
    "    l = np.where(n_dz_slice[:,0] != 0)[0].size  # number of layers\n",
    "    layer_vt = np.append(n_dz_slice[:l,0],n_dz_slice[0,1])\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    norm_wake = {}\n",
    "    norm_sleep = {}\n",
    "    for i in range(l):\n",
    "        if init_type == \"zero\":\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((layer_vt[i+1],layer_vt[i]+2))\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.zeros((layer_vt[l-i-1],layer_vt[l-i]+2))\n",
    "        elif init_type == \"random\":\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(layer_vt[i+1],layer_vt[i]+2)\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.random.randn(layer_vt[l-i-1],layer_vt[l-i]+2)\n",
    "        else:\n",
    "            raise Exception(\"Wrong Init Type\")\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)][:,-1] = 1          #scale\n",
    "        Theta[\"Theta_\" + str(l-i) + str(l-i-1)][:,-1] = 1\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)][:,-2] = 0              #bias\n",
    "        Theta[\"Theta_\" + str(l-i) + str(l-i-1)][:,-2] = 0\n",
    "        \n",
    "        norm_wake[\"l_\" + str(i+1)] = np.zeros((layer_vt[i+1]+1,3))\n",
    "        norm_sleep[\"l_\" + str(l-i-1)] = np.zeros((layer_vt[l-i-1]+1,3))\n",
    "        \n",
    "    return Phi,Theta,norm_wake,norm_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "89431a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_forward(x,activation_type,parameter_set,norm_set,add_norm):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input instantiation layer, numpy array of shape (n,1)\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l. The keys are ordered sequentially from layer x to y.\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    add_norm -- True or False. Update norm matrix or not\n",
    "    \n",
    "    Returns:\n",
    "    G -- activation of each layer including x\n",
    "    q -- probability of layer y\n",
    "    \"\"\"\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    G = {'z0': x}\n",
    "    g = x\n",
    "    \n",
    "    for i in range(l):\n",
    "        phi = parameter_set[p_keys[i]]\n",
    "        norm = norm_set[n_keys[i]]\n",
    "        \n",
    "        pre_ac = np.matmul(phi[:,:-2],g)+norm[:-1,2:]  # pre-activation; bias placeholder\n",
    "        K = phi[:,-1:] * pre_ac + phi[:,-2:-1]  # rescale linear term\n",
    "        if activation_type == \"sigmoid\":\n",
    "            g = sigmoid(K)\n",
    "        elif activation_type == \"tanh\":\n",
    "            g = np.tanh(K)\n",
    "        if i == l-1:\n",
    "            g = sigmoid(K)\n",
    "        G['z'+str(i+1)] = g\n",
    "        \n",
    "        if add_norm == True:\n",
    "            norm[:-1,0:1] += pre_ac\n",
    "            norm[-1,0] += 1\n",
    "            norm[:-1,1:2] += pre_ac**2\n",
    "            norm[-1,1] += 1\n",
    "            norm_set[n_keys[i]] = norm        \n",
    "    return G,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "7f1e8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_update(x,y,G,activation_type,parameter_set,value_set,norm_set,lr,\\\n",
    "                    fz_binary,fz_scale,fz_W,layer_norm,layer_norm_rate,check_lr):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input sampling layer, numpy array of shape (n,1)\n",
    "    y -- target sampling layer, numpy array of shape (m,1)\n",
    "    G -- output of one_step_forward_norm, activation of each layer including x\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    parameter_set -- parameters from x to y. Each array phi: -2 column bias, -1 column scale\n",
    "    value_set -- list or array [[a_x,b_x],[a_y,b_y]], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,3), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    lr -- learning rate, decimals\n",
    "    fz_binary -- True or Flase. If True, [a_x,b_x] are fixed values; if False, update [a_x,b_x]\n",
    "    fz_scale -- True or Flase. If True, last 2 columns of phi are fixed; if False, update adaptive scale and bias\n",
    "    fz_W -- True or Flase. If True, freeze weights; if False, update weights\n",
    "    layer_norm -- True or Flase, whether to adjust row distribution of W (more evenly distributed) by adjusting the \n",
    "    updating rate \"layer_norm_rate\" of dW\n",
    "    check_lr -- True or Flase. If True, check current lr and update it as needed\n",
    "    \n",
    "    Returns:\n",
    "    parameter_set -- updated parameters\n",
    "    loss -- value of loss function before updating, a number\n",
    "    grad_set -- gradients of parameters\n",
    "    lr -- updated learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    a_x = value_set[0,0]\n",
    "    b_x = value_set[0,1]\n",
    "    a_y = value_set[1,0]\n",
    "    b_y = value_set[1,1]\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    grad_set = {}\n",
    "    \n",
    "    # derivatives\n",
    "    q = G['z'+str(l)]\n",
    "    u = q - (y-b_y)/(a_y-b_y)   #[n_y,1]\n",
    "    loss = -np.sum(((y-b_y)*np.log(q) + (a_y-y)*np.log(1-q))/(a_y-b_y))  # layer entropy loss\n",
    "    if np.where(np.abs(y - a_y) < 1e-8)[0].size + np.where(np.abs(y - b_y) < 1e-8)[0].size != y.size:\n",
    "        raise Exception(\"Incorrect output layer\" + str(a_y)+ \" \"+str(b_y)+ \" \"+str(y))\n",
    "    # a = g(scale * [(Wz+b-mean)/deviation] + bias)\n",
    "    for i in range(l-1,-1,-1):\n",
    "        phi = parameter_set[p_keys[i]]\n",
    "        grad_set['d_'+p_keys[i]] = np.zeros(phi.shape)\n",
    "        W = phi[:,:-2]        #[n_y,n_z]\n",
    "        bias = phi[:,-2:-1]   #[n_y,1]\n",
    "        scale = phi[:,-1:]    #[n_y,1]\n",
    "        \n",
    "        z = G['z'+str(i)]      #[n_z,1]\n",
    "        \n",
    "        if fz_W == False:\n",
    "            dW = np.outer(u*scale,z)     #[n_y,n_z]\n",
    "            grad_set['d_'+p_keys[i]][:,:-2] = dW\n",
    "            # update weights\n",
    "            if layer_norm == True:\n",
    "                dW = layer_lr(W, dW, rate=layer_norm_rate)\n",
    "            parameter_set[p_keys[i]][:,:-2] -= lr * dW\n",
    "        \n",
    "        if fz_scale == False:\n",
    "            norm = norm_set[n_keys[i]]      #[n_y,3]\n",
    "            b = norm[:-1,2:]                #[n_y,1]\n",
    "            N = np.matmul(W,z)+b   #[n_y,1]\n",
    "            d_scale = u * N        #[n_y,1]\n",
    "            d_bias = u             #[n_y,1]\n",
    "            parameter_set[p_keys[i]][:,-1:] -= lr * d_scale\n",
    "            parameter_set[p_keys[i]][:,-2:-1] -= lr * d_bias\n",
    "            grad_set['d_'+p_keys[i]][:,-1:] = d_scale\n",
    "            grad_set['d_'+p_keys[i]][:,-2:-1] = d_bias\n",
    "        \n",
    "        dz = np.matmul(W.T,u*scale)  #[n_z,1]\n",
    "        if i > 0:\n",
    "            if activation_type == \"sigmoid\":\n",
    "                u = dz * z * (1-z)\n",
    "            elif activation_type == \"tanh\":\n",
    "                u = dz * (1-z**2)\n",
    "        else:\n",
    "            # input layer, parameters a_x, b_x\n",
    "            if fz_binary == False:\n",
    "                d_ax = np.mean(dz[np.where(np.abs(x - a_x) < 1e-8)[0]]) # use mean instead of sum\n",
    "                d_bx = np.mean(dz[np.where(np.abs(x - b_x) < 1e-8)[0]])\n",
    "                if np.where(np.abs(x - a_x) < 1e-8)[0].size + np.where(np.abs(x - b_x) < 1e-8)[0].size != dz.size:\n",
    "                    raise Exception(\"Incorrect input layer\" + str(a_x)+ \" \"+str(b_x)+ \" \"+str(x))\n",
    "                a_x -= lr * d_ax\n",
    "                b_x -= lr * d_bx\n",
    "                value_set[0,:] = [a_x,b_x]\n",
    "    \n",
    "    if check_lr == True:\n",
    "        G,q = one_step_forward(x,activation_type,parameter_set,norm_set,add_norm=False)\n",
    "        loss_new = -np.sum(((y-b_y)*np.log(q) + (a_y-y)*np.log(1-q))/(a_y-b_y))  # layer entropy loss\n",
    "        delta_loss = loss - loss_new\n",
    "        delta_f = 0\n",
    "        for keys in grad_set:\n",
    "            delta_f += np.sum(grad_set[keys]**2)\n",
    "        delta_f *= lr\n",
    "        print(\"delta_loss: \"+ str(delta_loss), \"delta_f: \"+ str(delta_f))\n",
    "        if delta_loss < delta_f:\n",
    "            lr /= delta_f/delta_loss\n",
    "            print(\"learning rate change: \" + str(lr))\n",
    "    \n",
    "    return lr,loss,grad_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "e988f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_lr(W, dW, rate):\n",
    "    \"\"\"\n",
    "    adjust row distribution of W (more evenly distributed) by adjusting the updating rate of dW\n",
    "    Arguments:\n",
    "    W -- phi[:,:-2], numpy array of sahpe (n_z,n_x)\n",
    "    dW -- derivatives of W\n",
    "    rate -- 10 folds, x rate; 100 folds, x 2 rate\n",
    "    \n",
    "    Returns:\n",
    "    layer_lr\n",
    "    \"\"\"\n",
    "    multiple = np.log10(np.abs(W)/np.min(np.abs(W),axis=1,keepdims=True)).astype(int)\n",
    "    index = np.where((multiple > 0) & (W*dW > 0))\n",
    "    dW[index] *= rate\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "b2e078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_norm_update(parameter_set,norm_set):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l. The keys are ordered sequentially from layer x to y.\n",
    "    Each array phi has: -2 column bias, -1 column scale\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    \n",
    "    Returns:\n",
    "    parameter_set -- updated parameters\n",
    "    norm_set -- reset mean, variance; update bias placeholder\n",
    "    \"\"\"\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    \n",
    "    for i in range(l):\n",
    "        phi = parameter_set[p_keys[i]]  #[n_z,n_x+2]\n",
    "        norm = norm_set[n_keys[i]]      #[n_z,1]\n",
    "        mean = norm[:-1,0:1]/norm[-1,0]\n",
    "        variance = norm[:-1,1:2]/norm[-1,1] - mean**2\n",
    "        print(\"mean: \", mean)\n",
    "        print(\"variance: \", variance)\n",
    "        \n",
    "        parameter_set[p_keys[i]][:,:-2] = phi[:,:-2]/np.sqrt(variance)  # scale every row of W by deviations\n",
    "        b = norm[:-1,2:]\n",
    "        norm_set[n_keys[i]][:-1,2:] = (b-mean)/np.sqrt(variance) # update bias placeholder\n",
    "        norm_set[n_keys[i]][:,:2] = 0\n",
    "    return parameter_set,norm_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "70349164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimalize(bnr):\n",
    "    \"\"\"\n",
    "    Arguments: bnr -- numpy array of binary values {0,1}, of shape (width,n)\n",
    "    Returns: dcm -- a decimalized intger array in range [0,2**width), shape (n,)\n",
    "    \"\"\"\n",
    "    width = bnr.shape[0]\n",
    "    n = bnr.shape[1]\n",
    "    dcm = np.zeros(n,dtype=int)\n",
    "    for i in range(n):\n",
    "        binary = ''.join(bnr[:,i].reshape(width,).astype(str))\n",
    "        dcm[i] = int(binary,2)\n",
    "    return dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "6772442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarilize(dcm,width):\n",
    "    \"\"\"\n",
    "    Arguments: dcm -- a decimalized intger array in range [0,2**width), shape (n,)\n",
    "    Returns:   bnr -- numpy array of binary values {0,1}, of shape (width,n)\n",
    "    \"\"\"\n",
    "    n = len(dcm)\n",
    "    bnr = np.zeros((width,n),dtype=int)\n",
    "    for i in range(n):\n",
    "        bnr[:,i:i+1] = np.array(list(np.binary_repr(dcm[i], width=width))).astype(int).reshape(-1,1)\n",
    "    return bnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1623,
   "id": "c8d27075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_prob_matrix(activation_type,n_x,parameter_set,norm_set):\n",
    "    x_ind = np.arange(2**n_x)\n",
    "    X = binarilize(x_ind,n_x)  #(n_x,2**n_x)\n",
    "    G,q = one_step_forward(X,activation_type,parameter_set,norm_set,add_norm=False)\n",
    "    \n",
    "    n_y = q.shape[0]           # q:(n_y,2**n_x)\n",
    "    y_ind = np.arange(2**n_y)\n",
    "    Y = binarilize(y_ind,n_y)  #(n_y,2**n_y)\n",
    "    \n",
    "    prob_mtx = np.zeros((2**n_x+1,2**n_y+1)) \n",
    "    # probability from X to Y, last row is sum, last column is row entropy\n",
    "    for i in range(2**n_x):\n",
    "        prob_mtx[i,:-1] = np.prod(q[:,i:i+1]**Y * (1-q[:,i:i+1])**(1-Y),axis=0)\n",
    "    prob_mtx[-1,:-1] = np.mean(prob_mtx[:-1,:-1],axis=0)\n",
    "    prob_mtx[:,-1] = -np.sum(prob_mtx[:,:-1]*np.log(prob_mtx[:,:-1]),axis=1)\n",
    "    return prob_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1624,
   "id": "2a837ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8,  6,  3,  1])"
      ]
     },
     "execution_count": 1624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(n_dz,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "id": "9ac27545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_M(n_x,n_y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_x -- number of neurons in input layer\n",
    "    n_y -- number of neurons in output layer\n",
    "    \n",
    "    Returns:\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+2), last row: summation; -2 column: summation; \n",
    "    last column: row entropy\n",
    "    \"\"\"\n",
    "#     M = np.zeros((2**n_x+1,2**n_y+2))\n",
    "#     epsilon = 1e-10\n",
    "#     M[0:2**n_x,0:2**n_y] = epsilon\n",
    "#     M[-1,:-2] = np.sum(M[:-1,:-2],axis=0)\n",
    "#     M[:,-2] = np.sum(M[:,:-2],axis=1)\n",
    "# #     M[:,-1] = np.log(2**n_y)\n",
    "\n",
    "    M = np.zeros((2**n_x+1,2**n_y+1)) #last row: sum; last column: entropy\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "id": "9cc8735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(H_prev,s,a):\n",
    "    \"\"\"\n",
    "    Iterative formula for entropy:\n",
    "    H_+1 = (s(H-log(s)) + (s+1)log(s+1) + [alog(a) - (a+1)log(a+1)])/(s+1)\n",
    "    \n",
    "    Arguments:\n",
    "    H_prev -- previous entropy\n",
    "    s -- total counts\n",
    "    a -- counts for the category being modified\n",
    "     \n",
    "    Returns:\n",
    "    H -- updated entropy\n",
    "    \"\"\"\n",
    "    H = (s*(H_prev-np.log(s)) + (s+1)*np.log(s+1) + [a*np.log(a) - (a+1)*np.log(a+1)])/(s+1)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1644,
   "id": "04de8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(Input):\n",
    "    \"\"\"\n",
    "    Find y for given x that increases the layer-wise accumulative mutual information\n",
    "    Iterative formula for entropy:\n",
    "    H_+1 = (s(H-log(s)) + (s+1)log(s+1) + [alog(a) - (a+1)log(a+1)])/(s+1)\n",
    "    \n",
    "    s = 1 -> H_+1 = H/2 + log(2) + [alog(a) - (a+1)log(a+1)]/2\n",
    "    \n",
    "    Arguments:\n",
    "    Input -- numpy array of shape (2,m+2), with each row comprises \n",
    "    [counts for m categories separately, sum of counts, entropy of this row]\n",
    "    first row: H(Y|x)    second row: H(Y) [summation over x]\n",
    "    \n",
    "    Returns:\n",
    "    MI_index -- list of indices of Y where the mutual information increases when this category counts +1, numpy array of shape (k, )\n",
    "    MI -- mutual information of each choice, numpy array of shape (k, )\n",
    "    \"\"\"\n",
    "#     s = Input[:,-2:-1]   #(2,1)\n",
    "#     H = Input[:,-1:]     #(2,1)\n",
    "#     C = s*(H - np.log(s)) + (s+1)*np.log(s+1)   #(2,1)\n",
    "    \n",
    "#     M = Input[:,:-2]     #(2,m)\n",
    "#     D = M*np.log(M) - (M+1)*np.log(M+1)     #(2,m)\n",
    "#     H_new = (C + D)/(s+1)      #(2,m)\n",
    "\n",
    "    H = Input[:,-1:]     #(2,1)\n",
    "    M = Input[:,:-1]     #(2,m)\n",
    "    H_new = H/2 + np.log(2) + (M*np.log(M) - (M+1)*np.log(M+1))/2  # +1\n",
    "    \n",
    "    \n",
    "    MI = H_new[1,:] - H_new[0,:]\n",
    "#     I_diff = I - (H[1,:] - H[0,:])  #(m, )\n",
    "#     MI_index = np.where(I_diff > 0)[0]\n",
    "#     MI = I[MI_index]\n",
    "    \n",
    "    return MI,H_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "id": "e9c0c095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 1373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_x = 3\n",
    "n_y = 2\n",
    "M = init_M(n_x,n_y)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "id": "54d71cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79244\\AppData\\Local\\Temp\\ipykernel_17324\\1705882109.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  elif np.all(np.abs(M[:,-1] + np.sum(M[:,:-2]*np.log(M[:,:-2]),axis=1)/M[:,-2]-np.log(M[:,-2])) > epsilon):\n",
      "C:\\Users\\79244\\AppData\\Local\\Temp\\ipykernel_17324\\1705882109.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  elif np.all(np.abs(M[:,-1] + np.sum(M[:,:-2]*np.log(M[:,:-2]),axis=1)/M[:,-2]-np.log(M[:,-2])) > epsilon):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_M(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "id": "246fc051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e-10, 1.00000000e+00, 1.00000000e-10, 1.00000000e-10,\n",
       "        1.00000000e+00, 6.65323756e-09],\n",
       "       [8.00000000e-10, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.00000000e+00, 1.09861229e+00]])"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_x = 2\n",
    "Input = M[[cat_x,-1],:]\n",
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "id": "151a505f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.93147183e-01, 3.43059081e-09, 6.93147183e-01, 6.93147183e-01],\n",
       "       [1.38629436e+00, 1.03972077e+00, 1.03972077e+00, 1.03972077e+00]])"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Input[:,-2:-1]   #(2,1)\n",
    "H = Input[:,-1:]     #(2,1)\n",
    "C = s*(H - np.log(s)) + (s+1)*np.log(s+1)   #(2,1)\n",
    "Mtx = Input[:,:-2]     #(2,m)\n",
    "D = Mtx*np.log(Mtx) - (Mtx+1)*np.log(Mtx+1)     #(2,m)\n",
    "H_new = (C + D)/(s+1)      #(2,m)\n",
    "H_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "id": "59556564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 1.03972077, 0.34657359, 0.34657359])"
      ]
     },
     "execution_count": 1078,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = H_new[1,:] - H_new[0,:]\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "id": "84a09e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40546511, -0.05889152, -0.75203869, -0.75203869])"
      ]
     },
     "execution_count": 1079,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_diff = I - (H[1,:] - H[0,:])  #(m, )\n",
    "I_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "e7783d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_index = np.where(I_diff > 0)[0]\n",
    "MI_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "id": "cf00a044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09861229, 1.09861229])"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI = I[MI_index]\n",
    "MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "id": "5effa761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 1071,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = 0.2\n",
    "q = np.array([[0.5],[0.5]])\n",
    "var_index = np.where(np.abs(q.reshape(-1,) - 0.5) < precision)[0]\n",
    "num = len(var_index)\n",
    "comb = binarilize(np.arange(2**num),num)   #(num, 2**num)\n",
    "prob = q[var_index]\n",
    "probability = np.prod(prob**comb * (1-prob)**(1-comb),axis=0)  #(2**num, )\n",
    "z = (q+0.5).astype(int)\n",
    "z_all = np.repeat(z, 2**num, axis=1)  #(n_q, 2**num)\n",
    "z_all[var_index,:] = comb\n",
    "p_index = decimalize(z_all)   #(2**num, )\n",
    "p_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "id": "25f5b189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 1072,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect, p_i, MI_i = np.intersect1d(p_index, MI_index,return_indices=True)\n",
    "intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "id": "a2e8ee81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1073,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.argsort(probability[p_i] * MI[MI_i])[::-1][0]  # the best y's index\n",
    "cat_y = p_index[p_i[ind]]    # the best y's category\n",
    "cat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "e462b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "M[cat_x,-1] = entropy(M[cat_x,-1],M[cat_x,-2],M[cat_x,cat_y])\n",
    "M[-1,-1] = entropy(M[-1,-1],M[-1,-2],M[-1,cat_y])\n",
    "M[cat_x,cat_y] += 1\n",
    "M[-1,cat_y] += 1\n",
    "M[cat_x,-2] += 1\n",
    "M[-1,-2] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "cc9496ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e+00,\n",
       "        1.00000000e+00, 6.65323756e-09],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e+00, 1.00000000e-10,\n",
       "        1.00000000e+00, 6.65323756e-09],\n",
       "       [1.00000000e-10, 1.00000000e+00, 1.00000000e-10, 1.00000000e-10,\n",
       "        1.00000000e+00, 6.65323756e-09],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10,\n",
       "        4.00000000e-10, 0.00000000e+00],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10,\n",
       "        4.00000000e-10, 0.00000000e+00],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10,\n",
       "        4.00000000e-10, 0.00000000e+00],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10,\n",
       "        4.00000000e-10, 0.00000000e+00],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10,\n",
       "        4.00000000e-10, 0.00000000e+00],\n",
       "       [8.00000000e-10, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.00000000e+00, 1.09861229e+00]])"
      ]
     },
     "execution_count": 1075,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "id": "7942ea71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_index[np.argsort(probability)[::-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "id": "25bc1111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(MI)[::-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "id": "7b8677ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_samples(q,precision):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    q -- sigmoid output of a given layer\n",
    "    precision -- a decimal number in [0,0.5)\n",
    "    \n",
    "    Returns:\n",
    "    p_index -- numpy array of shape (2**num, ), categorical number of possible y's\n",
    "    probability -- numpy array of shape (2**num, ), probability of each category\n",
    "    num -- max number of neurons being changed\n",
    "    \"\"\"\n",
    "    \n",
    "    var_index = np.where(np.abs(q.reshape(-1,) - 0.5) < precision)[0]\n",
    "    num = len(var_index)\n",
    "    comb = binarilize(np.arange(2**num),num)   #(num, 2**num)\n",
    "    prob = q[var_index]\n",
    "    probability = np.prod(prob**comb * (1-prob)**(1-comb),axis=0)  #(2**num, )\n",
    "    \n",
    "    z = (q+0.5).astype(int)\n",
    "    z_all = np.repeat(z, 2**num, axis=1)  #(n_q, 2**num)\n",
    "    z_all[var_index,:] = comb\n",
    "    p_index = decimalize(z_all)   #(2**num, )\n",
    "        \n",
    "    return p_index,probability,num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "id": "378f5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_y(M,cat_x,q,precision=0.2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+2), last row: summation; -2 column: summation; last column: entropy\n",
    "    cat_x -- categorical number of given x\n",
    "    q -- sigmoid output of input x\n",
    "    precision -- a decimal number in [0,0.5)\n",
    "    \n",
    "    Returns:\n",
    "    M -- updated count matrix\n",
    "    \"\"\"\n",
    "    M_pick = M[[cat_x,-1],:]\n",
    "    MI,H_new = mutual_info(M_pick)\n",
    "    p_index,probability,num = top_samples(q,precision)\n",
    "\n",
    "#     intersect, p_i, MI_i = np.intersect1d(p_index, MI_index,return_indices=True)\n",
    "#     if intersect.size > 0:\n",
    "#         ind = np.argsort(probability[p_i] * MI[MI_i])[::-1][0]  # the best y's index\n",
    "#         cat_y = p_index[p_i[ind]]    # the best y's category\n",
    "#     else:\n",
    "#     if MI_index.size == 0:\n",
    "#         cat_y = p_index[np.argsort(probability)[::-1][0]]\n",
    "#     else:\n",
    "\n",
    "#     cat_y = np.argsort(MI)[::-1][0]\n",
    "\n",
    "    ind = np.argsort(MI[p_index] * np.log(1+probability))[::-1][0]\n",
    "    cat_y = p_index[ind]\n",
    "    \n",
    "#     p_ind = np.argsort(MI)[::-1][0:5]      # top 5 max MI\n",
    "#     comb = binarilize(p_ind,width=len(q))  # (2**n_y,5)\n",
    "#     probability = np.prod(q**comb * (1-q)**(1-comb),axis=0)  #(5, )\n",
    "#     ii = np.argsort(probability * MI[p_ind])[::-1][0]\n",
    "#     cat_y = p_ind[ii]\n",
    "        \n",
    "#     # update count matrix\n",
    "#     M[cat_x,-1] = entropy(M[cat_x,-1],M[cat_x,-2],M[cat_x,cat_y])\n",
    "#     M[-1,-1] = entropy(M[-1,-1],M[-1,-2],M[-1,cat_y])\n",
    "#     M[cat_x,cat_y] += 1\n",
    "#     M[-1,cat_y] += 1\n",
    "#     M[cat_x,-2] += 1\n",
    "#     M[-1,-2] += 1    \n",
    "    \n",
    "    return cat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1763,
   "id": "39f984bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_M(M):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+1), last row: summation; last column: entropy\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    flag = True\n",
    "    epsilon = 1e-8\n",
    "    if np.all(np.abs(M[-1,:-1] - np.mean(M[:-1,:-1],axis=0)) > epsilon):\n",
    "        flag = False\n",
    "        raise Exception(\"row addition wrong\")\n",
    "    elif np.all(np.abs(np.sum(M[:,:-1],axis=1) - 1) > epsilon):\n",
    "        flag = False\n",
    "        raise Exception(\"column addition wrong\")\n",
    "    elif np.all(np.abs(M[:,-1] + np.sum(M[:,:-1]*np.log(M[:,:-1]),axis=1)) > epsilon):\n",
    "        flag = False\n",
    "        raise Exception(\"entropy computation wrong\")\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "id": "15ed00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helmholtz_machine:\n",
    "\n",
    "    def __init__(self, n_dz, activation_type, init_lr = 0.1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "        n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "        activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "        init_lr -- initial learning rate, decimal number\n",
    "        \n",
    "        Returns:\n",
    "        n_layer -- number of instantiation layers\n",
    "        lr -- different learning rate for each submodule, numpy array of shape (2,m-1)\n",
    "        \"\"\"\n",
    "        self.n_dz = n_dz\n",
    "        self.n_layer = self.n_dz.shape[1]\n",
    "        self.n_d = n_dz[0,0]\n",
    "        self.ac = activation_type\n",
    "        self.lr = np.ones((2,self.n_layer-1))*init_lr\n",
    "        \n",
    "    def parameter_init(self,init_type,value_set):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "        value_set -- numpy array [a,b], binary outcomes as {positive, negative}\n",
    "\n",
    "        Returns:\n",
    "        Phi, Theta -- -2 column bias, -1 column scale. \n",
    "        Eg. {Phi_01:{Phi_01,Phi_12}, Phi_12:{Phi_01,Phi_12,Phi_23}}, dictionary of dictionary\n",
    "        Norm_wake, Norm_sleep -- statistical mean and variance of each pre-activation layer. Norm_wake's last dic is idle.\n",
    "        Dictionary of dictionary, numpy array of shape (n_neuron+1,3), \n",
    "        last row is counts, column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "        Scalar -- numpy array of shape (n_layer,2), binary outcomes of every sample layer, shared between wake and sleep phases\n",
    "        \"\"\"\n",
    "        Phi = {}\n",
    "        Theta = {}\n",
    "        Norm_wake = {}\n",
    "        Norm_sleep = {}\n",
    "        \n",
    "        for i in range(self.n_layer-1):\n",
    "            Phi[\"Phi_\"+str(i)+str(i+1)],Theta[\"Theta_\"+str(i+1)+str(i)],Norm_wake['sl_'+str(i+1)], Norm_sleep['sl_'+str(i)] \\\n",
    "            = one_step_para_init(self.n_dz[:,i:i+2],init_type)\n",
    "            \n",
    "        self.Phi = Phi\n",
    "        self.Theta = Theta\n",
    "        self.Norm_wake = Norm_wake\n",
    "        self.Norm_sleep = Norm_sleep\n",
    "        self.Scalar = np.repeat(value_set.reshape(1,-1),self.n_layer,axis=0)\n",
    "          \n",
    "    def MI_mtx_init(self):\n",
    "        MI_matrix = {}\n",
    "        for i in range(self.n_layer-2):\n",
    "            MI_matrix[\"MI_\"+str(i)+str(i+1)] = init_M(self.n_dz[0,i],self.n_dz[0,i+1])\n",
    "        return MI_matrix\n",
    "    \n",
    "    def set_control(self,Fz_binary,fz_scale=True,fz_W=False,layer_norm=False,layer_norm_rate=2):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Fz_binary -- Boolean list of size n_layer, with last entry always as True\n",
    "        True or Flase. If True, last 2 columns of phi are fixed; if False, update adaptive scale and bias\n",
    "        fz_W -- True or Flase. If True, freeze weights; if False, update weights\n",
    "        layer_norm -- True or Flase, whether to adjust row distribution of W (more evenly distributed) by adjusting the \n",
    "        updating rate \"layer_norm_rate\" of dW\n",
    "        \"\"\"\n",
    "        self.Fz_binary = Fz_binary\n",
    "        self.fz_scale = fz_scale\n",
    "        self.fz_W = fz_W\n",
    "        self.layer_norm = layer_norm\n",
    "        self.ln_rate = layer_norm_rate\n",
    "        if fz_scale == False:\n",
    "            self.Scalar = self.Scalar.astype(float)\n",
    "    \n",
    "    def wake_sample(self,d0,add_norm=True):\n",
    "        \"\"\"\n",
    "        Stochastic sample in wake phase\n",
    "        Arguments:\n",
    "        d0 -- input pattern in {0,1}, numpy array of shape (n_d, 1)\n",
    "\n",
    "        Returns:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "        Norm -- updated mean and variance on every sampling\n",
    "        \"\"\"\n",
    "        a = self.Scalar[0,0]\n",
    "        b = self.Scalar[0,1]\n",
    "        S = d0*(a-b)+b  # input layer\n",
    "        Alpha_Q = {\"z0\":S}\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(S,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            S = ((q > np.random.rand(len(q),1)).astype(int))*(a-b)+b\n",
    "            Alpha_Q[\"z\"+str(i+1)] = S\n",
    "        Alpha_Q[\"z\"+str(self.n_layer-1)] = [[1]]\n",
    "        return Alpha_Q\n",
    "    \n",
    "    def sleep_sample(self,add_norm=True):\n",
    "        \"\"\"\n",
    "        Stochastic sample in sleep phase\n",
    "        Returns:\n",
    "        Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "        \"\"\"\n",
    "        S = [[1]]\n",
    "        Alpha_P = {\"z\"+str(self.n_layer-1):S}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            G,p = one_step_forward(S,self.ac,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)],add_norm)\n",
    "            a = self.Scalar[i-1,0]\n",
    "            b = self.Scalar[i-1,1]\n",
    "            S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "            Alpha_P[\"z\"+str(i-1)] = S\n",
    "        return Alpha_P\n",
    "    \n",
    "    def wake_update(self,Alpha_P,add_norm=True,check_lr=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "\n",
    "        Returns:\n",
    "        Phi, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        \"\"\"\n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "        for i in range(self.n_layer-2):\n",
    "            x = Alpha_P['z'+str(i)]\n",
    "            y = Alpha_P['z'+str(i+1)]\n",
    "            G,q = one_step_forward(x,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)    \n",
    "            \n",
    "            self.lr[0,i],loss,Grad_set[\"grad_Phi_\"+str(i)+str(i+1)] = one_step_update(x,y,G,self.ac,\\\n",
    "             self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Scalar[[i,i+1],:],self.Norm_wake['sl_'+str(i+1)],self.lr[0,i],\\\n",
    "                self.Fz_binary[i],self.fz_scale,self.fz_W,self.layer_norm,self.ln_rate,check_lr)\n",
    "            Loss[i] = loss\n",
    "            Loss[-1] += loss\n",
    "        return Loss,Grad_set\n",
    "    \n",
    "    def sleep_update(self,Alpha_Q,add_norm=True,check_lr=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "\n",
    "        Returns:\n",
    "        Theta, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        \"\"\"\n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            x = Alpha_Q['z'+str(i)]\n",
    "            y = Alpha_Q['z'+str(i-1)]\n",
    "            G,p = one_step_forward(x,self.ac,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)],add_norm)\n",
    "            \n",
    "            self.lr[1,i-1],loss,Grad_set[\"grad_Theta_\"+str(i)+str(i-1)] = one_step_update(x,y,G,self.ac,\\\n",
    "                self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Scalar[[i,i-1],:],self.Norm_sleep['sl_'+str(i-1)],self.lr[1,i-1],\\\n",
    "                self.Fz_binary[i],self.fz_scale,self.fz_W,self.layer_norm,self.ln_rate,check_lr)\n",
    "            Loss[i-1] = loss\n",
    "            Loss[-1] += loss\n",
    "        return Loss,Grad_set\n",
    "    \n",
    "    # update statistics\n",
    "    def norm_update(self):\n",
    "        \"\"\"\n",
    "        After eg.1000 steps,\n",
    "        Normalize layer pre-activation linear term to mean 0, variance 1 and offset norm matrices to count 0\n",
    "        Check current learning rate, and update it if needed\n",
    "        Check parameters, see if each matrix rows are evenly distributed, consider turning layer_norm on\n",
    "        \n",
    "        After enough training,\n",
    "        For last 2 columns of parameter matrix, adaptive scale and bias, may turn it on\n",
    "        For Scalar, binary values for each layer, could modify it by training\n",
    "        \"\"\"\n",
    "        for i in range(self.n_layer-2):\n",
    "            one_step_norm_update(self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)])\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            one_step_norm_update(self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)])\n",
    "            \n",
    "    def check_learning_rate(self,Alpha_P,Alpha_Q):\n",
    "        self.wake_update(Alpha_P,add_norm=False,check_lr=True) # see self.lr\n",
    "        self.sleep_update(Alpha_Q,add_norm=False,check_lr=True)\n",
    "    \n",
    "    # Compute latent statistics\n",
    "    def wake_sample_batch(self,D,add_norm=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        D -- data distriution represented in {0,1}, numpy array of shape (n_d, n_data)\n",
    "        Returns:\n",
    "        Alpha_Q_bnr -- assignment of each neuron as {1,0}, Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], n_data),i = 0,...m-1\n",
    "        Norm -- updated mean and variance on every sampling\n",
    "        \"\"\"\n",
    "        n_data = D.shape[1]\n",
    "        a = self.Scalar[0,0]\n",
    "        b = self.Scalar[0,1]\n",
    "        S = D*(a-b)+b  # input layer\n",
    "        Alpha_Q_bnr = {\"z0\":D}\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(S,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            S_bnr = (q > np.random.rand(len(q),n_data)).astype(int)\n",
    "            S = S_bnr*(a-b)+b\n",
    "            Alpha_Q_bnr[\"z\"+str(i+1)] = S_bnr\n",
    "        return Alpha_Q_bnr\n",
    "    \n",
    "    def sleep_sample_batch(self,n_sample,add_norm=False):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        Alpha_P_bnr -- assignment of each neuron (binary value) as {1,0}, Python dictionary of length m with each key-value pair \n",
    "        being a numpy array of shape (n_dz[0,i], n_data),i = m-1,...,0\n",
    "        \"\"\"\n",
    "        S = np.ones((1,n_sample))\n",
    "        Alpha_P_bnr = {\"z\"+str(self.n_layer-1):S}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            G,p = one_step_forward(S,self.ac,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)],add_norm)\n",
    "            a = self.Scalar[i-1,0]\n",
    "            b = self.Scalar[i-1,1]\n",
    "            S_bnr = (p > np.random.rand(len(p),n_sample)).astype(int)\n",
    "            S = S_bnr*(a-b)+b   # rejection sampling as a or b\n",
    "            Alpha_P_bnr[\"z\"+str(i-1)] = S_bnr\n",
    "        return Alpha_P_bnr\n",
    "    \n",
    "    # Compute latent distribution\n",
    "    def prob_matrix(self):\n",
    "        self.Prob_mtx_wake = {}\n",
    "        self.Prob_mtx_sleep = {}\n",
    "        for i in range(self.n_layer-2):\n",
    "            self.Prob_mtx_wake[\"Prob_\"+str(i)+str(i+1)] = one_step_prob_matrix(\\\n",
    "                self.ac,self.n_dz[0,i],self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)])\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            self.Prob_mtx_sleep[\"Prob_\"+str(i)+str(i-1)] = one_step_prob_matrix(\\\n",
    "                self.ac,self.n_dz[0,i],self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)])\n",
    "            \n",
    "    def MI_wake(self,d0,MI_matrix,precision=0.2,add_norm=True):\n",
    "        \"\"\"\n",
    "        Sample wake phase by maximize layer-wise mutual information\n",
    "        Arguments:\n",
    "        d0 -- input pattern in {0,1}, numpy array of shape (n_d, 1)\n",
    "\n",
    "        Returns:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "        Norm_wake, Phi, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        Nums -- numpy array of shape (3,m-2), row1: number of picked probabilities; row2: number of MI increase; \n",
    "        row3: number of intersection of two sets\n",
    "        \"\"\"\n",
    "        a = self.Scalar[0,0]\n",
    "        b = self.Scalar[0,1]\n",
    "        cat_x = decimalize(d0)[0]\n",
    "        x = d0*(a-b)+b  # input layer\n",
    "        Alpha_Q = {\"z0\":x}\n",
    "        \n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "        Num = np.zeros(self.n_layer-2)\n",
    "        keys = [*MI_matrix]\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(x,self.ac,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            \n",
    "            cat_y = determine_y(MI_matrix[keys[i]],cat_x,q,precision)\n",
    "            y_bnr = binarilize([cat_y],width=self.n_dz[0,i+1])\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            y = y_bnr*(a-b)+b\n",
    "            \n",
    "            self.lr[0,i],loss,Grad_set[\"grad_Phi_\"+str(i)+str(i+1)] = one_step_update(x,y,G,self.ac,\\\n",
    "             self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Scalar[[i,i+1],:],self.Norm_wake['sl_'+str(i+1)],self.lr[0,i],\\\n",
    "                self.Fz_binary[i],self.fz_scale,self.fz_W,self.layer_norm,self.ln_rate,check_lr=False)\n",
    "            Loss[i] = loss\n",
    "            Loss[-1] += loss\n",
    "            \n",
    "            Alpha_Q[\"z\"+str(i+1)] = y\n",
    "            cat_x = cat_y\n",
    "            x = y\n",
    "        Alpha_Q[\"z\"+str(self.n_layer-1)] = [[1]]\n",
    "        \n",
    "        return Alpha_Q,Loss,Grad_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c7acd",
   "metadata": {},
   "source": [
    "#### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "id": "c3094409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  6,  3,  1],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 1765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[10,8,6,3,1],\n",
    "             [0, 0,0,0,0],\n",
    "             [0, 0,0,0,0]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "id": "5b4fbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM = Helmholtz_machine(n_dz,'sigmoid',init_lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "id": "303a8f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 1767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_set = np.array([1,0])\n",
    "value_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "id": "21d59290",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM.parameter_init('random',value_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1769,
   "id": "6a2594d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True]"
      ]
     },
     "execution_count": 1769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fz_binary = [True]*HM.n_layer\n",
    "Fz_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1770,
   "id": "84375bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM.set_control(Fz_binary,fz_scale=True,fz_W=False,layer_norm=False,layer_norm_rate=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f8f97",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "ffb7ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_formed_generate(n_d):\n",
    "    \"\"\"\n",
    "    Well-formedness rules:\n",
    "        1. Start with 1\n",
    "        2. Forbid 00100 (no 100, 001 on the boundary)\n",
    "        3. Forbid 0000\n",
    "        \n",
    "    Arguments:\n",
    "    n_d -- length of input layer (single data point)\n",
    "    \n",
    "    Returns:\n",
    "    well_formed_set -- a dataset obeys the well-formedness rules, numpy array of shape (n_d,n_data) in {0,1}\n",
    "    n_data is the number of datapoints in the generated dataset\n",
    "    \"\"\"\n",
    "    well_formed_set = np.zeros([1,n_d],dtype=int)\n",
    "    well_formed_set[0,0] = 1\n",
    "\n",
    "    for i in range(1,n_d):\n",
    "        for j in range(np.shape(well_formed_set)[0]):\n",
    "            if i == 2 and np.array_equal(well_formed_set[j,i-2:i], [1,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            elif i > 3 and np.array_equal(well_formed_set[j,i-3:i], [0,0,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            elif i > 3 and np.array_equal(well_formed_set[j,i-4:i], [0,0,1,0]):\n",
    "                well_formed_set[j,i] = 1\n",
    "            else:\n",
    "                well_formed_set = np.append(well_formed_set, well_formed_set[j:j+1,:], axis=0)\n",
    "                well_formed_set[j,i] = 1\n",
    "\n",
    "    ind = np.array([], dtype=np.int8)\n",
    "    for i in range(well_formed_set.shape[0]):\n",
    "        if np.array_equal(well_formed_set[i,-3:], [0,0,1]):\n",
    "            ind = np.append(ind,i)\n",
    "\n",
    "    well_formed_set = np.delete(well_formed_set,ind,0)\n",
    "    well_formed_set = np.transpose(well_formed_set)\n",
    "    \n",
    "    return well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "0f08793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 0, 1, ..., 1, 0, 1],\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set = well_formed_generate(HM.n_d)\n",
    "well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "77a9d814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "5fdca947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0 = well_formed_set[:,4:5]\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "b5f20588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 1, ..., 0, 1, 1],\n",
       "       [0, 1, 0, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_set = binarilize(np.arange(2**HM.n_d),HM.n_d)\n",
    "entire_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "id": "1ac35ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_generate(k,n,n_data):\n",
    "    \"\"\"\n",
    "    The dataset is generated in a favor of Bayesian mixure of Gaussians. Given k mixture Gaussian components, we sample their \n",
    "    means u_1...u_k uniformly from [0,1]. Then we randomly assign each data to one of the components, and sample from its \n",
    "    Gaussian distribution (u_k, sigma). sigma is a hyperparameter, we default it to 1.\n",
    "    \n",
    "    The \"Bayesian mixure of Gaussians\" generation is just a way to generate dataset with non-singular distributions. The \n",
    "    generated data distribution is not identified with the mixure of Gaussian distributions that generated it. In other words, \n",
    "    the data is treated as sole evidence without any prior on how it's been generated thus its reconstruction is not convolved \n",
    "    with it's generative distributions, which is a major difference from varietional inference.\n",
    "        \n",
    "    Arguments:\n",
    "    k -- number of Gaussian components\n",
    "    n -- length of input layer (single data point)\n",
    "    n_data -- number of datapoints to generate\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    \n",
    "    Returns:\n",
    "    random_set -- generated dataset, numpy array of shape (n,n_data), n_data is the number of datapoints in the generated dataset\n",
    "    \"\"\"\n",
    "    u = np.random.rand(n,k)\n",
    "    c = np.random.randint(k, size=(n_data,))\n",
    "    data_mean = u[:,c]\n",
    "    prob = np.random.randn(n,n_data) + data_mean\n",
    "    random_set = (prob>0.5).astype(int)\n",
    "    \n",
    "    return random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "9a6cc913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 1, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 1233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "n_data = 300\n",
    "random_set = random_generate(k,HM.n_d,n_data)\n",
    "random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "id": "12eadddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 250)"
      ]
     },
     "execution_count": 1235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(random_set,axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2dcd7",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "id": "6efc5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = random_set\n",
    "n_data = dataset.shape[1]\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1725,
   "id": "3cd23238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [3.23 3.19 0.72 0.   7.14] Loss_P: [4.1  2.01 2.15 1.3  9.57]\n",
      "Loss_Q: [2.66 2.16 0.53 0.   5.36] Loss_P: [4.17 2.13 2.32 1.19 9.82]\n",
      "Loss_Q: [2.28 2.12 0.49 0.   4.89] Loss_P: [4.21 2.04 2.07 1.27 9.6 ]\n",
      "Loss_Q: [2.47 1.99 0.35 0.   4.81] Loss_P: [4.23 1.93 2.29 1.26 9.72]\n",
      "Loss_Q: [1.93 2.01 0.5  0.   4.43] Loss_P: [4.22 1.77 2.19 1.31 9.48]\n",
      "Loss_Q: [1.98 2.19 0.4  0.   4.57] Loss_P: [4.36 1.69 2.26 1.29 9.6 ]\n",
      "Loss_Q: [2.04 1.78 0.33 0.   4.15] Loss_P: [4.32 1.66 2.38 1.27 9.64]\n",
      "Loss_Q: [1.96 1.94 0.31 0.   4.2 ] Loss_P: [4.19 1.71 2.26 1.22 9.38]\n",
      "Loss_Q: [1.96 2.06 0.49 0.   4.51] Loss_P: [4.35 1.7  2.3  1.3  9.65]\n",
      "Loss_Q: [1.81 1.6  0.44 0.   3.84] Loss_P: [4.41 1.71 2.28 1.28 9.68]\n",
      "Loss_Q: [1.74 1.63 0.34 0.   3.7 ] Loss_P: [4.25 1.72 2.28 1.23 9.49]\n",
      "Loss_Q: [1.59 1.7  0.35 0.   3.63] Loss_P: [4.29 1.7  2.33 1.22 9.54]\n",
      "Loss_Q: [1.41 1.43 0.36 0.   3.2 ] Loss_P: [4.34 1.66 2.36 1.16 9.53]\n",
      "Loss_Q: [1.7  1.45 0.27 0.   3.43] Loss_P: [4.38 1.66 2.32 1.15 9.51]\n",
      "Loss_Q: [1.52 1.41 0.27 0.   3.2 ] Loss_P: [4.56 1.72 2.3  1.13 9.71]\n",
      "Loss_Q: [1.39 1.45 0.31 0.   3.16] Loss_P: [4.68 1.5  2.35 1.14 9.67]\n",
      "Loss_Q: [1.39 1.65 0.37 0.   3.41] Loss_P: [4.42 1.6  2.3  1.19 9.5 ]\n",
      "Loss_Q: [1.67 1.61 0.37 0.   3.64] Loss_P: [4.6  1.54 2.39 1.17 9.71]\n",
      "Loss_Q: [1.32 1.51 0.41 0.   3.24] Loss_P: [4.6  1.38 2.46 1.15 9.59]\n",
      "Loss_Q: [1.64 1.6  0.51 0.   3.75] Loss_P: [4.61 1.46 2.36 1.22 9.65]\n",
      "Loss_Q: [1.45 1.68 0.4  0.   3.53] Loss_P: [4.54 1.57 2.42 1.12 9.66]\n",
      "Loss_Q: [1.42 1.65 0.26 0.   3.34] Loss_P: [4.61 1.6  2.34 1.22 9.77]\n",
      "Loss_Q: [1.57 1.67 0.36 0.   3.6 ] Loss_P: [4.57 1.54 2.33 1.13 9.57]\n",
      "Loss_Q: [1.35 1.59 0.34 0.   3.28] Loss_P: [4.41 1.58 2.48 1.21 9.68]\n",
      "Loss_Q: [1.39 1.66 0.34 0.   3.39] Loss_P: [4.56 1.57 2.33 1.26 9.71]\n",
      "Loss_Q: [1.44 1.57 0.4  0.   3.41] Loss_P: [4.58 1.58 2.33 1.19 9.68]\n",
      "Loss_Q: [1.47 1.74 0.31 0.   3.52] Loss_P: [4.76 1.52 2.36 1.23 9.88]\n",
      "Loss_Q: [1.39 1.68 0.41 0.   3.47] Loss_P: [4.7  1.48 2.57 1.09 9.84]\n",
      "Loss_Q: [1.39 1.71 0.3  0.   3.4 ] Loss_P: [4.8  1.56 2.46 1.17 9.99]\n",
      "Loss_Q: [1.37 1.4  0.31 0.   3.08] Loss_P: [4.63 1.46 2.22 1.24 9.54]\n",
      "Loss_Q: [1.61 1.68 0.4  0.   3.68] Loss_P: [4.85 1.63 2.25 1.21 9.94]\n",
      "Loss_Q: [1.49 1.51 0.43 0.   3.43] Loss_P: [4.85 1.66 2.04 1.35 9.9 ]\n",
      "Loss_Q: [1.41 1.46 0.4  0.   3.28] Loss_P: [4.66 1.65 2.14 1.23 9.68]\n",
      "Loss_Q: [1.52 1.39 0.33 0.   3.23] Loss_P: [4.87 1.55 2.3  1.23 9.95]\n",
      "Loss_Q: [1.35 1.76 0.28 0.   3.39] Loss_P: [4.88 1.42 2.51 1.08 9.89]\n",
      "Loss_Q: [1.27 1.42 0.35 0.   3.04] Loss_P: [4.79 1.58 2.23 1.26 9.86]\n",
      "Loss_Q: [1.61 1.84 0.37 0.   3.82] Loss_P: [4.86 1.45 2.48 1.19 9.98]\n",
      "Loss_Q: [1.6  1.65 0.4  0.   3.65] Loss_P: [ 4.74  1.83  2.42  1.13 10.12]\n",
      "Loss_Q: [1.39 1.79 0.38 0.   3.56] Loss_P: [4.65 1.69 2.32 1.25 9.9 ]\n",
      "Loss_Q: [1.48 1.69 0.39 0.   3.55] Loss_P: [ 4.79  1.76  2.46  1.1  10.11]\n",
      "Loss_Q: [1.59 1.59 0.41 0.   3.59] Loss_P: [4.66 1.61 2.28 1.23 9.78]\n",
      "Loss_Q: [1.34 1.43 0.36 0.   3.14] Loss_P: [ 5.01  1.63  2.34  1.18 10.16]\n",
      "Loss_Q: [1.4  1.43 0.32 0.   3.16] Loss_P: [4.92 1.52 2.28 1.25 9.97]\n",
      "Loss_Q: [1.61 1.73 0.35 0.   3.69] Loss_P: [4.89 1.67 2.31 1.11 9.98]\n",
      "Loss_Q: [1.63 1.71 0.35 0.   3.69] Loss_P: [ 4.85  1.76  2.4   1.25 10.25]\n",
      "Loss_Q: [1.53 1.73 0.33 0.   3.58] Loss_P: [ 4.75  1.72  2.58  1.08 10.13]\n",
      "Loss_Q: [1.48 1.45 0.4  0.   3.33] Loss_P: [4.91 1.59 2.34 1.1  9.94]\n",
      "Loss_Q: [1.59 1.79 0.34 0.   3.72] Loss_P: [ 4.87  1.61  2.58  1.17 10.22]\n",
      "Loss_Q: [1.31 1.74 0.31 0.   3.36] Loss_P: [ 5.    1.55  2.46  1.12 10.12]\n",
      "Loss_Q: [1.53 1.76 0.42 0.   3.71] Loss_P: [ 5.01  1.69  2.5   1.14 10.33]\n",
      "Loss_Q: [1.57 1.9  0.4  0.   3.87] Loss_P: [ 5.01  1.57  2.59  1.17 10.34]\n",
      "Loss_Q: [1.56 1.6  0.42 0.   3.57] Loss_P: [ 4.86  1.79  2.37  1.18 10.19]\n",
      "Loss_Q: [1.53 1.49 0.37 0.   3.39] Loss_P: [ 4.86  1.75  2.39  1.18 10.18]\n",
      "Loss_Q: [1.34 1.56 0.28 0.   3.18] Loss_P: [ 4.95  1.87  2.36  1.21 10.39]\n",
      "Loss_Q: [1.76 1.55 0.48 0.   3.78] Loss_P: [ 4.79  1.94  2.42  1.13 10.29]\n",
      "Loss_Q: [1.83 1.62 0.48 0.   3.94] Loss_P: [ 4.73  2.14  2.45  1.19 10.51]\n",
      "Loss_Q: [1.73 1.4  0.42 0.   3.55] Loss_P: [ 4.81  1.84  2.26  1.19 10.1 ]\n",
      "Loss_Q: [1.52 1.69 0.43 0.   3.64] Loss_P: [ 4.71  2.04  2.44  1.18 10.37]\n",
      "Loss_Q: [1.62 1.52 0.41 0.   3.55] Loss_P: [ 4.83  2.06  2.37  1.15 10.4 ]\n",
      "Loss_Q: [1.57 1.36 0.45 0.   3.38] Loss_P: [ 5.05  2.02  2.34  1.17 10.58]\n",
      "Loss_Q: [1.64 1.68 0.38 0.   3.7 ] Loss_P: [ 4.87  2.    2.34  1.25 10.46]\n",
      "Loss_Q: [1.93 1.71 0.41 0.   4.04] Loss_P: [ 4.81  1.97  2.41  1.28 10.47]\n",
      "Loss_Q: [1.78 1.91 0.47 0.   4.16] Loss_P: [ 4.88  1.95  2.63  1.17 10.64]\n",
      "Loss_Q: [1.73 1.83 0.33 0.   3.89] Loss_P: [ 4.83  1.99  2.64  1.14 10.59]\n",
      "Loss_Q: [1.94 1.8  0.34 0.   4.08] Loss_P: [ 4.9   1.97  2.68  1.12 10.67]\n",
      "Loss_Q: [2.12 2.09 0.42 0.   4.63] Loss_P: [ 4.97  2.17  2.56  1.22 10.92]\n",
      "Loss_Q: [2.06 1.98 0.36 0.   4.4 ] Loss_P: [ 5.    2.07  2.81  1.06 10.94]\n",
      "Loss_Q: [1.88 1.95 0.41 0.   4.23] Loss_P: [ 5.07  2.02  2.42  1.2  10.7 ]\n",
      "Loss_Q: [2.   1.73 0.38 0.   4.12] Loss_P: [ 4.95  2.09  2.61  1.08 10.73]\n",
      "Loss_Q: [1.99 1.98 0.39 0.   4.36] Loss_P: [ 5.03  2.01  2.55  1.17 10.77]\n",
      "Loss_Q: [1.93 1.79 0.35 0.   4.07] Loss_P: [ 5.14  2.03  2.42  1.16 10.74]\n",
      "Loss_Q: [1.88 1.7  0.35 0.   3.93] Loss_P: [ 5.08  2.02  2.29  1.11 10.5 ]\n",
      "Loss_Q: [1.98 1.93 0.32 0.   4.23] Loss_P: [ 5.18  2.1   2.55  1.08 10.91]\n",
      "Loss_Q: [1.9  2.1  0.31 0.   4.32] Loss_P: [ 5.19  1.94  2.66  1.11 10.9 ]\n",
      "Loss_Q: [2.   1.72 0.33 0.   4.06] Loss_P: [ 5.1   2.13  2.52  1.13 10.88]\n",
      "Loss_Q: [2.05 1.52 0.51 0.   4.08] Loss_P: [ 5.12  2.14  2.46  1.22 10.94]\n",
      "Loss_Q: [2.08 1.74 0.44 0.   4.26] Loss_P: [ 4.97  2.24  2.25  1.26 10.72]\n",
      "Loss_Q: [1.96 1.52 0.46 0.   3.94] Loss_P: [ 5.14  2.18  2.01  1.25 10.58]\n",
      "Loss_Q: [1.99 1.62 0.41 0.   4.01] Loss_P: [ 5.21  2.22  2.25  1.3  10.98]\n",
      "Loss_Q: [2.07 1.84 0.48 0.   4.39] Loss_P: [ 5.16  2.11  2.42  1.24 10.93]\n",
      "Loss_Q: [1.87 1.98 0.37 0.   4.22] Loss_P: [ 5.21  1.99  2.71  1.21 11.12]\n",
      "Loss_Q: [1.84 2.03 0.49 0.   4.36] Loss_P: [ 5.37  1.72  2.59  1.24 10.93]\n",
      "Loss_Q: [1.86 2.2  0.44 0.   4.51] Loss_P: [ 5.3   1.66  2.83  1.23 11.02]\n",
      "Loss_Q: [1.74 2.13 0.55 0.   4.42] Loss_P: [ 5.45  1.6   2.68  1.29 11.03]\n",
      "Loss_Q: [1.67 2.01 0.46 0.   4.14] Loss_P: [ 5.34  1.72  2.32  1.3  10.69]\n",
      "Loss_Q: [1.65 1.72 0.48 0.   3.86] Loss_P: [ 5.58  1.84  2.06  1.26 10.73]\n",
      "Loss_Q: [1.85 1.79 0.49 0.   4.13] Loss_P: [ 5.36  1.74  2.51  1.27 10.88]\n",
      "Loss_Q: [1.87 1.74 0.43 0.   4.04] Loss_P: [ 5.34  1.88  2.18  1.35 10.75]\n",
      "Loss_Q: [1.76 2.01 0.38 0.   4.16] Loss_P: [ 5.43  1.77  2.78  1.04 11.02]\n",
      "Loss_Q: [1.85 2.18 0.44 0.   4.47] Loss_P: [ 5.27  1.84  2.81  1.21 11.13]\n",
      "Loss_Q: [1.89 2.1  0.36 0.   4.34] Loss_P: [ 5.35  1.74  2.5   1.2  10.79]\n",
      "Loss_Q: [1.93 2.03 0.42 0.   4.37] Loss_P: [ 5.44  2.11  2.62  1.27 11.44]\n",
      "Loss_Q: [1.77 1.87 0.45 0.   4.08] Loss_P: [ 5.42  1.75  2.24  1.23 10.64]\n",
      "Loss_Q: [1.64 1.45 0.42 0.   3.52] Loss_P: [ 5.54  1.68  2.05  1.21 10.48]\n",
      "Loss_Q: [2.08 2.2  0.42 0.   4.71] Loss_P: [ 5.36  2.    2.74  1.21 11.31]\n",
      "Loss_Q: [1.92 2.31 0.4  0.   4.63] Loss_P: [ 5.35  2.02  2.89  1.17 11.42]\n",
      "Loss_Q: [1.85 2.29 0.41 0.   4.55] Loss_P: [ 5.44  2.    2.58  1.2  11.22]\n",
      "Loss_Q: [2.05 2.11 0.34 0.   4.51] Loss_P: [ 5.48  2.1   2.72  1.17 11.47]\n",
      "Loss_Q: [2.07 2.25 0.41 0.   4.73] Loss_P: [ 5.38  2.33  2.71  1.15 11.57]\n",
      "Loss_Q: [2.42 2.39 0.47 0.   5.28] Loss_P: [ 5.59  2.3   2.8   1.27 11.96]\n"
     ]
    }
   ],
   "source": [
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(HM.n_layer)\n",
    "    Loss_P_total = np.zeros(HM.n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = HM.wake_sample(d0)\n",
    "        Loss_P,Grad_P = HM.sleep_update(Alpha_Q)\n",
    "        Alpha_P = HM.sleep_sample()\n",
    "        Loss_Q,Grad_Q = HM.wake_update(Alpha_P)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1746,
   "id": "09865248",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1771,
   "id": "5adac9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.62 1.62 0.71 0.   4.95] Loss_P: [ 6.51  4.33  3.71  1.44 15.99]\n",
      "Loss_Q: [1.78 1.1  0.41 0.   3.28] Loss_P: [ 5.3   2.79  2.55  1.35 11.99]\n",
      "Loss_Q: [1.45 0.8  0.25 0.   2.5 ] Loss_P: [ 4.88  2.76  2.19  1.29 11.12]\n",
      "Loss_Q: [1.23 0.58 0.18 0.   1.99] Loss_P: [ 4.63  2.64  2.06  1.25 10.58]\n",
      "Loss_Q: [1.05 0.47 0.15 0.   1.67] Loss_P: [ 4.44  2.4   2.03  1.27 10.15]\n",
      "Loss_Q: [0.9  0.39 0.1  0.   1.4 ] Loss_P: [ 4.35  2.37  2.04  1.23 10.  ]\n",
      "Loss_Q: [0.85 0.35 0.07 0.   1.26] Loss_P: [4.28 2.29 2.01 1.28 9.85]\n",
      "Loss_Q: [0.77 0.31 0.05 0.   1.13] Loss_P: [4.25 2.23 1.95 1.28 9.71]\n",
      "Loss_Q: [0.73 0.28 0.04 0.   1.05] Loss_P: [4.18 2.25 1.93 1.26 9.62]\n",
      "Loss_Q: [0.68 0.24 0.04 0.   0.96] Loss_P: [4.14 2.24 1.97 1.28 9.63]\n",
      "Loss_Q: [0.65 0.22 0.03 0.   0.9 ] Loss_P: [4.08 2.14 1.94 1.3  9.46]\n",
      "Loss_Q: [0.62 0.2  0.03 0.   0.86] Loss_P: [4.03 2.25 1.97 1.29 9.54]\n",
      "Loss_Q: [0.62 0.19 0.03 0.   0.84] Loss_P: [3.98 2.26 1.99 1.29 9.52]\n",
      "Loss_Q: [0.57 0.18 0.03 0.   0.78] Loss_P: [3.94 2.21 1.94 1.31 9.39]\n",
      "Loss_Q: [0.55 0.17 0.02 0.   0.74] Loss_P: [3.91 2.19 1.9  1.3  9.3 ]\n",
      "Loss_Q: [0.53 0.16 0.02 0.   0.71] Loss_P: [3.95 2.19 1.91 1.3  9.35]\n",
      "Loss_Q: [0.51 0.15 0.02 0.   0.69] Loss_P: [3.92 2.24 1.93 1.28 9.37]\n",
      "Loss_Q: [0.48 0.15 0.02 0.   0.64] Loss_P: [3.88 2.22 1.93 1.29 9.32]\n",
      "Loss_Q: [0.47 0.14 0.02 0.   0.63] Loss_P: [3.85 2.24 1.94 1.29 9.31]\n",
      "Loss_Q: [0.45 0.14 0.02 0.   0.61] Loss_P: [3.89 2.25 1.92 1.29 9.35]\n",
      "Loss_Q: [0.44 0.14 0.02 0.   0.59] Loss_P: [3.85 2.27 1.94 1.29 9.35]\n",
      "Loss_Q: [0.43 0.13 0.01 0.   0.57] Loss_P: [3.82 2.25 1.94 1.29 9.3 ]\n",
      "Loss_Q: [0.42 0.12 0.01 0.   0.55] Loss_P: [3.84 2.24 1.92 1.3  9.3 ]\n",
      "Loss_Q: [0.4  0.12 0.01 0.   0.53] Loss_P: [3.85 2.24 1.91 1.29 9.28]\n",
      "Loss_Q: [0.39 0.12 0.01 0.   0.51] Loss_P: [3.79 2.24 1.97 1.31 9.3 ]\n",
      "Loss_Q: [0.38 0.11 0.01 0.   0.5 ] Loss_P: [3.79 2.26 1.95 1.3  9.29]\n",
      "Loss_Q: [0.37 0.1  0.01 0.   0.49] Loss_P: [3.83 2.25 1.94 1.31 9.32]\n",
      "Loss_Q: [0.36 0.1  0.01 0.   0.47] Loss_P: [3.8  2.24 1.98 1.31 9.33]\n",
      "Loss_Q: [0.35 0.1  0.01 0.   0.46] Loss_P: [3.78 2.23 1.95 1.32 9.28]\n",
      "Loss_Q: [0.34 0.09 0.01 0.   0.44] Loss_P: [3.75 2.22 1.94 1.32 9.22]\n",
      "Loss_Q: [0.33 0.09 0.01 0.   0.43] Loss_P: [3.8  2.23 1.95 1.32 9.3 ]\n",
      "Loss_Q: [0.33 0.09 0.01 0.   0.43] Loss_P: [3.76 2.22 1.98 1.32 9.27]\n",
      "Loss_Q: [0.32 0.09 0.01 0.   0.42] Loss_P: [3.74 2.21 1.97 1.31 9.23]\n",
      "Loss_Q: [0.32 0.09 0.01 0.   0.41] Loss_P: [3.75 2.19 1.98 1.32 9.24]\n",
      "Loss_Q: [0.31 0.08 0.01 0.   0.4 ] Loss_P: [3.75 2.2  2.01 1.32 9.28]\n",
      "Loss_Q: [0.3  0.08 0.01 0.   0.39] Loss_P: [3.73 2.19 1.94 1.32 9.18]\n",
      "Loss_Q: [0.29 0.08 0.01 0.   0.38] Loss_P: [3.7  2.23 1.93 1.33 9.18]\n",
      "Loss_Q: [0.29 0.08 0.01 0.   0.38] Loss_P: [3.74 2.23 1.98 1.32 9.28]\n",
      "Loss_Q: [0.28 0.08 0.01 0.   0.37] Loss_P: [3.72 2.21 1.96 1.33 9.21]\n",
      "Loss_Q: [0.28 0.07 0.01 0.   0.36] Loss_P: [3.69 2.22 1.93 1.33 9.17]\n",
      "Loss_Q: [0.28 0.07 0.01 0.   0.35] Loss_P: [3.72 2.22 1.94 1.32 9.21]\n",
      "Loss_Q: [0.27 0.07 0.01 0.   0.34] Loss_P: [3.69 2.21 1.93 1.33 9.15]\n",
      "Loss_Q: [0.26 0.06 0.01 0.   0.33] Loss_P: [3.69 2.22 1.92 1.32 9.16]\n",
      "Loss_Q: [0.26 0.06 0.01 0.   0.32] Loss_P: [3.73 2.18 1.93 1.32 9.15]\n",
      "Loss_Q: [0.26 0.06 0.01 0.   0.32] Loss_P: [3.72 2.25 1.94 1.32 9.23]\n",
      "Loss_Q: [0.26 0.06 0.01 0.   0.32] Loss_P: [3.71 2.22 1.94 1.32 9.19]\n",
      "Loss_Q: [0.25 0.05 0.01 0.   0.31] Loss_P: [3.71 2.22 1.92 1.32 9.17]\n",
      "Loss_Q: [0.25 0.05 0.01 0.   0.3 ] Loss_P: [3.74 2.2  1.92 1.32 9.18]\n",
      "Loss_Q: [0.24 0.05 0.01 0.   0.3 ] Loss_P: [3.7  2.21 1.92 1.32 9.15]\n",
      "Loss_Q: [0.24 0.05 0.01 0.   0.3 ] Loss_P: [3.75 2.21 1.92 1.32 9.2 ]\n",
      "Loss_Q: [0.24 0.05 0.01 0.   0.29] Loss_P: [3.72 2.19 1.92 1.32 9.15]\n",
      "Loss_Q: [0.23 0.05 0.01 0.   0.29] Loss_P: [3.71 2.22 1.92 1.32 9.16]\n",
      "Loss_Q: [0.23 0.05 0.01 0.   0.28] Loss_P: [3.72 2.23 1.92 1.32 9.19]\n",
      "Loss_Q: [0.23 0.05 0.01 0.   0.28] Loss_P: [3.72 2.19 1.94 1.32 9.17]\n",
      "Loss_Q: [0.23 0.04 0.01 0.   0.28] Loss_P: [3.71 2.22 1.93 1.32 9.18]\n",
      "Loss_Q: [0.23 0.04 0.01 0.   0.28] Loss_P: [3.66 2.23 1.93 1.33 9.15]\n",
      "Loss_Q: [0.22 0.04 0.01 0.   0.27] Loss_P: [3.68 2.21 1.92 1.32 9.14]\n",
      "Loss_Q: [0.22 0.04 0.01 0.   0.27] Loss_P: [3.69 2.2  1.92 1.32 9.13]\n",
      "Loss_Q: [0.22 0.04 0.01 0.   0.26] Loss_P: [3.69 2.22 1.92 1.33 9.15]\n",
      "Loss_Q: [0.22 0.04 0.01 0.   0.26] Loss_P: [3.74 2.22 1.93 1.32 9.21]\n",
      "Loss_Q: [0.22 0.04 0.01 0.   0.26] Loss_P: [3.7  2.21 1.93 1.33 9.17]\n",
      "Loss_Q: [0.21 0.04 0.   0.   0.26] Loss_P: [3.71 2.21 1.94 1.33 9.19]\n",
      "Loss_Q: [0.21 0.04 0.   0.   0.25] Loss_P: [3.7  2.23 1.93 1.32 9.19]\n",
      "Loss_Q: [0.21 0.04 0.   0.   0.25] Loss_P: [3.72 2.22 1.94 1.33 9.2 ]\n",
      "Loss_Q: [0.21 0.04 0.   0.   0.25] Loss_P: [3.73 2.21 1.93 1.33 9.19]\n",
      "Loss_Q: [0.21 0.04 0.   0.   0.25] Loss_P: [3.7  2.21 1.94 1.32 9.18]\n",
      "Loss_Q: [0.21 0.04 0.   0.   0.25] Loss_P: [3.7  2.21 1.92 1.32 9.16]\n",
      "Loss_Q: [0.2  0.04 0.   0.   0.24] Loss_P: [3.7  2.19 1.94 1.32 9.16]\n",
      "Loss_Q: [0.2  0.03 0.   0.   0.24] Loss_P: [3.73 2.2  1.94 1.32 9.19]\n",
      "Loss_Q: [0.2  0.03 0.   0.   0.24] Loss_P: [3.72 2.2  1.94 1.32 9.18]\n",
      "Loss_Q: [0.2  0.03 0.   0.   0.24] Loss_P: [3.72 2.19 1.95 1.32 9.18]\n",
      "Loss_Q: [0.2  0.03 0.   0.   0.24] Loss_P: [3.72 2.22 1.94 1.32 9.2 ]\n",
      "Loss_Q: [0.2  0.03 0.   0.   0.23] Loss_P: [3.73 2.2  1.93 1.32 9.19]\n",
      "Loss_Q: [0.19 0.03 0.   0.   0.23] Loss_P: [3.71 2.22 1.93 1.33 9.19]\n",
      "Loss_Q: [0.19 0.03 0.   0.   0.23] Loss_P: [3.71 2.2  1.94 1.32 9.17]\n",
      "Loss_Q: [0.19 0.03 0.   0.   0.23] Loss_P: [3.69 2.2  1.93 1.31 9.14]\n",
      "Loss_Q: [0.19 0.03 0.   0.   0.22] Loss_P: [3.69 2.22 1.94 1.33 9.18]\n",
      "Loss_Q: [0.19 0.03 0.   0.   0.22] Loss_P: [3.72 2.2  1.94 1.32 9.18]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.22] Loss_P: [3.72 2.2  1.94 1.32 9.18]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.22] Loss_P: [3.7  2.22 1.94 1.32 9.17]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.22] Loss_P: [3.73 2.19 1.94 1.32 9.18]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.21] Loss_P: [3.7  2.21 1.94 1.32 9.17]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.21] Loss_P: [3.71 2.2  1.92 1.33 9.16]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.21] Loss_P: [3.72 2.21 1.94 1.33 9.2 ]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.21] Loss_P: [3.71 2.19 1.94 1.32 9.15]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.21] Loss_P: [3.71 2.2  1.94 1.33 9.18]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.21] Loss_P: [3.71 2.22 1.93 1.32 9.17]\n",
      "Loss_Q: [0.18 0.03 0.   0.   0.21] Loss_P: [3.69 2.22 1.95 1.32 9.18]\n",
      "Loss_Q: [0.17 0.03 0.   0.   0.2 ] Loss_P: [3.7  2.21 1.93 1.32 9.16]\n",
      "Loss_Q: [0.17 0.03 0.   0.   0.2 ] Loss_P: [3.68 2.19 1.94 1.33 9.15]\n",
      "Loss_Q: [0.17 0.03 0.   0.   0.2 ] Loss_P: [3.68 2.2  1.94 1.32 9.13]\n",
      "Loss_Q: [0.17 0.03 0.   0.   0.2 ] Loss_P: [3.69 2.2  1.93 1.32 9.15]\n",
      "Loss_Q: [0.17 0.02 0.   0.   0.2 ] Loss_P: [3.69 2.22 1.92 1.32 9.15]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.19] Loss_P: [3.71 2.21 1.93 1.32 9.17]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.19] Loss_P: [3.69 2.21 1.94 1.32 9.15]\n",
      "Loss_Q: [0.17 0.02 0.   0.   0.19] Loss_P: [3.7  2.2  1.95 1.32 9.17]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.19] Loss_P: [3.7  2.2  1.93 1.33 9.16]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.19] Loss_P: [3.71 2.17 1.93 1.33 9.14]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.19] Loss_P: [3.7  2.18 1.94 1.33 9.14]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.19] Loss_P: [3.72 2.2  1.94 1.32 9.19]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.18] Loss_P: [3.71 2.19 1.93 1.32 9.16]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.18] Loss_P: [3.71 2.18 1.94 1.32 9.16]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.18] Loss_P: [3.68 2.19 1.94 1.33 9.13]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.18] Loss_P: [3.69 2.21 1.93 1.32 9.15]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.19] Loss_P: [3.71 2.21 1.93 1.32 9.17]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.18] Loss_P: [3.7  2.2  1.93 1.33 9.16]\n",
      "Loss_Q: [0.16 0.02 0.   0.   0.18] Loss_P: [3.71 2.22 1.93 1.33 9.19]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.18] Loss_P: [3.68 2.19 1.93 1.32 9.13]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.7  2.18 1.93 1.31 9.12]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.71 2.2  1.93 1.32 9.17]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.7  2.22 1.94 1.32 9.18]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.67 2.21 1.94 1.31 9.13]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.71 2.21 1.93 1.32 9.17]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.71 2.2  1.94 1.33 9.17]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.68 2.19 1.94 1.32 9.13]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.71 2.21 1.94 1.33 9.17]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.69 2.2  1.93 1.32 9.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.69 2.21 1.94 1.32 9.16]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.17] Loss_P: [3.68 2.19 1.93 1.31 9.12]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.7  2.21 1.94 1.32 9.16]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.7  2.19 1.93 1.32 9.14]\n",
      "Loss_Q: [0.15 0.02 0.   0.   0.17] Loss_P: [3.73 2.22 1.93 1.32 9.2 ]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.72 2.2  1.94 1.32 9.18]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.72 2.19 1.93 1.33 9.17]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.71 2.22 1.94 1.33 9.2 ]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.69 2.19 1.93 1.32 9.14]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.69 2.21 1.93 1.33 9.16]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.67 2.21 1.94 1.32 9.14]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.71 2.21 1.93 1.33 9.18]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.73 2.22 1.93 1.32 9.21]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.7  2.21 1.94 1.33 9.18]\n",
      "Loss_Q: [0.14 0.02 0.   0.   0.16] Loss_P: [3.66 2.21 1.94 1.32 9.12]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.7  2.18 1.93 1.32 9.13]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.69 2.22 1.94 1.33 9.19]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.68 2.21 1.93 1.32 9.14]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.7  2.21 1.94 1.33 9.18]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.7  2.22 1.94 1.33 9.19]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.7  2.2  1.94 1.33 9.17]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.69 2.22 1.94 1.32 9.17]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.7  2.21 1.93 1.33 9.16]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.66 2.21 1.94 1.32 9.14]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.71 2.2  1.94 1.32 9.18]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.71 2.24 1.94 1.33 9.21]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.71 2.22 1.93 1.33 9.19]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.69 2.2  1.94 1.32 9.14]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.15] Loss_P: [3.69 2.2  1.94 1.33 9.15]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.14] Loss_P: [3.69 2.19 1.94 1.33 9.16]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.14] Loss_P: [3.7  2.21 1.94 1.32 9.17]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.14] Loss_P: [3.69 2.19 1.94 1.33 9.15]\n",
      "Loss_Q: [0.13 0.02 0.   0.   0.14] Loss_P: [3.71 2.22 1.94 1.33 9.19]\n"
     ]
    }
   ],
   "source": [
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(HM.n_layer)\n",
    "    Loss_P_total = np.zeros(HM.n_layer)\n",
    "    Nums_total = np.zeros((HM.n_layer-2))\n",
    "    \n",
    "    HM.prob_matrix()\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q,Loss_Q,Grad_Q = HM.MI_wake(d0,HM.Prob_mtx_wake,precision=0.2)\n",
    "        Loss_P,Grad_P = HM.sleep_update(Alpha_Q)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "        \n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    \n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1772,
   "id": "fe694d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Prob_01': array([[ 3.90625000e-03,  3.90625000e-03,  3.90625000e-03, ...,\n",
       "          3.90625000e-03,  3.90625000e-03,  5.54517744e+00],\n",
       "        [ 5.64872661e-19,  7.43147839e-18,  1.67497628e-12, ...,\n",
       "          8.18540456e-03,  1.07687380e-01,  2.69445327e+00],\n",
       "        [ 5.35041386e-03,  1.33271960e-01,  1.19128492e-04, ...,\n",
       "          4.06556111e-11,  1.00000000e+00, -5.44142472e-01],\n",
       "        ...,\n",
       "        [ 3.89971472e-21,  2.73729427e-10,  1.52925728e-29, ...,\n",
       "          4.97995319e-36,  3.49553706e-25,  3.87518289e-01],\n",
       "        [ 1.55019459e-29,  1.43150621e-17,  1.80256960e-31, ...,\n",
       "          2.86858837e-28,  1.00000000e+00, -1.37623947e+00],\n",
       "        [ 9.30426375e-06,  1.00205431e+00,  5.29388339e-06, ...,\n",
       "          1.40079374e-05,  3.01009836e+02,  4.04834871e+00]]),\n",
       " 'Prob_12': array([[ 1.56250000e-02,  1.56250000e-02,  1.56250000e-02, ...,\n",
       "          1.56250000e-02,  1.56250000e-02,  4.15888308e+00],\n",
       "        [ 4.60960899e-09,  7.16100356e-07,  7.74429587e-11, ...,\n",
       "          2.82138758e-06,  1.00043830e+00, -1.32758196e+00],\n",
       "        [ 2.89857798e-08,  3.06141307e-08,  3.88618975e-08, ...,\n",
       "          1.44078094e-06,  1.52172053e-06,  2.42239218e+00],\n",
       "        ...,\n",
       "        [ 8.76307897e-17,  1.29715885e-15,  3.48737863e-12, ...,\n",
       "          1.19283220e-06,  1.76569544e-05,  3.90281654e-01],\n",
       "        [ 1.04823815e-19,  2.41049481e-16,  7.00842105e-17, ...,\n",
       "          8.73335782e-07,  1.00200829e+00, -1.35303763e+00],\n",
       "        [ 1.23106601e-04,  5.27279542e-04,  1.71711061e-04, ...,\n",
       "          2.75932137e-02,  3.00002943e+02,  2.48616752e+00]]),\n",
       " 'Prob_23': array([[ 1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "          1.25000000e-01,  1.25000000e-01,  1.25000000e-01,\n",
       "          1.25000000e-01,  1.25000000e-01,  2.07944154e+00],\n",
       "        [ 1.33707128e-03,  6.69099017e-05,  1.04364935e-02,\n",
       "          5.22264418e-04,  1.06816249e-01,  5.34531318e-03,\n",
       "          8.33752922e-01,  4.17227765e-02,  6.12060553e-01],\n",
       "        [ 3.37198876e-02,  1.04063306e-02,  3.07539899e-01,\n",
       "          9.49102173e-02,  4.17876684e-02,  1.28961371e-02,\n",
       "          3.81121535e-01,  1.17618325e-01,  1.55611511e+00],\n",
       "        [ 1.36414716e-04,  2.10672843e-06,  9.71128319e-03,\n",
       "          1.49976754e-04,  1.35053551e-02,  2.08570720e-04,\n",
       "          9.61438266e-01,  1.48480266e-02,  2.07789558e-01],\n",
       "        [ 2.71832231e-01,  7.57582191e-02,  5.10214146e-01,\n",
       "          1.42194010e-01,  3.78781158e-07,  1.05564325e-07,\n",
       "          7.10951399e-07,  1.98138431e-07,  1.17026706e+00],\n",
       "        [ 6.30099607e-02,  8.78767219e-04,  9.23125611e-01,\n",
       "          1.28743538e-02,  7.01422010e-06,  9.78236871e-08,\n",
       "          1.02761629e-04,  1.43316311e-06,  3.11296963e-01],\n",
       "        [ 5.08209464e-02,  4.37102263e-03,  8.69980749e-01,\n",
       "          7.48255555e-02,  8.77591054e-08,  7.54801048e-09,\n",
       "          1.50230835e-06,  1.29210971e-07,  4.90355759e-01],\n",
       "        [ 7.39553829e-03,  3.18306809e-05,  9.88181525e-01,\n",
       "          4.25317125e-03,  1.02023917e-06,  4.39114857e-09,\n",
       "          1.36322937e-04,  5.86739156e-07,  7.28255936e-02],\n",
       "        [ 2.26611260e-02,  5.57464855e-03,  3.68845471e-01,\n",
       "          9.07361740e-02,  2.37929879e-02,  5.85308716e-03,\n",
       "          3.87268305e-01,  9.52682003e-02,  1.41077136e+00],\n",
       "        [ 9.08910202e-05,  1.11890398e-06,  1.15473943e-02,\n",
       "          1.42152936e-04,  7.62379552e-03,  9.38519020e-05,\n",
       "          9.68577231e-01,  1.19235642e-02,  1.75420553e-01],\n",
       "        [ 2.70259169e-03,  2.05176654e-04,  4.01197996e-01,\n",
       "          3.04583423e-02,  3.51649462e-03,  2.66966928e-04,\n",
       "          5.22021363e-01,  3.96310687e-02,  9.79819449e-01],\n",
       "        [ 8.18521008e-06,  3.10966188e-08,  9.48436361e-03,\n",
       "          3.60322626e-05,  8.50828666e-04,  3.23240264e-06,\n",
       "          9.85871878e-01,  3.74544840e-03,  8.56543739e-02],\n",
       "        [ 2.96618529e-02,  2.03358856e-03,  9.06176493e-01,\n",
       "          6.21266027e-02,  4.33963489e-08,  2.97521261e-09,\n",
       "          1.32576854e-06,  9.08934361e-08,  3.78872918e-01],\n",
       "        [ 4.16130735e-03,  1.42767857e-05,  9.92303116e-01,\n",
       "          3.40443464e-03,  4.86370776e-07,  1.66866101e-09,\n",
       "          1.15979714e-04,  3.97908008e-07,  5.10487377e-02],\n",
       "        [ 3.50203505e-03,  7.40963478e-05,  9.75776496e-01,\n",
       "          2.06455600e-02,  6.34946928e-09,  1.34342597e-10,\n",
       "          1.76916073e-06,  3.74320493e-08,  1.24568563e-01],\n",
       "        [ 4.59037283e-04,  4.86026676e-07,  9.98338617e-01,\n",
       "          1.05703658e-03,  6.64886723e-08,  7.03979167e-11,\n",
       "          1.44603089e-04,  1.53105121e-07,  1.37205325e-02],\n",
       "        [ 3.39595162e-01,  2.03307501e-01,  6.98676739e-07,\n",
       "          4.18281051e-07,  2.85921135e-01,  1.71174144e-01,\n",
       "          5.88248799e-07,  3.52170485e-07,  1.35078845e+00],\n",
       "        [ 1.42231662e-02,  4.26112253e-04,  2.28407900e-07,\n",
       "          6.84287896e-09,  9.56673649e-01,  2.86610139e-02,\n",
       "          1.53630926e-05,  4.60263341e-07,  2.08161118e-01],\n",
       "        [ 4.13057825e-01,  7.63157647e-02,  7.75070116e-06,\n",
       "          1.43200455e-06,  4.30980520e-01,  7.96271271e-02,\n",
       "          8.08700626e-06,  1.49413963e-06,  1.12603024e+00],\n",
       "        [ 1.17443976e-02,  1.08585037e-04,  1.72012865e-06,\n",
       "          1.59037730e-08,  9.78949513e-01,  9.05106186e-03,\n",
       "          1.43380628e-04,  1.32565257e-06,  1.17909145e-01],\n",
       "        [ 8.57005421e-01,  1.42989544e-01,  3.30940489e-06,\n",
       "          5.52167215e-07,  1.00543906e-06,  1.67755383e-07,\n",
       "          3.88259496e-12,  6.47802768e-13,  4.10423955e-01],\n",
       "        [ 9.91596883e-01,  8.27926336e-03,  2.98883220e-05,\n",
       "          2.49550290e-07,  9.29373099e-05,  7.75973058e-07,\n",
       "          2.80127973e-09,  2.33890738e-11,  4.92474474e-02],\n",
       "        [ 9.50995518e-01,  4.89678102e-02,  3.34934708e-05,\n",
       "          1.72461583e-06,  1.38265132e-06,  7.11942448e-08,\n",
       "          4.86961199e-11,  2.50741703e-12,  1.95887400e-01],\n",
       "        [ 9.97039969e-01,  2.56909766e-03,  2.74090699e-04,\n",
       "          7.06256315e-07,  1.15805593e-04,  2.98399150e-07,\n",
       "          3.18354700e-08,  8.20312465e-11,  2.15910181e-02],\n",
       "        [ 4.62633602e-01,  6.81341882e-02,  1.54922656e-05,\n",
       "          2.28161753e-06,  4.08968102e-01,  6.02306221e-02,\n",
       "          1.36951627e-05,  2.01694989e-06,  1.07490367e+00],\n",
       "        [ 1.38565778e-02,  1.02122098e-04,  3.62187777e-06,\n",
       "          2.66930091e-08,  9.78568001e-01,  7.21198401e-03,\n",
       "          2.55781315e-04,  1.88509205e-06,  1.19187232e-01],\n",
       "        [ 4.56325998e-01,  2.07402805e-02,  1.39369686e-04,\n",
       "          6.33443282e-06,  4.99907321e-01,  2.27210768e-02,\n",
       "          1.52680160e-04,  6.93940156e-06,  8.73718535e-01],\n",
       "        [ 1.12447030e-02,  2.55754470e-05,  2.68065635e-05,\n",
       "          6.09700272e-08,  9.84113153e-01,  2.23831024e-03,\n",
       "          2.34605501e-03,  5.33597070e-06,  9.47066823e-02],\n",
       "        [ 9.60512024e-01,  3.94238947e-02,  6.03715201e-05,\n",
       "          2.47792884e-06,  1.18315730e-06,  4.85622954e-08,\n",
       "          7.43655495e-11,  3.05230910e-12,  1.66805875e-01],\n",
       "        [ 9.97362744e-01,  2.04854587e-03,  4.89308199e-04,\n",
       "          1.00502079e-06,  9.81466736e-05,  2.01589606e-07,\n",
       "          4.81509584e-08,  9.89002724e-11,  1.99689166e-02],\n",
       "        [ 9.86924341e-01,  1.25012109e-02,  5.65755148e-04,\n",
       "          7.16632888e-06,  1.50655698e-06,  1.90833134e-08,\n",
       "          8.63634966e-10,  1.09395243e-11,  7.21050213e-02],\n",
       "        [ 9.94793462e-01,  6.30575328e-04,  4.45120527e-03,\n",
       "          2.82151053e-06,  1.21315806e-04,  7.68991324e-08,\n",
       "          5.42827808e-07,  3.44085317e-10,  3.50800723e-02],\n",
       "        [ 1.53317684e-02,  1.05358627e-04,  8.49337883e-02,\n",
       "          5.83657870e-04,  1.36536224e-01,  9.38265484e-04,\n",
       "          7.56373204e-01,  5.19773323e-03,  7.95740968e-01],\n",
       "        [ 3.17164518e-05,  1.09068233e-08,  1.37142760e-03,\n",
       "          4.71613870e-07,  2.25643588e-02,  7.75955258e-06,\n",
       "          9.75688730e-01,  3.35525067e-04,  1.21714838e-01],\n",
       "        [ 1.60907752e-03,  3.41245017e-06,  8.12981310e-02,\n",
       "          1.72412962e-04,  1.77580236e-02,  3.76603178e-05,\n",
       "          8.97218507e-01,  1.90277560e-03,  3.97108369e-01],\n",
       "        [ 2.86521817e-06,  3.04076515e-10,  1.12995549e-03,\n",
       "          1.19918592e-07,  2.52614410e-03,  2.68091660e-07,\n",
       "          1.99962349e+01,  1.90001057e+01, -2.08295215e-01],\n",
       "        [ 8.75677418e-02,  1.67706852e-04,  9.10508367e-01,\n",
       "          1.74377561e-03,  1.08664380e-06,  2.08110439e-09,\n",
       "          1.12986615e-05,  2.16388240e-08,  3.11297521e-01],\n",
       "        [ 1.21582145e-02,  1.16523170e-06,  9.86755685e-01,\n",
       "          9.45697252e-05,  1.20530129e-05,  1.15514928e-09,\n",
       "          9.78217565e-04,  9.37514399e-08,  7.45800148e-02],\n",
       "        [ 1.04285981e-02,  6.16373686e-06,  9.88965340e-01,\n",
       "          5.84519804e-04,  1.60372910e-07,  9.47870852e-11,\n",
       "          1.52084919e-05,  8.98885362e-09,  6.31582944e-02],\n",
       "        [ 1.34744929e-03,  3.98534955e-08,  1.59973960e+01,\n",
       "          2.94999721e-05,  1.65538978e-06,  4.89614488e-11,\n",
       "          1.22533674e-03,  1.50000000e+01, -2.48896182e-01],\n",
       "        [ 1.05797271e-03,  1.78849744e-06,  9.53950149e-02,\n",
       "          1.61264784e-04,  9.89230738e-03,  1.67228949e-05,\n",
       "          8.91967063e-01,  1.50786575e-03,  3.90451517e-01],\n",
       "        [ 1.89674284e-06,  1.60456806e-10,  1.33493376e-03,\n",
       "          1.12930020e-07,  1.41681894e-03,  1.19857177e-07,\n",
       "          2.69971618e+01,  2.60000844e+01, -1.64098499e-01],\n",
       "        [ 9.64370417e-05,  5.03117284e-08,  7.93068031e-02,\n",
       "          4.13747899e-05,  1.11745242e-03,  5.82980995e-07,\n",
       "          9.18957874e-01,  4.79425314e-04,  2.91241094e-01],\n",
       "        [ 1.68080594e-07,  4.38812272e-12,  1.07890743e-03,\n",
       "          2.81673101e-08,  1.55591367e-04,  4.06206331e-09,\n",
       "          4.89987392e+01,  4.80000261e+01, -1.01588197e-01],\n",
       "        [ 5.87122094e-03,  2.76612046e-06,  4.99364485e+00,\n",
       "          4.68137950e-04,  7.64960445e-08,  3.60397395e-11,\n",
       "          1.29461829e-05,  4.00000001e+00, -6.22941890e-01],\n",
       "        [ 7.55625336e-04,  1.78149664e-08,  2.09981810e+01,\n",
       "          2.35335700e-05,  7.86501996e-07,  1.85429286e-11,\n",
       "          1.03896911e-03,  2.00000000e+01, -2.00739288e-01],\n",
       "        [ 6.47337053e-04,  9.41205880e-08,  2.59991911e+01,\n",
       "          1.45278966e-04,  1.04520877e-08,  1.51969771e-12,\n",
       "          1.61332236e-05,  2.50000000e+01, -1.69439147e-01],\n",
       "        [ 8.28863098e-05,  6.03077050e-10,  4.69986216e+01,\n",
       "          7.26592578e-06,  1.06914878e-07,  7.77907828e-13,\n",
       "          1.28811993e-03,  4.60000000e+01, -1.05089672e-01],\n",
       "        [ 1.17192473e-01,  4.82135906e-04,  1.33568102e-06,\n",
       "          5.49506091e-09,  8.78698981e-01,  3.61501314e-03,\n",
       "          1.00148202e-05,  4.12014893e-08,  3.89022818e-01],\n",
       "        [ 1.66618939e-03,  3.43028301e-07,  1.48227043e-07,\n",
       "          3.05163814e-11,  9.98039042e-01,  2.05472223e-04,\n",
       "          8.87872513e-05,  1.82791586e-08,  1.51985916e-02],\n",
       "        [ 9.70309199e-02,  1.23194525e-04,  1.00862222e-05,\n",
       "          1.28058907e-08,  9.01597242e-01,  1.14470567e-03,\n",
       "          9.37197149e-05,  1.18990480e-07,  3.29589644e-01],\n",
       "        [ 1.34415801e-03,  8.54018766e-08,  1.09060819e-06,\n",
       "          6.92924383e-11,  2.19977817e+01,  6.33946491e-05,\n",
       "          8.09569136e-04,  2.10000001e+01, -1.93385826e-01],\n",
       "        [ 9.98822946e-01,  1.14521512e-03,  2.13669774e-05,\n",
       "          2.44986219e-08,  1.04355738e-05,  1.19650604e-08,\n",
       "          2.23239434e-10,  2.55958453e-13,  9.28206950e-03],\n",
       "        [ 9.98941909e-01,  5.73158358e-05,  1.66799544e-04,\n",
       "          9.57038162e-09,  8.33778742e-04,  4.78393438e-08,\n",
       "          1.39221223e-07,  7.98803281e-12,  8.98256391e-03],\n",
       "        [ 2.29994383e+01,  3.53643819e-04,  1.94996105e-04,\n",
       "          6.89979203e-08,  1.29403449e-05,  4.57884471e-09,\n",
       "          2.52473488e-09,  2.20000000e+01, -1.86894171e-01],\n",
       "        [ 2.09974300e+01,  1.76615306e-05,  1.51898127e-03,\n",
       "          2.68966576e-08,  1.03170384e-03,  1.82684182e-08,\n",
       "          1.57117668e-06,  2.00000000e+01, -2.00624913e-01],\n",
       "        [ 1.12574066e-01,  1.13931579e-04,  2.08835516e-05,\n",
       "          2.11353830e-08,  8.86229610e-01,  8.96916514e-04,\n",
       "          1.64403955e-04,  1.66386477e-07,  3.61904323e-01],\n",
       "        [ 1.58514480e-03,  8.02805866e-08,  2.29527640e-06,\n",
       "          1.16245617e-10,  1.19969184e+01,  5.04895154e-05,\n",
       "          1.44352948e-03,  1.10000001e+01, -3.12215313e-01],\n",
       "        [ 9.27857118e-02,  2.89799991e-05,  1.56986555e-04,\n",
       "          4.90320129e-08,  9.05213512e-01,  2.82727656e-04,\n",
       "          1.53155424e-03,  4.78354261e-07,  3.24662338e-01],\n",
       "        [ 1.26469491e-03,  1.97669138e-08,  1.67019497e-05,\n",
       "          2.61047938e-10,  2.98568571e+00,  1.54060589e-05,\n",
       "          1.30172684e-02,  2.00000020e+00, -9.46872110e-01],\n",
       "        [ 7.99935909e+00,  2.81874726e-04,  3.47967848e-04,\n",
       "          9.81462449e-08,  1.09626838e-05,  3.09208524e-09,\n",
       "          3.81710790e-09,  7.00000000e+00, -4.30404054e-01],\n",
       "        [ 1.49964023e+01,  1.40638790e-05,  2.70802278e-03,\n",
       "          3.82228192e-08,  8.73198137e-04,  1.23248943e-08,\n",
       "          2.37317845e-06,  1.40000000e+01, -2.61728684e-01],\n",
       "        [ 9.96734089e-01,  8.67611666e-05,  3.16528056e-03,\n",
       "          2.75523269e-07,  1.35499176e-05,  1.17945866e-09,\n",
       "          4.30298223e-08,  3.74555022e-12,  2.24466073e-02],\n",
       "        [ 9.74749194e-01,  4.24594914e-06,  2.41616078e-02,\n",
       "          1.05246517e-07,  1.05860268e-03,  4.61120991e-09,\n",
       "          2.62401271e-05,  1.14300423e-10,  1.22466090e-01],\n",
       "        [ 6.32906207e+01,  1.33435439e-02,  1.10260400e+02,\n",
       "          1.04284363e-02,  3.42123970e+01,  8.40406243e-03,\n",
       "          9.31972283e+01,  3.00007178e+02,  1.28443173e+00]])}"
      ]
     },
     "execution_count": 1772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Prob_mtx_wake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1721,
   "id": "afcdf12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.680781374717379"
      ]
     },
     "execution_count": 1721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.prob_matrix()\n",
    "(HM.Prob_mtx_wake['Prob_12'][-1,:-1] * np.log(HM.Prob_mtx_wake['Prob_12'][-1,:-1])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1722,
   "id": "3636af63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.6730563979879642"
      ]
     },
     "execution_count": 1722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Prob_mtx_wake['Prob_12'][-1,1] += 1\n",
    "(HM.Prob_mtx_wake['Prob_12'][-1,:-1] * np.log(HM.Prob_mtx_wake['Prob_12'][-1,:-1])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1724,
   "id": "7daabdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0296753795539275"
      ]
     },
     "execution_count": 1724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(HM.Prob_mtx_wake['Prob_12'][-1,:-1]/2 * np.log(HM.Prob_mtx_wake['Prob_12'][-1,:-1]/2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "id": "b253d22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_M(HM.Prob_mtx_wake['Prob_12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "id": "e7dfce9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.51255143, 0.60690873, 0.58530356, 0.60263829, 0.31615883,\n",
       "        0.60088686, 0.58888015, 0.6028854 , 0.68880302, 0.90463516,\n",
       "        0.60575849, 0.60799333, 0.3792804 , 0.60056428, 0.59771875,\n",
       "        0.60299595, 0.46592698, 0.59916467, 0.58955471, 0.60271819,\n",
       "        0.45651492, 0.59413038, 0.5990413 , 0.60261077, 0.52892298,\n",
       "        0.60405236, 0.6027437 , 0.60292358, 0.58786925, 0.60296805,\n",
       "        0.60298119, 0.6029953 , 0.54215891, 0.5792465 , 0.52543661,\n",
       "        0.6026024 , 0.57745008, 0.59080641, 0.58294648, 0.60286382,\n",
       "        0.85983496, 0.9199371 , 0.58945983, 0.60821857, 0.6027992 ,\n",
       "        0.60302442, 0.60289193, 0.60299469, 0.58858301, 0.55734297,\n",
       "        0.58188582, 0.60164367, 0.56383163, 0.53485548, 0.58529454,\n",
       "        0.59787462, 0.59940603, 0.59159166, 0.58728942, 0.59749926,\n",
       "        0.60295692, 0.60278075, 0.60280302, 0.60295215]),\n",
       " array([[1.42350526, 1.42276665, 1.43015122, 1.4301437 , 1.43021466,\n",
       "         1.43021462, 1.430215  , 1.430215  , 1.14782335, 1.1246182 ,\n",
       "         1.42562312, 1.42511281, 1.43018556, 1.43018207, 1.43021478,\n",
       "         1.43021475, 1.43020004, 1.43019826, 1.43021489, 1.43021488,\n",
       "         1.430215  , 1.430215  , 1.43021501, 1.43021501, 1.42906386,\n",
       "         1.42893252, 1.43020504, 1.43020385, 1.43021495, 1.43021495,\n",
       "         1.430215  , 1.430215  , 1.42256066, 1.4217211 , 1.43014159,\n",
       "         1.43013295, 1.43021461, 1.43021456, 1.430215  , 1.430215  ,\n",
       "         1.1183047 , 1.09323457, 1.42497038, 1.42438939, 1.43018109,\n",
       "         1.43017707, 1.43021474, 1.43021471, 1.43019776, 1.43019571,\n",
       "         1.43021487, 1.43021486, 1.430215  , 1.430215  , 1.43021501,\n",
       "         1.43021501, 1.42889578, 1.42874558, 1.43020352, 1.43020215,\n",
       "         1.43021495, 1.43021494, 1.430215  , 1.430215  ],\n",
       "        [1.93605669, 2.02967538, 2.01545477, 2.03278199, 1.74637349,\n",
       "         2.03110148, 2.01909515, 2.03310041, 1.83662637, 2.02925336,\n",
       "         2.0313816 , 2.03310614, 1.80946596, 2.03074635, 2.02793353,\n",
       "         2.0332107 , 1.89612702, 2.02936292, 2.0197696 , 2.03293307,\n",
       "         1.88672993, 2.02434538, 2.02925631, 2.03282578, 1.95798684,\n",
       "         2.03298488, 2.03294874, 2.03312743, 2.01808421, 2.033183  ,\n",
       "         2.0331962 , 2.0332103 , 1.96471957, 2.0009676 , 1.9555782 ,\n",
       "         2.03273535, 2.00766469, 2.02102097, 2.01316148, 2.03307882,\n",
       "         1.97813966, 2.01317167, 2.01443021, 2.03260796, 2.03298028,\n",
       "         2.03320149, 2.03310667, 2.0332094 , 2.01878077, 1.98753868,\n",
       "         2.01210069, 2.03185853, 1.99404663, 1.96507049, 2.01550954,\n",
       "         2.02808963, 2.02830181, 2.02033723, 2.01749294, 2.02770141,\n",
       "         2.03317187, 2.03299569, 2.03301803, 2.03316715]]))"
      ]
     },
     "execution_count": 1704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info(HM.Prob_mtx_wake['Prob_12'][[1,-1],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fcf86",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1773,
   "id": "526b91c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 1, 1, 0],\n",
       "       [1, 1, 0, ..., 1, 1, 1],\n",
       "       [0, 1, 1, ..., 0, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 1773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 10000\n",
    "Alpha_P_bnr = HM.sleep_sample_batch(n_sample)\n",
    "generation = Alpha_P_bnr['z0']\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1774,
   "id": "4fada9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1658, 4171, 1172, 2998,    1], dtype=int64)"
      ]
     },
     "execution_count": 1774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(Alpha_P_bnr['z3'],axis=1, return_counts=True) #[10,8,6,3,1]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1775,
   "id": "79d3b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha_Q_bnr = HM.wake_sample_batch(np.repeat(dataset,30,axis=1),n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1776,
   "id": "90182fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1871,    1, 3325, 1021, 2782], dtype=int64)"
      ]
     },
     "execution_count": 1776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(Alpha_Q_bnr['z3'],axis=1, return_counts=True) #[10,8,6,3,1]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1777,
   "id": "ef02b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dcm = decimalize(dataset)\n",
    "gen_dcm = decimalize(generation)\n",
    "values_d, counts_d = np.unique(data_dcm, return_counts=True)\n",
    "values_g, counts_g = np.unique(gen_dcm, return_counts=True)\n",
    "values_dg = np.unique(np.append(values_d,values_g))\n",
    "dist_d = counts_d\n",
    "dist_g = counts_g/n_sample*n_data\n",
    "dist = np.zeros((2,values_dg.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1778,
   "id": "f7d6675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy, d_ind, dg_ind_d = np.intersect1d(values_d, values_dg, return_indices=True)\n",
    "xy, g_ind, dg_ind_g = np.intersect1d(values_g, values_dg, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1779,
   "id": "65896d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist[0,dg_ind_d] = dist_d\n",
    "dist[1,dg_ind_g] = dist_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1780,
   "id": "63376188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425.63199999999995"
      ]
     },
     "execution_count": 1780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = np.sum((dist[0,:] - dist[1,:])**2)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1781,
   "id": "4a399104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4003"
      ]
     },
     "execution_count": 1781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistics\n",
    "percent = np.in1d(gen_dcm,values_d).sum()/gen_dcm.size\n",
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1715,
   "id": "7841985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 1715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_n = np.in1d(values_g,values_d,invert=True).sum()\n",
    "outlier_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1716,
   "id": "517043fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595"
      ]
     },
     "execution_count": 1716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_index = np.where(np.in1d(values_g,values_d,invert=True))\n",
    "outlier = np.array([values_g[outlier_index],counts_g[outlier_index]])\n",
    "counts_g[outlier_index].sum()/n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1717,
   "id": "36c886dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 1717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_n = np.in1d(values_d,values_g,invert=True).sum()\n",
    "fp_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1718,
   "id": "6a79d932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 1718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_index = np.where(np.in1d(values_d,values_g,invert=True))\n",
    "fp = np.array([values_d[fp_index],counts_d[fp_index]])\n",
    "counts_d[fp_index].sum()/n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1719,
   "id": "1061516b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21a679f1d90>"
      ]
     },
     "execution_count": 1719,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization (original order)\n",
    "x_lim = 2**HM.n_d\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d, dist_d,label = \"Ground Truth\")\n",
    "ax.bar(values_g,dist_g,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "78c001e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 650,  651,  652, ..., 1008, 1009, 1017], dtype=int64)"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set = np.append(values_d,np.where(np.in1d(np.arange(0,x_lim),values_d,invert=True)))\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "f6093a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy, g_ind, re_ind = np.intersect1d(values_g, reordered_set, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "d7367dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21a5a41a1d0>"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization (re-order)\n",
    "x_lim = 2**HM.n_d\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - counts_d.size))] = counts_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(np.arange(counts_d.size),dist_d,label = \"Ground Truth\")\n",
    "ax.bar(np.sort(re_ind),dist_g[np.argsort(re_ind)],label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482919c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cda822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f984a1",
   "metadata": {},
   "source": [
    "#### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "e2b4d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HM.Norm_wake\n",
    "# HM.Norm_sleep\n",
    "# HM.norm_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "d04bcac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_loss: 0.05438910316629375 delta_f: 0.05638372414047777\n",
      "learning rate change: 0.06804622607667567\n",
      "delta_loss: 0.016245949003610094 delta_f: 0.016597544835692143\n",
      "learning rate change: 0.08256297998449529\n",
      "delta_loss: 0.00100050853950151 delta_f: 0.0010051795081557305\n",
      "learning rate change: 0.09664104923354642\n",
      "delta_loss: 0.07150847706109653 delta_f: 0.07195960043278823\n",
      "learning rate change: 0.09632229959867626\n",
      "delta_loss: 0.1887106331828612 delta_f: 0.18983020678770893\n",
      "learning rate change: 0.09626077653282775\n",
      "delta_loss: 0.15021801480739239 delta_f: 0.15449018873574802\n",
      "learning rate change: 0.08361333660640764\n",
      "delta_loss: 0.16437682773261697 delta_f: 0.1691848956994138\n",
      "learning rate change: 0.0792985805094706\n"
     ]
    }
   ],
   "source": [
    "HM.check_learning_rate(Alpha_P,Alpha_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "a3b67cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09223127, 0.0965636 , 0.0994897 , 0.1       ],\n",
       "       [0.09640789, 0.09784066, 0.09949712, 0.09955754]])"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "328501be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  [[ 0.20505972]\n",
      " [-2.34546241]\n",
      " [-1.12998377]\n",
      " [-2.20587891]\n",
      " [ 4.23259693]\n",
      " [ 1.67613006]\n",
      " [-2.53318631]\n",
      " [-3.15303168]]\n",
      "variance:  [[ 9.5840907 ]\n",
      " [ 4.93115383]\n",
      " [10.75898034]\n",
      " [ 6.57241381]\n",
      " [ 3.01927054]\n",
      " [ 4.29380434]\n",
      " [ 7.21834511]\n",
      " [ 3.81451885]]\n",
      "mean:  [[ 0.49790018]\n",
      " [-3.28049798]\n",
      " [-1.78900284]\n",
      " [-3.09031354]\n",
      " [ 2.10576247]\n",
      " [ 3.13281887]]\n",
      "variance:  [[9.62717795]\n",
      " [4.15156993]\n",
      " [2.48673745]\n",
      " [2.60442977]\n",
      " [5.24644926]\n",
      " [2.02691862]]\n",
      "mean:  [[ 3.59183105]\n",
      " [ 1.9120008 ]\n",
      " [-2.38617924]]\n",
      "variance:  [[2.8296228 ]\n",
      " [2.07283743]\n",
      " [6.51634957]]\n",
      "mean:  [[ 2.99705049]\n",
      " [ 1.7650585 ]\n",
      " [-1.96320875]]\n",
      "variance:  [[0.9919234 ]\n",
      " [1.26148602]\n",
      " [3.85914269]]\n",
      "mean:  [[ 0.72382662]\n",
      " [-3.0462023 ]\n",
      " [-1.64544818]\n",
      " [-2.93320947]\n",
      " [ 1.89010369]\n",
      " [ 3.11356005]]\n",
      "variance:  [[3.88699664]\n",
      " [3.0803616 ]\n",
      " [1.78767406]\n",
      " [1.89114895]\n",
      " [2.90990065]\n",
      " [1.8444683 ]]\n",
      "mean:  [[ 0.17705639]\n",
      " [-1.9474096 ]\n",
      " [-0.53805177]\n",
      " [-1.58487726]\n",
      " [ 4.1729943 ]\n",
      " [ 1.66812241]\n",
      " [-1.99651384]\n",
      " [-2.99240441]]\n",
      "variance:  [[2.43091329]\n",
      " [2.82521496]\n",
      " [1.72365973]\n",
      " [2.08910757]\n",
      " [2.73762018]\n",
      " [4.05643603]\n",
      " [3.21044353]\n",
      " [2.92046643]]\n",
      "mean:  [[ 8.19592861]\n",
      " [ 1.2615309 ]\n",
      " [ 1.52692769]\n",
      " [ 0.2135285 ]\n",
      " [ 0.32017369]\n",
      " [ 0.3893867 ]\n",
      " [ 0.53681853]\n",
      " [ 1.11457551]\n",
      " [ 0.65643367]\n",
      " [-0.17140047]]\n",
      "variance:  [[5.62011878]\n",
      " [5.23870987]\n",
      " [4.97706821]\n",
      " [0.57307119]\n",
      " [0.84012025]\n",
      " [0.94002901]\n",
      " [0.84558964]\n",
      " [4.05409048]\n",
      " [3.02124629]\n",
      " [1.56779384]]\n"
     ]
    }
   ],
   "source": [
    "HM.norm_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "id": "8a2d1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "HM.prob_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1498,
   "id": "6c6ad259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.15888308, 1.6026435 , 1.54191883, 4.05182657, 1.13117255,\n",
       "       2.47574146, 0.84914124, 1.11057367, 1.42324828, 2.11913664,\n",
       "       1.87224667, 1.56805419, 1.96717821, 1.6683238 , 0.50787017,\n",
       "       1.80186696, 2.943097  , 1.6153151 , 2.06727769, 3.15733438,\n",
       "       2.30347043, 2.56520868, 1.1098868 , 2.141564  , 1.20750777,\n",
       "       1.8468537 , 1.63167354, 1.22179728, 1.56914046, 1.3551652 ,\n",
       "       1.17200027, 1.67760733, 1.48653911, 1.42668707, 2.1046045 ,\n",
       "       1.20204613, 2.05460493, 0.70418627, 0.59523651, 2.09278418,\n",
       "       2.65223476, 2.07489558, 1.07142565, 2.45381939, 0.9846837 ,\n",
       "       2.62191631, 0.10535773, 0.9085679 , 2.24637625, 2.16353798,\n",
       "       1.71804373, 1.87125311, 1.82330209, 1.23389537, 0.79127325,\n",
       "       1.72415463, 2.2365382 , 1.24615391, 1.96149212, 2.35461751,\n",
       "       1.38964204, 3.14904622, 0.25074432, 1.23946851, 2.28819933,\n",
       "       2.70275918, 1.15374625, 2.43416832, 1.0384842 , 1.96811814,\n",
       "       0.47524835, 0.81290594, 0.90915943, 1.82412822, 0.97362833,\n",
       "       0.89405671, 0.62642521, 1.1777669 , 0.6229906 , 0.74206245,\n",
       "       2.10825501, 2.65376469, 1.15414031, 2.2994013 , 1.27623781,\n",
       "       2.20661078, 0.95987203, 1.14248126, 1.20427369, 1.34000694,\n",
       "       1.32220812, 1.19215793, 0.53972757, 1.15534424, 0.79222504,\n",
       "       0.60975217, 1.8260951 , 1.31147306, 1.71303313, 1.52949991,\n",
       "       0.95191775, 2.07011648, 0.18477178, 0.97277384, 1.67527196,\n",
       "       1.45330167, 0.82069571, 1.84861987, 0.55834819, 1.87224061,\n",
       "       0.09322486, 0.43720461, 2.27998727, 1.20509987, 1.74599056,\n",
       "       2.061342  , 1.06601374, 2.11671906, 0.38865726, 1.02103561,\n",
       "       1.48990747, 1.63306576, 1.27875605, 1.58419034, 0.91393492,\n",
       "       1.60535553, 0.27269282, 0.84025265, 0.07082675, 0.28009743,\n",
       "       0.51463171, 0.08397037, 0.22232218, 0.0722201 , 1.00843217,\n",
       "       0.32266027, 0.13833083, 0.04187443, 0.65670036, 0.17532437,\n",
       "       0.04975165, 0.00880547, 0.63942311, 0.06932499, 0.13233079,\n",
       "       0.36494135, 0.8493735 , 0.13774689, 0.06964158, 0.04325213,\n",
       "       0.54250661, 0.10482335, 0.34640986, 0.07274793, 0.37001968,\n",
       "       0.41490622, 0.07150867, 0.00718842, 0.73611683, 0.09383954,\n",
       "       0.57545421, 1.35819995, 1.71271762, 0.7376744 , 1.30993772,\n",
       "       0.80243607, 1.07088585, 1.33877835, 0.46756443, 0.3826778 ,\n",
       "       0.52230715, 0.56001046, 0.45634223, 0.12145176, 1.94710287,\n",
       "       0.61378757, 0.61765428, 1.06718074, 1.15339557, 0.73297283,\n",
       "       0.71195772, 0.48883825, 1.76352492, 0.90863532, 0.73971367,\n",
       "       0.54236229, 0.18205704, 0.77837272, 0.33280583, 0.07725248,\n",
       "       1.07764553, 0.43509423, 0.46219013, 0.1148657 , 0.8498844 ,\n",
       "       0.54597428, 0.69374097, 0.2226348 , 1.48694649, 0.7795334 ,\n",
       "       0.62486628, 0.2055751 , 0.1470067 , 0.56417138, 0.69200593,\n",
       "       0.07716613, 0.86454479, 0.79516296, 0.70946769, 0.16830315,\n",
       "       0.47902665, 0.74326038, 0.43170452, 0.07123705, 1.29108919,\n",
       "       0.52204469, 0.32310786, 0.46297678, 0.04850053, 0.26462913,\n",
       "       0.74427752, 0.10748327, 0.34216415, 0.74570428, 1.40739629,\n",
       "       0.94161633, 0.93178883, 1.50724594, 0.96719904, 0.8821027 ,\n",
       "       1.2087594 , 1.13114345, 0.49494379, 0.66160568, 0.68125769,\n",
       "       0.48130317, 1.42770952, 0.559019  , 0.67835166, 1.42020525,\n",
       "       0.99543168, 0.97108334, 0.85161824, 0.99217053, 1.33255062,\n",
       "       0.62940442, 1.0819565 , 1.46948756, 0.17271247, 0.85171242,\n",
       "       0.29520791, 0.16452797, 0.9271929 , 0.45782881, 0.78353932,\n",
       "       0.91486664, 2.62223577])"
      ]
     },
     "execution_count": 1498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Prob_mtx_wake['Prob_12'][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "id": "067673e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1588830833596715"
      ]
     },
     "execution_count": 1227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(2**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "id": "b4605c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.15888308, 1.46899102, 2.39385178, 1.67337806, 2.9677934 ,\n",
       "       1.01966291, 2.12550257, 1.01882204])"
      ]
     },
     "execution_count": 1263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Prob_mtx_sleep['Prob_32'][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "id": "ded669e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.70050677e-02, 5.00777871e-03, 2.35999519e-03, 3.19370681e-04,\n",
       "       6.54096777e-01, 8.85168469e-02, 4.17149688e-02, 5.64515472e-03,\n",
       "       2.41645666e-03, 3.27011433e-04, 1.54109328e-04, 2.08551276e-05,\n",
       "       4.27129745e-02, 5.78021167e-03, 2.72401648e-03, 3.68632530e-04,\n",
       "       2.21056629e-03, 2.99148941e-04, 1.40978686e-04, 1.90781994e-05,\n",
       "       3.90736830e-02, 5.28771786e-03, 2.49192096e-03, 3.37223778e-04,\n",
       "       1.44351516e-04, 1.95346339e-05, 9.20600624e-06, 1.24581969e-06,\n",
       "       2.55153868e-03, 3.45291655e-04, 1.62724172e-04, 2.20209472e-05,\n",
       "       2.26650134e-03, 3.06718454e-04, 1.44545939e-04, 1.95609445e-05,\n",
       "       4.00623837e-02, 5.42151560e-03, 2.55497527e-03, 3.45756718e-04,\n",
       "       1.48004114e-04, 2.00289284e-05, 9.43895037e-06, 1.27734328e-06,\n",
       "       2.61610152e-03, 3.54028740e-04, 1.66841662e-04, 2.25781542e-05,\n",
       "       1.35393657e-04, 1.83223952e-05, 8.63471950e-06, 1.16850927e-06,\n",
       "       2.39320072e-03, 3.23864280e-04, 1.52626182e-04, 2.06544182e-05,\n",
       "       8.84130000e-06, 1.19646515e-06, 5.63853190e-07, 7.63044679e-08,\n",
       "       1.56277672e-04, 2.11485628e-05, 9.96659583e-06, 1.34874787e-06,\n",
       "       1.46899102e+00])"
      ]
     },
     "execution_count": 1265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HM.Prob_mtx_sleep['Prob_32'][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ec544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
