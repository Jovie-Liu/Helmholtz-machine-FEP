{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 2</span> \n",
    "\n",
    "# Helmholtz Machine - Effect of Structural Modifications (Continue...)\n",
    "\n",
    "Continue with the structural modification of the Helmholtz machine and put forward more questions:\n",
    "\n",
    "1. Change the binary value from {0,1} to {-1,1} -- what if we change it to {2,1}, {-10,1}, even {100,1}?\n",
    "2. Add bias term to the linear combination of neurons in the previous layer -- what if we only add bias on the phenotype layer?\n",
    "\n",
    "The reasons are stated elsewhere so let's take it for granted now, and see how these modifications affect the model behavior.\n",
    "\n",
    "*Updated:* December 10, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use is the well-formed set generated with the well-formedness rules:\n",
    "\n",
    "1. Start with 1\n",
    "2. Forbid 00100 (no 100, 001 on the boundary)\n",
    "3. Forbid 0000\n",
    "\n",
    "We use a 10-node pattern with binary values thus there are 1024 combinations of all patterns, of which the well-formed set contains 256 instances. The dataset is generated as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 0., ..., 1., 1., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "well_formed_set = np.zeros([1,n])\n",
    "well_formed_set[0,0] = 1\n",
    "\n",
    "for i in range(1,n):\n",
    "    for j in range(np.shape(well_formed_set)[0]):\n",
    "        if i == 2 and np.array_equal(well_formed_set[j,i-2:i], [1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-3:i], [0,0,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-4:i], [0,0,1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        else:\n",
    "            well_formed_set = np.append(well_formed_set, well_formed_set[j:j+1,:], axis=0)\n",
    "            well_formed_set[j,i] = 1\n",
    "            \n",
    "ind = np.array([], dtype=np.int8)\n",
    "for i in range(well_formed_set.shape[0]):\n",
    "    if np.array_equal(well_formed_set[i,-3:], [0,0,1]):\n",
    "        ind = np.append(ind,i)\n",
    "\n",
    "well_formed_set = np.delete(well_formed_set,ind,0)\n",
    "well_formed_set = np.transpose(well_formed_set)\n",
    "well_formed_set_0 = well_formed_set\n",
    "well_formed_set_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       [ 1., -1.,  1., ...,  1., -1.,  1.],\n",
       "       [ 1.,  1., -1., ...,  1.,  1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set = (well_formed_set_0 - 0.5)*2\n",
    "well_formed_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Helmholtz machine structure is given as below:\n",
    "\n",
    "<img src=\"Helmz.jpg\" style=\"width:550px\">\n",
    "<caption><center> **Figure 4**: The Helmholtz Machine  </center></caption>\n",
    "\n",
    "I previously wrote in Document 2 about the variational objective function and deduction of the parameter updating rules of the Helmholtz machine, and I will give a more systematic analysis on these matters in upcoming notebooks, so here let's skip these steps and say we know the parameter updating local delta rule is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{F}}{\\partial \\phi_{k,n}^{m-1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m-1}(1-q_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m-1} \\centerdot q_n^m & \\text{if } s_n^m = 0 \\text{ or } -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial \\theta_{k,n}^{m+1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m = 0 \\text{ or } -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Let's write the functions to run this model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always have the bottom layer as data input, denoted as $d_0$. In our case illustrated in Figure 4, we have $m = 4$ layers, with various numbers of neurons, $n_{d_0} = 10$, $n_{z_1} = 8$, $n_{z_2} = 5$, $n_{z_3} = 3$. These are the hyperparameters to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(init_type,n_dz):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (m, ), where m is the number of layers\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \"\"\"\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    m = len(n_dz)\n",
    "    if init_type == \"zero\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((n_dz[i+1],n_dz[i]+1))\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.zeros((n_dz[i],n_dz[i+1]+1))\n",
    "        Theta[\"Theta_k\"] = np.zeros((n_dz[-1],1))\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(n_dz[i+1],n_dz[i]+1)\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.random.randn(n_dz[i],n_dz[i+1]+1)\n",
    "        Theta[\"Theta_k\"] = np.random.randn(n_dz[-1],1)\n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "    return Phi, Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization_databias(init_type,n_dz):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (m, ), where m is the number of layers\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi), where the last column represents bias b's\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}), where the last column represents bias b's\n",
    "    \"\"\"\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    m = len(n_dz)\n",
    "    if init_type == \"zero\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((n_dz[i+1],n_dz[i]))\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.zeros((n_dz[i],n_dz[i+1]))\n",
    "        Theta[\"Theta_k\"] = np.zeros((n_dz[-1],1))\n",
    "        Theta['Theta_10'] = np.append(Theta['Theta_10'], np.zeros((n_dz[0],1)),axis=1)\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(n_dz[i+1],n_dz[i])\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.random.randn(n_dz[i],n_dz[i+1])\n",
    "        Theta[\"Theta_k\"] = np.random.randn(n_dz[-1],1)\n",
    "        Theta['Theta_10'] = np.append(Theta['Theta_10'], np.random.randn(n_dz[0],1),axis=1)\n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "    return Phi, Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi, Theta = parameter_initialization_databias(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- probability of each neuron (taking value 1), Python dictionary of same length as Phi with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = {}\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0)))\n",
    "        S = ((q > np.random.rand(len(q),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        Q[\"q\"+str(i+1)] = q\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = well_formed_set_1[:,5:6]\n",
    "Q, Alpha_Q = wake_forward(d0,Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward_nobias(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- probability of each neuron (taking value 1), Python dictionary of same length as Phi with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = {}\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,S))\n",
    "        S = ((q > np.random.rand(len(q),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        Q[\"q\"+str(i+1)] = q\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = well_formed_set[:,5:6]\n",
    "Q, Alpha_Q = wake_forward_nobias(d0,Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- probability of each neuron (taking value 1), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    \n",
    "    m = len(Theta)\n",
    "    P = {\"p\"+str(m-1):p}\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        P[\"p\"+str(i-1)] = p\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    return P, Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Alpha_P = sleep_forward(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward_databias(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- probability of each neuron (taking value 1), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    \n",
    "    m = len(Theta)\n",
    "    P = {\"p\"+str(m-1):p}\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        if i == 1:\n",
    "            S = np.append(S,[[1]], axis=0)\n",
    "        p = sigmoid(np.matmul(theta,S))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        P[\"p\"+str(i-1)] = p\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    return P, Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Alpha_P = sleep_forward_databias(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    for i in range(n):\n",
    "        S_bias = np.append(Alpha_P[\"z\"+str(i)],[[1]], axis=0)\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S_bias))\n",
    "        gain = q - (1+Alpha_P[\"z\"+str(i+1)])/2\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "Phi, info_gain_wake,error_Q = wake_update_delta(Phi,Alpha_P,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta_nobias(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    for i in range(n):\n",
    "        S = Alpha_P[\"z\"+str(i)]\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S))\n",
    "        gain = q - (1+Alpha_P[\"z\"+str(i+1)])/2\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "Phi, info_gain_wake,error_Q = wake_update_delta_nobias(Phi,Alpha_P,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information gain, we can read the error of the given assignment *Alpha_P* for parameters *Phi*, on each neuron it computes. The value of info_gain is within $[-1,1]$, where positive value indicates the current neuron takes value $-1$, while negative value indicates that the current neuron takes value $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - (1+Alpha_Q[\"z\"+str(n-1)])/2\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,0,-1):\n",
    "        S_bias = np.append(Alpha_Q[\"z\"+str(i)],[[1]], axis=0)\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S_bias))\n",
    "        gain = p - (1+Alpha_Q[\"z\"+str(i-1)])/2\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta, info_gain_sleep, error_P = sleep_update_delta(Theta,Alpha_Q,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta_databias(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - (1+Alpha_Q[\"z\"+str(n-1)])/2\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,0,-1):\n",
    "        S = Alpha_Q[\"z\"+str(i)]\n",
    "        if i == 1:\n",
    "            S = np.append(S,[[1]], axis=0)\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S))\n",
    "        gain = p - (1+Alpha_Q[\"z\"+str(i-1)])/2\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta, info_gain_sleep, error_P = sleep_update_delta_databias(Theta,Alpha_Q,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_databias(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,0,-1):\n",
    "        if i == 1:\n",
    "            S = np.append(S,[[1]], axis=0)\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,S))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  1., ..., -1.,  1., -1.],\n",
       "       [ 1.,  1., -1., ...,  1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "entire_set = np.zeros((2,10))\n",
    "entire_set[0,0] = 1\n",
    "for i in range(1,n):\n",
    "    for j in range(entire_set.shape[0]):\n",
    "        entire_set = np.append(entire_set, entire_set[j:j+1,:], axis=0)\n",
    "        entire_set[j,i] = 1\n",
    "entire_set = (entire_set - 0.5)*2\n",
    "entire_set = np.transpose(entire_set)\n",
    "entire_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., ...,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       ...,\n",
       "       [-1., -1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1., -1., ..., -1., -1., -1.],\n",
       "       [-1.,  1., -1., ..., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_set = np.unique(entire_set, axis=1)\n",
    "well_formed_set = np.unique(well_formed_set, axis=1)\n",
    "reordered_set = np.zeros(entire_set.shape)\n",
    "reordered_set[:,:well_formed_set.shape[1]] = well_formed_set\n",
    "\n",
    "k = well_formed_set.shape[1]\n",
    "for i in range(entire_set.shape[1]):\n",
    "    flag = 0\n",
    "    for j in range(well_formed_set.shape[1]):\n",
    "        if np.array_equal(entire_set[:,i], well_formed_set[:,j]):\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 0:\n",
    "        reordered_set[:,k] = entire_set[:,i]\n",
    "        k += 1\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set_0 = (reordered_set+1)/2\n",
    "reordered_set_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmholtz Machine ({-1,1}, Data bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"zero\"\n",
    "Phi, Theta = parameter_initialization_databias(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like stochastic gradient descent, we update the parameters for each data input (no batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18826421660021633 0.07381059194150875\n",
      "0.18786861072099392 0.07221894431509084\n",
      "0.18524468524964738 0.06808185132852419\n",
      "0.18703478748974167 0.06844561431700254\n",
      "0.1887515462801898 0.07552903245745472\n",
      "0.1856713579327965 0.06324535663541006\n",
      "0.1850496865719389 0.06819816323870201\n",
      "0.18933333131061955 0.0683428348418914\n",
      "0.1878786117981575 0.07187951599696277\n",
      "0.19170161141502393 0.0736461856946673\n",
      "0.1880115210551621 0.07238471469901812\n",
      "0.19218706146946496 0.07674457010538162\n",
      "0.18977861856949546 0.07268673196626087\n",
      "0.19250555588380958 0.08037893368493837\n",
      "0.19488370103985245 0.08353953921466681\n",
      "0.19492005976221188 0.0853152329779941\n",
      "0.1934347909427599 0.07644727711344332\n",
      "0.19603304956704234 0.08357001767474179\n",
      "0.19710648557480218 0.08403439674629583\n",
      "0.1958179379042467 0.08681706437375306\n",
      "0.19557276535977702 0.08609296303542655\n",
      "0.201184187678188 0.08395867338908404\n",
      "0.19889157734552235 0.0906001756963732\n",
      "0.19465959096278465 0.08115238690906879\n",
      "0.1949720979401089 0.07762737798655761\n",
      "0.19373506088428602 0.07536893869396906\n",
      "0.19369554043615644 0.07597465794536612\n",
      "0.1943295975035289 0.06990595305807902\n",
      "0.19507728892127452 0.08188198878789152\n",
      "0.1948288601575056 0.07787066397238458\n",
      "0.19560105457920188 0.08379922144923603\n",
      "0.19562203604673983 0.07813205113096011\n",
      "0.19761016220405828 0.07832767199760408\n",
      "0.19607926254675437 0.08019701490495407\n",
      "0.19866655471471287 0.08267328391471426\n",
      "0.1956134082544645 0.08337362781274413\n",
      "0.19713913583696768 0.08076351470196952\n",
      "0.19352570469754685 0.07636949897416835\n",
      "0.19144081059709783 0.07454187692570367\n",
      "0.19383174213132615 0.0769391331417548\n",
      "0.19378619817602252 0.08114364960731175\n",
      "0.1902687844131518 0.07788447008193394\n",
      "0.19319489379739918 0.06653260485358183\n",
      "0.19026238700891562 0.07021507941122766\n",
      "0.19113517735223876 0.0671785378512654\n",
      "0.18607764966291834 0.06812600816086446\n",
      "0.18753013123157442 0.06398917160225027\n",
      "0.18870637719767075 0.07026563601833039\n",
      "0.18666660451533154 0.07090782057194886\n",
      "0.19027000158537924 0.06974863588419698\n",
      "0.18977765949490163 0.06741813770720838\n",
      "0.1885534447623688 0.07704696724358348\n",
      "0.19198196477022847 0.07101034374043198\n",
      "0.19118601770579316 0.06923625889991006\n",
      "0.19045641468390362 0.07660098847220756\n",
      "0.19018039030951542 0.0756116604206901\n",
      "0.1888401377508131 0.08086386605667575\n",
      "0.1884309920459626 0.07266979954309694\n",
      "0.19058983911711802 0.0703342532720231\n",
      "0.19098860938449466 0.07323859613673184\n",
      "0.192342004584644 0.07745799892669444\n",
      "0.19558985746904983 0.07300543205544362\n",
      "0.1929897389815831 0.0772339850622976\n",
      "0.1924009893533928 0.08060062525674488\n",
      "0.1923623316541449 0.07317973129162736\n",
      "0.1956620111001802 0.07352427264943295\n",
      "0.1911870375159355 0.07329978905396359\n",
      "0.1927854619642012 0.07593985510995975\n",
      "0.19117386039753495 0.07800492894400764\n",
      "0.1934707643342649 0.07450003941448242\n",
      "0.19381042774566595 0.0795651044408963\n",
      "0.19145343476899113 0.07495504113888367\n",
      "0.19296576949589306 0.06985388484779552\n",
      "0.19238851200992474 0.0733374906084869\n",
      "0.19216317972054864 0.0788611306154385\n",
      "0.19188773942909942 0.07696759421252243\n",
      "0.19359457930747098 0.08191967365128916\n",
      "0.19808429691605567 0.07955145604814262\n",
      "0.19811433639281636 0.08182161433744341\n",
      "0.19630656844877903 0.07962901241130357\n",
      "0.1980344523476819 0.08777993788341923\n",
      "0.19930837033031307 0.08177015413239884\n",
      "0.20057683671640864 0.0849070604571063\n",
      "0.19960901811278658 0.08766976652486463\n",
      "0.1963121137430958 0.08264079862991693\n",
      "0.19548436620006351 0.08324709310816872\n",
      "0.1966232009870806 0.08291205652701204\n",
      "0.19618172666917927 0.08525147036619159\n",
      "0.19355712461733304 0.07939000389952548\n",
      "0.19368329623135133 0.07590679842950797\n",
      "0.1934072258993244 0.08282669099232119\n",
      "0.19552401755830476 0.07861011237783973\n",
      "0.1977233011200806 0.08368962493760236\n",
      "0.2005168811048495 0.0856700512652164\n",
      "0.19983992716671137 0.08892692329222317\n",
      "0.19804531622457824 0.08571391898864322\n",
      "0.19900804982176462 0.08366570918229001\n",
      "0.19685428019733497 0.08235847978569202\n",
      "0.19528368047974756 0.08312524629472912\n",
      "0.20083158613572163 0.08184477850671501\n",
      "0.19669967829489585 0.07711879391750216\n",
      "0.1947824324974275 0.08024220174783309\n",
      "0.19603031976662436 0.07900276873209035\n",
      "0.1965598658128722 0.07705084633953423\n",
      "0.19554218771470958 0.07490652537877812\n",
      "0.19332019724232008 0.0777434444037812\n",
      "0.19237423772173928 0.07343441046111227\n",
      "0.19302535483402497 0.07083470457504368\n",
      "0.19155003065705672 0.0714229850143659\n",
      "0.1957461382113683 0.08456925494690563\n",
      "0.19504599926899593 0.0877715539300059\n",
      "0.19942609081562795 0.08319458866552006\n",
      "0.19746035173495993 0.08329456082054366\n",
      "0.19894695208767826 0.08110894931255072\n",
      "0.1985092060199518 0.08587118329487518\n",
      "0.1986107052733908 0.08090689848736209\n",
      "0.19168253389821774 0.0733907212282138\n",
      "0.19047688424375836 0.07133416306249529\n",
      "0.19255133319307224 0.0796127698740247\n",
      "0.19207989699594596 0.07600174328777741\n",
      "0.19489241285428638 0.07892160918330722\n",
      "0.18980440209367716 0.07146022452190075\n",
      "0.19197315688655903 0.07249954256075322\n",
      "0.19528098024635754 0.0752673931194599\n",
      "0.1936889051364386 0.07351246089861574\n",
      "0.1947129072536264 0.07734323131588594\n",
      "0.19339555455039698 0.07965320579057208\n",
      "0.1926111816996284 0.08090884129538628\n",
      "0.19429139011359983 0.07143396185451843\n",
      "0.1898201571894024 0.06864159282366816\n",
      "0.1898423538999435 0.06946146944611535\n",
      "0.19240263834794627 0.07665369506148842\n",
      "0.19761994361349625 0.08296055958943066\n",
      "0.1936589616301275 0.07677315390945574\n",
      "0.19580521971050452 0.07865166163622572\n",
      "0.19022493914095243 0.07392771046174114\n",
      "0.1904944139201008 0.07016643792031245\n",
      "0.18862512323158553 0.06929659098623187\n",
      "0.18607249512963903 0.06448576360756536\n",
      "0.18771327736668608 0.06838151552046139\n",
      "0.18865311144448368 0.07008728987552416\n",
      "0.18754855316262642 0.0738643890117186\n",
      "0.18804832137702301 0.07286098178116554\n",
      "0.1886177709410395 0.0669257790425814\n",
      "0.18651312702712394 0.06687223620057624\n",
      "0.18744051597540867 0.0644550688947549\n",
      "0.18742376421697865 0.07092156936142173\n",
      "0.18967381696908037 0.06999345598885902\n",
      "0.1910102731607635 0.0716836703758741\n",
      "0.1920192141947795 0.07104834052085977\n",
      "0.19186637119344996 0.07159114204453929\n",
      "0.1933390384779764 0.0729892144257452\n",
      "0.1902278533268167 0.07288521577308658\n",
      "0.19063714956822944 0.07358717007500258\n",
      "0.1891304762594431 0.07115960406477903\n",
      "0.18566107208176832 0.06487798791746632\n",
      "0.18753809071277353 0.07044404367171112\n",
      "0.18836678329797898 0.07389250410964598\n",
      "0.1920754400834693 0.07148388938709564\n",
      "0.19375732695145512 0.07365068394151496\n",
      "0.19347793470002148 0.07199625008113775\n",
      "0.19508596415964324 0.07674368379241919\n",
      "0.18804770258330925 0.06817247922502166\n",
      "0.18934650596975575 0.0654376325664308\n",
      "0.18567259165171798 0.06605851856594556\n",
      "0.1872453768082765 0.06679287552556679\n",
      "0.19147358340290943 0.07401461556204465\n",
      "0.18859696932561099 0.07245587662151846\n",
      "0.19131957005016303 0.06538698140464017\n",
      "0.18684423661891147 0.062477437357534665\n",
      "0.1864920688404238 0.06724007886515626\n",
      "0.18647970660386246 0.06503063380232127\n",
      "0.1842514224733486 0.07023467344082132\n",
      "0.1876885288403074 0.0728089633036896\n",
      "0.1870116265697216 0.061895334891497344\n",
      "0.18425066264322343 0.06688490758477043\n",
      "0.18695918566059114 0.06645482528550567\n",
      "0.18317253533242364 0.06186305933215334\n",
      "0.1809163029054771 0.0649061880910563\n",
      "0.18175565588335638 0.06085691413377885\n",
      "0.1820436428606898 0.06785134585738758\n",
      "0.17997885400988617 0.06470885936482271\n",
      "0.18156043254954252 0.06566906847438918\n",
      "0.18236931849134527 0.06256770444258485\n",
      "0.18576561866641758 0.07249165509825864\n",
      "0.1848139521450657 0.06528863725723012\n",
      "0.18596160282973745 0.06652342186829205\n",
      "0.18557681986131566 0.06446289796514629\n",
      "0.18537532580523897 0.06539712591362379\n",
      "0.18826763200717886 0.06614973735480409\n",
      "0.1871669548744644 0.06474349512795663\n",
      "0.18492999358276485 0.06263383377523242\n",
      "0.1839295818788514 0.061348571983366115\n",
      "0.18460401448957314 0.06320085027160875\n",
      "0.18345330488816594 0.06249939421577344\n",
      "0.18367209381188157 0.06570693278775858\n",
      "0.1878582222622315 0.0665452392528002\n",
      "0.18713606555495452 0.059931727273695994\n",
      "0.1838512310565955 0.060359980156668866\n",
      "0.18430084322023482 0.0599419621302576\n",
      "0.18610330922053092 0.06243996378414954\n",
      "0.1826626643621903 0.06797040335297479\n",
      "0.18164916828603725 0.06252065492850635\n",
      "0.18086243789288334 0.05833234804833562\n",
      "0.1838729941268095 0.06672218597900315\n",
      "0.18703604981151567 0.0674028516046221\n",
      "0.1856951406510364 0.06177727885517191\n",
      "0.18720541592877143 0.06185179873682304\n",
      "0.1877069307364732 0.06255498619940525\n",
      "0.186870835099614 0.07025585805140824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1871636345512407 0.06382178372508769\n",
      "0.1876066276835732 0.06458423339145201\n",
      "0.1875172207650644 0.062016952144493105\n",
      "0.19078128620994136 0.06726564162380799\n",
      "0.19308335097917073 0.07346865772730542\n",
      "0.19024402764544263 0.07161147731782325\n",
      "0.18962047952457145 0.06489985849469786\n",
      "0.1883757475906117 0.06808125920198822\n",
      "0.18733624185340036 0.06203695938739965\n",
      "0.1869451198907303 0.06408159689431846\n",
      "0.19183616310394697 0.07019261599997186\n",
      "0.19139693189315768 0.07621213877121326\n",
      "0.19076048615569283 0.07163376879210248\n",
      "0.1893397277636422 0.0747054868759332\n",
      "0.18981139458440616 0.0674063943477723\n",
      "0.18830772295306303 0.07102597138017269\n",
      "0.187852007513533 0.06732950730617054\n",
      "0.18546182034923472 0.06832062765279805\n",
      "0.18914751546708147 0.06879551358222517\n",
      "0.18612021623588385 0.06425562173141483\n",
      "0.18492690392302116 0.06323995200644117\n",
      "0.1848243853686539 0.06805679287829758\n",
      "0.1868330695727967 0.06785946221614907\n",
      "0.18731231254840855 0.06826094570402752\n",
      "0.18672081818720532 0.06335830458401183\n",
      "0.18898318230911632 0.07174482714893164\n",
      "0.1928987639883173 0.07668752357337007\n",
      "0.19017105137655488 0.0805729369327894\n",
      "0.18515258823112743 0.0662868831402275\n",
      "0.18833917122656693 0.07394157898466704\n",
      "0.18807398008682327 0.06888504139895081\n",
      "0.1860646786403788 0.06240851478862452\n",
      "0.1830014533906861 0.058723456567257304\n",
      "0.18307099257669573 0.05990505379638737\n",
      "0.1873136927172589 0.06951640779261625\n",
      "0.18668849937240847 0.06580109620569714\n",
      "0.18691529006682694 0.06954869836392268\n",
      "0.18727671268373772 0.068195725693108\n",
      "0.18539914870612276 0.06188722619103288\n",
      "0.18482197214936272 0.06400684832330948\n",
      "0.18677933905042934 0.06834123614353632\n",
      "0.18629364751975705 0.061614106464155334\n",
      "0.18527968088044233 0.06182278105549214\n",
      "0.18617087155491935 0.061441611786958845\n",
      "0.18488933895461826 0.0624056777382708\n",
      "0.1854830297752135 0.06198746946248318\n",
      "0.18772448079707318 0.0664912590723978\n",
      "0.19235731781847631 0.07262101715888782\n",
      "0.18698027070652493 0.06935709921776247\n",
      "0.18498138881890103 0.06830053426750485\n",
      "0.18731901058153758 0.06610257105243313\n",
      "0.19105158606577402 0.06324726298503504\n",
      "0.18562081298486474 0.07025187457468177\n",
      "0.1864403558184922 0.06382136946198566\n",
      "0.18578113293047213 0.0634618425021283\n",
      "0.1862572604819417 0.06610283263881457\n",
      "0.18749088706236477 0.07132647813393879\n",
      "0.18593416499122647 0.06746238101363851\n",
      "0.18444308567852913 0.06414058123356675\n",
      "0.18541506294355867 0.06324871695549827\n",
      "0.1871622962093804 0.0663392821228908\n",
      "0.18888050299146192 0.07127623730592411\n",
      "0.19028354531786237 0.06515658554463613\n",
      "0.18644732637482392 0.06907546260386051\n",
      "0.18797367578455793 0.07153731430637117\n",
      "0.18852594624742036 0.06245018925291242\n",
      "0.184594694437997 0.06541845985576494\n",
      "0.18534437948939092 0.064834349046715\n",
      "0.18462288654494932 0.06738705200711953\n",
      "0.18799857893488417 0.0653699449028151\n",
      "0.1875256798313043 0.06908343933095916\n",
      "0.1883144476340789 0.06761507549928061\n",
      "0.18909532745486154 0.06529549265287724\n",
      "0.18492547373403898 0.06286395457551078\n",
      "0.18681351638928181 0.06047963095132093\n",
      "0.18701100673347057 0.06481849601477943\n",
      "0.18947457326849731 0.06818796138844574\n",
      "0.19069678747286656 0.06958458116316794\n",
      "0.1892726292811744 0.07340598001696946\n",
      "0.1873734570127557 0.06836690598239413\n",
      "0.19101243756358596 0.07292836138608409\n",
      "0.19485848263942748 0.07468800008210683\n",
      "0.19111865811207124 0.07340815025318206\n",
      "0.19258807084499094 0.07182505038175764\n",
      "0.19450385354926542 0.07778032379767783\n",
      "0.1991272906181963 0.07925403664821576\n",
      "0.19319045625505873 0.0793044135674171\n",
      "0.19486601928040678 0.07785256691388096\n",
      "0.19621926926025934 0.07530395874884531\n",
      "0.19548419251534876 0.07680340643906478\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set[:,i:i+1]\n",
    "        Q, Alpha_Q = wake_forward_nobias(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_delta_databias(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        P, Alpha_P = sleep_forward_databias(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_delta_nobias(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to examine the distribution of generation. We let the model generate, say 10000 instances, then use the samples to approximate its generative distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate_databias(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reordered_set, we set the first 256 columns as the well-formed set and the rest as negative samples outside of well-formed bound. Now we calculate the distribution of generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values < 256].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,   6,   6,   6,  12,   7,   5,   8,  22,  11,   5,   9,   4,\n",
       "         7,  13,  12,   9,  11,  18,  18,  29,  23,   4,  18,  13,  28,\n",
       "        12,  26,  18,  25,  28,   9,  20,  40,  30,  21,  16,  17,  19,\n",
       "         6,  25,  18,   5,  20,  19,  14,  21,  12,  24,  18,  17,  15,\n",
       "        10,  33,   9,  17,  26,  14,  19,  22,  14,  28,  39,  17,  16,\n",
       "        18,  13,  21,  18,   6,  30,  26,  29,  15,  31,  11,  47,  39,\n",
       "        31,  11,  25,  36,  37,  34,  36,  24,  23,  48,  46,   1,   5,\n",
       "         2,   3,   5,   0,   3,   2,   3,   3,   6,   2,   1,   2,   5,\n",
       "         1,   1,   5,   4,   4,   2,  10,   5,   2,  10,   3,   4,   3,\n",
       "         4,   4,  12,   6,   7,  12,   7,  12,   7,  12,   9,  15,   9,\n",
       "         5,   9,  13,  13,  31,   7,  13,   7,  18,  21,  21,  25,  10,\n",
       "        17,  24,  16,  25,   8,  23,  32,  12,  26,  22,  23,  27,  36,\n",
       "        12,  25,  33,  28,  18,  19,  26,  18,  17,  21,  40,  10,  24,\n",
       "        25,  28,  22,  29,  18,  23,  21,  39,  24,  24,  28,  60,  53,\n",
       "        61,  30,   9,  32,  47,  39,  48,  26,  61,  58,  56,  47,  44,\n",
       "        64,  72,  66,  24,  61,  64,  52,  91,  50,  71,  78,  40,  74,\n",
       "       118,  33,  73,  49,  51,  85,  89,  87,  79,  75,  57,  50,  66,\n",
       "        89, 121, 111,  79,  70,  44, 121,  91, 140,  76,  79,  84, 140,\n",
       "       158, 122,  81, 190, 137, 166, 108, 133, 140, 222, 125, 103,  95,\n",
       "       167, 117, 191, 122, 129, 149,  89, 166, 182], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.array([values, counts])\n",
    "for i in range(values.size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        statistics = np.append(statistics, np.array([[values[i]+j],[0]]),axis = 1)\n",
    "statistics = np.unique(statistics,axis = 1)\n",
    "statistics[1,0:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': array([[ 8.71492027e-03,  9.12434144e-03,  6.02576278e-03,\n",
       "          4.56707485e-03,  1.40426963e-01, -2.64134319e-02,\n",
       "         -1.41354080e-02,  2.28364279e-02,  9.34058319e+00],\n",
       "        [-3.59750468e+00,  7.63219786e-01,  8.10174458e-01,\n",
       "          1.12514441e-01,  5.05244048e+00,  7.12797267e-02,\n",
       "         -2.02223557e-01, -1.74391774e-01,  1.48554736e+00],\n",
       "        [ 3.73917546e-01, -4.30142096e-01,  4.21451488e-02,\n",
       "         -3.59766109e-01, -3.27345167e+00,  1.30574930e-01,\n",
       "          2.41698034e-01, -5.06731981e-01,  5.12685560e+00],\n",
       "        [-3.92591520e+00,  3.32678431e-01,  9.31633969e-01,\n",
       "         -5.58039338e-01, -3.22484072e+00, -4.44504000e-01,\n",
       "          2.31454644e-01,  6.12653580e-01,  9.06336087e-01],\n",
       "        [ 2.56821233e-01,  1.04978145e-01,  5.74783385e-01,\n",
       "          3.04381683e+00, -8.11428674e-02, -1.13423488e+00,\n",
       "          5.64197413e-02,  7.59015355e-01,  1.81758608e+00],\n",
       "        [ 4.85569393e-01,  2.63767722e+00, -2.28252093e-01,\n",
       "         -6.37142212e-01, -3.79750277e-01,  1.04774923e-02,\n",
       "         -4.29757937e-01,  7.29344681e-02,  1.15192215e+00],\n",
       "        [ 4.02838502e-01, -6.51205423e-01, -1.02741216e+00,\n",
       "         -8.52206357e-01,  3.53407753e-01, -1.53216582e+00,\n",
       "         -9.17101815e-01,  1.36943330e+00,  1.10092177e+00],\n",
       "        [-2.06700450e-01, -9.94442490e-02, -9.64140548e-02,\n",
       "          3.95688320e-01, -3.23531836e-01,  4.07749831e+00,\n",
       "         -5.94192156e-01,  1.58908960e+00,  3.41240247e+00],\n",
       "        [ 9.27698435e-01, -6.67015807e-02,  6.64473695e-01,\n",
       "         -4.81457947e-01,  4.03081107e-01, -8.22476978e-01,\n",
       "          3.82804880e+00,  4.71828518e-01,  2.28573746e+00],\n",
       "        [ 8.76573388e-01,  7.92528494e-02,  2.01611889e+00,\n",
       "         -4.11907801e-01, -4.33135295e-02,  3.95222129e-01,\n",
       "         -4.52534260e-01,  2.76823631e-01, -3.56169268e-01]]),\n",
       " 'Theta_21': array([[ 0.23183097, -0.33604563, -1.12751987, -0.0119594 ,  0.39778416],\n",
       "        [-0.17259673,  2.20810437, -0.66961706, -0.81959432, -0.05126043],\n",
       "        [-0.00979659, -0.08283426, -0.67774119,  1.10708674, -2.42219531],\n",
       "        [ 0.25155976, -0.24606711, -0.16064952, -0.01801522,  0.05852895],\n",
       "        [-0.02519265,  0.69563087,  1.12377481,  0.34747103, -0.71442328],\n",
       "        [-1.53192454, -0.32484913, -0.43830906, -1.46203779, -0.93528655],\n",
       "        [-1.29048203,  0.78430181, -0.20574726,  3.43165038,  1.0251638 ],\n",
       "        [-0.7343564 , -0.50276703, -0.05423149, -0.24498894, -0.45941235]]),\n",
       " 'Theta_32': array([[-0.0847605 ,  0.3379717 , -0.01552924],\n",
       "        [ 5.28748223,  0.2749043 ,  0.35277976],\n",
       "        [ 0.28492135,  5.21569548,  0.65675424],\n",
       "        [ 0.28749646, -0.16602374,  0.35591479],\n",
       "        [ 0.11342309,  0.06200991, -5.51995041]]),\n",
       " 'Theta_k': array([[0.53880465],\n",
       "        [0.79640325],\n",
       "        [0.66043512]])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmholtz Machine({-1,1}, No bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"zero\"\n",
    "Phi, Theta = parameter_initialization_nobias(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21144180894898457 0.10851161797378958\n",
      "0.21132218444951734 0.11041370360960563\n",
      "0.20963734927898703 0.10508960687117773\n",
      "0.20856072818619567 0.1093300148223108\n",
      "0.20664167159803032 0.10361713764264813\n",
      "0.20549348340252216 0.1040275821708045\n",
      "0.20454381159117738 0.10088668441397694\n",
      "0.20301456959811318 0.10111009914665102\n",
      "0.2057424674733209 0.10499410642121869\n",
      "0.20829631805675475 0.0992223926358681\n",
      "0.21175743834841926 0.10986235749122338\n",
      "0.20597512970673817 0.10246561574911481\n",
      "0.21062967646260233 0.10723418483124735\n",
      "0.2064784579142833 0.10371673551099105\n",
      "0.2088966437906742 0.11557368297022494\n",
      "0.2055922432720056 0.10605810485977038\n",
      "0.2094548804890874 0.10883617525145055\n",
      "0.2106084088470429 0.10045104638073768\n",
      "0.19950855691556468 0.09204157784967124\n",
      "0.19903568999728843 0.09744932705645219\n",
      "0.19998074742742417 0.09531109726650068\n",
      "0.20522118613456766 0.10252656717032972\n",
      "0.21103229824324435 0.10313281105960694\n",
      "0.20816947030655594 0.10450733581686715\n",
      "0.204332590919339 0.10110490224182393\n",
      "0.20029852056864295 0.0987032789102787\n",
      "0.20120450571979498 0.09612892323227638\n",
      "0.19681070166266362 0.09926249476727632\n",
      "0.1981314818187533 0.09419377905221804\n",
      "0.20431696091967186 0.1008732498961662\n",
      "0.20221979636996892 0.0983192095652139\n",
      "0.19927553597232195 0.09446508526897573\n",
      "0.20008817146076105 0.09872154961842479\n",
      "0.20065994352856367 0.09540925439609517\n",
      "0.20297757844157818 0.09989391159197662\n",
      "0.20482700910757637 0.09651918960507151\n",
      "0.20281476053496222 0.0982230535442292\n",
      "0.20295492010125832 0.10639795193824993\n",
      "0.207741998483687 0.0964006053328509\n",
      "0.20777880360387924 0.10038839522551811\n",
      "0.20863220113082495 0.10416484552479673\n",
      "0.20819821609225636 0.1044683149305855\n",
      "0.20465809205265256 0.1040353196527838\n",
      "0.20368520248510932 0.09787876517467828\n",
      "0.20392934802516113 0.09788659762469244\n",
      "0.20626450102577 0.10562642096379411\n",
      "0.20576096285607537 0.1050830796027359\n",
      "0.21237325316384634 0.11163829525443578\n",
      "0.21143820755726497 0.10175219131390281\n",
      "0.2073074974994169 0.10085987055047979\n",
      "0.20961990948265455 0.107565789498279\n",
      "0.20812531577886867 0.10099616973652592\n",
      "0.2059309769519683 0.10202736144748274\n",
      "0.20509540270534196 0.09882160377792967\n",
      "0.2054370776127119 0.10622796712800427\n",
      "0.2108134919611134 0.09684570284019663\n",
      "0.20590893114524417 0.10214058192305357\n",
      "0.20038298640214236 0.09648404411684067\n",
      "0.2065545469594104 0.09755877455196404\n",
      "0.20922556540026005 0.08438609010746667\n",
      "0.2074749043797221 0.10061508353825724\n",
      "0.20419277499987654 0.1001214102887306\n",
      "0.2059589191123021 0.10158817364073089\n",
      "0.20815869600086204 0.10473825468918983\n",
      "0.20777218650519358 0.10343261115592746\n",
      "0.20812703951708086 0.10060949754101083\n",
      "0.20862438915558926 0.09293209690816517\n",
      "0.207974323766061 0.10073007031089952\n",
      "0.2067618231837915 0.10120830888292577\n",
      "0.21142234745920735 0.10795603094730798\n",
      "0.2092228938321767 0.10980624396717528\n",
      "0.2045645051388147 0.10040834756969684\n",
      "0.20891905257469437 0.11518194431972092\n",
      "0.2114884346423014 0.10613268390272045\n",
      "0.20779635883920547 0.10200013602550444\n",
      "0.21014676293531367 0.10204189098727576\n",
      "0.2127489234717564 0.10355271474009763\n",
      "0.21008389322089457 0.10016889017193584\n",
      "0.2078664285534818 0.09977805735276918\n",
      "0.20199355981042239 0.09673347955424677\n",
      "0.20282914328326535 0.09795138510195252\n",
      "0.20483351610020478 0.0970185739538325\n",
      "0.20040715564561554 0.08856809767640171\n",
      "0.1927740959088742 0.08483789219609962\n",
      "0.19588497286266215 0.08801847220090572\n",
      "0.1978977543979014 0.08588049506507925\n",
      "0.2017354941362091 0.08991523911713405\n",
      "0.20034994446129975 0.08976529594132936\n",
      "0.1975296039866653 0.09127257012576842\n",
      "0.197273763816651 0.08680589500190103\n",
      "0.1978348456175804 0.08405538267326836\n",
      "0.20007815066608564 0.08631024735373857\n",
      "0.19704877348037927 0.08690743123918153\n",
      "0.199301534260022 0.09646147731503975\n",
      "0.20087150398658812 0.09642003909844261\n",
      "0.20336860530799897 0.09162248011126853\n",
      "0.1997082384674988 0.09280814608615663\n",
      "0.20394994338688724 0.09409643397129762\n",
      "0.2010464421623231 0.09123498968149332\n",
      "0.20517873317677376 0.09221174718168643\n",
      "0.20311402327753153 0.08876684055314177\n",
      "0.20509559610375558 0.09332388893539673\n",
      "0.20531027622678943 0.08753326439751102\n",
      "0.19989339551109728 0.09109615864354986\n",
      "0.2086401943737117 0.09401262524364615\n",
      "0.20725519179313703 0.09442624910405162\n",
      "0.2061391802878648 0.0968222849283479\n",
      "0.20155111932943265 0.09967529664393208\n",
      "0.20656026626580412 0.09308009662451994\n",
      "0.20666937216206369 0.0914301243825223\n",
      "0.20281810145847692 0.10034907183641538\n",
      "0.20711484371637817 0.09890565640765585\n",
      "0.20633673960711116 0.09760977992810327\n",
      "0.20984081081223438 0.10574142292945898\n",
      "0.2038734266164497 0.09725655801780476\n",
      "0.20525674327362906 0.09243770308642295\n",
      "0.20634054175215794 0.09386888262799403\n",
      "0.20245983602531134 0.09777275192566953\n",
      "0.2042211625211425 0.09289852774065396\n",
      "0.20863343991441247 0.09639162711724719\n",
      "0.20547732997488485 0.10179293172173022\n",
      "0.20115419346827562 0.09790317480714844\n",
      "0.20154489407658155 0.09239442296589921\n",
      "0.20006439278610802 0.0956985887838047\n",
      "0.20575039582315288 0.0984152431943588\n",
      "0.20092835638351791 0.09153629676189463\n",
      "0.2051849330745545 0.1016192141277959\n",
      "0.20748474557154065 0.10206324846872654\n",
      "0.2047135588575543 0.09560792348012374\n",
      "0.20575704739341255 0.09829321998625087\n",
      "0.2054711177868189 0.09376808558747693\n",
      "0.20328561158113467 0.09392216227663362\n",
      "0.2088635529470606 0.10152289671664348\n",
      "0.20713145790878787 0.09950753382679708\n",
      "0.20632607773660588 0.10576472376717057\n",
      "0.20455397614337684 0.09977265865199109\n",
      "0.20890118563234977 0.10367158228567154\n",
      "0.20783536078578185 0.10820602553606815\n",
      "0.2101970502318705 0.10924013539026839\n",
      "0.2065552126109065 0.10255622876094235\n",
      "0.20550439926887376 0.1046214687637445\n",
      "0.21462330758953374 0.10926123477888786\n",
      "0.21212727448499527 0.11014194810003321\n",
      "0.21646140055458887 0.10820197188799992\n",
      "0.2129575014949848 0.11412054644112823\n",
      "0.22193818314436914 0.11643310210245766\n",
      "0.21776886926554156 0.11042885309759749\n",
      "0.21655328990125308 0.11201922817732674\n",
      "0.2128859293125195 0.10452778276595363\n",
      "0.21666327195961968 0.10805041913112018\n",
      "0.21188791066399232 0.10397002938049592\n",
      "0.21227817698982346 0.10539950153687118\n",
      "0.21230925411884763 0.11067070900441658\n",
      "0.21767818371899397 0.10706831075970667\n",
      "0.21239067044488547 0.10703468688827876\n",
      "0.21158386746391275 0.10266445786210261\n",
      "0.21466616169308186 0.10813249583166412\n",
      "0.21712202759240393 0.11461814456878237\n",
      "0.21536666602856253 0.10857855218604619\n",
      "0.21727207626143996 0.11379437762836041\n",
      "0.21292230973135842 0.11106629744546712\n",
      "0.21401016136021833 0.10603803296509494\n",
      "0.2115482639462749 0.1028979689277683\n",
      "0.2109569445285086 0.10205311240646496\n",
      "0.21110675099242596 0.10583773833937396\n",
      "0.20951673489956907 0.10453310788623103\n",
      "0.20721438906693546 0.10239800439341042\n",
      "0.2176173903014362 0.10406295311005129\n",
      "0.21622945109000938 0.10986824688552885\n",
      "0.22233574419933505 0.11131789894772609\n",
      "0.21301395785990784 0.10506333752733663\n",
      "0.21343998710624437 0.10166820526937499\n",
      "0.21248496714691414 0.10876746704976649\n",
      "0.21656338526091368 0.11002854294480538\n",
      "0.21453177631024592 0.11002152795854918\n",
      "0.21520317204878678 0.10399165161505555\n",
      "0.21188881150562364 0.10321452827407886\n",
      "0.2137801068984983 0.10839723872070613\n",
      "0.21753054661427612 0.10492626437394609\n",
      "0.21432827165065574 0.11239958426964156\n",
      "0.2199476073048086 0.10598592542736113\n",
      "0.2140552581144834 0.10884566625354954\n",
      "0.21703650245242964 0.10481974278152571\n",
      "0.22070399018535564 0.1140046389848533\n",
      "0.21897743637669556 0.10688305204541013\n",
      "0.22166217064827573 0.12152503313687989\n",
      "0.22079730601967135 0.11318484199614573\n",
      "0.22356917998390546 0.11759472157233597\n",
      "0.22632666085492653 0.11796559489710332\n",
      "0.2229063996059238 0.12191801908119711\n",
      "0.22359285108644297 0.125316133860085\n",
      "0.2255375572372265 0.12369073799734026\n",
      "0.22334743834830706 0.11822778473726346\n",
      "0.22846262256138314 0.12600821887357785\n",
      "0.23046677466765678 0.1330002165198652\n",
      "0.23525812400239116 0.13346721043114976\n",
      "0.23291286529605565 0.1287415541985371\n",
      "0.23172025332406201 0.13174413116745204\n",
      "0.23510618640156564 0.1318152520436121\n",
      "0.2354429660701038 0.13096047478201506\n",
      "0.22984819676725526 0.12838266810655363\n",
      "0.23376446757080568 0.12622530463650958\n",
      "0.23574206793790703 0.13737721224122706\n",
      "0.23771290119430413 0.13632925578742344\n",
      "0.23432688575397695 0.14653733094205199\n",
      "0.23558337858058293 0.1397264735723651\n",
      "0.2412950738051219 0.14614499483561483\n",
      "0.24149912513112057 0.14713545364325004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24547984192430858 0.14750785431393817\n",
      "0.24321753268638144 0.14762216016268476\n",
      "0.23827044330216365 0.14833077671257971\n",
      "0.24266973008906262 0.14798305590109395\n",
      "0.23863733981729648 0.14059427466118946\n",
      "0.2354465791118978 0.13351694232326441\n",
      "0.23477840685041332 0.13220651349190254\n",
      "0.2321246991513295 0.1275829735990318\n",
      "0.23386504624866564 0.13573166579737736\n",
      "0.2330392827642621 0.1339235984314507\n",
      "0.228114456391656 0.12689569382962648\n",
      "0.23167132273762653 0.1276749452608936\n",
      "0.23027968989058253 0.13003043664496486\n",
      "0.23054294057172983 0.13071580833944946\n",
      "0.2317638417607181 0.1313039199374044\n",
      "0.2294066534639206 0.1267765355827092\n",
      "0.23411498623989943 0.13405602146254336\n",
      "0.23492040041151505 0.13647693752045947\n",
      "0.22973447244123976 0.12372331194653056\n",
      "0.22842678937101465 0.12575980205006224\n",
      "0.23106625239919462 0.121067961627299\n",
      "0.2292695650130827 0.11629029196904883\n",
      "0.22840783217151467 0.12190065553299811\n",
      "0.22619203164494145 0.11700679890544542\n",
      "0.22843372803007989 0.12341597215498575\n",
      "0.22389611195570516 0.12549334910341886\n",
      "0.2250617893344204 0.12407782497578894\n",
      "0.22692618077944232 0.11729886319437792\n",
      "0.22481798600280947 0.11776057646736307\n",
      "0.226375093003841 0.12054989841200789\n",
      "0.2222605192237221 0.11087767322287326\n",
      "0.22258336803746664 0.11679834832507158\n",
      "0.22360860979025146 0.12374496265161128\n",
      "0.22428085683220045 0.12537276936192385\n",
      "0.22357302553530484 0.12110375605132827\n",
      "0.22298910311226844 0.11866609012163924\n",
      "0.22281641948021103 0.11994729343350467\n",
      "0.2277542297660022 0.13151791592909295\n",
      "0.22974347525414354 0.12797487879982014\n",
      "0.23056795707490282 0.1339320200655893\n",
      "0.22900063052737946 0.12292049814117888\n",
      "0.22894729115999954 0.12748493693409108\n",
      "0.23275865837130408 0.12612668310463537\n",
      "0.23196100883072676 0.13208024217260708\n",
      "0.2358435877551556 0.12738922227070018\n",
      "0.2346178798899157 0.12529941751327592\n",
      "0.22772380682565307 0.12472044802659588\n",
      "0.2277022987845924 0.12287847261296489\n",
      "0.2252538402690935 0.12381534097346411\n",
      "0.22521477440819682 0.12545808341871337\n",
      "0.22554628482328815 0.12346161863555871\n",
      "0.2246996542210361 0.12031950612539959\n",
      "0.22650050557329254 0.1226296172473653\n",
      "0.22334789027618496 0.11440477409541225\n",
      "0.22639948083527442 0.11479830884787436\n",
      "0.2213218058771708 0.10649506767892694\n",
      "0.2207001257652005 0.11178327248783156\n",
      "0.223525926213152 0.11821467302467639\n",
      "0.22459380046580166 0.11444648378646861\n",
      "0.22645585810687702 0.11649951786089274\n",
      "0.22149525615233726 0.1177312979768022\n",
      "0.22140793350710639 0.11561312597144338\n",
      "0.22222304114949568 0.11437025574643556\n",
      "0.22522734072235892 0.11264078204931624\n",
      "0.2232205783367169 0.111342686031988\n",
      "0.22701068995609697 0.11722644783185539\n",
      "0.22181482002057099 0.11123617506658057\n",
      "0.22475924539854458 0.11592841054329608\n",
      "0.22341186692441767 0.1140183548125168\n",
      "0.22017303412001776 0.11430234779783144\n",
      "0.2242532652181008 0.11119127478889033\n",
      "0.22183610253587985 0.11247932443999144\n",
      "0.22319999540402524 0.115178146953242\n",
      "0.21996750662540615 0.11164362981182162\n",
      "0.2182462664869916 0.11602159982047627\n",
      "0.22702569869253267 0.11928922972822162\n",
      "0.22557666154525283 0.12416490254705086\n",
      "0.22093456202964715 0.11940443712890847\n",
      "0.23021738561660385 0.127705284683322\n",
      "0.2306111977992058 0.12583585416739188\n",
      "0.23108464952041474 0.12409868650158347\n",
      "0.22662500957299667 0.12035369980543471\n",
      "0.23046769032000594 0.12764216704606207\n",
      "0.22818994069088452 0.12021239993010453\n",
      "0.23297035879791891 0.12175658660845677\n",
      "0.22945067833768212 0.1158162049734556\n",
      "0.22876669151241197 0.12191883909138888\n",
      "0.23162235520105928 0.12230768983965697\n",
      "0.22722965502907985 0.12098186762221749\n",
      "0.22622485459024222 0.11633639984910338\n",
      "0.22616235054373043 0.12486212558678399\n",
      "0.23026351847562782 0.12361023296293502\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set[:,i:i+1]\n",
    "        Q, Alpha_Q = wake_forward_nobias(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_delta_nobias(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        P, Alpha_P = sleep_forward_nobias(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_delta_nobias(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate_nobias(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8143"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values < 256].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255],\n",
       "       [  2,   1,   5,   4,   4,   9,   1,   7,   3,   6,   6,  10,  12,\n",
       "         12,  10,  10,  11,   4,   2,   5,   3,   5,   3,   8,   5,   4,\n",
       "          8,   9,   9,  22,  11,  12,  15,  11,  14,   9,  18,  24,  16,\n",
       "          7,   9,  19,   4,   7,   9,   8,   6,   3,   5,   6,  17,   9,\n",
       "          3,  11,   8,  11,  25,  12,  27,  22,  11,  14,  13,  21,  15,\n",
       "          7,   9,  10,  15,  13,  16,  12,  16,  14,  16,  23,  39,  23,\n",
       "         17,  18,  15,  34,  16,  32,  16,  33,  18,  23,  25,   4,   6,\n",
       "          1,   7,   5,   8,   7,   9,   4,   3,   1,  13,   5,   8,   6,\n",
       "          9,   5,   2,   9,  13,  11,  12,   8,  12,  11,  10,  14,   5,\n",
       "          6,   6,   2,   7,   2,   8,   3,   6,   2,   4,   1,   4,   5,\n",
       "          9,  16,  13,   8,  28,   5,   9,  17,   5,   7,   8,  13,   5,\n",
       "         17,   9,  15,  22,   4,   8,  10,  10,  19,   7,  20,  17,  41,\n",
       "         34,  41,  15,  58,  12,  18,  31,  19,  11,   9,  15,  28,  14,\n",
       "         12,  34,  46,  31,  44,  55,  35,  31,  54,  82,  46,  72,  58,\n",
       "        109,  67,  26,  17,  28,  40,  27,  28,  43,  47,  36,  42,  59,\n",
       "         63,  42,  50,  60,  85,  83,  69,  65,  90,  94,  87,  61, 132,\n",
       "         79,  27,  23,  27,  46,  21,  24,  46,  43,  57,  28,  46,  70,\n",
       "         74,  87,  93,  67,  86,  87,  96,  89, 101, 106,  55,  47,  43,\n",
       "         69,  77,  72,  71,  28,  84, 103,  90, 120,  99, 116,  78, 120,\n",
       "        132, 171, 146, 146, 118, 204, 114, 157, 107]], dtype=int64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.array([values, counts])\n",
    "for i in range(values.size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        statistics = np.append(statistics, np.array([[values[i]+j],[0]]),axis = 1)\n",
    "statistics = np.unique(statistics,axis = 1)\n",
    "statistics[:,0:256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmholtz Machine({-1,1}, With bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"zero\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25099032671736404 0.17689707891277448\n",
      "0.2499040624152378 0.17292468640030267\n",
      "0.255835949025975 0.17802833340270194\n",
      "0.25648814557706084 0.18091978680818713\n",
      "0.255026505460202 0.1852384158511488\n",
      "0.2533620477777227 0.18865158156615017\n",
      "0.2587343287207544 0.18116280162373524\n",
      "0.255313903559481 0.1813033905189412\n",
      "0.2539962373143932 0.17218119153967712\n",
      "0.24703645614497147 0.17363856159813465\n",
      "0.24524641429384603 0.17501286669150187\n",
      "0.2457181289304028 0.16528536020331916\n",
      "0.2541811745631462 0.17691160895455904\n",
      "0.24942357084480055 0.17456789113173524\n",
      "0.24712348761815636 0.17836732418659793\n",
      "0.2495884787214872 0.17106514536880793\n",
      "0.24231569343807668 0.1728583136079073\n",
      "0.24012683072244123 0.17077932111074523\n",
      "0.2457051436525877 0.17147674005864122\n",
      "0.23917072922242813 0.17288779329773424\n",
      "0.24808802710273825 0.17466717406228816\n",
      "0.2567044577872202 0.18111961593805065\n",
      "0.25074206983192965 0.18349442553655199\n",
      "0.24714197596225174 0.16838081099354033\n",
      "0.24361190601190594 0.16907528863649868\n",
      "0.2455885421999716 0.17764464670796135\n",
      "0.2452008570748281 0.16813456104302904\n",
      "0.23974999150487264 0.1684080014478721\n",
      "0.2370845681875434 0.1634124979310233\n",
      "0.2347060219385953 0.15832426003119973\n",
      "0.24142684489908978 0.1677714378202068\n",
      "0.24453760452262002 0.17534002263523685\n",
      "0.24096266706286362 0.17386047525056234\n",
      "0.2390540502053388 0.16445269017530537\n",
      "0.24298078967203493 0.16384094455601664\n",
      "0.23839809236919277 0.16995286593534426\n",
      "0.23423612001529348 0.1618442033292647\n",
      "0.24660172765931446 0.17312477284225553\n",
      "0.24380539474134114 0.17452573336252164\n",
      "0.25018056350174095 0.17738484651309042\n",
      "0.23999647141399522 0.17074486554190682\n",
      "0.2428570254867725 0.1670818507568996\n",
      "0.23870584398823474 0.16496312039749217\n",
      "0.23031394895057838 0.14932593980843703\n",
      "0.23071035628325695 0.1586747308812716\n",
      "0.23008585307197388 0.14948464165832445\n",
      "0.22758774168564067 0.14126279354948337\n",
      "0.2297311780926989 0.1512000666298419\n",
      "0.22676050960656843 0.14058633178188762\n",
      "0.22587468925328608 0.1450813674468646\n",
      "0.2325403178864877 0.1524303921444404\n",
      "0.23105330713989217 0.15624726876788309\n",
      "0.23199750202724073 0.1487743128325001\n",
      "0.24089767624308528 0.16001645924097374\n",
      "0.23633860485699845 0.15999879231418423\n",
      "0.2301279023803474 0.15399376833124653\n",
      "0.23039281010076298 0.15032505265820076\n",
      "0.2284693855093739 0.1378447811767057\n",
      "0.2207640191318794 0.14026759452139237\n",
      "0.22941661735851948 0.15725272079203584\n",
      "0.23359243976236688 0.1542796887296223\n",
      "0.2363335029838403 0.15457592800782852\n",
      "0.24091937093321206 0.16113758869821598\n",
      "0.23534729749628433 0.15287290414432664\n",
      "0.23287128448410538 0.1538141077695327\n",
      "0.2300043555276331 0.14972311695792498\n",
      "0.22651604215750715 0.15175653938548983\n",
      "0.2209011114900476 0.14523266724442674\n",
      "0.2279246827832748 0.15470843496122877\n",
      "0.2345786958341074 0.15689521262825637\n",
      "0.23058182860529927 0.1642547355461809\n",
      "0.23459803354745457 0.16751082406650386\n",
      "0.2374303689163629 0.16030051935199519\n",
      "0.22859766257252245 0.15168603249063733\n",
      "0.22651258288363804 0.15433095465182484\n",
      "0.2235930324110674 0.13860869785174545\n",
      "0.22739680349021285 0.14970704624450307\n",
      "0.21935144787693214 0.13991313330375102\n",
      "0.22509587165030842 0.1485572424809709\n",
      "0.22441264736156682 0.14276809229621837\n",
      "0.22320680449022948 0.143177251007351\n",
      "0.2312375001595242 0.154419787240321\n",
      "0.22822748303040563 0.14371648430332226\n",
      "0.22571242211211598 0.14561492044385757\n",
      "0.22537076171202397 0.14199959583962207\n",
      "0.2291070115272367 0.15079301182289448\n",
      "0.2330281973121249 0.15081582390115966\n",
      "0.23648305480739268 0.15287000517562416\n",
      "0.22806387012933246 0.1440819542247454\n",
      "0.22436018197903615 0.1442112761707684\n",
      "0.21764479397379913 0.12923796426756828\n",
      "0.2103215328060312 0.12687228743634607\n",
      "0.22033387532916743 0.13080061353999992\n",
      "0.2171748571027424 0.12196335189306934\n",
      "0.21585509389319155 0.12053070632799226\n",
      "0.21364508018127293 0.12243522965592855\n",
      "0.21408721401531763 0.12709747075852784\n",
      "0.2135311006521388 0.12857082411173895\n",
      "0.21655051674672174 0.13504351839621884\n",
      "0.2190007836290678 0.1281610336179179\n",
      "0.2209675737554998 0.1292544558920947\n",
      "0.22166149897379875 0.14177236360914106\n",
      "0.223855947358772 0.13866710012639893\n",
      "0.2225820399953352 0.13810262249764954\n",
      "0.2246426253720031 0.13668879722335162\n",
      "0.23062187084330454 0.14493802749454177\n",
      "0.22214014605302015 0.14377200990748695\n",
      "0.22293998830012815 0.13480819318650447\n",
      "0.2229146436088678 0.136182961154766\n",
      "0.2149806023951437 0.12924821083174415\n",
      "0.22002809996417266 0.12634320438338148\n",
      "0.2178783245255287 0.13738103116000636\n",
      "0.21612087607265468 0.1179739630840251\n",
      "0.2125677032584423 0.12356035060365382\n",
      "0.21687621740033533 0.12308848136635879\n",
      "0.21037442627228342 0.12175934600385546\n",
      "0.2153356902123196 0.12652547845224732\n",
      "0.21504458743287838 0.12497381043685477\n",
      "0.21507159227258085 0.12979271831240924\n",
      "0.21626347492516448 0.12787671501741785\n",
      "0.21639091400393312 0.13011903745039763\n",
      "0.2197917142167781 0.12258617179199639\n",
      "0.21519037949945047 0.13127800714696025\n",
      "0.22125385416525795 0.12107724535839741\n",
      "0.21864398686826486 0.13093085366146776\n",
      "0.21618816401693491 0.13066557490394445\n",
      "0.2129154251213537 0.12636357301245535\n",
      "0.21674482948017468 0.13015965481583605\n",
      "0.22217108724939807 0.13284062653647902\n",
      "0.21667111831078764 0.12804213323984923\n",
      "0.2164686969329585 0.1307965654447219\n",
      "0.21506422639846906 0.12824334301277399\n",
      "0.22076701248708824 0.12635759267263186\n",
      "0.21590682758495294 0.13326011881868208\n",
      "0.214123786708323 0.12363251111449722\n",
      "0.21298964731950576 0.1298306065903798\n",
      "0.21669761811634713 0.13346355325557868\n",
      "0.214190779956388 0.12765732549323955\n",
      "0.2209688844266264 0.12932292544235016\n",
      "0.22391994950368424 0.14177990663541234\n",
      "0.22036056773789756 0.1346298236930885\n",
      "0.2230840947251132 0.14059147633433403\n",
      "0.22152270664798446 0.13477364078010387\n",
      "0.22180406456566534 0.1431098951787548\n",
      "0.22479671713992602 0.1328219701696943\n",
      "0.21532646490911939 0.13076825911349593\n",
      "0.21644949479630848 0.13355083151036504\n",
      "0.21617177663284134 0.13318243309878666\n",
      "0.21457900874556218 0.12789623687939292\n",
      "0.21875490803487094 0.13066017266775215\n",
      "0.21259891131661773 0.11557543351322902\n",
      "0.2163625022785844 0.12531518383160892\n",
      "0.21589544976816682 0.12509392258164162\n",
      "0.22425529741903008 0.12809339148324791\n",
      "0.21963856655029548 0.13307661133348783\n",
      "0.22046123327148406 0.1348549644786654\n",
      "0.2168903616842203 0.1302365446968102\n",
      "0.21230514094121525 0.12521752386597804\n",
      "0.21362538651212318 0.11639504412611959\n",
      "0.215576098119007 0.13066304040441912\n",
      "0.20621377236258015 0.11938920629931467\n",
      "0.20499622213918592 0.12442201734487292\n",
      "0.20675234637668838 0.12270342476188174\n",
      "0.2158594313357932 0.1316008430967415\n",
      "0.21504736482094852 0.12553872215614417\n",
      "0.22063853750807666 0.1366608446255904\n",
      "0.225349986319138 0.14518942400376703\n",
      "0.22261863330483375 0.14323016107083544\n",
      "0.22119460724155734 0.13888594428838025\n",
      "0.22079891893593895 0.13445886829992612\n",
      "0.21663339767172263 0.13102918439107072\n",
      "0.22210354286748843 0.1370877047066983\n",
      "0.22553635742799769 0.14639263495342505\n",
      "0.22608953482164196 0.14286068845909755\n",
      "0.2253643543471203 0.1460883188617473\n",
      "0.22572017394096391 0.14135005231223804\n",
      "0.21945691923240446 0.1371889873021823\n",
      "0.21883251936943046 0.1349263162544622\n",
      "0.21586374327911922 0.12435456214006715\n",
      "0.21308709183000613 0.1265800075170522\n",
      "0.21389315786822552 0.13392802600448112\n",
      "0.20876322050410034 0.12912571818019628\n",
      "0.20462769187453417 0.11273754916081716\n",
      "0.21135577706759628 0.12382186517238865\n",
      "0.2120375916423825 0.12441938247719522\n",
      "0.21092501138108408 0.12585261084851024\n",
      "0.20216817510060725 0.11416835028443582\n",
      "0.19978960267338036 0.10761329086169966\n",
      "0.20814639378791513 0.12717907819919314\n",
      "0.21022603127156367 0.12398805235207551\n",
      "0.20555200174582353 0.12861819995246915\n",
      "0.2042134624052284 0.11752127640334438\n",
      "0.20135779563176479 0.12548563535074858\n",
      "0.20520945227225976 0.11757679260749052\n",
      "0.20503464974800317 0.11844229760225738\n",
      "0.2028318418872429 0.11426331354733434\n",
      "0.20776325311564406 0.12537018812470405\n",
      "0.2081344721181393 0.12171797594250224\n",
      "0.2036237977996632 0.11499635866951946\n",
      "0.20086887046381927 0.10936586024405072\n",
      "0.201885925373157 0.10555843785854202\n",
      "0.2024899765886072 0.1038800002490542\n",
      "0.2110716760915966 0.12206586219575644\n",
      "0.2088451090650348 0.11761520749638955\n",
      "0.2115586927325902 0.11907461694135156\n",
      "0.21291840257238998 0.1291050360943928\n",
      "0.2094985505193784 0.12127653905012928\n",
      "0.21207483252317194 0.12696871859253145\n",
      "0.2109047342818761 0.12401035490908262\n",
      "0.21041007205875836 0.12308524706840134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20519022111385823 0.11663903770801559\n",
      "0.20348374281348008 0.1105575004929603\n",
      "0.20616161260543703 0.11873257911248301\n",
      "0.20955583779320835 0.1201110807057696\n",
      "0.20604578105873428 0.11472834305720628\n",
      "0.20438941767390217 0.1170844155553509\n",
      "0.207179404417221 0.11167431088470849\n",
      "0.20558913354819144 0.11663693734184997\n",
      "0.2017672673191107 0.1073925209426348\n",
      "0.1944399216312345 0.09855335569552601\n",
      "0.18826439999311204 0.08851178518421643\n",
      "0.19121444967707638 0.10054267153800123\n",
      "0.19223068085059425 0.09517592746015882\n",
      "0.19199366649481658 0.09928155112624087\n",
      "0.19208128887906992 0.10094185645912511\n",
      "0.2005356890975852 0.10560642722252217\n",
      "0.2042874863526665 0.11112583611579854\n",
      "0.20579546621951733 0.10797465025442068\n",
      "0.19975464715564828 0.10754070459440962\n",
      "0.2006443121581505 0.1096227157465536\n",
      "0.20071205200463774 0.11390717012517196\n",
      "0.19708564246728047 0.10909293499274857\n",
      "0.20169691872738582 0.10824583605155466\n",
      "0.2020354939750461 0.10940875023188587\n",
      "0.20019332080253022 0.09939271915560613\n",
      "0.2007984101786757 0.10933988526471074\n",
      "0.1997193601213432 0.10722188663989186\n",
      "0.20782703379278572 0.11912686071593638\n",
      "0.20910666330039765 0.1193864326837879\n",
      "0.2090075243299528 0.11860893314527382\n",
      "0.20890911452633187 0.1257149323734677\n",
      "0.20681982536270752 0.11208497257346643\n",
      "0.20486329338291848 0.11038866297678271\n",
      "0.20815490439009848 0.11573622817415305\n",
      "0.20268255858553447 0.11349349918856406\n",
      "0.21139648956730914 0.11784424953410312\n",
      "0.2083457235792759 0.11408280352089613\n",
      "0.20670498160431178 0.11628872346828391\n",
      "0.2087227429003408 0.1187816813777942\n",
      "0.20666475905732845 0.11696254575128576\n",
      "0.2118462452815346 0.12141600139177477\n",
      "0.21777494382470225 0.12465633593257229\n",
      "0.2143147347569511 0.12333250978861078\n",
      "0.22325564471281895 0.12800823762596278\n",
      "0.21852340231161346 0.13998386842405378\n",
      "0.21598870099998463 0.13072138363620928\n",
      "0.2207245356440687 0.1384352138214904\n",
      "0.21983144722542908 0.1324214348352526\n",
      "0.2224653342535574 0.12784988674686745\n",
      "0.22277202087790987 0.13812483887005195\n",
      "0.22305348522497906 0.13534356834669242\n",
      "0.22471767417614139 0.13332807308604902\n",
      "0.2238498562926463 0.13910703528282717\n",
      "0.2265739044684711 0.14602736814956307\n",
      "0.2241362566393133 0.14857627111524846\n",
      "0.22423848623087847 0.1402854855680947\n",
      "0.22872357853885789 0.1491372003534588\n",
      "0.2229268139263151 0.14204179073091858\n",
      "0.22535499241895562 0.1397640357493934\n",
      "0.2236720826937237 0.13566691928279437\n",
      "0.2184537377954831 0.13659035819894472\n",
      "0.21719662383068006 0.13120300235064478\n",
      "0.21643267410533082 0.13229213725282996\n",
      "0.21662026114064412 0.1288375805768597\n",
      "0.21447997465755983 0.12796596667752913\n",
      "0.21600365439314143 0.1293272622070529\n",
      "0.20728795141987133 0.12110678243958542\n",
      "0.20404392082526393 0.11257368505864482\n",
      "0.20499298723563228 0.10940135671151786\n",
      "0.20234826075430987 0.11055830201725875\n",
      "0.20030188030644525 0.09690379373499046\n",
      "0.19132220210547654 0.10161174796910238\n",
      "0.1955514913930823 0.10279858956314222\n",
      "0.19662527945873823 0.0995021971090379\n",
      "0.1999897729932587 0.1115642906234475\n",
      "0.1963724280986656 0.09672809240081043\n",
      "0.19867316585918193 0.09663948474487366\n",
      "0.19702032828491423 0.09911902065639358\n",
      "0.19906151348184234 0.10709236150313758\n",
      "0.19429137408568545 0.09756956630763926\n",
      "0.19765064224543835 0.10821500916254272\n",
      "0.20311056328184035 0.10797329366103398\n",
      "0.20819321659452356 0.12124711616475795\n",
      "0.21041305564837634 0.12265903985810156\n",
      "0.2130237848436249 0.12522650765766655\n",
      "0.20883253753602724 0.12686166905504032\n",
      "0.20836567672345627 0.12636452244924257\n",
      "0.2088255476466416 0.11799552660946615\n",
      "0.20651275080240766 0.10982214468769481\n",
      "0.19940341967751138 0.10721014628470034\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set[:,i:i+1]\n",
    "        Q, Alpha_Q = wake_forward(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_delta(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        P, Alpha_P = sleep_forward(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_delta(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9817"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values < 256].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "         41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "         54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n",
       "         67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
       "         80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "        119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
       "        132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
       "        145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
       "        158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
       "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
       "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
       "        223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
       "        236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
       "        249, 250, 251, 252, 253, 254, 255, 256, 257],\n",
       "       [  2,   1,   2,   5,   0,   2,   2,   3,   4,   2,   7,   7,   3,\n",
       "          9,   5,   4,   6,   2,   7,  10,  14,   4,   5,   2,   2,   4,\n",
       "          8,   6,   9,   7,  11,  12,   4,  18,   3,   4,  15,   5,  20,\n",
       "         10,   7,   2,   6,   4,   3,  13,   6,  18,  29,   3,   3,   6,\n",
       "         15,  16,  21,  11,  16,   6,  23,  15,  38,  31,   5,  11,  27,\n",
       "         15,  24,  20,  14,  25,  32,  23,  25,  39,  14,  17,  22,  58,\n",
       "         22,  37,  37,  19,  29,  52,  52,  66,  55,   0,   0,   0,   0,\n",
       "          0,   1,   1,   1,   0,   0,   0,   2,   2,   2,   4,   1,   0,\n",
       "          1,   3,   2,   0,   0,   1,   2,   9,  10,   7,  13,   2,   3,\n",
       "          6,   0,   0,   1,   5,   7,   2,   2,   2,   4,   4,   3,   3,\n",
       "          2,   4,   2,   3,   6,   9,   6,   4,   5,   4,   8,  12,   0,\n",
       "          4,   8,  12,  23,  25,  23,  13,   9,  13,  11,  11,  10,   7,\n",
       "          7,  10,  19,  28,  38,  28,   9,  27,  13,  10,  24,   7,   9,\n",
       "          2,   8,   5,   6,   8,   6,  21,  30,  22,  34,  31,  20,  13,\n",
       "         43,  45,  16,  23,  19,  34,  40,  53,  35,  30,  25,  32,  83,\n",
       "         50,  28,  26,  32,  39,  63,  50,  62,  91,  50,  60,  62,  34,\n",
       "         18,  40,  69,  73,  69,  63,  63, 107,  31,  34,  45,  63,  63,\n",
       "         74,  54, 107,  96, 122,  81, 145,  98,  79, 126, 103,  81, 129,\n",
       "        120, 173, 258, 175, 171, 143, 247, 135, 137, 190, 184, 199, 232,\n",
       "        201, 354, 362, 378, 248, 357, 342,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.array([values, counts])\n",
    "for i in range(values.size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        statistics = np.append(statistics, np.array([[values[i]+j],[0]]),axis = 1)\n",
    "statistics = np.unique(statistics,axis = 1)\n",
    "statistics[:,0:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': array([[-8.36540413e-01, -3.06410059e-02,  3.33452507e-01,\n",
       "          1.86871027e-01,  1.47692185e-01,  2.98797954e-01,\n",
       "         -1.08444891e-01,  4.25839228e-01,  8.40150359e+00],\n",
       "        [-1.43060245e-01,  2.51773504e-01,  8.52681208e-02,\n",
       "          1.67024130e-01,  2.61627890e-01,  7.43944446e+00,\n",
       "          1.03944058e-01,  4.00275901e-01,  3.59300167e-01],\n",
       "        [-5.08203449e-01, -5.17815952e-01, -1.02730378e+00,\n",
       "         -7.17975308e-02, -8.73390849e-02, -2.47069606e+00,\n",
       "         -5.00927901e-01, -6.25650531e-01,  6.14668125e+00],\n",
       "        [-3.86091530e-01, -6.08207898e-01,  3.36814234e+00,\n",
       "         -5.63881592e-01,  5.57727338e-01, -3.37088739e-01,\n",
       "          9.29598615e-02,  9.81238007e-01, -2.46873801e+00],\n",
       "        [ 1.12020512e-02,  2.77705710e+00,  6.26229663e-01,\n",
       "          1.62100903e-01,  6.34233795e-01, -1.11120596e-01,\n",
       "          1.48634333e-01, -1.53470477e+00,  4.17997041e+00],\n",
       "        [-7.06556236e-02, -8.49730182e-01,  6.69893414e-01,\n",
       "          2.99775682e+00,  4.95095020e-01,  1.32496414e-02,\n",
       "         -4.69456929e-02,  6.46514516e-01, -1.65955358e+00],\n",
       "        [ 2.86666670e+00,  2.91297665e-01, -1.25888257e+00,\n",
       "         -1.68403261e-01, -6.09914400e-01,  2.97279495e-01,\n",
       "          4.70618027e-01,  1.89852936e+00,  3.63841815e+00],\n",
       "        [-3.84393572e+00,  2.57286771e-01,  4.28722951e-01,\n",
       "          1.68092603e-02, -1.88074673e-01,  1.25416663e-01,\n",
       "         -4.90676340e+00,  1.09011765e+00, -2.32164458e+00],\n",
       "        [-4.23207736e+00,  2.34378807e-01,  5.69510654e-01,\n",
       "          7.32647055e-03, -9.70639951e-01,  3.46902155e-01,\n",
       "          3.55681303e+00,  1.24270074e-01, -4.33371609e-01],\n",
       "        [-3.28125710e+00, -1.31425974e-01, -5.20025950e-01,\n",
       "         -3.24922599e-01,  1.04623786e+00, -6.77081365e-01,\n",
       "          2.47928356e-02,  7.21484577e-02, -2.77622818e+00]]),\n",
       " 'Theta_21': array([[-1.30321575,  0.13423677, -0.04996752,  0.02879005, -0.01349665,\n",
       "         -2.7028098 ],\n",
       "        [-0.14553845,  3.974861  ,  0.07001097, -0.0054688 ,  0.12119729,\n",
       "          0.87182971],\n",
       "        [-0.28558021, -0.12809189, -0.51109279,  0.04824339, -0.47804433,\n",
       "          2.99069929],\n",
       "        [-0.04366623, -0.29438215, -0.32725623, -0.19746076, -1.54338588,\n",
       "          1.45150406],\n",
       "        [-0.54382961,  0.10682493, -0.53286674,  1.59549217, -0.6678321 ,\n",
       "          0.53289433],\n",
       "        [ 0.06684973, -0.38570032,  0.07324427,  3.53416133,  0.14872119,\n",
       "          0.49618305],\n",
       "        [ 3.34804535, -0.29556525,  0.5850995 , -0.47819966,  0.07068406,\n",
       "          0.32219781],\n",
       "        [-0.81047134, -0.91815315,  0.26293311,  0.44821171,  0.30336483,\n",
       "          1.65592192]]),\n",
       " 'Theta_32': array([[-1.20581943, -0.76375712, -0.12016846, -0.99648983],\n",
       "        [ 0.03557946,  0.36294932, -0.60199639, -2.22322451],\n",
       "        [ 0.56349474, -1.59262509, -0.30715716, -3.29214385],\n",
       "        [-2.29781793, -0.10977511, -0.37684362,  0.83384474],\n",
       "        [ 0.94986956,  0.70750481, -1.4956831 ,  0.55122908]]),\n",
       " 'Theta_k': array([[-1.99944761],\n",
       "        [ 5.1837616 ],\n",
       "        [-1.72169456]])}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmholtz Machine({-1,1}, Bias on data only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
