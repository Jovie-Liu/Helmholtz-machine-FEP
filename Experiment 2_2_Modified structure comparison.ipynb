{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 2</span> \n",
    "\n",
    "# Helmholtz Machine - Effect of Structural Modifications\n",
    "\n",
    "I did two modifications to the structure of the vanilla Helmholtz machine\n",
    "\n",
    "1. Change the binary value from {0,1} to {-1,1}\n",
    "2. Add bias term to the linear combination of neurons in the previous layer\n",
    "\n",
    "The reasons are stated elsewhere so let's take it for granted now, and see how these modifications affect the model behavior.\n",
    "\n",
    "*Updated:* December 9, 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use is the well-formed set generated with the well-formedness rules:\n",
    "\n",
    "1. Start with 1\n",
    "2. Forbid 00100 (no 100, 001 on the boundary)\n",
    "3. Forbid 0000\n",
    "\n",
    "We use a 10-node pattern with binary values thus there are 1024 combinations of all patterns, of which the well-formed set contains 256 instances. The dataset is generated as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 0., ..., 1., 1., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "well_formed_set = np.zeros([1,n])\n",
    "well_formed_set[0,0] = 1\n",
    "\n",
    "for i in range(1,n):\n",
    "    for j in range(np.shape(well_formed_set)[0]):\n",
    "        if i == 2 and np.array_equal(well_formed_set[j,i-2:i], [1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-3:i], [0,0,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        elif i > 3 and np.array_equal(well_formed_set[j,i-4:i], [0,0,1,0]):\n",
    "            well_formed_set[j,i] = 1\n",
    "        else:\n",
    "            well_formed_set = np.append(well_formed_set, well_formed_set[j:j+1,:], axis=0)\n",
    "            well_formed_set[j,i] = 1\n",
    "            \n",
    "ind = np.array([], dtype=np.int8)\n",
    "for i in range(well_formed_set.shape[0]):\n",
    "    if np.array_equal(well_formed_set[i,-3:], [0,0,1]):\n",
    "        ind = np.append(ind,i)\n",
    "\n",
    "well_formed_set = np.delete(well_formed_set,ind,0)\n",
    "well_formed_set = np.transpose(well_formed_set)\n",
    "well_formed_set_0 = well_formed_set\n",
    "well_formed_set_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set_1 = (well_formed_set_0 - 0.5)*2\n",
    "well_formed_set_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Helmholtz machine structure is given as below:\n",
    "\n",
    "<img src=\"Helmz.jpg\" style=\"width:550px\">\n",
    "<caption><center> **Figure 4**: The Helmholtz Machine  </center></caption>\n",
    "\n",
    "I previously wrote in Document 2 about the variational objective function and deduction of the parameter updating rules of the Helmholtz machine, and I will give a more systematic analysis on these matters in upcoming notebooks, so here let's skip these steps and say we know the parameter updating local delta rule is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tilde{F}}{\\partial \\phi_{k,n}^{m-1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m-1}(1-q_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m-1} \\centerdot q_n^m & \\text{if } s_n^m = 0 \\text{ or } -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial \\theta_{k,n}^{m+1,m}} = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-s_k^{m+1}(1-p_n^m) & \\text{if } s_n^m = 1 \\\\\n",
    "s_k^{m+1} \\centerdot p_n^m & \\text{if } s_n^m = 0 \\text{ or } -1\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Let's write the functions to run this model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always have the bottom layer as data input, denoted as $d_0$. In our case illustrated in Figure 4, we have $m = 4$ layers, with various numbers of neurons, $n_{d_0} = 10$, $n_{z_1} = 8$, $n_{z_2} = 5$, $n_{z_3} = 3$. These are the hyperparameters to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(init_type,n_dz):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (m, ), where m is the number of layers\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \"\"\"\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    m = len(n_dz)\n",
    "    if init_type == \"zero\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((n_dz[i+1],n_dz[i]+1))\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.zeros((n_dz[i],n_dz[i+1]+1))\n",
    "        Theta[\"Theta_k\"] = np.zeros((n_dz[-1],1))\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(n_dz[i+1],n_dz[i]+1)\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.random.randn(n_dz[i],n_dz[i+1]+1)\n",
    "        Theta[\"Theta_k\"] = np.random.randn(n_dz[-1],1)\n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "    return Phi, Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization_nobias(init_type,n_dz):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (m, ), where m is the number of layers\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi), where the last column represents bias b's\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}), where the last column represents bias b's\n",
    "    \"\"\"\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    m = len(n_dz)\n",
    "    if init_type == \"zero\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((n_dz[i+1],n_dz[i]))\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.zeros((n_dz[i],n_dz[i+1]))\n",
    "        Theta[\"Theta_k\"] = np.zeros((n_dz[-1],1))\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(m-1):\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(n_dz[i+1],n_dz[i])\n",
    "            Theta[\"Theta_\" + str(i+1) + str(i)] = np.random.randn(n_dz[i],n_dz[i+1])\n",
    "        Theta[\"Theta_k\"] = np.random.randn(n_dz[-1],1)\n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "    return Phi, Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"random\"\n",
    "Phi_nobias, Theta_nobias = parameter_initialization_nobias(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- probability of each neuron (taking value 1), Python dictionary of same length as Phi with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = {}\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,np.append(S,[[1]], axis=0)))\n",
    "        S = ((q > np.random.rand(len(q),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        Q[\"q\"+str(i+1)] = q\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = well_formed_set_1[:,5:6]\n",
    "Q, Alpha_Q = wake_forward(d0,Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward_nobias(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- probability of each neuron (taking value 1), Python dictionary of same length as Phi with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = {}\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,S))\n",
    "        S = ((q > np.random.rand(len(q),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        Q[\"q\"+str(i+1)] = q\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = well_formed_set_1[:,5:6]\n",
    "Q, Alpha_Q_nobias = wake_forward_nobias(d0,Phi_nobias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_forward_nobias_zero(d0,Phi):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    Q -- probability of each neuron (taking value 1), Python dictionary of same length as Phi with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length len(Phi)+1 with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    S = d0  # assignment of each layer\n",
    "    Q = {}\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    for i in range(n):\n",
    "        phi = Phi[\"Phi_\" + str(i) + str(i+1)]\n",
    "        q = sigmoid(np.matmul(phi,S))\n",
    "        S = (q > np.random.rand(len(q),1)).astype(int)  # rejection sampling\n",
    "        Q[\"q\"+str(i+1)] = q\n",
    "        Alpha_Q[\"z\"+str(i+1)] = S\n",
    "    return Q, Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = well_formed_set_0[:,5:6]\n",
    "Q, Alpha_Q_nobias_zero = wake_forward_nobias_zero(d0,Phi_nobias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- probability of each neuron (taking value 1), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    \n",
    "    m = len(Theta)\n",
    "    P = {\"p\"+str(m-1):p}\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        P[\"p\"+str(i-1)] = p\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    return P, Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Alpha_P = sleep_forward(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward_nobias(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- probability of each neuron (taking value 1), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    \n",
    "    m = len(Theta)\n",
    "    P = {\"p\"+str(m-1):p}\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,S))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "        P[\"p\"+str(i-1)] = p\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    return P, Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Alpha_P_nobias = sleep_forward_nobias(Theta_nobias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_forward_nobias_zero(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    P -- probability of each neuron (taking value 1), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    Alpha_P -- assignment of each neuron (binary value), Python dictionary of same length as Theta with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = (p > np.random.rand(len(p),1)).astype(int)\n",
    "    \n",
    "    m = len(Theta)\n",
    "    P = {\"p\"+str(m-1):p}\n",
    "    Alpha_P = {\"z\"+str(m-1):S}\n",
    "    \n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,S))\n",
    "        S = (p > np.random.rand(len(p),1)).astype(int)    # rejection sampling\n",
    "        P[\"p\"+str(i-1)] = p\n",
    "        Alpha_P[\"z\"+str(i-1)] = S\n",
    "    return P, Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Alpha_P_nobias_zero = sleep_forward_nobias_zero(Theta_nobias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    for i in range(n):\n",
    "        S_bias = np.append(Alpha_P[\"z\"+str(i)],[[1]], axis=0)\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S_bias))\n",
    "        gain = q - (1+Alpha_P[\"z\"+str(i+1)])/2\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "Phi, info_gain_wake,error_Q = wake_update_delta(Phi,Alpha_P,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta_nobias(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    for i in range(n):\n",
    "        S = Alpha_P[\"z\"+str(i)]\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S))\n",
    "        gain = q - (1+Alpha_P[\"z\"+str(i+1)])/2\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "Phi_nobias, info_gain_wake,error_Q = wake_update_delta_nobias(Phi_nobias,Alpha_P_nobias,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wake_update_delta_nobias_zero(Phi,Alpha_P,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    Alpha_P -- Generative assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    lr -- learning rate, decimals\n",
    "    \n",
    "    Returns:\n",
    "    Phi -- Updated recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    info_gain_wake -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_Q -- cumsum of all terms in info_gain_wake, a measurement of discrepancy between the generative assignment and \n",
    "    the recognition model\n",
    "    \"\"\"\n",
    "    n = len(Phi)\n",
    "    info_gain_wake = {}\n",
    "    error_Q = 0\n",
    "    for i in range(n):\n",
    "        S = Alpha_P[\"z\"+str(i)]\n",
    "        q = sigmoid(np.matmul(Phi[\"Phi_\" + str(i) + str(i+1)],S))\n",
    "        gain = q - Alpha_P[\"z\"+str(i+1)]\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)] -= lr * np.outer(gain,S)\n",
    "        info_gain_wake[\"z\"+str(i+1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_Q += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Phi, info_gain_wake,error_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "Phi_nobias, info_gain_wake,error_Q = wake_update_delta_nobias_zero(Phi_nobias,Alpha_P_nobias_zero,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information gain, we can read the error of the given assignment *Alpha_P* for parameters *Phi*, on each neuron it computes. The value of info_gain is within $[-1,1]$, where positive value indicates the current neuron takes value $-1$, while negative value indicates that the current neuron takes value $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - (1+Alpha_Q[\"z\"+str(n-1)])/2\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,0,-1):\n",
    "        S_bias = np.append(Alpha_Q[\"z\"+str(i)],[[1]], axis=0)\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S_bias))\n",
    "        gain = p - (1+Alpha_Q[\"z\"+str(i-1)])/2\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S_bias)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta, info_gain_sleep, error_P = sleep_update_delta(Theta,Alpha_Q,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta_nobias(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - (1+Alpha_Q[\"z\"+str(n-1)])/2\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,0,-1):\n",
    "        S = Alpha_Q[\"z\"+str(i)]\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S))\n",
    "        gain = p - (1+Alpha_Q[\"z\"+str(i-1)])/2\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta_nobias, info_gain_sleep, error_P = sleep_update_delta_nobias(Theta_nobias,Alpha_Q_nobias,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_update_delta_nobias_zero(Theta,Alpha_Q,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    Alpha_Q -- Recognition assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "    a numpy array of shape (n_z, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Theta -- Updated generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    info_gain_sleep -- information gain of each neuron by single update, quantifies the error and the amount of parameter change \n",
    "    associated with each neuron, Python dictionary of length m-1 with each key-value pair being a numpy array of shape (n_z, 1)\n",
    "    error_P -- cumsum of all terms in info_gain_sleep, a measurement of discrepancy between the recognition assignment and \n",
    "    the generative model\n",
    "    \"\"\"\n",
    "    n = len(Theta)\n",
    "    info_gain_sleep = {}\n",
    "    error_P = 0\n",
    "    \n",
    "    p = sigmoid(Theta[\"Theta_k\"])\n",
    "    gain = p - Alpha_Q[\"z\"+str(n-1)]\n",
    "    Theta[\"Theta_k\"] -= lr * gain\n",
    "    info_gain_sleep[\"z\"+str(n-1)] = gain\n",
    "    error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    for i in range(n-1,0,-1):\n",
    "        S = Alpha_Q[\"z\"+str(i)]\n",
    "        p = sigmoid(np.matmul(Theta[\"Theta_\" + str(i) + str(i-1)],S))\n",
    "        gain = p - Alpha_Q[\"z\"+str(i-1)]\n",
    "        Theta[\"Theta_\" + str(i) + str(i-1)] -= lr * np.outer(gain,S)\n",
    "        info_gain_sleep[\"z\"+str(i-1)] = gain # positive(s=-1) & negative(s=1)\n",
    "        error_P += (np.abs(gain)).sum()\n",
    "    \n",
    "    return Theta, info_gain_sleep, error_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta_nobias, info_gain_sleep, error_P = sleep_update_delta_nobias_zero(Theta_nobias,Alpha_Q_nobias_zero,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,np.append(S,[[1]], axis=0)))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nobias(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,S))\n",
    "        S = ((p > np.random.rand(len(p),1)).astype(int) - 0.5)*2    # rejection sampling\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nobias_zero(Theta):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "    \n",
    "    Returns:\n",
    "    S -- generation of one instance, numpy array of shape (n_d, )\n",
    "    \"\"\"\n",
    "    theta = Theta[\"Theta_k\"]\n",
    "    p = sigmoid(theta)\n",
    "    S = (p > np.random.rand(len(p),1)).astype(int)\n",
    "    m = len(Theta)\n",
    "    for i in range(m-1,0,-1):\n",
    "        theta = Theta[\"Theta_\" + str(i) + str(i-1)]\n",
    "        p = sigmoid(np.matmul(theta,S))\n",
    "        S = (p > np.random.rand(len(p),1)).astype(int)    # rejection sampling\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  1., ..., -1.,  1., -1.],\n",
       "       [ 1.,  1., -1., ...,  1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       ...,\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1.,  1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "entire_set = np.zeros((2,10))\n",
    "entire_set[0,0] = 1\n",
    "for i in range(1,n):\n",
    "    for j in range(entire_set.shape[0]):\n",
    "        entire_set = np.append(entire_set, entire_set[j:j+1,:], axis=0)\n",
    "        entire_set[j,i] = 1\n",
    "entire_set = (entire_set - 0.5)*2\n",
    "entire_set = np.transpose(entire_set)\n",
    "entire_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       [-1., -1., -1., ...,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
       "       ...,\n",
       "       [-1., -1.,  1., ..., -1., -1., -1.],\n",
       "       [ 1.,  1., -1., ..., -1., -1., -1.],\n",
       "       [-1.,  1., -1., ..., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_set = np.unique(entire_set, axis=1)\n",
    "well_formed_set = np.unique(well_formed_set, axis=1)\n",
    "reordered_set = np.zeros(entire_set.shape)\n",
    "reordered_set[:,:well_formed_set.shape[1]] = well_formed_set\n",
    "\n",
    "k = well_formed_set.shape[1]\n",
    "for i in range(entire_set.shape[1]):\n",
    "    flag = 0\n",
    "    for j in range(well_formed_set.shape[1]):\n",
    "        if np.array_equal(entire_set[:,i], well_formed_set[:,j]):\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 0:\n",
    "        reordered_set[:,k] = entire_set[:,i]\n",
    "        k += 1\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set_0 = (reordered_set+1)/2\n",
    "reordered_set_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Helmholtz ({0,1}, No bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"zero\"\n",
    "Phi, Theta = parameter_initialization_nobias(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like stochastic gradient descent, we update the parameters for each data input (no batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4512408241986824 0.4771955535214047\n",
      "0.4455348670979606 0.4668768904083609\n",
      "0.44226677250524155 0.4676133832531709\n",
      "0.44342842204617844 0.4623762194860265\n",
      "0.4348607295606624 0.45209059297375437\n",
      "0.42393253369878714 0.43338195051651074\n",
      "0.4231244769398459 0.43194325085553853\n",
      "0.4126367765449228 0.42011269611780544\n",
      "0.4065061406721173 0.41028120668258855\n",
      "0.4083673120832688 0.4091654809852304\n",
      "0.41039128960857757 0.4138568728117954\n",
      "0.39976827355502065 0.39399292914445627\n",
      "0.39877285910970384 0.39731701065762776\n",
      "0.3858335651498432 0.386495320719365\n",
      "0.39505603950881163 0.3919209612208473\n",
      "0.39002688597789475 0.3882136875988232\n",
      "0.3903037106003247 0.3771105450229234\n",
      "0.3850545729738855 0.3805714874092258\n",
      "0.3739789767963079 0.36382902511753895\n",
      "0.3790900225048797 0.37724598673887605\n",
      "0.38638165975641203 0.38640381844910365\n",
      "0.3897499716850858 0.3843844069400341\n",
      "0.390485068222139 0.3854985220087329\n",
      "0.3821925907022479 0.3762448731639817\n",
      "0.3821083311311943 0.3828349313885993\n",
      "0.384381111741185 0.37519312397169974\n",
      "0.3815719038110449 0.37267759591116856\n",
      "0.37470007573688524 0.3645450560048113\n",
      "0.37905426448583013 0.3704305925512413\n",
      "0.3726483683987359 0.35526987937196647\n",
      "0.3678621240031139 0.3488306563431634\n",
      "0.3740012853363759 0.36193389440699975\n",
      "0.3695689483297709 0.3512879907872619\n",
      "0.36853477464950585 0.36007233209705763\n",
      "0.37743578258702465 0.3750630068107755\n",
      "0.3713010158524426 0.3695108560849467\n",
      "0.3758540927592593 0.3684719980069937\n",
      "0.37738842168540593 0.3702083078372606\n",
      "0.38651250811183907 0.38086455289097726\n",
      "0.38281307532466263 0.376885686188983\n",
      "0.3810525582424129 0.3725146278348129\n",
      "0.37486966285929124 0.3674025169221192\n",
      "0.36851696845439386 0.3562975767673871\n",
      "0.3644478574905655 0.34908980293605235\n",
      "0.37034696063992617 0.35401657580951323\n",
      "0.3668651962880411 0.34403971040453146\n",
      "0.3623519429971092 0.3493960446879003\n",
      "0.366955269055839 0.3557777709471387\n",
      "0.36657442617403496 0.3500138147490314\n",
      "0.3655862048709638 0.3481126241155248\n",
      "0.36778505512733156 0.33809672846726724\n",
      "0.3584575764146815 0.34155444433540083\n",
      "0.3608149369855313 0.34090848590409256\n",
      "0.35177537132587344 0.33123047942282274\n",
      "0.3553894480012609 0.3204232489597318\n",
      "0.35772399362166385 0.33472875871456076\n",
      "0.3538288004468392 0.33125642383089876\n",
      "0.353193839283007 0.3331049998668749\n",
      "0.3539567588723667 0.3254143931884893\n",
      "0.35933385188369965 0.34132414264618643\n",
      "0.3575873863650115 0.33676732590612873\n",
      "0.35272755205625933 0.33542163494599425\n",
      "0.35509990123440216 0.3386252298069881\n",
      "0.35374876305576153 0.33789706589685675\n",
      "0.3534205214173113 0.33206123357654227\n",
      "0.3458154294780996 0.32255639079841025\n",
      "0.3417036178384303 0.3159655754620285\n",
      "0.34468192500939576 0.32270127759489625\n",
      "0.35561694020344015 0.32279238280363887\n",
      "0.34885748642092457 0.32698934736375745\n",
      "0.3545395010026447 0.3222297091604612\n",
      "0.3526549866694254 0.3257123836695494\n",
      "0.3539894941532744 0.32551017400248905\n",
      "0.3541739754519366 0.3265312763215858\n",
      "0.3457755459805507 0.3182101222326169\n",
      "0.351084415785406 0.32151863355326266\n",
      "0.35273285290553724 0.31611398741751695\n",
      "0.34319489318349083 0.31384135577573125\n",
      "0.3412592981490754 0.3133840061246313\n",
      "0.3476400406199371 0.3164523003679083\n",
      "0.34292017879051223 0.3074151740744333\n",
      "0.3466132578514383 0.32063736829009126\n",
      "0.3455756526838534 0.31432702383314365\n",
      "0.3455744942231759 0.3104444878999966\n",
      "0.3388700804722726 0.3128652864731301\n",
      "0.33534146108740676 0.3056720701829912\n",
      "0.3385140960252326 0.30199302056271826\n",
      "0.3458215049321121 0.31238053305989516\n",
      "0.34164216178645923 0.316849868765083\n",
      "0.3437084103069777 0.310093209888825\n",
      "0.34747041452148053 0.3180518587942597\n",
      "0.34750620413673133 0.31367004635355233\n",
      "0.346374556940623 0.3121909394471637\n",
      "0.3408005980800473 0.31379812214246067\n",
      "0.3381908544588951 0.31392200731803593\n",
      "0.34046076914712475 0.3064884976611869\n",
      "0.3343132321732499 0.3078700793545637\n",
      "0.3402994017780485 0.3079744650692366\n",
      "0.34017138002604 0.30674769543404456\n",
      "0.3402527614098041 0.3035453035452511\n",
      "0.3362705091221334 0.3004967685922409\n",
      "0.3354540600327091 0.2969238604692871\n",
      "0.33243824952170986 0.3060032583231356\n",
      "0.33661045607730805 0.30553128758120174\n",
      "0.3439819696634717 0.3086129638584098\n",
      "0.332234155291515 0.29784228339888064\n",
      "0.32933594102200353 0.2954344603344196\n",
      "0.33256729076715585 0.3018005718098651\n",
      "0.34146096259588593 0.30628204910469453\n",
      "0.34025944093441507 0.30694311769841265\n",
      "0.3440999049532107 0.3137453869215351\n",
      "0.3452766074537848 0.31323617055348346\n",
      "0.3515815552608039 0.3188209806048216\n",
      "0.34681011053447786 0.3152857652455102\n",
      "0.3448857513620525 0.32096469286710766\n",
      "0.3461100000832406 0.3178005416728942\n",
      "0.34363281321076244 0.32087001889483546\n",
      "0.34445222244006013 0.317656688981412\n",
      "0.3420263653117514 0.30902768053777857\n",
      "0.3463836998348224 0.3214961557993407\n",
      "0.3474349705516484 0.31724717652298406\n",
      "0.3433172161865264 0.31261233916663167\n",
      "0.34100692779403413 0.31301702748421545\n",
      "0.33676591023781843 0.30566331941327635\n",
      "0.3353766078282625 0.29996067248139796\n",
      "0.33535071951286194 0.3099815649426856\n",
      "0.33272913007850063 0.3079785796534752\n",
      "0.33992792427346386 0.31456079120405506\n",
      "0.3383544937530956 0.30325380434330756\n",
      "0.33428273908716094 0.30044290556670916\n",
      "0.33674414040787504 0.3093693317108321\n",
      "0.3361897868625045 0.31423058924342234\n",
      "0.34030216691940357 0.305220762614862\n",
      "0.3391615288261649 0.30905390358852186\n",
      "0.341410720193149 0.3182307228365096\n",
      "0.3414164459012868 0.3133976289248484\n",
      "0.33979345877785455 0.313071478767916\n",
      "0.3366998869258234 0.30825169064356484\n",
      "0.3413527322971849 0.30804896109440366\n",
      "0.3426750562632651 0.3085881384982293\n",
      "0.34705936824374295 0.32137279253393314\n",
      "0.3477664691654025 0.3201336868798198\n",
      "0.34290033342619725 0.3157304269203608\n",
      "0.33976893984185075 0.30480456500296305\n",
      "0.33812947070181415 0.30309785452118404\n",
      "0.33082644610533257 0.29326019119923247\n",
      "0.3354937499497779 0.30340973577412356\n",
      "0.33538000621214453 0.30634307236276415\n",
      "0.33311910990470384 0.3022444842634168\n",
      "0.3330820222389787 0.2978993825941926\n",
      "0.32614174412107216 0.29729471788150696\n",
      "0.33415145230304005 0.3124824326712874\n",
      "0.3392359909620553 0.312308233507394\n",
      "0.3369269925805121 0.3077235297601032\n",
      "0.3296286978928553 0.3016086260548654\n",
      "0.3319231958934291 0.2930116550300703\n",
      "0.33628551308339627 0.29908552079436074\n",
      "0.3360217142214855 0.3023348619446842\n",
      "0.3362337159611991 0.3055717997757313\n",
      "0.3331286049427724 0.29311054153855587\n",
      "0.32994290459319997 0.29113725352766445\n",
      "0.3296311761979941 0.3027340509833074\n",
      "0.3279820007947226 0.2943792107226548\n",
      "0.32320023046482527 0.28793029362726935\n",
      "0.3195644548753323 0.28262420310526837\n",
      "0.3204992938814316 0.281533927954898\n",
      "0.32671087916756014 0.28434263516065683\n",
      "0.3333354306229817 0.3003679196705986\n",
      "0.3318978355009887 0.3022852065414812\n",
      "0.33194210887043535 0.3003796103156067\n",
      "0.3269507245066234 0.28556179164195605\n",
      "0.32213503075324484 0.2824329860621836\n",
      "0.32329422843633643 0.29148373964057445\n",
      "0.32629555291538 0.2814321111086691\n",
      "0.3214961138450345 0.278327821348983\n",
      "0.307420366993662 0.2546289911812387\n",
      "0.3209715060651873 0.2795198889686861\n",
      "0.3222299634412172 0.29149690447606097\n",
      "0.32049511542502124 0.28440612925314424\n",
      "0.32211768359428505 0.2900926456509141\n",
      "0.3226184893477332 0.28692554034176004\n",
      "0.32901241751153315 0.28527271419377953\n",
      "0.3164369683603464 0.27574602185671887\n",
      "0.316990406938611 0.27314135931167366\n",
      "0.31111377981301375 0.2637880377559759\n",
      "0.3207190622561732 0.28338385928088894\n",
      "0.3231080591987605 0.2922058527454864\n",
      "0.3257502642292306 0.28015451077976516\n",
      "0.32585354986841714 0.2836202453055843\n",
      "0.3234772589416358 0.2844728100198283\n",
      "0.3234433430358589 0.2867143552175504\n",
      "0.32008175460438004 0.2813154206365544\n",
      "0.3191144823821138 0.2763735776015741\n",
      "0.3257822155827024 0.2795537664491392\n",
      "0.3205191767530829 0.285899397788947\n",
      "0.3097641692036034 0.27304286218925855\n",
      "0.31797692621757323 0.27281640954598607\n",
      "0.3202037556364558 0.2787557967727565\n",
      "0.32806819027548684 0.2856251587830864\n",
      "0.3260502077730321 0.28815900123683835\n",
      "0.3272825737160613 0.29219371162773744\n",
      "0.32680587113705206 0.2931331899136106\n",
      "0.33250871683363903 0.3000348443469774\n",
      "0.3339817724596551 0.29430014102457946\n",
      "0.3290780556070474 0.2883982807889019\n",
      "0.32969300973197113 0.29159952714656506\n",
      "0.3369547854530451 0.3004081360436874\n",
      "0.3340085994598965 0.30499185943801393\n",
      "0.3350660176248053 0.3010039143289617\n",
      "0.33128796196512605 0.30163576324466823\n",
      "0.33663030155759005 0.30575046710028697\n",
      "0.3374250731905084 0.3054386592322005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33707538759461064 0.3080571060138567\n",
      "0.3373544001364402 0.3054811239668528\n",
      "0.3472332974756901 0.31287117824737354\n",
      "0.34372957390405356 0.3059914735481026\n",
      "0.34197854955273715 0.304256138079006\n",
      "0.34006787290179236 0.30950239282541914\n",
      "0.33793493633421123 0.30695759526130745\n",
      "0.34134021257182734 0.3114702139333474\n",
      "0.3384619437979058 0.30425997500793883\n",
      "0.34015267530434096 0.30306300375198925\n",
      "0.3348544398308365 0.2987935859558781\n",
      "0.3302580922708278 0.2947009498141879\n",
      "0.3362995503925505 0.31032850406730933\n",
      "0.33567408307789437 0.3044808005149989\n",
      "0.3298691003933496 0.29676139797032824\n",
      "0.329774159160135 0.2943984699107677\n",
      "0.33090005167496084 0.3003711977418332\n",
      "0.3278250975630413 0.2990197016948478\n",
      "0.3299295819484131 0.28834772849971496\n",
      "0.3253622171021529 0.2894181469022424\n",
      "0.31703503367970887 0.27210689305186986\n",
      "0.3297974515715203 0.29634350223727235\n",
      "0.32773433679575503 0.283825158639731\n",
      "0.32021644265536303 0.2803602132873959\n",
      "0.317503679314819 0.27171802094959574\n",
      "0.31910337548930967 0.28318994147131876\n",
      "0.32227570328972555 0.28103845227055124\n",
      "0.3129196713723198 0.2752327356951085\n",
      "0.3099042702464316 0.26606078726143706\n",
      "0.3000835104541042 0.2476300873097471\n",
      "0.30227160353344146 0.24861022286907505\n",
      "0.30732656059057917 0.2664666924015688\n",
      "0.31552427342218 0.2753457596844729\n",
      "0.31670480457404626 0.2761264437254482\n",
      "0.3201231665991978 0.27860286081752256\n",
      "0.31832452980374326 0.2710748023163282\n",
      "0.3095470582325957 0.27040351156575065\n",
      "0.3077340458444773 0.260992809437859\n",
      "0.31158644348022213 0.2647002631245786\n",
      "0.3176991443445705 0.2647048096202777\n",
      "0.31643354263700085 0.26771846982329206\n",
      "0.3117447427767991 0.26521059535306185\n",
      "0.3128282793471675 0.26170278335210706\n",
      "0.3134892704850255 0.26667519967452397\n",
      "0.31108110046316917 0.26995125984681817\n",
      "0.31998258062269036 0.2695662643260348\n",
      "0.3147615935729327 0.2667940236732553\n",
      "0.31540153390623027 0.27259419886606917\n",
      "0.3202809760343799 0.27710931140160583\n",
      "0.31739275919311644 0.27269467532316116\n",
      "0.3137662516124309 0.27114371576635365\n",
      "0.31746003169151304 0.268151412059579\n",
      "0.3157244529828495 0.27047672425982233\n",
      "0.3173121310111543 0.2704077732365265\n",
      "0.31244252349010165 0.2590071172272562\n",
      "0.3055540797490674 0.24838866563182468\n",
      "0.3057077407975386 0.24536755092069437\n",
      "0.3049966015906203 0.25728534715444396\n",
      "0.30954416571335064 0.26524260535337246\n",
      "0.3067777458236756 0.2645118070261568\n",
      "0.3041954241241919 0.2563718934880309\n",
      "0.29945090995380275 0.2568730633098268\n",
      "0.3063758439256583 0.2556463977713313\n",
      "0.30775851022596146 0.25729308964062914\n",
      "0.3076479673178624 0.262146400102107\n",
      "0.3074587760037973 0.2603445836338412\n",
      "0.3014340922552907 0.25671265776491464\n",
      "0.3051799826208228 0.2498532555817731\n",
      "0.3066900137612357 0.25842980237845614\n",
      "0.3160241578996738 0.272571611802282\n",
      "0.31944598793836454 0.2777828915104433\n",
      "0.3237581600026979 0.2773972382206521\n",
      "0.3253205244808739 0.2904382787819735\n",
      "0.328164866133736 0.29052918579250353\n",
      "0.3335068505084254 0.30107354259971547\n",
      "0.3353402191210226 0.3090291171436211\n",
      "0.3315934142224391 0.3048041097046242\n",
      "0.32806252211534864 0.29837933605197414\n",
      "0.3271536330706475 0.2876006211864715\n",
      "0.3273118401381818 0.28262236583716027\n",
      "0.3284258620861708 0.293112842564765\n",
      "0.32877865290900776 0.30002952052654686\n",
      "0.33868784741782815 0.30249005500080967\n",
      "0.32900176596224956 0.297869517715474\n",
      "0.3324398102235775 0.2992421341107988\n",
      "0.33048527773155495 0.2978219280500618\n",
      "0.3236376955992245 0.2834587975606772\n",
      "0.3247661376616708 0.29103780938279133\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set_0.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set_0[:,i:i+1]\n",
    "        Q, Alpha_Q = wake_forward_nobias_zero(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_delta_nobias_zero(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        P, Alpha_P = sleep_forward_nobias_zero(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_delta_nobias_zero(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to examine the distribution of generation. We let the model generate, say 10000 instances, then use the samples to approximate its generative distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate_nobias_zero(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reordered_set, we set the first 256 columns as the well-formed set and the rest as negative samples outside of well-formed bound. Now we calculate the distribution of generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set_0.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set_0[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 257,  282,  284,  319,  321,  328,  349,  370,  416,  423,  436,\n",
       "        458,  469,  474,  485,  488,  489,  506,  509,  516,  538,  553,\n",
       "        573,  598,  610,  614,  618,  630,  637,  639,  664,  667,  674,\n",
       "        680,  714,  728,  743,  772,  775,  776,  777,  778,  780,  781,\n",
       "        782,  783,  784,  786,  787,  788,  792,  793,  794,  795,  796,\n",
       "        797,  798,  800,  802,  804,  806,  807,  808,  809,  810,  811,\n",
       "        812,  813,  814,  816,  818,  820,  821,  822,  824,  825,  826,\n",
       "        827,  828,  829,  830,  831,  832,  834,  836,  838,  840,  841,\n",
       "        842,  844,  846,  848,  849,  850,  852,  853,  854,  855,  856,\n",
       "        858,  859,  860,  861,  862,  863,  864,  865,  866,  867,  868,\n",
       "        870,  872,  873,  874,  875,  876,  877,  878,  880,  882,  884,\n",
       "        885,  886,  887,  888,  890,  892,  893,  896,  898,  899,  900,\n",
       "        901,  902,  903,  904,  905,  906,  907,  908,  909,  910,  911,\n",
       "        912,  913,  914,  915,  916,  917,  918,  919,  920,  921,  922,\n",
       "        923,  924,  925,  926,  927,  928,  929,  930,  931,  932,  933,\n",
       "        934,  935,  936,  939,  940,  941,  942,  943,  944,  945,  946,\n",
       "        947,  948,  949,  950,  951,  952,  953,  954,  955,  956,  957,\n",
       "        958,  959,  960,  961,  962,  963,  964,  965,  966,  967,  968,\n",
       "        969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
       "        980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,\n",
       "        991,  993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002,\n",
       "       1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013,\n",
       "       1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[values >= 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = np.zeros((2,),dtype = int)\n",
    "k = 0\n",
    "for i in range(values[values < 256].size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        FN[k] = values[i]+j\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 99, 102])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7932"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values < 256].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 254)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([values[values < 256], counts[values < 256]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.array([values, counts])\n",
    "for i in range(values.size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        statistics = np.append(statistics, np.array([[values[i]+j],[0]]),axis = 1)\n",
    "statistics = np.unique(statistics,axis = 1)\n",
    "statistics[0,0:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_d, counts_d  = np.unique(well_formed_set_0, axis=1, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmholtz Machine({-1,1}, No bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"zero\"\n",
    "Phi, Theta = parameter_initialization_nobias(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21144180894898457 0.10851161797378958\n",
      "0.21132218444951734 0.11041370360960563\n",
      "0.20963734927898703 0.10508960687117773\n",
      "0.20856072818619567 0.1093300148223108\n",
      "0.20664167159803032 0.10361713764264813\n",
      "0.20549348340252216 0.1040275821708045\n",
      "0.20454381159117738 0.10088668441397694\n",
      "0.20301456959811318 0.10111009914665102\n",
      "0.2057424674733209 0.10499410642121869\n",
      "0.20829631805675475 0.0992223926358681\n",
      "0.21175743834841926 0.10986235749122338\n",
      "0.20597512970673817 0.10246561574911481\n",
      "0.21062967646260233 0.10723418483124735\n",
      "0.2064784579142833 0.10371673551099105\n",
      "0.2088966437906742 0.11557368297022494\n",
      "0.2055922432720056 0.10605810485977038\n",
      "0.2094548804890874 0.10883617525145055\n",
      "0.2106084088470429 0.10045104638073768\n",
      "0.19950855691556468 0.09204157784967124\n",
      "0.19903568999728843 0.09744932705645219\n",
      "0.19998074742742417 0.09531109726650068\n",
      "0.20522118613456766 0.10252656717032972\n",
      "0.21103229824324435 0.10313281105960694\n",
      "0.20816947030655594 0.10450733581686715\n",
      "0.204332590919339 0.10110490224182393\n",
      "0.20029852056864295 0.0987032789102787\n",
      "0.20120450571979498 0.09612892323227638\n",
      "0.19681070166266362 0.09926249476727632\n",
      "0.1981314818187533 0.09419377905221804\n",
      "0.20431696091967186 0.1008732498961662\n",
      "0.20221979636996892 0.0983192095652139\n",
      "0.19927553597232195 0.09446508526897573\n",
      "0.20008817146076105 0.09872154961842479\n",
      "0.20065994352856367 0.09540925439609517\n",
      "0.20297757844157818 0.09989391159197662\n",
      "0.20482700910757637 0.09651918960507151\n",
      "0.20281476053496222 0.0982230535442292\n",
      "0.20295492010125832 0.10639795193824993\n",
      "0.207741998483687 0.0964006053328509\n",
      "0.20777880360387924 0.10038839522551811\n",
      "0.20863220113082495 0.10416484552479673\n",
      "0.20819821609225636 0.1044683149305855\n",
      "0.20465809205265256 0.1040353196527838\n",
      "0.20368520248510932 0.09787876517467828\n",
      "0.20392934802516113 0.09788659762469244\n",
      "0.20626450102577 0.10562642096379411\n",
      "0.20576096285607537 0.1050830796027359\n",
      "0.21237325316384634 0.11163829525443578\n",
      "0.21143820755726497 0.10175219131390281\n",
      "0.2073074974994169 0.10085987055047979\n",
      "0.20961990948265455 0.107565789498279\n",
      "0.20812531577886867 0.10099616973652592\n",
      "0.2059309769519683 0.10202736144748274\n",
      "0.20509540270534196 0.09882160377792967\n",
      "0.2054370776127119 0.10622796712800427\n",
      "0.2108134919611134 0.09684570284019663\n",
      "0.20590893114524417 0.10214058192305357\n",
      "0.20038298640214236 0.09648404411684067\n",
      "0.2065545469594104 0.09755877455196404\n",
      "0.20922556540026005 0.08438609010746667\n",
      "0.2074749043797221 0.10061508353825724\n",
      "0.20419277499987654 0.1001214102887306\n",
      "0.2059589191123021 0.10158817364073089\n",
      "0.20815869600086204 0.10473825468918983\n",
      "0.20777218650519358 0.10343261115592746\n",
      "0.20812703951708086 0.10060949754101083\n",
      "0.20862438915558926 0.09293209690816517\n",
      "0.207974323766061 0.10073007031089952\n",
      "0.2067618231837915 0.10120830888292577\n",
      "0.21142234745920735 0.10795603094730798\n",
      "0.2092228938321767 0.10980624396717528\n",
      "0.2045645051388147 0.10040834756969684\n",
      "0.20891905257469437 0.11518194431972092\n",
      "0.2114884346423014 0.10613268390272045\n",
      "0.20779635883920547 0.10200013602550444\n",
      "0.21014676293531367 0.10204189098727576\n",
      "0.2127489234717564 0.10355271474009763\n",
      "0.21008389322089457 0.10016889017193584\n",
      "0.2078664285534818 0.09977805735276918\n",
      "0.20199355981042239 0.09673347955424677\n",
      "0.20282914328326535 0.09795138510195252\n",
      "0.20483351610020478 0.0970185739538325\n",
      "0.20040715564561554 0.08856809767640171\n",
      "0.1927740959088742 0.08483789219609962\n",
      "0.19588497286266215 0.08801847220090572\n",
      "0.1978977543979014 0.08588049506507925\n",
      "0.2017354941362091 0.08991523911713405\n",
      "0.20034994446129975 0.08976529594132936\n",
      "0.1975296039866653 0.09127257012576842\n",
      "0.197273763816651 0.08680589500190103\n",
      "0.1978348456175804 0.08405538267326836\n",
      "0.20007815066608564 0.08631024735373857\n",
      "0.19704877348037927 0.08690743123918153\n",
      "0.199301534260022 0.09646147731503975\n",
      "0.20087150398658812 0.09642003909844261\n",
      "0.20336860530799897 0.09162248011126853\n",
      "0.1997082384674988 0.09280814608615663\n",
      "0.20394994338688724 0.09409643397129762\n",
      "0.2010464421623231 0.09123498968149332\n",
      "0.20517873317677376 0.09221174718168643\n",
      "0.20311402327753153 0.08876684055314177\n",
      "0.20509559610375558 0.09332388893539673\n",
      "0.20531027622678943 0.08753326439751102\n",
      "0.19989339551109728 0.09109615864354986\n",
      "0.2086401943737117 0.09401262524364615\n",
      "0.20725519179313703 0.09442624910405162\n",
      "0.2061391802878648 0.0968222849283479\n",
      "0.20155111932943265 0.09967529664393208\n",
      "0.20656026626580412 0.09308009662451994\n",
      "0.20666937216206369 0.0914301243825223\n",
      "0.20281810145847692 0.10034907183641538\n",
      "0.20711484371637817 0.09890565640765585\n",
      "0.20633673960711116 0.09760977992810327\n",
      "0.20984081081223438 0.10574142292945898\n",
      "0.2038734266164497 0.09725655801780476\n",
      "0.20525674327362906 0.09243770308642295\n",
      "0.20634054175215794 0.09386888262799403\n",
      "0.20245983602531134 0.09777275192566953\n",
      "0.2042211625211425 0.09289852774065396\n",
      "0.20863343991441247 0.09639162711724719\n",
      "0.20547732997488485 0.10179293172173022\n",
      "0.20115419346827562 0.09790317480714844\n",
      "0.20154489407658155 0.09239442296589921\n",
      "0.20006439278610802 0.0956985887838047\n",
      "0.20575039582315288 0.0984152431943588\n",
      "0.20092835638351791 0.09153629676189463\n",
      "0.2051849330745545 0.1016192141277959\n",
      "0.20748474557154065 0.10206324846872654\n",
      "0.2047135588575543 0.09560792348012374\n",
      "0.20575704739341255 0.09829321998625087\n",
      "0.2054711177868189 0.09376808558747693\n",
      "0.20328561158113467 0.09392216227663362\n",
      "0.2088635529470606 0.10152289671664348\n",
      "0.20713145790878787 0.09950753382679708\n",
      "0.20632607773660588 0.10576472376717057\n",
      "0.20455397614337684 0.09977265865199109\n",
      "0.20890118563234977 0.10367158228567154\n",
      "0.20783536078578185 0.10820602553606815\n",
      "0.2101970502318705 0.10924013539026839\n",
      "0.2065552126109065 0.10255622876094235\n",
      "0.20550439926887376 0.1046214687637445\n",
      "0.21462330758953374 0.10926123477888786\n",
      "0.21212727448499527 0.11014194810003321\n",
      "0.21646140055458887 0.10820197188799992\n",
      "0.2129575014949848 0.11412054644112823\n",
      "0.22193818314436914 0.11643310210245766\n",
      "0.21776886926554156 0.11042885309759749\n",
      "0.21655328990125308 0.11201922817732674\n",
      "0.2128859293125195 0.10452778276595363\n",
      "0.21666327195961968 0.10805041913112018\n",
      "0.21188791066399232 0.10397002938049592\n",
      "0.21227817698982346 0.10539950153687118\n",
      "0.21230925411884763 0.11067070900441658\n",
      "0.21767818371899397 0.10706831075970667\n",
      "0.21239067044488547 0.10703468688827876\n",
      "0.21158386746391275 0.10266445786210261\n",
      "0.21466616169308186 0.10813249583166412\n",
      "0.21712202759240393 0.11461814456878237\n",
      "0.21536666602856253 0.10857855218604619\n",
      "0.21727207626143996 0.11379437762836041\n",
      "0.21292230973135842 0.11106629744546712\n",
      "0.21401016136021833 0.10603803296509494\n",
      "0.2115482639462749 0.1028979689277683\n",
      "0.2109569445285086 0.10205311240646496\n",
      "0.21110675099242596 0.10583773833937396\n",
      "0.20951673489956907 0.10453310788623103\n",
      "0.20721438906693546 0.10239800439341042\n",
      "0.2176173903014362 0.10406295311005129\n",
      "0.21622945109000938 0.10986824688552885\n",
      "0.22233574419933505 0.11131789894772609\n",
      "0.21301395785990784 0.10506333752733663\n",
      "0.21343998710624437 0.10166820526937499\n",
      "0.21248496714691414 0.10876746704976649\n",
      "0.21656338526091368 0.11002854294480538\n",
      "0.21453177631024592 0.11002152795854918\n",
      "0.21520317204878678 0.10399165161505555\n",
      "0.21188881150562364 0.10321452827407886\n",
      "0.2137801068984983 0.10839723872070613\n",
      "0.21753054661427612 0.10492626437394609\n",
      "0.21432827165065574 0.11239958426964156\n",
      "0.2199476073048086 0.10598592542736113\n",
      "0.2140552581144834 0.10884566625354954\n",
      "0.21703650245242964 0.10481974278152571\n",
      "0.22070399018535564 0.1140046389848533\n",
      "0.21897743637669556 0.10688305204541013\n",
      "0.22166217064827573 0.12152503313687989\n",
      "0.22079730601967135 0.11318484199614573\n",
      "0.22356917998390546 0.11759472157233597\n",
      "0.22632666085492653 0.11796559489710332\n",
      "0.2229063996059238 0.12191801908119711\n",
      "0.22359285108644297 0.125316133860085\n",
      "0.2255375572372265 0.12369073799734026\n",
      "0.22334743834830706 0.11822778473726346\n",
      "0.22846262256138314 0.12600821887357785\n",
      "0.23046677466765678 0.1330002165198652\n",
      "0.23525812400239116 0.13346721043114976\n",
      "0.23291286529605565 0.1287415541985371\n",
      "0.23172025332406201 0.13174413116745204\n",
      "0.23510618640156564 0.1318152520436121\n",
      "0.2354429660701038 0.13096047478201506\n",
      "0.22984819676725526 0.12838266810655363\n",
      "0.23376446757080568 0.12622530463650958\n",
      "0.23574206793790703 0.13737721224122706\n",
      "0.23771290119430413 0.13632925578742344\n",
      "0.23432688575397695 0.14653733094205199\n",
      "0.23558337858058293 0.1397264735723651\n",
      "0.2412950738051219 0.14614499483561483\n",
      "0.24149912513112057 0.14713545364325004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24547984192430858 0.14750785431393817\n",
      "0.24321753268638144 0.14762216016268476\n",
      "0.23827044330216365 0.14833077671257971\n",
      "0.24266973008906262 0.14798305590109395\n",
      "0.23863733981729648 0.14059427466118946\n",
      "0.2354465791118978 0.13351694232326441\n",
      "0.23477840685041332 0.13220651349190254\n",
      "0.2321246991513295 0.1275829735990318\n",
      "0.23386504624866564 0.13573166579737736\n",
      "0.2330392827642621 0.1339235984314507\n",
      "0.228114456391656 0.12689569382962648\n",
      "0.23167132273762653 0.1276749452608936\n",
      "0.23027968989058253 0.13003043664496486\n",
      "0.23054294057172983 0.13071580833944946\n",
      "0.2317638417607181 0.1313039199374044\n",
      "0.2294066534639206 0.1267765355827092\n",
      "0.23411498623989943 0.13405602146254336\n",
      "0.23492040041151505 0.13647693752045947\n",
      "0.22973447244123976 0.12372331194653056\n",
      "0.22842678937101465 0.12575980205006224\n",
      "0.23106625239919462 0.121067961627299\n",
      "0.2292695650130827 0.11629029196904883\n",
      "0.22840783217151467 0.12190065553299811\n",
      "0.22619203164494145 0.11700679890544542\n",
      "0.22843372803007989 0.12341597215498575\n",
      "0.22389611195570516 0.12549334910341886\n",
      "0.2250617893344204 0.12407782497578894\n",
      "0.22692618077944232 0.11729886319437792\n",
      "0.22481798600280947 0.11776057646736307\n",
      "0.226375093003841 0.12054989841200789\n",
      "0.2222605192237221 0.11087767322287326\n",
      "0.22258336803746664 0.11679834832507158\n",
      "0.22360860979025146 0.12374496265161128\n",
      "0.22428085683220045 0.12537276936192385\n",
      "0.22357302553530484 0.12110375605132827\n",
      "0.22298910311226844 0.11866609012163924\n",
      "0.22281641948021103 0.11994729343350467\n",
      "0.2277542297660022 0.13151791592909295\n",
      "0.22974347525414354 0.12797487879982014\n",
      "0.23056795707490282 0.1339320200655893\n",
      "0.22900063052737946 0.12292049814117888\n",
      "0.22894729115999954 0.12748493693409108\n",
      "0.23275865837130408 0.12612668310463537\n",
      "0.23196100883072676 0.13208024217260708\n",
      "0.2358435877551556 0.12738922227070018\n",
      "0.2346178798899157 0.12529941751327592\n",
      "0.22772380682565307 0.12472044802659588\n",
      "0.2277022987845924 0.12287847261296489\n",
      "0.2252538402690935 0.12381534097346411\n",
      "0.22521477440819682 0.12545808341871337\n",
      "0.22554628482328815 0.12346161863555871\n",
      "0.2246996542210361 0.12031950612539959\n",
      "0.22650050557329254 0.1226296172473653\n",
      "0.22334789027618496 0.11440477409541225\n",
      "0.22639948083527442 0.11479830884787436\n",
      "0.2213218058771708 0.10649506767892694\n",
      "0.2207001257652005 0.11178327248783156\n",
      "0.223525926213152 0.11821467302467639\n",
      "0.22459380046580166 0.11444648378646861\n",
      "0.22645585810687702 0.11649951786089274\n",
      "0.22149525615233726 0.1177312979768022\n",
      "0.22140793350710639 0.11561312597144338\n",
      "0.22222304114949568 0.11437025574643556\n",
      "0.22522734072235892 0.11264078204931624\n",
      "0.2232205783367169 0.111342686031988\n",
      "0.22701068995609697 0.11722644783185539\n",
      "0.22181482002057099 0.11123617506658057\n",
      "0.22475924539854458 0.11592841054329608\n",
      "0.22341186692441767 0.1140183548125168\n",
      "0.22017303412001776 0.11430234779783144\n",
      "0.2242532652181008 0.11119127478889033\n",
      "0.22183610253587985 0.11247932443999144\n",
      "0.22319999540402524 0.115178146953242\n",
      "0.21996750662540615 0.11164362981182162\n",
      "0.2182462664869916 0.11602159982047627\n",
      "0.22702569869253267 0.11928922972822162\n",
      "0.22557666154525283 0.12416490254705086\n",
      "0.22093456202964715 0.11940443712890847\n",
      "0.23021738561660385 0.127705284683322\n",
      "0.2306111977992058 0.12583585416739188\n",
      "0.23108464952041474 0.12409868650158347\n",
      "0.22662500957299667 0.12035369980543471\n",
      "0.23046769032000594 0.12764216704606207\n",
      "0.22818994069088452 0.12021239993010453\n",
      "0.23297035879791891 0.12175658660845677\n",
      "0.22945067833768212 0.1158162049734556\n",
      "0.22876669151241197 0.12191883909138888\n",
      "0.23162235520105928 0.12230768983965697\n",
      "0.22722965502907985 0.12098186762221749\n",
      "0.22622485459024222 0.11633639984910338\n",
      "0.22616235054373043 0.12486212558678399\n",
      "0.23026351847562782 0.12361023296293502\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set[:,i:i+1]\n",
    "        Q, Alpha_Q = wake_forward_nobias(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_delta_nobias(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        P, Alpha_P = sleep_forward_nobias(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_delta_nobias(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate_nobias(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCEAAAI/CAYAAABJfsMvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3df7Dld13f8dfbBH/bBszC0PyYCzRSkZGAOxlaikWwNVDHQC1KxmJKsSsz0KK1U1c7U6wdZ2wrYh0tTpCU0MEA5YfQLlozkUodC7qBGIIBCYiwkCYrQaDFwSa8+8f9rtwsd7N37znnc37cx2Pmzp7zvd9zzuf8vvs8n+/3VHcHAAAAYNG+bNkDAAAAAA4GEQIAAAAYQoQAAAAAhhAhAAAAgCFECAAAAGAIEQIAAAAY4vxlDyBJLrzwwt7a2lr2MAAAAIDT3HzzzX/S3YfmcV4rESG2trZy/PjxZQ8DAAAAOE1V/fG8zsvmGAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEMDCbB09tuwhAAAAK0SEAAAAAIYQIQAAAIAhRAgAAABgCBECAAAAGEKEAAAAAIY4a4Soqkuq6u1VdXtVva+qXjwtf0hV3VhVH5z+ffC0vKrq56vqjqq6taqesOgrAQAAAKy+vcyEuDfJj3T3NyZ5YpIXVtVjkhxNclN3X5bkpul4kjw9yWXTz5EkL5/7qAEAAIC1c9YI0d13dve7p8OfTXJ7kouSXJXk+mm165M8czp8VZJX97Z3Jrmgqh4+95EDAAAAa+Wc9glRVVtJHp/kXUke1t13JtuhIslDp9UuSvKxHSc7MS0DAAAADrA9R4iq+tokb0zyQ939mQdadZdlvcv5Hamq41V1/OTJk3sdBgAAALCm9hQhqupB2Q4Qr+nuN02L7zq1mcX0793T8hNJLtlx8ouTfOL08+zua7v7cHcfPnTo0H7HDwAAAKyJvXw7RiV5ZZLbu/tnd/zqrUmumQ5fk+QtO5Z///QtGU9M8ulTm20AAAAAB9f5e1jnSUmem+S9VXXLtOzHk/x0ktdX1fOTfDTJs6ffvS3JM5LckeRzSZ431xEDAAAAa+msEaK7fzu77+chSZ62y/qd5IUzjgsAAADYMOf07RgAAAAA+yVCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQZ40QVXVdVd1dVbftWPa6qrpl+vlIVd0yLd+qqj/b8btfWuTgAQAAgPVx/h7WeVWSX0jy6lMLuvt7Tx2uqpcm+fSO9T/U3ZfPa4AAAADAZjhrhOjud1TV1m6/q6pK8j1JnjrfYQEAAACbZtZ9Qjw5yV3d/cEdyx5RVe+pqt+qqifPeP4AAADAhtjL5hgP5OokN+w4fmeSS7v7k1X1LUl+taq+qbs/c/oJq+pIkiNJcumll844DAAAAGDV7XsmRFWdn+TvJXndqWXd/fnu/uR0+OYkH0ryDbudvruv7e7D3X340KFD+x0GAAAAsCZm2Rzj25O8v7tPnFpQVYeq6rzp8COTXJbkw7MNEQAAANgEe/mKzhuS/K8kj66qE1X1/OlXz8n9N8VIkm9NcmtV/X6SNyR5QXffM88BAwAAAOtpL9+OcfUZlv/DXZa9MckbZx8WAAAAsGlm/XYMAAAAgD0RIQAAAIAhRAgAAABgCBECAAAAGEKEAAAAAIYQIYC52Tp6bNlDAAAAVpgIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBDAwm0dPbbsIQAAACtAhAAAAACGECEAAACAIUQIAAAAYAgRAgAAABhChAAAAACGECEAAACAIUQIAAAAYAgRAgAAABhChAAAAACGECEAAACAIUQIAAAAYAgRAgAAABhChAAAAACGOGuEqKrrquruqrptx7KfqKqPV9Ut088zdvzux6rqjqr6QFV9x6IGDgAAAKyXvcyEeFWSK3dZ/rLuvnz6eVuSVNVjkjwnyTdNp/mPVXXevAYLAAAArK+zRojufkeSe/Z4flcleW13f767/yjJHUmumGF8AAAAwIaYZZ8QL6qqW6fNNR48Lbsoycd2rHNiWgYAAAAccPuNEC9P8qgklye5M8lLp+W1y7q92xlU1ZGqOl5Vx0+ePLnPYQAAAADrYl8Rorvv6u77uvsLSV6RL25ycSLJJTtWvTjJJ85wHtd29+HuPnzo0KH9DAMAAABYI/uKEFX18B1Hn5Xk1DdnvDXJc6rqK6rqEUkuS/K7sw0RAAAA2ATnn22FqrohyVOSXFhVJ5K8JMlTqurybG9q8ZEkP5gk3f2+qnp9kj9Icm+SF3b3fYsZOgAAALBOzhohuvvqXRa/8gHW/6kkPzXLoAAAAIDNM8u3YwAAAADsmQgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBzNXW0WPZOnps2cMAAABWkAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgSska2jx5Y9hDNa5bEBAACrQYQAAAAAhhAhAAAAgCFECAAAAGAIEQIAAAAYQoQAAAAAhhAhAAAAgCFECAAAAGAIEQIAAAAYQoQAAAAAhhAhAAAAgCFECAAAAGAIEQIAAAAYQoQAAAAAhjhrhKiq66rq7qq6bceyf19V76+qW6vqzVV1wbR8q6r+rKpumX5+aZGDBwAAANbHXmZCvCrJlactuzHJY7v7m5P8YZIf2/G7D3X35dPPC+YzTAAAAGDdnTVCdPc7ktxz2rLf6O57p6PvTHLxAsYGAAAAbJB57BPiHyX5tR3HH1FV76mq36qqJ8/h/AEAAIANcP4sJ66qf5nk3iSvmRbdmeTS7v5kVX1Lkl+tqm/q7s/sctojSY4kyaWXXjrLMAAAAIA1sO+ZEFV1TZLvTPJ93d1J0t2f7+5PTodvTvKhJN+w2+m7+9ruPtzdhw8dOrTfYQAAAABrYl8RoqquTPKjSb6ruz+3Y/mhqjpvOvzIJJcl+fA8BgoAAACst7NujlFVNyR5SpILq+pEkpdk+9swviLJjVWVJO+cvgnjW5P8ZFXdm+S+JC/o7nt2PWMAAADgQDlrhOjuq3dZ/MozrPvGJG+cdVAAAADA5pnHt2MAAAAAnJUIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBDAzLaOHlv2EAAAgDUgQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgALYWeVAADA6UQIAAAAYAgRAgAAABhChAAAAACGECEAAACAIUQIYN/sfBIAADgXIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgSwUHZeCQAAnCJCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEMAQW0ePLXsIAADAkokQAAAAwBAiBAAAADCECAEksbkEAACweCIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAyxpwhRVddV1d1VdduOZQ+pqhur6oPTvw+elldV/XxV3VFVt1bVExY1eAAAAGB97HUmxKuSXHnasqNJburuy5LcNB1PkqcnuWz6OZLk5bMPE1ikraPHlj0EAADgANhThOjudyS557TFVyW5fjp8fZJn7lj+6t72ziQXVNXD5zFYAAAAYH3Nsk+Ih3X3nUky/fvQaflFST62Y70T0zIAAADgAFvEjilrl2X9JStVHamq41V1/OTJkwsYBgAAALBKZokQd53azGL69+5p+Ykkl+xY7+Iknzj9xN19bXcf7u7Dhw4dmmEYAAAAwDqYJUK8Nck10+Frkrxlx/Lvn74l44lJPn1qsw0AAADg4Dp/LytV1Q1JnpLkwqo6keQlSX46yeur6vlJPprk2dPqb0vyjCR3JPlckufNecwAAADAGtpThOjuq8/wq6ftsm4neeEsgwIAAAA2zyJ2TAkAAADwJUQIAAAAYAgRAgAAABhChAAAAACGECEAAACAIUQIOMC2jh5b9hAAAIADRIQAAAAAhhAhAAAAgCFECAAAAGAIEQIAAAAYQoSAFWfnkQAAwKYQIQAAAIAhRAgAAABgCBECDhCbdgAAAMskQgAAAABDiBAAAADAECIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIErDBfqQkAAGwSEQIAAAAYQoQAAAAAhhAh4ICzyQcAADCKCAEAAAAMIUIAAAAAQ4gQAAAAwBAiBAAAADCECAFrYB47jzz9POyQEgAAGE2EAAAAAIYQIQAAAIAhRAg4gGyKAQAALIMIAQAAAAwhQsAKMUMBAADYZCIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIErBg7pwQAADaVCAEAAAAMIUIAAAAAQ4gQAAAAwBAiBAAAADCECAEAAAAMIUIAAAAAQ4gQAAAAwBAiBAAAADCECAEAAAAMIULAmtk6emzZQwAAANgXEQIAAAAYQoQAAAAAhhAhgJnYPAQAANgrEQIAAAAYQoQAAAAAhjh/vyesqkcned2ORY9M8q+SXJDkHyc5OS3/8e5+275HCAAAAGyEfUeI7v5AksuTpKrOS/LxJG9O8rwkL+vun5nLCAEAAICNMK/NMZ6W5EPd/cdzOj8AAABgw8wrQjwnyQ07jr+oqm6tquuq6sFzugwAAABgjc0cIarqy5N8V5L/Mi16eZJHZXtTjTuTvPQMpztSVcer6vjJkyd3WwUAAADYIPOYCfH0JO/u7ruSpLvv6u77uvsLSV6R5IrdTtTd13b34e4+fOjQoTkMAwAAAFhl84gQV2fHphhV9fAdv3tWktvmcBkAAADAmtv3t2MkSVV9dZK/neQHdyz+d1V1eZJO8pHTfgcswdbRY8seAgAAwGwRors/l+TrT1v23JlGBAAAAGykeX07BgAAAMADEiEAAACAIUQIAAAAYAgRAgAAABhChAAAAACGECEAAACAIUQIAAAAYAgRAgAAABhChIANt3X02LKHAAAAkESEAAAAAAYRIQAAAIAhRAhYQTahAAAANpEIAQAAAAwhQgAAAABDiBAAAADAECIEAAAAMIQIAQeMnV4CAADLIkIAAAAAQ4gQAAAAwBAiBAAAADCECAEAAAAMIULAirIDSQAAYNOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIELBEdj4JAAAcJCIEAAAAMIQIAQAAAAwhQgAAAABDiBAAAADAECIErAk7sQQAANadCAEAAAAMIUIAAAAAQ4gQsCJsbgEAAGw6EQIAAAAYQoSAJTHzAQAAOGhECAAAAGAIEQIAAAAYQoTgwLEZBAAAwHKIEAAAAMAQIgRsILM9AACAVSRCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCwDnY9B0+bvr1AwAAlkuEAAAAAIYQIQAAAIAhRAgAAABgCBECAAAAGEKEAAAAAIYQIQAAAIAhRAgAAABgCBECAAAAGOL8Wc+gqj6S5LNJ7ktyb3cfrqqHJHldkq0kH0nyPd39qVkvCwAAAFhf85oJ8W3dfXl3H56OH01yU3dfluSm6TgAAABwgC1qc4yrklw/Hb4+yTMXdDkAAADAmphHhOgkv1FVN1fVkWnZw7r7ziSZ/n3oHC4HAAAAWGMz7xMiyZO6+xNV9dAkN1bV+/dyoilYHEmSSy+9dA7DgPW1dfTYsocAAACwcDPPhOjuT0z/3p3kzUmuSHJXVT08SaZ/797ldNd29+HuPnzo0KFZhwEAAACsuJkiRFV9TVV93anDSf5OktuSvDXJNdNq1yR5yyyXAwAAAKy/WTfHeFiSN1fVqfP6le7+9ar6vSSvr6rnJ/lokmfPeDkAAADAmpspQnT3h5M8bpfln0zytFnOGwAAANgsi/qKTgAAAID7ESEAAACAIUQIAAAAYAgRAgbaOnps2UMAAABYGhECAAAAGEKEgLNY9dkLO8d3+lhXfewAAMDBIkIAAAAAQ4gQAAAAwBAiBAAAADCECAEAAAAMIULAHtnJIwAAwGxECAAAAGAIEQIAAAAYQoSAQUZtzmGzEQAAYFWJEAAAAMAQIgQswc7ZCmYuAAAAB4UIAQAAAAwhQgAAAABDiBBsvFObO9jsAQAAYLlECAAAAGAIEQJYGWarAADAZhMhAAAAgCFECAAAAGAIEQIAAAAYQoQAAAAAhhAhAAAAgCFECAAAAGAIEQIAAAAYQoQAAAAAhhAhAAAAgCFECBhs6+ixZQ8BAABgKUQIAAAAYAgRAgAAABhChIBzZHMKAACA/REhAAAAgCFECAAAAGAIEWLN2TRgb/Z7O7l9AQAA5keEAAAAAIYQITaMT+4BAABYVSIEAAAAMIQIAQAAAAwhQmwIm2EAAACw6kQIAAAAYAgRggNtrzNITl/PzBMAAIBzJ0IAAAAAQ4gQAAAAwBAiBAAAADCECAEAAAAMIUIAAAAAQ4gQAAAAwBAiBAAAADCECMFK2Tp6bNlDWMgYlnG9VuG2BAAA2EmEAAAAAIYQIQbxqTTzdOrx5HEFAACsk31HiKq6pKreXlW3V9X7qurF0/KfqKqPV9Ut088z5jdcAAAAYF2dP8Np703yI9397qr6uiQ3V9WN0+9e1t0/M/vwAAAAgE2x7wjR3XcmuXM6/Nmquj3JRfMaGAAAALBZ5rJPiKraSvL4JO+aFr2oqm6tquuq6sHzuAwAAABgvc0cIarqa5O8MckPdfdnkrw8yaOSXJ7tmRIvPcPpjlTV8ao6fvLkyVmHsbKWueNAOy1k3XkMAwDAZpkpQlTVg7IdIF7T3W9Kku6+q7vv6+4vJHlFkit2O213X9vdh7v78KFDh2YZBgAAALAGZvl2jEryyiS3d/fP7lj+8B2rPSvJbfsfHgAAALApZpkJ8aQkz03y1NO+jvPfVdV7q+rWJN+W5IfnMdCRTAFnHXncAgAAq26Wb8f47SS1y6/etv/hAAAAAJtqLt+Owerxqfjm24T7eBOuAwAAsHciBAAAADCECAEAAAAMcSAjxLKmgJt6Pn9uU9bVbo9dj2dg0bzOALBsBzJCAAAAAOOJEAAAAMAQIsQSmRLJvHgsAQAA60CEAAAAAIYQIdhYZgesnkXfJ+5zAABYbSIEAAAAMIQIAQAAAAwhQuyBKd7r5VzvL/fvePu9zd1XAACw3kQIAAAAYAgR4jTr8knrA41zXa7DsrmdOIhW4XG/CmOAVeI5AcBBIkIAAAAAQ4gQAAAAwBAixJyYSjl/Z7pNz+W2dr+srr3eN5t8H27ydQMAgN2IEAAAAMAQIsQZrNMnlKfGuk5jhtMdpMfvKlzXVRgDAACrZ9F/J4oQAAAAwBAiBAAAADDEgY8Qy56SPMvlL3vsy7af63+20xz023QZ3OYAAHBmm/b38oGPEAAAAMAYIsRkZF3atJKVbN512rTrAwAAsApECAAAAGAIEQIAAAAYQoRYklWc7r/qO21c9uUzH/O4H5fxWJjXZXocAwCwX5vwt6QIAQAAAAxxoCPEqIq0CbVqp1PXZ52v117Gvs7Xb91tHT22kffRKo93lcfG5hn9ePP4BmBdbeJ72IGOEAAAAMA4IgQAAAAwxIGNEDuntcx7issDba6w7tNpTh//ul+fUzblemyCc70vFnXfLfsxsYjLP9N5Lvu6nm7VxgMAwPwc2AgBAAAAjLV2EcInZKtnVe6Tve7MkM2wbrNyHmh86zJDgdXhsbG+3HcA7McmvX+sXYQAAAAA1pMIAQAAAAyxshFiUTtlO9cp0ecy5XuWMS9jes3ZLnORO+88m3MZ26jLZD3M837c1OelxzoAAMuyshECAAAA2CwHJkKM/nR0lT9pnMcsg4MwM4L1t66PizPNylr0DKBl317Lvvx14DYCgM0xj/f1EX8bzPsyDkyEAAAAAJZLhAAAAACGWPkIscypp+eyU8p5Xs6ijJ52Pa/L2M/5mLK8uc51c6FNfSws4/XoINyuAADrZt3+Llv5CAEAAABsBhECAAAAGGJtIsSi9gy/X+s0LXnk5gyrdj8B5+5cn8OzPue9ZpCc2+NgnR4z6zRWgHkY/a2EIy17PKv2DYX7tTYRAgAAAFhvKxUhFlFaFl2Lll3DztXZxjvLzjgXsSPKUTsHhXN1psfmLLOIFrEuD8xtuRo25ZOdRVyOmUYAq2evr62Lnl26yNf4RZ73SkUIAAAAYHOJEAAAAMAQKxchFrFjw1WYDrks57r5xTrahOvAanmgx9Q67ZR2Fpty3Vbx/WQdL3tRznSdZp2Oeq7nu0637W5j3cv41+k6AizaOu4GYJOsXIQAAAAANtNaRIhVrUr7/TRi1azjmGGZDuJzZpbr/ECvlaN3PrsJn4TPYl2v5yyf9C9rZtNeZyKu03Vb18cPsF5WfWeLs+yM/Fz+/7gqO21exDjWIkIAAAAA60+EAAAAAIZYiQjx3o9/etlD4AHMMtXI1E32YtbHyajH2blMU1vU90evgkW8JhyE6eL7Hdtedtg8y44c97M5wDw3z3mgx8Ssmz3ud5OEeb8mnf7438/rw7ncHsva3AlgN7u9Ju3lfW0ddro7+j101rGsioVFiKq6sqo+UFV3VNXRRV0OAAAAsB4WEiGq6rwkv5jk6Ukek+TqqnrMA51mU3byuKoOwld1srnmteOfRV/2fj/tnOcYFnG+D/TJxZk+qR35mn76pyW7jfGBxrZz+enrPtC493I/73Z+83K2T5b2cr3P9unM6acfsePQWWdK7OUyznR8Htdx1hlTqz6LasTjYN3N61PPZd7Om3ofn+t7+qbeDstwttfyWf4m2evr7sjX9/2c3yo83kaNYVEzIa5Ickd3f7i7/zzJa5NctaDLAgAAANbAoiLERUk+tuP4iWkZAAAAcEBVd8//TKueneQ7uvsHpuPPTXJFd/+THescSXJkOvrYJLfNfSDAqrkwyZ8sexCsHY8b+CLPB7g/zwkY49Hd/XXzOKPz53EmuziR5JIdxy9O8omdK3T3tUmuTZKqOt7dhxc0FmBFeK6zHx438EWeD3B/nhMwRlUdn9d5LWpzjN9LcllVPaKqvjzJc5K8dUGXBQAAAKyBhcyE6O57q+pFSf57kvOSXNfd71vEZQEAAADrYVGbY6S735bkbXtc/dpFjQNYKZ7r7IfHDXyR5wPcn+cEjDG359pCdkwJAAAAcLpF7RMCAAAA4H6WHiGq6sqq+kBV3VFVR5c9HmB/quqSqnp7Vd1eVe+rqhdPy3+iqj5eVbdMP8+Ylm9V1Z/tWP5Ly70GLFNVnVdV76mq/zYdf0RVvauqPlhVr5t2cpyq+orp+B3T77eWOW5YhKq6oKreUFXvn15T/3pVPaSqbpyeEzdW1YOndauqfn56TtxaVU9Y9vhhnqrqh6e/K26rqhuq6iu9R8Dsquq6qrq7qm7bsexM7zXfN73H3FpVv1NVj5uWf2VV/W5V/f70PP3Xe7nspUaIqjovyS8meXqSxyS5uqoes8wxAft2b5If6e5vTPLEJC/c8Xx+WXdfPv3s3FfMh3Ysf8HwEbNKXpzk9h3H/222HzeXJflUkudPy5+f5FPd/VeTvGxaDzbNf0jy693915I8LtvPjaNJbpqeEzdNx5Ptv6Eum36OJHn5+OHCYlTVRUn+aZLD3f3YbO/w/jnxHgHz8KokV5627EzvNX+U5G919zcn+Tf54v4hPp/kqd39uCSXJ7myqp54tgte9kyIK5Lc0d0f7u4/T/LaJFcteUzAPnT3nd397unwZ7P9R/NFyx0V66CqLk7yd5P88nS8kjw1yRumVa5P8szp8FXT8Uy/f9q0PmyEqvpLSb41ySuTpLv/vLv/NPd/7J/+nHh1b3tnkusUa0AAAAPwSURBVAuq6uGDhw2LdH6Sr6qq85N8dZI74z0CZtbd70hyz2mLd32v6e7f6e5PTcvfmeTiaXl39/+Zlj9o+jnrTieXHSEuSvKxHcdPxH9aYO1N0x8fn+Rd06IXTdO3rjs1rWvyiGkK/m9V1ZNHj5OV8XNJ/kWSL0zHvz7Jn3b3vdPxne8Nf/G+Mf3+09P6sCkemeRkkv80vT7+clV9TZKHdfedyXb0TfLQaX1/S7GxuvvjSX4myUezHR8+neTmeI+ARTnTe81Oz0/ya6eOTJvU3pLk7iQ3dve7djnN/Sw7QuxWJn1dB6yxqvraJG9M8kPd/ZlsTw1+VLanaN2Z5KXTqncmubS7H5/knyX5lekTQA6QqvrOJHd39807F++yau/hd7AJzk/yhCQvn14f/2++OB12N54TbKzpg4urkjwiyV9J8jXZ3gTpdN4jYICq+rZsR4gfPbWsu+/r7suzPTviiqp67NnOZ9kR4kSSS3YcvzjJJ5Y0FmBGVfWgbAeI13T3m5Kku++aXpy+kOQV2d4MK939+e7+5HT45iQfSvINyxk5S/SkJN9VVR/J9iZ5T832zIgLpqm3yf3fG/7ifWP6/V/Ol04lhHV2IsmJHZ8kvSHbUeKuU5tZTP/evWN9f0uxqb49yR9198nu/n9J3pTkb8R7BCzKmd5rUlXfnO1NZ6869Tf8TtOmg/8jX7qfiS+x7Ajxe0kum/Zw++XZ3tHMW5c8JmAfpm0uX5nk9u7+2R3Ld26b/Kwkt03LD007p01VPTLbO1X78LgRswq6+8e6++Lu3sr2e8Bvdvf3JXl7kr8/rXZNkrdMh986Hc/0+9/sbp9ysTG6+38n+VhVPXpa9LQkf5D7P/ZPf058//QtGU9M8ulTU2lhA3w0yROr6qunvzNOPR+8R8Bi7PpeU1WXZjsCPre7//DUytPf8xdMh78q2+Hw/We7kFr287K2v67v57K9t9vruvunljogYF+q6m8m+Z9J3psvbtv/40muzvamGJ3kI0l+sLvvrKrvTvKT2f5WjfuSvKS7/+vocbM6quopSf55d3/nFKZem+QhSd6T5B909+er6iuT/Ods73PkniTP6W7xio1SVZdn+9OmL892nH1etj84en2SS7P9H7Nnd/c903/MfiHbnzx9Lsnzuvv4UgYOCzB95d/3Zvvvhfck+YFs7/vBewTMoKpuSPKUJBcmuSvJS5L8anZ/r/nlJN+d5I+nk9/b3Yen2RHXZ/v/8l+W5PXd/ZNnvexlRwgAAADgYFj25hgAAADAASFCAAAAAEOIEAAAAMAQIgQAAAAwhAgBAAAADCFCAAAAAEOIEAAAAMAQIgQAAAAwxP8HGYnxa3KTgRUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8143"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values < 256].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255],\n",
       "       [  2,   1,   5,   4,   4,   9,   1,   7,   3,   6,   6,  10,  12,\n",
       "         12,  10,  10,  11,   4,   2,   5,   3,   5,   3,   8,   5,   4,\n",
       "          8,   9,   9,  22,  11,  12,  15,  11,  14,   9,  18,  24,  16,\n",
       "          7,   9,  19,   4,   7,   9,   8,   6,   3,   5,   6,  17,   9,\n",
       "          3,  11,   8,  11,  25,  12,  27,  22,  11,  14,  13,  21,  15,\n",
       "          7,   9,  10,  15,  13,  16,  12,  16,  14,  16,  23,  39,  23,\n",
       "         17,  18,  15,  34,  16,  32,  16,  33,  18,  23,  25,   4,   6,\n",
       "          1,   7,   5,   8,   7,   9,   4,   3,   1,  13,   5,   8,   6,\n",
       "          9,   5,   2,   9,  13,  11,  12,   8,  12,  11,  10,  14,   5,\n",
       "          6,   6,   2,   7,   2,   8,   3,   6,   2,   4,   1,   4,   5,\n",
       "          9,  16,  13,   8,  28,   5,   9,  17,   5,   7,   8,  13,   5,\n",
       "         17,   9,  15,  22,   4,   8,  10,  10,  19,   7,  20,  17,  41,\n",
       "         34,  41,  15,  58,  12,  18,  31,  19,  11,   9,  15,  28,  14,\n",
       "         12,  34,  46,  31,  44,  55,  35,  31,  54,  82,  46,  72,  58,\n",
       "        109,  67,  26,  17,  28,  40,  27,  28,  43,  47,  36,  42,  59,\n",
       "         63,  42,  50,  60,  85,  83,  69,  65,  90,  94,  87,  61, 132,\n",
       "         79,  27,  23,  27,  46,  21,  24,  46,  43,  57,  28,  46,  70,\n",
       "         74,  87,  93,  67,  86,  87,  96,  89, 101, 106,  55,  47,  43,\n",
       "         69,  77,  72,  71,  28,  84, 103,  90, 120,  99, 116,  78, 120,\n",
       "        132, 171, 146, 146, 118, 204, 114, 157, 107]], dtype=int64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.array([values, counts])\n",
    "for i in range(values.size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        statistics = np.append(statistics, np.array([[values[i]+j],[0]]),axis = 1)\n",
    "statistics = np.unique(statistics,axis = 1)\n",
    "statistics[:,0:256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helmholtz Machine({-1,1}, With bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_dz = np.array([10,8,5,3])\n",
    "init_type = \"zero\"\n",
    "Phi, Theta = parameter_initialization(init_type,n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25099032671736404 0.17689707891277448\n",
      "0.2499040624152378 0.17292468640030267\n",
      "0.255835949025975 0.17802833340270194\n",
      "0.25648814557706084 0.18091978680818713\n",
      "0.255026505460202 0.1852384158511488\n",
      "0.2533620477777227 0.18865158156615017\n",
      "0.2587343287207544 0.18116280162373524\n",
      "0.255313903559481 0.1813033905189412\n",
      "0.2539962373143932 0.17218119153967712\n",
      "0.24703645614497147 0.17363856159813465\n",
      "0.24524641429384603 0.17501286669150187\n",
      "0.2457181289304028 0.16528536020331916\n",
      "0.2541811745631462 0.17691160895455904\n",
      "0.24942357084480055 0.17456789113173524\n",
      "0.24712348761815636 0.17836732418659793\n",
      "0.2495884787214872 0.17106514536880793\n",
      "0.24231569343807668 0.1728583136079073\n",
      "0.24012683072244123 0.17077932111074523\n",
      "0.2457051436525877 0.17147674005864122\n",
      "0.23917072922242813 0.17288779329773424\n",
      "0.24808802710273825 0.17466717406228816\n",
      "0.2567044577872202 0.18111961593805065\n",
      "0.25074206983192965 0.18349442553655199\n",
      "0.24714197596225174 0.16838081099354033\n",
      "0.24361190601190594 0.16907528863649868\n",
      "0.2455885421999716 0.17764464670796135\n",
      "0.2452008570748281 0.16813456104302904\n",
      "0.23974999150487264 0.1684080014478721\n",
      "0.2370845681875434 0.1634124979310233\n",
      "0.2347060219385953 0.15832426003119973\n",
      "0.24142684489908978 0.1677714378202068\n",
      "0.24453760452262002 0.17534002263523685\n",
      "0.24096266706286362 0.17386047525056234\n",
      "0.2390540502053388 0.16445269017530537\n",
      "0.24298078967203493 0.16384094455601664\n",
      "0.23839809236919277 0.16995286593534426\n",
      "0.23423612001529348 0.1618442033292647\n",
      "0.24660172765931446 0.17312477284225553\n",
      "0.24380539474134114 0.17452573336252164\n",
      "0.25018056350174095 0.17738484651309042\n",
      "0.23999647141399522 0.17074486554190682\n",
      "0.2428570254867725 0.1670818507568996\n",
      "0.23870584398823474 0.16496312039749217\n",
      "0.23031394895057838 0.14932593980843703\n",
      "0.23071035628325695 0.1586747308812716\n",
      "0.23008585307197388 0.14948464165832445\n",
      "0.22758774168564067 0.14126279354948337\n",
      "0.2297311780926989 0.1512000666298419\n",
      "0.22676050960656843 0.14058633178188762\n",
      "0.22587468925328608 0.1450813674468646\n",
      "0.2325403178864877 0.1524303921444404\n",
      "0.23105330713989217 0.15624726876788309\n",
      "0.23199750202724073 0.1487743128325001\n",
      "0.24089767624308528 0.16001645924097374\n",
      "0.23633860485699845 0.15999879231418423\n",
      "0.2301279023803474 0.15399376833124653\n",
      "0.23039281010076298 0.15032505265820076\n",
      "0.2284693855093739 0.1378447811767057\n",
      "0.2207640191318794 0.14026759452139237\n",
      "0.22941661735851948 0.15725272079203584\n",
      "0.23359243976236688 0.1542796887296223\n",
      "0.2363335029838403 0.15457592800782852\n",
      "0.24091937093321206 0.16113758869821598\n",
      "0.23534729749628433 0.15287290414432664\n",
      "0.23287128448410538 0.1538141077695327\n",
      "0.2300043555276331 0.14972311695792498\n",
      "0.22651604215750715 0.15175653938548983\n",
      "0.2209011114900476 0.14523266724442674\n",
      "0.2279246827832748 0.15470843496122877\n",
      "0.2345786958341074 0.15689521262825637\n",
      "0.23058182860529927 0.1642547355461809\n",
      "0.23459803354745457 0.16751082406650386\n",
      "0.2374303689163629 0.16030051935199519\n",
      "0.22859766257252245 0.15168603249063733\n",
      "0.22651258288363804 0.15433095465182484\n",
      "0.2235930324110674 0.13860869785174545\n",
      "0.22739680349021285 0.14970704624450307\n",
      "0.21935144787693214 0.13991313330375102\n",
      "0.22509587165030842 0.1485572424809709\n",
      "0.22441264736156682 0.14276809229621837\n",
      "0.22320680449022948 0.143177251007351\n",
      "0.2312375001595242 0.154419787240321\n",
      "0.22822748303040563 0.14371648430332226\n",
      "0.22571242211211598 0.14561492044385757\n",
      "0.22537076171202397 0.14199959583962207\n",
      "0.2291070115272367 0.15079301182289448\n",
      "0.2330281973121249 0.15081582390115966\n",
      "0.23648305480739268 0.15287000517562416\n",
      "0.22806387012933246 0.1440819542247454\n",
      "0.22436018197903615 0.1442112761707684\n",
      "0.21764479397379913 0.12923796426756828\n",
      "0.2103215328060312 0.12687228743634607\n",
      "0.22033387532916743 0.13080061353999992\n",
      "0.2171748571027424 0.12196335189306934\n",
      "0.21585509389319155 0.12053070632799226\n",
      "0.21364508018127293 0.12243522965592855\n",
      "0.21408721401531763 0.12709747075852784\n",
      "0.2135311006521388 0.12857082411173895\n",
      "0.21655051674672174 0.13504351839621884\n",
      "0.2190007836290678 0.1281610336179179\n",
      "0.2209675737554998 0.1292544558920947\n",
      "0.22166149897379875 0.14177236360914106\n",
      "0.223855947358772 0.13866710012639893\n",
      "0.2225820399953352 0.13810262249764954\n",
      "0.2246426253720031 0.13668879722335162\n",
      "0.23062187084330454 0.14493802749454177\n",
      "0.22214014605302015 0.14377200990748695\n",
      "0.22293998830012815 0.13480819318650447\n",
      "0.2229146436088678 0.136182961154766\n",
      "0.2149806023951437 0.12924821083174415\n",
      "0.22002809996417266 0.12634320438338148\n",
      "0.2178783245255287 0.13738103116000636\n",
      "0.21612087607265468 0.1179739630840251\n",
      "0.2125677032584423 0.12356035060365382\n",
      "0.21687621740033533 0.12308848136635879\n",
      "0.21037442627228342 0.12175934600385546\n",
      "0.2153356902123196 0.12652547845224732\n",
      "0.21504458743287838 0.12497381043685477\n",
      "0.21507159227258085 0.12979271831240924\n",
      "0.21626347492516448 0.12787671501741785\n",
      "0.21639091400393312 0.13011903745039763\n",
      "0.2197917142167781 0.12258617179199639\n",
      "0.21519037949945047 0.13127800714696025\n",
      "0.22125385416525795 0.12107724535839741\n",
      "0.21864398686826486 0.13093085366146776\n",
      "0.21618816401693491 0.13066557490394445\n",
      "0.2129154251213537 0.12636357301245535\n",
      "0.21674482948017468 0.13015965481583605\n",
      "0.22217108724939807 0.13284062653647902\n",
      "0.21667111831078764 0.12804213323984923\n",
      "0.2164686969329585 0.1307965654447219\n",
      "0.21506422639846906 0.12824334301277399\n",
      "0.22076701248708824 0.12635759267263186\n",
      "0.21590682758495294 0.13326011881868208\n",
      "0.214123786708323 0.12363251111449722\n",
      "0.21298964731950576 0.1298306065903798\n",
      "0.21669761811634713 0.13346355325557868\n",
      "0.214190779956388 0.12765732549323955\n",
      "0.2209688844266264 0.12932292544235016\n",
      "0.22391994950368424 0.14177990663541234\n",
      "0.22036056773789756 0.1346298236930885\n",
      "0.2230840947251132 0.14059147633433403\n",
      "0.22152270664798446 0.13477364078010387\n",
      "0.22180406456566534 0.1431098951787548\n",
      "0.22479671713992602 0.1328219701696943\n",
      "0.21532646490911939 0.13076825911349593\n",
      "0.21644949479630848 0.13355083151036504\n",
      "0.21617177663284134 0.13318243309878666\n",
      "0.21457900874556218 0.12789623687939292\n",
      "0.21875490803487094 0.13066017266775215\n",
      "0.21259891131661773 0.11557543351322902\n",
      "0.2163625022785844 0.12531518383160892\n",
      "0.21589544976816682 0.12509392258164162\n",
      "0.22425529741903008 0.12809339148324791\n",
      "0.21963856655029548 0.13307661133348783\n",
      "0.22046123327148406 0.1348549644786654\n",
      "0.2168903616842203 0.1302365446968102\n",
      "0.21230514094121525 0.12521752386597804\n",
      "0.21362538651212318 0.11639504412611959\n",
      "0.215576098119007 0.13066304040441912\n",
      "0.20621377236258015 0.11938920629931467\n",
      "0.20499622213918592 0.12442201734487292\n",
      "0.20675234637668838 0.12270342476188174\n",
      "0.2158594313357932 0.1316008430967415\n",
      "0.21504736482094852 0.12553872215614417\n",
      "0.22063853750807666 0.1366608446255904\n",
      "0.225349986319138 0.14518942400376703\n",
      "0.22261863330483375 0.14323016107083544\n",
      "0.22119460724155734 0.13888594428838025\n",
      "0.22079891893593895 0.13445886829992612\n",
      "0.21663339767172263 0.13102918439107072\n",
      "0.22210354286748843 0.1370877047066983\n",
      "0.22553635742799769 0.14639263495342505\n",
      "0.22608953482164196 0.14286068845909755\n",
      "0.2253643543471203 0.1460883188617473\n",
      "0.22572017394096391 0.14135005231223804\n",
      "0.21945691923240446 0.1371889873021823\n",
      "0.21883251936943046 0.1349263162544622\n",
      "0.21586374327911922 0.12435456214006715\n",
      "0.21308709183000613 0.1265800075170522\n",
      "0.21389315786822552 0.13392802600448112\n",
      "0.20876322050410034 0.12912571818019628\n",
      "0.20462769187453417 0.11273754916081716\n",
      "0.21135577706759628 0.12382186517238865\n",
      "0.2120375916423825 0.12441938247719522\n",
      "0.21092501138108408 0.12585261084851024\n",
      "0.20216817510060725 0.11416835028443582\n",
      "0.19978960267338036 0.10761329086169966\n",
      "0.20814639378791513 0.12717907819919314\n",
      "0.21022603127156367 0.12398805235207551\n",
      "0.20555200174582353 0.12861819995246915\n",
      "0.2042134624052284 0.11752127640334438\n",
      "0.20135779563176479 0.12548563535074858\n",
      "0.20520945227225976 0.11757679260749052\n",
      "0.20503464974800317 0.11844229760225738\n",
      "0.2028318418872429 0.11426331354733434\n",
      "0.20776325311564406 0.12537018812470405\n",
      "0.2081344721181393 0.12171797594250224\n",
      "0.2036237977996632 0.11499635866951946\n",
      "0.20086887046381927 0.10936586024405072\n",
      "0.201885925373157 0.10555843785854202\n",
      "0.2024899765886072 0.1038800002490542\n",
      "0.2110716760915966 0.12206586219575644\n",
      "0.2088451090650348 0.11761520749638955\n",
      "0.2115586927325902 0.11907461694135156\n",
      "0.21291840257238998 0.1291050360943928\n",
      "0.2094985505193784 0.12127653905012928\n",
      "0.21207483252317194 0.12696871859253145\n",
      "0.2109047342818761 0.12401035490908262\n",
      "0.21041007205875836 0.12308524706840134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20519022111385823 0.11663903770801559\n",
      "0.20348374281348008 0.1105575004929603\n",
      "0.20616161260543703 0.11873257911248301\n",
      "0.20955583779320835 0.1201110807057696\n",
      "0.20604578105873428 0.11472834305720628\n",
      "0.20438941767390217 0.1170844155553509\n",
      "0.207179404417221 0.11167431088470849\n",
      "0.20558913354819144 0.11663693734184997\n",
      "0.2017672673191107 0.1073925209426348\n",
      "0.1944399216312345 0.09855335569552601\n",
      "0.18826439999311204 0.08851178518421643\n",
      "0.19121444967707638 0.10054267153800123\n",
      "0.19223068085059425 0.09517592746015882\n",
      "0.19199366649481658 0.09928155112624087\n",
      "0.19208128887906992 0.10094185645912511\n",
      "0.2005356890975852 0.10560642722252217\n",
      "0.2042874863526665 0.11112583611579854\n",
      "0.20579546621951733 0.10797465025442068\n",
      "0.19975464715564828 0.10754070459440962\n",
      "0.2006443121581505 0.1096227157465536\n",
      "0.20071205200463774 0.11390717012517196\n",
      "0.19708564246728047 0.10909293499274857\n",
      "0.20169691872738582 0.10824583605155466\n",
      "0.2020354939750461 0.10940875023188587\n",
      "0.20019332080253022 0.09939271915560613\n",
      "0.2007984101786757 0.10933988526471074\n",
      "0.1997193601213432 0.10722188663989186\n",
      "0.20782703379278572 0.11912686071593638\n",
      "0.20910666330039765 0.1193864326837879\n",
      "0.2090075243299528 0.11860893314527382\n",
      "0.20890911452633187 0.1257149323734677\n",
      "0.20681982536270752 0.11208497257346643\n",
      "0.20486329338291848 0.11038866297678271\n",
      "0.20815490439009848 0.11573622817415305\n",
      "0.20268255858553447 0.11349349918856406\n",
      "0.21139648956730914 0.11784424953410312\n",
      "0.2083457235792759 0.11408280352089613\n",
      "0.20670498160431178 0.11628872346828391\n",
      "0.2087227429003408 0.1187816813777942\n",
      "0.20666475905732845 0.11696254575128576\n",
      "0.2118462452815346 0.12141600139177477\n",
      "0.21777494382470225 0.12465633593257229\n",
      "0.2143147347569511 0.12333250978861078\n",
      "0.22325564471281895 0.12800823762596278\n",
      "0.21852340231161346 0.13998386842405378\n",
      "0.21598870099998463 0.13072138363620928\n",
      "0.2207245356440687 0.1384352138214904\n",
      "0.21983144722542908 0.1324214348352526\n",
      "0.2224653342535574 0.12784988674686745\n",
      "0.22277202087790987 0.13812483887005195\n",
      "0.22305348522497906 0.13534356834669242\n",
      "0.22471767417614139 0.13332807308604902\n",
      "0.2238498562926463 0.13910703528282717\n",
      "0.2265739044684711 0.14602736814956307\n",
      "0.2241362566393133 0.14857627111524846\n",
      "0.22423848623087847 0.1402854855680947\n",
      "0.22872357853885789 0.1491372003534588\n",
      "0.2229268139263151 0.14204179073091858\n",
      "0.22535499241895562 0.1397640357493934\n",
      "0.2236720826937237 0.13566691928279437\n",
      "0.2184537377954831 0.13659035819894472\n",
      "0.21719662383068006 0.13120300235064478\n",
      "0.21643267410533082 0.13229213725282996\n",
      "0.21662026114064412 0.1288375805768597\n",
      "0.21447997465755983 0.12796596667752913\n",
      "0.21600365439314143 0.1293272622070529\n",
      "0.20728795141987133 0.12110678243958542\n",
      "0.20404392082526393 0.11257368505864482\n",
      "0.20499298723563228 0.10940135671151786\n",
      "0.20234826075430987 0.11055830201725875\n",
      "0.20030188030644525 0.09690379373499046\n",
      "0.19132220210547654 0.10161174796910238\n",
      "0.1955514913930823 0.10279858956314222\n",
      "0.19662527945873823 0.0995021971090379\n",
      "0.1999897729932587 0.1115642906234475\n",
      "0.1963724280986656 0.09672809240081043\n",
      "0.19867316585918193 0.09663948474487366\n",
      "0.19702032828491423 0.09911902065639358\n",
      "0.19906151348184234 0.10709236150313758\n",
      "0.19429137408568545 0.09756956630763926\n",
      "0.19765064224543835 0.10821500916254272\n",
      "0.20311056328184035 0.10797329366103398\n",
      "0.20819321659452356 0.12124711616475795\n",
      "0.21041305564837634 0.12265903985810156\n",
      "0.2130237848436249 0.12522650765766655\n",
      "0.20883253753602724 0.12686166905504032\n",
      "0.20836567672345627 0.12636452244924257\n",
      "0.2088255476466416 0.11799552660946615\n",
      "0.20651275080240766 0.10982214468769481\n",
      "0.19940341967751138 0.10721014628470034\n"
     ]
    }
   ],
   "source": [
    "n_q = n_dz[1:].sum()\n",
    "n_p = n_dz.sum()\n",
    "lr = 0.05\n",
    "epoch = 300\n",
    "n_data = well_formed_set.shape[1]\n",
    "\n",
    "for e in range(epoch):\n",
    "    error_P_all = 0\n",
    "    error_Q_all = 0\n",
    "    for i in range(n_data):\n",
    "        d0 = well_formed_set[:,i:i+1]\n",
    "        Q, Alpha_Q = wake_forward(d0,Phi)\n",
    "        Theta, info_gain_sleep, error_P = sleep_update_delta(Theta,Alpha_Q,lr)\n",
    "        error_P_all += error_P/n_p\n",
    "\n",
    "        P, Alpha_P = sleep_forward(Theta)\n",
    "        Phi, info_gain_wake,error_Q = wake_update_delta(Phi,Alpha_P,lr)\n",
    "        error_Q_all += error_Q/n_q\n",
    "\n",
    "    error_P_all = error_P_all/n_data\n",
    "    error_Q_all = error_Q_all/n_data\n",
    "    print(error_P_all,error_Q_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = np.zeros((n_dz[0],n_sample))\n",
    "for i in range(n_sample):\n",
    "    generation[:,i:i+1] = generate(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = np.zeros((generation.shape[1], ),dtype = int)\n",
    "for i in range(generation.shape[1]):\n",
    "    for j in range(reordered_set.shape[1]):\n",
    "        if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "            distribution[i] = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, counts = np.unique(distribution, return_counts=True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values,counts)\n",
    "ax.set(xlim=(0, 1023), xticks=np.array([0,255,400,600,800,1023]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9817"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of correct instances among all generations\n",
    "counts[values < 256].sum()/generation.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[values < 256].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "         41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "         54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,\n",
       "         67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
       "         80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
       "         93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
       "        119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
       "        132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
       "        145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
       "        158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
       "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
       "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
       "        223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
       "        236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
       "        249, 250, 251, 252, 253, 254, 255, 256, 257],\n",
       "       [  2,   1,   2,   5,   0,   2,   2,   3,   4,   2,   7,   7,   3,\n",
       "          9,   5,   4,   6,   2,   7,  10,  14,   4,   5,   2,   2,   4,\n",
       "          8,   6,   9,   7,  11,  12,   4,  18,   3,   4,  15,   5,  20,\n",
       "         10,   7,   2,   6,   4,   3,  13,   6,  18,  29,   3,   3,   6,\n",
       "         15,  16,  21,  11,  16,   6,  23,  15,  38,  31,   5,  11,  27,\n",
       "         15,  24,  20,  14,  25,  32,  23,  25,  39,  14,  17,  22,  58,\n",
       "         22,  37,  37,  19,  29,  52,  52,  66,  55,   0,   0,   0,   0,\n",
       "          0,   1,   1,   1,   0,   0,   0,   2,   2,   2,   4,   1,   0,\n",
       "          1,   3,   2,   0,   0,   1,   2,   9,  10,   7,  13,   2,   3,\n",
       "          6,   0,   0,   1,   5,   7,   2,   2,   2,   4,   4,   3,   3,\n",
       "          2,   4,   2,   3,   6,   9,   6,   4,   5,   4,   8,  12,   0,\n",
       "          4,   8,  12,  23,  25,  23,  13,   9,  13,  11,  11,  10,   7,\n",
       "          7,  10,  19,  28,  38,  28,   9,  27,  13,  10,  24,   7,   9,\n",
       "          2,   8,   5,   6,   8,   6,  21,  30,  22,  34,  31,  20,  13,\n",
       "         43,  45,  16,  23,  19,  34,  40,  53,  35,  30,  25,  32,  83,\n",
       "         50,  28,  26,  32,  39,  63,  50,  62,  91,  50,  60,  62,  34,\n",
       "         18,  40,  69,  73,  69,  63,  63, 107,  31,  34,  45,  63,  63,\n",
       "         74,  54, 107,  96, 122,  81, 145,  98,  79, 126, 103,  81, 129,\n",
       "        120, 173, 258, 175, 171, 143, 247, 135, 137, 190, 184, 199, 232,\n",
       "        201, 354, 362, 378, 248, 357, 342,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = np.array([values, counts])\n",
    "for i in range(values.size-1):\n",
    "    diff = values[i+1] - values[i]\n",
    "    for j in range(1,diff):\n",
    "        statistics = np.append(statistics, np.array([[values[i]+j],[0]]),axis = 1)\n",
    "statistics = np.unique(statistics,axis = 1)\n",
    "statistics[:,0:256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': array([[-8.36540413e-01, -3.06410059e-02,  3.33452507e-01,\n",
       "          1.86871027e-01,  1.47692185e-01,  2.98797954e-01,\n",
       "         -1.08444891e-01,  4.25839228e-01,  8.40150359e+00],\n",
       "        [-1.43060245e-01,  2.51773504e-01,  8.52681208e-02,\n",
       "          1.67024130e-01,  2.61627890e-01,  7.43944446e+00,\n",
       "          1.03944058e-01,  4.00275901e-01,  3.59300167e-01],\n",
       "        [-5.08203449e-01, -5.17815952e-01, -1.02730378e+00,\n",
       "         -7.17975308e-02, -8.73390849e-02, -2.47069606e+00,\n",
       "         -5.00927901e-01, -6.25650531e-01,  6.14668125e+00],\n",
       "        [-3.86091530e-01, -6.08207898e-01,  3.36814234e+00,\n",
       "         -5.63881592e-01,  5.57727338e-01, -3.37088739e-01,\n",
       "          9.29598615e-02,  9.81238007e-01, -2.46873801e+00],\n",
       "        [ 1.12020512e-02,  2.77705710e+00,  6.26229663e-01,\n",
       "          1.62100903e-01,  6.34233795e-01, -1.11120596e-01,\n",
       "          1.48634333e-01, -1.53470477e+00,  4.17997041e+00],\n",
       "        [-7.06556236e-02, -8.49730182e-01,  6.69893414e-01,\n",
       "          2.99775682e+00,  4.95095020e-01,  1.32496414e-02,\n",
       "         -4.69456929e-02,  6.46514516e-01, -1.65955358e+00],\n",
       "        [ 2.86666670e+00,  2.91297665e-01, -1.25888257e+00,\n",
       "         -1.68403261e-01, -6.09914400e-01,  2.97279495e-01,\n",
       "          4.70618027e-01,  1.89852936e+00,  3.63841815e+00],\n",
       "        [-3.84393572e+00,  2.57286771e-01,  4.28722951e-01,\n",
       "          1.68092603e-02, -1.88074673e-01,  1.25416663e-01,\n",
       "         -4.90676340e+00,  1.09011765e+00, -2.32164458e+00],\n",
       "        [-4.23207736e+00,  2.34378807e-01,  5.69510654e-01,\n",
       "          7.32647055e-03, -9.70639951e-01,  3.46902155e-01,\n",
       "          3.55681303e+00,  1.24270074e-01, -4.33371609e-01],\n",
       "        [-3.28125710e+00, -1.31425974e-01, -5.20025950e-01,\n",
       "         -3.24922599e-01,  1.04623786e+00, -6.77081365e-01,\n",
       "          2.47928356e-02,  7.21484577e-02, -2.77622818e+00]]),\n",
       " 'Theta_21': array([[-1.30321575,  0.13423677, -0.04996752,  0.02879005, -0.01349665,\n",
       "         -2.7028098 ],\n",
       "        [-0.14553845,  3.974861  ,  0.07001097, -0.0054688 ,  0.12119729,\n",
       "          0.87182971],\n",
       "        [-0.28558021, -0.12809189, -0.51109279,  0.04824339, -0.47804433,\n",
       "          2.99069929],\n",
       "        [-0.04366623, -0.29438215, -0.32725623, -0.19746076, -1.54338588,\n",
       "          1.45150406],\n",
       "        [-0.54382961,  0.10682493, -0.53286674,  1.59549217, -0.6678321 ,\n",
       "          0.53289433],\n",
       "        [ 0.06684973, -0.38570032,  0.07324427,  3.53416133,  0.14872119,\n",
       "          0.49618305],\n",
       "        [ 3.34804535, -0.29556525,  0.5850995 , -0.47819966,  0.07068406,\n",
       "          0.32219781],\n",
       "        [-0.81047134, -0.91815315,  0.26293311,  0.44821171,  0.30336483,\n",
       "          1.65592192]]),\n",
       " 'Theta_32': array([[-1.20581943, -0.76375712, -0.12016846, -0.99648983],\n",
       "        [ 0.03557946,  0.36294932, -0.60199639, -2.22322451],\n",
       "        [ 0.56349474, -1.59262509, -0.30715716, -3.29214385],\n",
       "        [-2.29781793, -0.10977511, -0.37684362,  0.83384474],\n",
       "        [ 0.94986956,  0.70750481, -1.4956831 ,  0.55122908]]),\n",
       " 'Theta_k': array([[-1.99944761],\n",
       "        [ 5.1837616 ],\n",
       "        [-1.72169456]])}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
