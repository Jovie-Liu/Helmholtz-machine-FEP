{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6df78",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 4_5</span> \n",
    "\n",
    "# Helmholtz Machine on Mutual Information Algorithm\n",
    "\n",
    "\n",
    "*Created:* December 29, 2023\n",
    "\n",
    "*Updated:* December 31, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cb0071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  6,  3,  1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[10,8,6,3,1]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e8efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, Theta = ut.parameter_initialization(\"zero\",n_dz)  # \"zero\" or \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e0d756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_set = [1,0]\n",
    "activation_type = \"tanh\"\n",
    "bias = [False,False,False] # [instantiation bias, MLP bias,data bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cda5cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 1],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 1, 1, 1],\n",
       "       [1, 0, 0, ..., 0, 1, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = n_dz[0,0]\n",
    "k = 3\n",
    "n_data = 300\n",
    "random_set = ut.random_generate(k,n,n_data,value_set)\n",
    "random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c0dd709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values,counts = np.unique(random_set, axis=1, return_counts = True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1929b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = values\n",
    "entire_set = ut.all_comb(n, value_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17cdeb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set = ut.reorder_all_comb(entire_set,dataset)\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa978494",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1e7a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 2000\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cd202b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.18 2.26 1.2  0.   5.65] Loss_P: [ 3.83  2.43  2.88  1.28 10.41]\n",
      "Loss_Q: [2.1  2.37 1.2  0.   5.66] Loss_P: [ 3.84  2.46  2.91  1.22 10.43]\n",
      "Loss_Q: [2.11 2.3  1.19 0.   5.59] Loss_P: [ 3.85  2.45  2.87  1.27 10.45]\n",
      "Loss_Q: [2.18 2.31 1.17 0.   5.66] Loss_P: [ 3.77  2.49  2.86  1.27 10.38]\n",
      "Loss_Q: [2.15 2.33 1.19 0.   5.66] Loss_P: [ 3.78  2.48  2.9   1.27 10.43]\n",
      "Loss_Q: [2.16 2.24 1.23 0.   5.63] Loss_P: [ 3.8   2.54  2.9   1.26 10.51]\n",
      "Loss_Q: [2.13 2.31 1.22 0.   5.67] Loss_P: [ 3.83  2.55  2.89  1.27 10.53]\n",
      "Loss_Q: [2.17 2.27 1.18 0.   5.63] Loss_P: [ 3.78  2.46  2.88  1.27 10.39]\n",
      "Loss_Q: [2.2  2.28 1.22 0.   5.7 ] Loss_P: [ 3.8   2.51  2.89  1.24 10.44]\n",
      "Loss_Q: [2.19 2.37 1.25 0.   5.81] Loss_P: [ 3.8   2.54  2.9   1.27 10.5 ]\n",
      "Loss_Q: [2.12 2.31 1.21 0.   5.64] Loss_P: [ 3.77  2.45  2.92  1.27 10.41]\n",
      "Loss_Q: [2.1  2.33 1.23 0.   5.65] Loss_P: [ 3.76  2.49  2.92  1.28 10.45]\n",
      "Loss_Q: [2.2  2.33 1.24 0.   5.77] Loss_P: [ 3.84  2.42  2.94  1.25 10.46]\n",
      "Loss_Q: [2.24 2.34 1.24 0.   5.82] Loss_P: [ 3.77  2.49  2.94  1.3  10.5 ]\n",
      "Loss_Q: [2.26 2.33 1.24 0.   5.82] Loss_P: [ 3.78  2.53  2.93  1.3  10.54]\n",
      "Loss_Q: [2.2  2.34 1.26 0.   5.8 ] Loss_P: [ 3.84  2.53  2.92  1.31 10.6 ]\n",
      "Loss_Q: [2.18 2.29 1.26 0.   5.73] Loss_P: [ 3.79  2.56  2.91  1.32 10.58]\n",
      "Loss_Q: [2.22 2.34 1.28 0.   5.84] Loss_P: [ 3.82  2.46  2.93  1.34 10.55]\n",
      "Loss_Q: [2.22 2.32 1.26 0.   5.81] Loss_P: [ 3.84  2.53  2.9   1.35 10.62]\n",
      "Loss_Q: [2.21 2.36 1.22 0.   5.79] Loss_P: [ 3.81  2.5   2.89  1.32 10.52]\n",
      "Loss_Q: [2.29 2.29 1.26 0.   5.84] Loss_P: [ 3.85  2.44  2.86  1.33 10.48]\n",
      "Loss_Q: [2.22 2.34 1.34 0.   5.89] Loss_P: [ 3.73  2.56  2.89  1.35 10.53]\n",
      "Loss_Q: [2.28 2.29 1.27 0.   5.84] Loss_P: [ 3.76  2.48  2.88  1.3  10.42]\n",
      "Loss_Q: [2.25 2.31 1.26 0.   5.82] Loss_P: [ 3.78  2.46  2.87  1.32 10.44]\n",
      "Loss_Q: [2.17 2.28 1.24 0.   5.7 ] Loss_P: [ 3.78  2.51  2.87  1.28 10.44]\n",
      "Loss_Q: [2.17 2.34 1.27 0.   5.78] Loss_P: [ 3.87  2.48  2.88  1.29 10.52]\n",
      "Loss_Q: [2.2  2.34 1.33 0.   5.87] Loss_P: [ 3.85  2.45  2.87  1.36 10.53]\n",
      "Loss_Q: [2.1  2.31 1.27 0.   5.68] Loss_P: [ 3.87  2.48  2.89  1.39 10.63]\n",
      "Loss_Q: [2.18 2.28 1.29 0.   5.75] Loss_P: [ 3.78  2.46  2.87  1.34 10.45]\n",
      "Loss_Q: [2.18 2.33 1.31 0.   5.82] Loss_P: [ 3.81  2.46  2.88  1.36 10.51]\n",
      "Loss_Q: [2.25 2.29 1.29 0.   5.83] Loss_P: [ 3.81  2.5   2.86  1.36 10.52]\n",
      "Loss_Q: [2.24 2.29 1.34 0.   5.87] Loss_P: [ 3.84  2.42  2.9   1.39 10.55]\n",
      "Loss_Q: [2.32 2.28 1.31 0.   5.91] Loss_P: [ 3.82  2.48  2.93  1.37 10.6 ]\n",
      "Loss_Q: [2.24 2.34 1.34 0.   5.91] Loss_P: [ 3.89  2.44  2.9   1.36 10.58]\n",
      "Loss_Q: [2.18 2.3  1.3  0.   5.78] Loss_P: [ 3.78  2.51  2.88  1.36 10.53]\n",
      "Loss_Q: [2.16 2.31 1.28 0.   5.76] Loss_P: [ 3.82  2.49  2.9   1.32 10.53]\n",
      "Loss_Q: [2.19 2.32 1.28 0.   5.79] Loss_P: [ 3.73  2.54  2.87  1.36 10.5 ]\n",
      "Loss_Q: [2.14 2.26 1.31 0.   5.72] Loss_P: [ 3.78  2.57  2.87  1.36 10.58]\n",
      "Loss_Q: [2.22 2.21 1.27 0.   5.7 ] Loss_P: [ 3.84  2.49  2.86  1.33 10.52]\n",
      "Loss_Q: [2.2  2.28 1.28 0.   5.76] Loss_P: [ 3.81  2.5   2.85  1.35 10.52]\n",
      "Loss_Q: [2.17 2.25 1.32 0.   5.74] Loss_P: [ 3.83  2.45  2.85  1.35 10.47]\n",
      "Loss_Q: [2.22 2.32 1.3  0.   5.84] Loss_P: [ 3.78  2.59  2.88  1.34 10.59]\n",
      "Loss_Q: [2.16 2.31 1.29 0.   5.75] Loss_P: [ 3.81  2.52  2.86  1.35 10.54]\n",
      "Loss_Q: [2.15 2.32 1.3  0.   5.77] Loss_P: [ 3.85  2.43  2.84  1.34 10.47]\n",
      "Loss_Q: [2.14 2.27 1.26 0.   5.67] Loss_P: [ 3.86  2.41  2.81  1.34 10.42]\n",
      "Loss_Q: [2.15 2.33 1.28 0.   5.76] Loss_P: [ 3.82  2.43  2.84  1.34 10.42]\n",
      "Loss_Q: [2.06 2.34 1.29 0.   5.69] Loss_P: [ 3.87  2.43  2.85  1.32 10.46]\n",
      "Loss_Q: [2.01 2.23 1.25 0.   5.5 ] Loss_P: [ 3.82  2.35  2.85  1.32 10.34]\n",
      "Loss_Q: [2.15 2.35 1.26 0.   5.76] Loss_P: [ 3.85  2.39  2.86  1.32 10.42]\n",
      "Loss_Q: [2.18 2.29 1.25 0.   5.72] Loss_P: [ 3.8   2.4   2.84  1.32 10.36]\n",
      "Loss_Q: [2.21 2.3  1.29 0.   5.8 ] Loss_P: [ 3.82  2.41  2.87  1.31 10.42]\n",
      "Loss_Q: [2.15 2.3  1.22 0.   5.67] Loss_P: [ 3.84  2.38  2.84  1.33 10.39]\n",
      "Loss_Q: [2.02 2.3  1.26 0.   5.58] Loss_P: [ 3.82  2.37  2.85  1.3  10.33]\n",
      "Loss_Q: [2.08 2.31 1.24 0.   5.64] Loss_P: [ 3.79  2.44  2.83  1.31 10.37]\n",
      "Loss_Q: [2.19 2.24 1.24 0.   5.66] Loss_P: [ 3.78  2.54  2.83  1.31 10.46]\n",
      "Loss_Q: [2.08 2.29 1.25 0.   5.61] Loss_P: [ 3.87  2.5   2.83  1.29 10.49]\n",
      "Loss_Q: [2.17 2.31 1.25 0.   5.73] Loss_P: [ 3.81  2.47  2.81  1.31 10.4 ]\n",
      "Loss_Q: [2.14 2.23 1.27 0.   5.64] Loss_P: [ 3.86  2.41  2.84  1.31 10.42]\n",
      "Loss_Q: [2.2  2.36 1.28 0.   5.84] Loss_P: [ 3.77  2.51  2.84  1.36 10.47]\n",
      "Loss_Q: [2.16 2.3  1.28 0.   5.74] Loss_P: [ 3.85  2.49  2.83  1.31 10.48]\n",
      "Loss_Q: [2.23 2.31 1.3  0.   5.84] Loss_P: [ 3.85  2.45  2.86  1.34 10.5 ]\n",
      "Loss_Q: [2.1  2.34 1.3  0.   5.74] Loss_P: [ 3.83  2.44  2.88  1.37 10.52]\n",
      "Loss_Q: [2.12 2.3  1.31 0.   5.73] Loss_P: [ 3.77  2.45  2.86  1.38 10.46]\n",
      "Loss_Q: [2.09 2.35 1.27 0.   5.71] Loss_P: [ 3.86  2.52  2.89  1.34 10.61]\n",
      "Loss_Q: [2.1  2.3  1.33 0.   5.73] Loss_P: [ 3.76  2.51  2.85  1.37 10.49]\n",
      "Loss_Q: [2.11 2.29 1.35 0.   5.75] Loss_P: [ 3.83  2.4   2.89  1.38 10.51]\n",
      "Loss_Q: [2.12 2.34 1.33 0.   5.79] Loss_P: [ 3.86  2.45  2.86  1.38 10.56]\n",
      "Loss_Q: [2.1  2.37 1.33 0.   5.81] Loss_P: [ 3.85  2.4   2.84  1.37 10.46]\n",
      "Loss_Q: [2.09 2.35 1.31 0.   5.75] Loss_P: [ 3.75  2.44  2.89  1.34 10.43]\n",
      "Loss_Q: [2.11 2.34 1.26 0.   5.71] Loss_P: [ 3.88  2.41  2.88  1.37 10.53]\n",
      "Loss_Q: [2.19 2.34 1.31 0.   5.84] Loss_P: [ 3.83  2.47  2.85  1.34 10.48]\n",
      "Loss_Q: [2.11 2.37 1.28 0.   5.76] Loss_P: [ 3.83  2.45  2.84  1.35 10.47]\n",
      "Loss_Q: [2.11 2.29 1.26 0.   5.66] Loss_P: [ 3.8   2.46  2.85  1.32 10.44]\n",
      "Loss_Q: [2.15 2.29 1.33 0.   5.78] Loss_P: [ 3.83  2.45  2.88  1.32 10.49]\n",
      "Loss_Q: [2.1  2.3  1.31 0.   5.71] Loss_P: [ 3.8   2.46  2.83  1.36 10.44]\n",
      "Loss_Q: [2.13 2.31 1.32 0.   5.76] Loss_P: [ 3.81  2.4   2.83  1.38 10.42]\n",
      "Loss_Q: [2.05 2.31 1.29 0.   5.64] Loss_P: [ 3.8   2.41  2.8   1.36 10.38]\n",
      "Loss_Q: [2.15 2.34 1.28 0.   5.77] Loss_P: [ 3.91  2.35  2.85  1.34 10.44]\n",
      "Loss_Q: [2.13 2.3  1.33 0.   5.76] Loss_P: [ 3.83  2.28  2.82  1.35 10.28]\n",
      "Loss_Q: [2.19 2.25 1.31 0.   5.76] Loss_P: [ 3.81  2.38  2.84  1.38 10.41]\n",
      "Loss_Q: [2.08 2.32 1.29 0.   5.69] Loss_P: [ 3.85  2.33  2.82  1.35 10.35]\n",
      "Loss_Q: [2.19 2.32 1.32 0.   5.83] Loss_P: [ 3.88  2.42  2.82  1.33 10.45]\n",
      "Loss_Q: [2.06 2.3  1.31 0.   5.66] Loss_P: [ 3.85  2.41  2.84  1.34 10.45]\n",
      "Loss_Q: [2.17 2.3  1.31 0.   5.79] Loss_P: [ 3.84  2.38  2.88  1.36 10.46]\n",
      "Loss_Q: [2.03 2.32 1.31 0.   5.66] Loss_P: [ 3.79  2.45  2.84  1.36 10.43]\n",
      "Loss_Q: [2.05 2.26 1.31 0.   5.63] Loss_P: [ 3.82  2.35  2.88  1.34 10.39]\n",
      "Loss_Q: [2.13 2.34 1.34 0.   5.81] Loss_P: [ 3.8   2.4   2.84  1.39 10.44]\n",
      "Loss_Q: [2.17 2.32 1.31 0.   5.79] Loss_P: [ 3.86  2.47  2.85  1.4  10.59]\n",
      "Loss_Q: [2.14 2.29 1.34 0.   5.76] Loss_P: [ 3.85  2.32  2.87  1.38 10.42]\n",
      "Loss_Q: [2.08 2.3  1.35 0.   5.73] Loss_P: [ 3.85  2.39  2.84  1.38 10.45]\n",
      "Loss_Q: [2.11 2.26 1.32 0.   5.69] Loss_P: [ 3.83  2.34  2.82  1.36 10.34]\n",
      "Loss_Q: [2.26 2.32 1.3  0.   5.88] Loss_P: [ 3.83  2.39  2.85  1.37 10.43]\n",
      "Loss_Q: [2.07 2.3  1.35 0.   5.73] Loss_P: [ 3.85  2.39  2.82  1.37 10.43]\n",
      "Loss_Q: [2.09 2.3  1.34 0.   5.74] Loss_P: [ 3.84  2.4   2.83  1.39 10.47]\n",
      "Loss_Q: [2.07 2.25 1.32 0.   5.64] Loss_P: [ 3.83  2.37  2.85  1.41 10.46]\n",
      "Loss_Q: [2.08 2.27 1.34 0.   5.69] Loss_P: [ 3.84  2.41  2.84  1.38 10.46]\n",
      "Loss_Q: [2.09 2.35 1.37 0.   5.81] Loss_P: [ 3.87  2.4   2.83  1.39 10.48]\n",
      "Loss_Q: [2.15 2.33 1.33 0.   5.81] Loss_P: [ 3.82  2.45  2.8   1.38 10.46]\n",
      "Loss_Q: [2.14 2.3  1.33 0.   5.77] Loss_P: [ 3.79  2.4   2.84  1.36 10.39]\n",
      "Loss_Q: [2.11 2.3  1.32 0.   5.73] Loss_P: [ 3.8   2.43  2.83  1.37 10.42]\n",
      "Loss_Q: [2.08 2.29 1.28 0.   5.66] Loss_P: [ 3.76  2.42  2.85  1.38 10.42]\n",
      "Loss_Q: [2.09 2.29 1.31 0.   5.68] Loss_P: [ 3.88  2.41  2.85  1.36 10.49]\n",
      "Loss_Q: [2.17 2.28 1.26 0.   5.71] Loss_P: [ 3.85  2.36  2.86  1.35 10.42]\n",
      "Loss_Q: [2.07 2.26 1.28 0.   5.6 ] Loss_P: [ 3.81  2.39  2.84  1.34 10.38]\n",
      "Loss_Q: [2.09 2.3  1.28 0.   5.67] Loss_P: [ 3.82  2.36  2.8   1.32 10.29]\n",
      "Loss_Q: [2.02 2.22 1.24 0.   5.48] Loss_P: [ 3.82  2.42  2.77  1.3  10.31]\n",
      "Loss_Q: [2.18 2.27 1.24 0.   5.7 ] Loss_P: [ 3.91  2.4   2.78  1.34 10.43]\n",
      "Loss_Q: [2.13 2.19 1.3  0.   5.62] Loss_P: [ 3.9   2.4   2.78  1.34 10.42]\n",
      "Loss_Q: [2.21 2.16 1.25 0.   5.63] Loss_P: [ 3.8   2.33  2.76  1.36 10.26]\n",
      "Loss_Q: [2.06 2.21 1.28 0.   5.55] Loss_P: [ 3.82  2.4   2.74  1.34 10.3 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.13 2.21 1.28 0.   5.62] Loss_P: [ 3.8   2.49  2.79  1.35 10.43]\n",
      "Loss_Q: [2.14 2.25 1.27 0.   5.66] Loss_P: [ 3.82  2.45  2.77  1.38 10.42]\n",
      "Loss_Q: [2.09 2.21 1.27 0.   5.58] Loss_P: [ 3.86  2.43  2.74  1.34 10.37]\n",
      "Loss_Q: [2.1  2.27 1.31 0.   5.68] Loss_P: [ 3.8   2.44  2.77  1.36 10.38]\n",
      "Loss_Q: [2.16 2.29 1.31 0.   5.76] Loss_P: [ 3.81  2.45  2.81  1.35 10.43]\n",
      "Loss_Q: [2.14 2.28 1.32 0.   5.74] Loss_P: [ 3.78  2.52  2.79  1.36 10.45]\n",
      "Loss_Q: [2.24 2.23 1.29 0.   5.76] Loss_P: [ 3.79  2.5   2.8   1.34 10.43]\n",
      "Loss_Q: [2.12 2.24 1.26 0.   5.62] Loss_P: [ 3.83  2.54  2.82  1.38 10.57]\n",
      "Loss_Q: [2.18 2.2  1.33 0.   5.71] Loss_P: [ 3.82  2.51  2.83  1.34 10.5 ]\n",
      "Loss_Q: [2.22 2.27 1.3  0.   5.79] Loss_P: [ 3.81  2.5   2.84  1.38 10.53]\n",
      "Loss_Q: [2.19 2.26 1.34 0.   5.79] Loss_P: [ 3.84  2.49  2.81  1.39 10.52]\n",
      "Loss_Q: [2.21 2.25 1.34 0.   5.8 ] Loss_P: [ 3.82  2.51  2.81  1.4  10.54]\n",
      "Loss_Q: [2.21 2.17 1.36 0.   5.74] Loss_P: [ 3.89  2.51  2.79  1.4  10.6 ]\n",
      "Loss_Q: [2.19 2.26 1.34 0.   5.78] Loss_P: [ 3.81  2.52  2.81  1.41 10.55]\n",
      "Loss_Q: [2.19 2.28 1.36 0.   5.82] Loss_P: [ 3.84  2.51  2.83  1.4  10.58]\n",
      "Loss_Q: [2.31 2.23 1.39 0.   5.92] Loss_P: [ 3.9   2.51  2.82  1.43 10.66]\n",
      "Loss_Q: [2.21 2.28 1.36 0.   5.85] Loss_P: [ 3.81  2.5   2.86  1.41 10.57]\n",
      "Loss_Q: [2.14 2.27 1.39 0.   5.8 ] Loss_P: [ 3.79  2.56  2.86  1.44 10.65]\n",
      "Loss_Q: [2.22 2.3  1.37 0.   5.88] Loss_P: [ 3.8   2.52  2.83  1.43 10.58]\n",
      "Loss_Q: [2.2  2.28 1.38 0.   5.86] Loss_P: [ 3.77  2.49  2.89  1.43 10.57]\n",
      "Loss_Q: [2.15 2.25 1.37 0.   5.76] Loss_P: [ 3.83  2.37  2.85  1.41 10.46]\n",
      "Loss_Q: [2.13 2.24 1.35 0.   5.72] Loss_P: [ 3.8   2.44  2.84  1.4  10.48]\n",
      "Loss_Q: [2.19 2.28 1.35 0.   5.81] Loss_P: [ 3.82  2.4   2.85  1.39 10.46]\n",
      "Loss_Q: [2.16 2.23 1.33 0.   5.73] Loss_P: [ 3.92  2.36  2.86  1.42 10.56]\n",
      "Loss_Q: [2.04 2.27 1.38 0.   5.7 ] Loss_P: [ 3.89  2.43  2.86  1.39 10.57]\n",
      "Loss_Q: [2.03 2.32 1.3  0.   5.64] Loss_P: [ 3.81  2.42  2.84  1.39 10.45]\n",
      "Loss_Q: [2.18 2.26 1.35 0.   5.79] Loss_P: [ 3.84  2.47  2.86  1.39 10.55]\n",
      "Loss_Q: [2.23 2.28 1.34 0.   5.85] Loss_P: [ 3.8   2.5   2.87  1.41 10.59]\n",
      "Loss_Q: [2.25 2.29 1.32 0.   5.87] Loss_P: [ 3.85  2.42  2.86  1.39 10.53]\n",
      "Loss_Q: [2.18 2.33 1.32 0.   5.83] Loss_P: [ 3.89  2.44  2.85  1.37 10.56]\n",
      "Loss_Q: [2.04 2.32 1.31 0.   5.67] Loss_P: [ 3.87  2.46  2.85  1.37 10.56]\n",
      "Loss_Q: [2.07 2.31 1.32 0.   5.7 ] Loss_P: [ 3.84  2.47  2.85  1.37 10.54]\n",
      "Loss_Q: [2.14 2.31 1.32 0.   5.77] Loss_P: [ 3.83  2.44  2.86  1.37 10.5 ]\n",
      "Loss_Q: [2.19 2.28 1.35 0.   5.81] Loss_P: [ 3.85  2.46  2.81  1.39 10.51]\n",
      "Loss_Q: [2.19 2.29 1.36 0.   5.84] Loss_P: [ 3.83  2.44  2.86  1.4  10.53]\n",
      "Loss_Q: [2.12 2.28 1.32 0.   5.73] Loss_P: [ 3.84  2.52  2.83  1.4  10.59]\n",
      "Loss_Q: [2.18 2.24 1.34 0.   5.75] Loss_P: [ 3.79  2.46  2.85  1.39 10.48]\n",
      "Loss_Q: [2.15 2.32 1.36 0.   5.83] Loss_P: [ 3.83  2.4   2.85  1.39 10.47]\n",
      "Loss_Q: [2.16 2.26 1.34 0.   5.76] Loss_P: [ 3.82  2.5   2.82  1.39 10.53]\n",
      "Loss_Q: [2.19 2.27 1.34 0.   5.8 ] Loss_P: [ 3.82  2.5   2.79  1.38 10.5 ]\n",
      "Loss_Q: [2.22 2.28 1.33 0.   5.83] Loss_P: [ 3.79  2.5   2.79  1.37 10.44]\n",
      "Loss_Q: [2.21 2.27 1.32 0.   5.8 ] Loss_P: [ 3.84  2.48  2.81  1.36 10.49]\n",
      "Loss_Q: [2.13 2.3  1.26 0.   5.69] Loss_P: [ 3.88  2.42  2.77  1.35 10.43]\n",
      "Loss_Q: [2.13 2.23 1.31 0.   5.67] Loss_P: [ 3.89  2.42  2.81  1.35 10.46]\n",
      "Loss_Q: [2.19 2.26 1.3  0.   5.75] Loss_P: [ 3.83  2.41  2.81  1.34 10.39]\n",
      "Loss_Q: [2.17 2.19 1.25 0.   5.61] Loss_P: [ 3.81  2.39  2.79  1.33 10.31]\n",
      "Loss_Q: [2.19 2.23 1.23 0.   5.66] Loss_P: [ 3.82  2.41  2.78  1.32 10.33]\n",
      "Loss_Q: [2.17 2.26 1.22 0.   5.65] Loss_P: [ 3.84  2.51  2.77  1.31 10.43]\n",
      "Loss_Q: [2.24 2.24 1.23 0.   5.71] Loss_P: [ 3.87  2.43  2.81  1.31 10.42]\n",
      "Loss_Q: [2.17 2.17 1.17 0.   5.5 ] Loss_P: [ 3.92  2.47  2.77  1.31 10.46]\n",
      "Loss_Q: [2.19 2.21 1.17 0.   5.57] Loss_P: [ 3.84  2.41  2.76  1.28 10.31]\n",
      "Loss_Q: [2.13 2.26 1.16 0.   5.56] Loss_P: [ 3.88  2.45  2.77  1.23 10.34]\n",
      "Loss_Q: [2.17 2.19 1.14 0.   5.51] Loss_P: [ 3.87  2.4   2.71  1.25 10.24]\n",
      "Loss_Q: [2.15 2.13 1.13 0.   5.42] Loss_P: [ 3.9   2.38  2.72  1.21 10.21]\n",
      "Loss_Q: [2.14 2.2  1.14 0.   5.48] Loss_P: [ 3.79  2.44  2.69  1.24 10.16]\n",
      "Loss_Q: [2.09 2.19 1.12 0.   5.4 ] Loss_P: [ 3.77  2.46  2.72  1.22 10.18]\n",
      "Loss_Q: [2.15 2.15 1.14 0.   5.44] Loss_P: [ 3.83  2.37  2.71  1.21 10.13]\n",
      "Loss_Q: [2.17 2.15 1.17 0.   5.49] Loss_P: [ 3.88  2.42  2.73  1.25 10.27]\n",
      "Loss_Q: [2.18 2.19 1.17 0.   5.55] Loss_P: [ 3.87  2.45  2.68  1.22 10.22]\n",
      "Loss_Q: [2.19 2.18 1.16 0.   5.52] Loss_P: [ 3.83  2.39  2.68  1.24 10.14]\n",
      "Loss_Q: [2.12 2.13 1.12 0.   5.37] Loss_P: [ 3.8   2.41  2.64  1.23 10.09]\n",
      "Loss_Q: [2.23 2.1  1.21 0.   5.55] Loss_P: [ 3.83  2.52  2.68  1.25 10.27]\n",
      "Loss_Q: [2.19 2.14 1.18 0.   5.5 ] Loss_P: [ 3.84  2.43  2.7   1.23 10.19]\n",
      "Loss_Q: [2.12 2.09 1.08 0.   5.28] Loss_P: [3.79 2.41 2.61 1.17 9.98]\n",
      "Loss_Q: [2.25 2.08 1.08 0.   5.42] Loss_P: [ 3.85  2.45  2.61  1.21 10.11]\n",
      "Loss_Q: [2.19 2.11 1.14 0.   5.45] Loss_P: [ 3.82  2.51  2.63  1.2  10.16]\n",
      "Loss_Q: [2.19 2.1  1.12 0.   5.42] Loss_P: [ 3.8   2.41  2.66  1.27 10.15]\n",
      "Loss_Q: [2.17 2.12 1.16 0.   5.45] Loss_P: [ 3.79  2.43  2.65  1.23 10.09]\n",
      "Loss_Q: [2.13 2.17 1.21 0.   5.52] Loss_P: [ 3.81  2.45  2.69  1.23 10.18]\n",
      "Loss_Q: [2.16 2.12 1.18 0.   5.46] Loss_P: [ 3.84  2.47  2.66  1.26 10.24]\n",
      "Loss_Q: [2.11 2.14 1.18 0.   5.44] Loss_P: [ 3.85  2.43  2.64  1.24 10.16]\n",
      "Loss_Q: [2.12 2.07 1.16 0.   5.35] Loss_P: [ 3.86  2.43  2.62  1.26 10.18]\n",
      "Loss_Q: [2.12 2.12 1.17 0.   5.41] Loss_P: [ 3.87  2.38  2.64  1.28 10.18]\n",
      "Loss_Q: [2.14 2.1  1.18 0.   5.42] Loss_P: [ 3.86  2.37  2.63  1.28 10.13]\n",
      "Loss_Q: [2.13 2.2  1.19 0.   5.51] Loss_P: [ 3.83  2.39  2.64  1.28 10.15]\n",
      "Loss_Q: [2.11 2.09 1.2  0.   5.4 ] Loss_P: [ 3.77  2.49  2.66  1.29 10.21]\n",
      "Loss_Q: [2.13 2.16 1.18 0.   5.47] Loss_P: [ 3.86  2.41  2.61  1.25 10.12]\n",
      "Loss_Q: [2.07 2.04 1.18 0.   5.29] Loss_P: [ 3.96  2.31  2.58  1.24 10.09]\n",
      "Loss_Q: [2.04 2.09 1.2  0.   5.33] Loss_P: [ 3.83  2.42  2.51  1.25 10.  ]\n",
      "Loss_Q: [2.02 2.02 1.18 0.   5.22] Loss_P: [3.89 2.34 2.5  1.25 9.98]\n",
      "Loss_Q: [2.03 2.03 1.18 0.   5.24] Loss_P: [ 3.91  2.31  2.55  1.26 10.03]\n",
      "Loss_Q: [2.01 2.05 1.21 0.   5.27] Loss_P: [ 3.86  2.34  2.54  1.29 10.03]\n",
      "Loss_Q: [1.98 1.98 1.18 0.   5.15] Loss_P: [ 3.87  2.37  2.49  1.28 10.01]\n",
      "Loss_Q: [2.04 2.04 1.2  0.   5.28] Loss_P: [ 3.89  2.36  2.56  1.28 10.09]\n",
      "Loss_Q: [2.13 1.99 1.19 0.   5.3 ] Loss_P: [ 3.94  2.36  2.54  1.28 10.12]\n",
      "Loss_Q: [2.08 2.09 1.19 0.   5.37] Loss_P: [ 3.81  2.41  2.59  1.28 10.1 ]\n",
      "Loss_Q: [2.07 2.06 1.22 0.   5.35] Loss_P: [ 3.86  2.33  2.59  1.29 10.06]\n",
      "Loss_Q: [2.09 2.14 1.21 0.   5.44] Loss_P: [ 3.83  2.4   2.64  1.3  10.17]\n",
      "Loss_Q: [2.17 2.15 1.2  0.   5.53] Loss_P: [ 3.89  2.31  2.66  1.28 10.15]\n",
      "Loss_Q: [2.09 2.19 1.23 0.   5.51] Loss_P: [ 3.88  2.39  2.7   1.29 10.27]\n",
      "Loss_Q: [2.15 2.17 1.21 0.   5.52] Loss_P: [ 3.84  2.33  2.73  1.27 10.16]\n",
      "Loss_Q: [2.12 2.15 1.24 0.   5.52] Loss_P: [ 3.94  2.28  2.7   1.3  10.22]\n",
      "Loss_Q: [2.11 2.16 1.26 0.   5.53] Loss_P: [ 3.8   2.41  2.68  1.33 10.22]\n",
      "Loss_Q: [2.04 2.15 1.25 0.   5.44] Loss_P: [ 3.86  2.39  2.63  1.3  10.18]\n",
      "Loss_Q: [2.12 2.14 1.25 0.   5.51] Loss_P: [ 3.94  2.32  2.69  1.34 10.29]\n",
      "Loss_Q: [2.12 2.19 1.22 0.   5.53] Loss_P: [ 3.88  2.29  2.67  1.28 10.12]\n",
      "Loss_Q: [2.09 2.25 1.2  0.   5.54] Loss_P: [ 3.83  2.31  2.7   1.27 10.11]\n",
      "Loss_Q: [2.09 2.17 1.22 0.   5.48] Loss_P: [ 3.9   2.28  2.66  1.28 10.12]\n",
      "Loss_Q: [2.11 2.13 1.22 0.   5.47] Loss_P: [ 3.89  2.35  2.69  1.29 10.22]\n",
      "Loss_Q: [2.09 2.15 1.19 0.   5.44] Loss_P: [ 3.88  2.33  2.64  1.28 10.12]\n",
      "Loss_Q: [2.04 2.08 1.22 0.   5.34] Loss_P: [ 3.93  2.25  2.61  1.31 10.1 ]\n",
      "Loss_Q: [2.03 2.18 1.22 0.   5.42] Loss_P: [ 3.96  2.27  2.63  1.3  10.17]\n",
      "Loss_Q: [2.07 2.22 1.2  0.   5.49] Loss_P: [ 3.89  2.32  2.68  1.29 10.18]\n",
      "Loss_Q: [1.97 2.24 1.19 0.   5.39] Loss_P: [ 3.82  2.34  2.72  1.26 10.14]\n",
      "Loss_Q: [2.03 2.24 1.15 0.   5.42] Loss_P: [ 3.89  2.26  2.7   1.26 10.11]\n",
      "Loss_Q: [2.03 2.24 1.18 0.   5.45] Loss_P: [ 3.89  2.34  2.76  1.21 10.2 ]\n",
      "Loss_Q: [2.11 2.24 1.17 0.   5.51] Loss_P: [ 3.86  2.36  2.73  1.27 10.22]\n",
      "Loss_Q: [2.09 2.21 1.19 0.   5.49] Loss_P: [ 3.92  2.29  2.74  1.25 10.2 ]\n",
      "Loss_Q: [2.09 2.18 1.12 0.   5.39] Loss_P: [ 3.89  2.32  2.78  1.23 10.22]\n",
      "Loss_Q: [2.06 2.19 1.16 0.   5.41] Loss_P: [ 3.89  2.3   2.71  1.21 10.1 ]\n",
      "Loss_Q: [2.09 2.21 1.16 0.   5.46] Loss_P: [ 3.92  2.38  2.74  1.21 10.24]\n",
      "Loss_Q: [2.04 2.19 1.17 0.   5.4 ] Loss_P: [ 3.83  2.46  2.75  1.19 10.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.11 2.19 1.15 0.   5.45] Loss_P: [ 3.84  2.38  2.72  1.18 10.12]\n",
      "Loss_Q: [2.18 2.2  1.14 0.   5.52] Loss_P: [ 3.84  2.47  2.73  1.17 10.21]\n",
      "Loss_Q: [2.13 2.21 1.12 0.   5.46] Loss_P: [ 3.79  2.4   2.75  1.17 10.12]\n",
      "Loss_Q: [2.07 2.22 1.15 0.   5.43] Loss_P: [ 3.86  2.42  2.76  1.19 10.23]\n",
      "Loss_Q: [2.05 2.23 1.12 0.   5.4 ] Loss_P: [ 3.8   2.45  2.75  1.2  10.2 ]\n",
      "Loss_Q: [2.14 2.23 1.11 0.   5.48] Loss_P: [ 3.87  2.35  2.76  1.17 10.15]\n",
      "Loss_Q: [2.02 2.23 1.11 0.   5.37] Loss_P: [ 3.86  2.31  2.73  1.16 10.07]\n",
      "Loss_Q: [2.19 2.23 1.09 0.   5.51] Loss_P: [ 3.85  2.36  2.78  1.21 10.19]\n",
      "Loss_Q: [2.17 2.27 1.14 0.   5.58] Loss_P: [ 3.86  2.39  2.74  1.14 10.13]\n",
      "Loss_Q: [2.07 2.31 1.14 0.   5.52] Loss_P: [ 3.85  2.43  2.81  1.18 10.27]\n",
      "Loss_Q: [2.05 2.21 1.07 0.   5.33] Loss_P: [ 3.86  2.43  2.77  1.2  10.27]\n",
      "Loss_Q: [2.06 2.28 1.19 0.   5.53] Loss_P: [ 3.86  2.29  2.73  1.22 10.1 ]\n",
      "Loss_Q: [2.16 2.16 1.16 0.   5.48] Loss_P: [ 3.85  2.46  2.72  1.23 10.26]\n",
      "Loss_Q: [2.13 2.26 1.14 0.   5.53] Loss_P: [ 3.84  2.38  2.8   1.24 10.25]\n",
      "Loss_Q: [2.1  2.19 1.21 0.   5.5 ] Loss_P: [ 3.87  2.41  2.76  1.26 10.3 ]\n",
      "Loss_Q: [2.1  2.24 1.17 0.   5.52] Loss_P: [ 3.85  2.35  2.78  1.24 10.21]\n",
      "Loss_Q: [2.06 2.27 1.14 0.   5.47] Loss_P: [ 3.83  2.43  2.77  1.25 10.28]\n",
      "Loss_Q: [2.15 2.29 1.16 0.   5.61] Loss_P: [ 3.91  2.35  2.82  1.29 10.36]\n",
      "Loss_Q: [2.05 2.23 1.17 0.   5.45] Loss_P: [ 3.88  2.28  2.78  1.28 10.23]\n",
      "Loss_Q: [2.02 2.3  1.18 0.   5.49] Loss_P: [ 3.95  2.25  2.78  1.26 10.24]\n",
      "Loss_Q: [1.96 2.27 1.15 0.   5.38] Loss_P: [ 3.85  2.32  2.77  1.27 10.21]\n",
      "Loss_Q: [2.12 2.23 1.18 0.   5.53] Loss_P: [ 3.89  2.33  2.75  1.27 10.24]\n",
      "Loss_Q: [2.09 2.25 1.23 0.   5.57] Loss_P: [ 3.94  2.33  2.79  1.28 10.33]\n",
      "Loss_Q: [2.11 2.22 1.19 0.   5.51] Loss_P: [ 3.88  2.37  2.81  1.26 10.32]\n",
      "Loss_Q: [2.03 2.31 1.21 0.   5.56] Loss_P: [ 3.82  2.33  2.78  1.27 10.2 ]\n",
      "Loss_Q: [2.03 2.32 1.13 0.   5.47] Loss_P: [ 3.87  2.3   2.77  1.24 10.18]\n",
      "Loss_Q: [2.01 2.27 1.2  0.   5.49] Loss_P: [ 3.86  2.32  2.78  1.26 10.23]\n",
      "Loss_Q: [2.14 2.19 1.16 0.   5.49] Loss_P: [ 3.87  2.31  2.8   1.25 10.23]\n",
      "Loss_Q: [2.08 2.25 1.18 0.   5.51] Loss_P: [ 3.87  2.34  2.76  1.24 10.21]\n",
      "Loss_Q: [2.13 2.25 1.17 0.   5.56] Loss_P: [ 3.87  2.34  2.79  1.27 10.27]\n",
      "Loss_Q: [2.1  2.25 1.15 0.   5.51] Loss_P: [ 3.88  2.41  2.83  1.25 10.36]\n",
      "Loss_Q: [2.08 2.29 1.14 0.   5.51] Loss_P: [ 3.84  2.33  2.79  1.22 10.19]\n",
      "Loss_Q: [2.11 2.26 1.12 0.   5.49] Loss_P: [ 3.8   2.44  2.8   1.23 10.27]\n",
      "Loss_Q: [2.2  2.22 1.19 0.   5.62] Loss_P: [ 3.9   2.41  2.79  1.22 10.31]\n",
      "Loss_Q: [2.1  2.22 1.13 0.   5.45] Loss_P: [ 3.8   2.46  2.77  1.23 10.26]\n",
      "Loss_Q: [2.14 2.26 1.18 0.   5.58] Loss_P: [ 3.79  2.5   2.79  1.23 10.31]\n",
      "Loss_Q: [2.09 2.22 1.16 0.   5.47] Loss_P: [ 3.88  2.43  2.8   1.25 10.36]\n",
      "Loss_Q: [2.18 2.25 1.19 0.   5.62] Loss_P: [ 3.78  2.52  2.83  1.3  10.42]\n",
      "Loss_Q: [2.2  2.18 1.16 0.   5.53] Loss_P: [ 3.88  2.47  2.77  1.28 10.4 ]\n",
      "Loss_Q: [2.24 2.32 1.19 0.   5.75] Loss_P: [ 3.85  2.49  2.78  1.26 10.39]\n",
      "Loss_Q: [2.2  2.24 1.2  0.   5.64] Loss_P: [ 3.9   2.52  2.8   1.28 10.49]\n",
      "Loss_Q: [2.21 2.24 1.22 0.   5.67] Loss_P: [ 3.8   2.46  2.75  1.26 10.27]\n",
      "Loss_Q: [2.12 2.26 1.2  0.   5.58] Loss_P: [ 3.82  2.48  2.78  1.31 10.39]\n",
      "Loss_Q: [2.22 2.26 1.24 0.   5.72] Loss_P: [ 3.86  2.47  2.79  1.31 10.43]\n",
      "Loss_Q: [2.21 2.3  1.25 0.   5.75] Loss_P: [ 3.84  2.46  2.76  1.35 10.41]\n",
      "Loss_Q: [2.24 2.22 1.26 0.   5.72] Loss_P: [ 3.87  2.46  2.8   1.34 10.48]\n",
      "Loss_Q: [2.14 2.29 1.28 0.   5.71] Loss_P: [ 3.94  2.46  2.83  1.35 10.58]\n",
      "Loss_Q: [2.18 2.25 1.32 0.   5.75] Loss_P: [ 3.79  2.51  2.82  1.38 10.49]\n",
      "Loss_Q: [2.14 2.26 1.27 0.   5.68] Loss_P: [ 3.86  2.47  2.82  1.36 10.51]\n",
      "Loss_Q: [2.2  2.32 1.28 0.   5.81] Loss_P: [ 3.82  2.44  2.83  1.38 10.46]\n",
      "Loss_Q: [2.25 2.3  1.31 0.   5.86] Loss_P: [ 3.9   2.43  2.8   1.4  10.52]\n",
      "Loss_Q: [2.23 2.3  1.28 0.   5.81] Loss_P: [ 3.8   2.52  2.81  1.4  10.53]\n",
      "Loss_Q: [2.2  2.21 1.29 0.   5.71] Loss_P: [ 3.85  2.38  2.79  1.37 10.4 ]\n",
      "Loss_Q: [2.14 2.27 1.27 0.   5.67] Loss_P: [ 3.83  2.43  2.81  1.37 10.44]\n",
      "Loss_Q: [2.19 2.25 1.31 0.   5.75] Loss_P: [ 3.86  2.48  2.84  1.4  10.59]\n",
      "Loss_Q: [2.15 2.27 1.3  0.   5.72] Loss_P: [ 3.87  2.41  2.83  1.41 10.52]\n",
      "Loss_Q: [2.22 2.28 1.32 0.   5.82] Loss_P: [ 3.83  2.45  2.82  1.41 10.51]\n",
      "Loss_Q: [2.23 2.23 1.32 0.   5.78] Loss_P: [ 3.78  2.47  2.81  1.41 10.47]\n",
      "Loss_Q: [2.27 2.22 1.32 0.   5.81] Loss_P: [ 3.8   2.47  2.83  1.4  10.5 ]\n",
      "Loss_Q: [2.18 2.29 1.29 0.   5.77] Loss_P: [ 3.85  2.44  2.81  1.39 10.49]\n",
      "Loss_Q: [2.22 2.22 1.28 0.   5.72] Loss_P: [ 3.83  2.49  2.81  1.4  10.53]\n",
      "Loss_Q: [2.29 2.2  1.31 0.   5.8 ] Loss_P: [ 3.88  2.42  2.79  1.4  10.49]\n",
      "Loss_Q: [2.13 2.22 1.31 0.   5.67] Loss_P: [ 3.78  2.49  2.76  1.38 10.4 ]\n",
      "Loss_Q: [2.24 2.2  1.33 0.   5.77] Loss_P: [ 3.8   2.46  2.75  1.43 10.43]\n",
      "Loss_Q: [2.26 2.2  1.32 0.   5.78] Loss_P: [ 3.86  2.54  2.74  1.39 10.53]\n",
      "Loss_Q: [2.26 2.13 1.32 0.   5.71] Loss_P: [ 3.86  2.56  2.75  1.39 10.56]\n",
      "Loss_Q: [2.2  2.22 1.32 0.   5.75] Loss_P: [ 3.83  2.53  2.68  1.38 10.42]\n",
      "Loss_Q: [2.19 2.23 1.31 0.   5.73] Loss_P: [ 3.85  2.48  2.75  1.39 10.47]\n",
      "Loss_Q: [2.2  2.14 1.31 0.   5.66] Loss_P: [ 3.87  2.53  2.72  1.4  10.51]\n",
      "Loss_Q: [2.21 2.19 1.31 0.   5.7 ] Loss_P: [ 3.82  2.56  2.72  1.38 10.48]\n",
      "Loss_Q: [2.2  2.28 1.27 0.   5.75] Loss_P: [ 3.88  2.5   2.69  1.38 10.45]\n",
      "Loss_Q: [2.2  2.18 1.29 0.   5.67] Loss_P: [ 3.78  2.54  2.74  1.38 10.43]\n",
      "Loss_Q: [2.2  2.2  1.26 0.   5.66] Loss_P: [ 3.85  2.57  2.68  1.39 10.49]\n",
      "Loss_Q: [2.23 2.14 1.26 0.   5.62] Loss_P: [ 3.78  2.55  2.71  1.36 10.4 ]\n",
      "Loss_Q: [2.18 2.16 1.23 0.   5.57] Loss_P: [ 3.77  2.49  2.72  1.34 10.32]\n",
      "Loss_Q: [2.21 2.21 1.28 0.   5.7 ] Loss_P: [ 3.8   2.46  2.71  1.35 10.33]\n",
      "Loss_Q: [2.19 2.17 1.26 0.   5.61] Loss_P: [ 3.76  2.53  2.76  1.33 10.38]\n",
      "Loss_Q: [2.15 2.24 1.26 0.   5.65] Loss_P: [ 3.8   2.53  2.73  1.36 10.43]\n",
      "Loss_Q: [2.16 2.19 1.25 0.   5.59] Loss_P: [ 3.83  2.51  2.76  1.38 10.48]\n",
      "Loss_Q: [2.25 2.24 1.27 0.   5.77] Loss_P: [ 3.81  2.47  2.78  1.35 10.41]\n",
      "Loss_Q: [2.13 2.25 1.26 0.   5.64] Loss_P: [ 3.79  2.56  2.75  1.34 10.43]\n",
      "Loss_Q: [2.21 2.23 1.22 0.   5.67] Loss_P: [ 3.78  2.54  2.73  1.3  10.35]\n",
      "Loss_Q: [2.19 2.23 1.2  0.   5.62] Loss_P: [ 3.83  2.55  2.71  1.32 10.41]\n",
      "Loss_Q: [2.22 2.22 1.28 0.   5.72] Loss_P: [ 3.84  2.49  2.7   1.34 10.37]\n",
      "Loss_Q: [2.26 2.2  1.24 0.   5.69] Loss_P: [ 3.78  2.59  2.78  1.36 10.51]\n",
      "Loss_Q: [2.24 2.22 1.28 0.   5.74] Loss_P: [ 3.81  2.53  2.78  1.35 10.47]\n",
      "Loss_Q: [2.2  2.26 1.25 0.   5.7 ] Loss_P: [ 3.79  2.54  2.79  1.32 10.43]\n",
      "Loss_Q: [2.17 2.27 1.22 0.   5.66] Loss_P: [ 3.84  2.52  2.79  1.32 10.48]\n",
      "Loss_Q: [2.22 2.22 1.24 0.   5.69] Loss_P: [ 3.85  2.55  2.81  1.3  10.5 ]\n",
      "Loss_Q: [2.22 2.27 1.25 0.   5.74] Loss_P: [ 3.84  2.47  2.83  1.33 10.46]\n",
      "Loss_Q: [2.19 2.32 1.23 0.   5.73] Loss_P: [ 3.81  2.52  2.77  1.33 10.43]\n",
      "Loss_Q: [2.31 2.25 1.22 0.   5.78] Loss_P: [ 3.78  2.54  2.74  1.33 10.39]\n",
      "Loss_Q: [2.19 2.25 1.23 0.   5.66] Loss_P: [ 3.81  2.54  2.74  1.31 10.39]\n",
      "Loss_Q: [2.22 2.28 1.23 0.   5.72] Loss_P: [ 3.82  2.43  2.75  1.32 10.32]\n",
      "Loss_Q: [2.27 2.25 1.27 0.   5.79] Loss_P: [ 3.8   2.57  2.76  1.37 10.5 ]\n",
      "Loss_Q: [2.28 2.21 1.26 0.   5.75] Loss_P: [ 3.79  2.53  2.81  1.3  10.43]\n",
      "Loss_Q: [2.24 2.31 1.24 0.   5.79] Loss_P: [ 3.79  2.59  2.75  1.33 10.45]\n",
      "Loss_Q: [2.21 2.23 1.26 0.   5.71] Loss_P: [ 3.84  2.51  2.75  1.35 10.45]\n",
      "Loss_Q: [2.28 2.21 1.26 0.   5.75] Loss_P: [ 3.86  2.47  2.74  1.34 10.41]\n",
      "Loss_Q: [2.21 2.23 1.21 0.   5.65] Loss_P: [ 3.9   2.5   2.75  1.32 10.46]\n",
      "Loss_Q: [2.24 2.22 1.23 0.   5.68] Loss_P: [ 3.87  2.45  2.75  1.35 10.42]\n",
      "Loss_Q: [2.23 2.21 1.25 0.   5.69] Loss_P: [ 3.86  2.54  2.72  1.33 10.45]\n",
      "Loss_Q: [2.23 2.25 1.27 0.   5.75] Loss_P: [ 3.82  2.5   2.71  1.35 10.38]\n",
      "Loss_Q: [2.2  2.26 1.19 0.   5.64] Loss_P: [ 3.8   2.58  2.7   1.34 10.42]\n",
      "Loss_Q: [2.26 2.21 1.23 0.   5.71] Loss_P: [ 3.83  2.58  2.73  1.36 10.5 ]\n",
      "Loss_Q: [2.29 2.24 1.26 0.   5.79] Loss_P: [ 3.89  2.54  2.73  1.36 10.51]\n",
      "Loss_Q: [2.32 2.27 1.26 0.   5.85] Loss_P: [ 3.83  2.52  2.74  1.37 10.46]\n",
      "Loss_Q: [2.29 2.22 1.25 0.   5.76] Loss_P: [ 3.89  2.51  2.74  1.36 10.5 ]\n",
      "Loss_Q: [2.27 2.18 1.28 0.   5.72] Loss_P: [ 3.87  2.6   2.74  1.4  10.61]\n",
      "Loss_Q: [2.24 2.24 1.33 0.   5.8 ] Loss_P: [ 3.92  2.46  2.74  1.38 10.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.25 2.24 1.3  0.   5.79] Loss_P: [ 3.83  2.54  2.74  1.4  10.51]\n",
      "Loss_Q: [2.26 2.17 1.28 0.   5.7 ] Loss_P: [ 3.89  2.47  2.73  1.37 10.46]\n",
      "Loss_Q: [2.21 2.28 1.28 0.   5.77] Loss_P: [ 3.94  2.5   2.76  1.38 10.58]\n",
      "Loss_Q: [2.23 2.24 1.29 0.   5.76] Loss_P: [ 3.9   2.52  2.74  1.35 10.51]\n",
      "Loss_Q: [2.25 2.22 1.27 0.   5.75] Loss_P: [ 3.93  2.51  2.77  1.36 10.56]\n",
      "Loss_Q: [2.19 2.26 1.22 0.   5.68] Loss_P: [ 3.81  2.49  2.74  1.35 10.38]\n",
      "Loss_Q: [2.19 2.24 1.24 0.   5.68] Loss_P: [ 3.91  2.46  2.75  1.35 10.46]\n",
      "Loss_Q: [2.18 2.21 1.2  0.   5.6 ] Loss_P: [ 3.9   2.48  2.74  1.33 10.44]\n",
      "Loss_Q: [2.25 2.16 1.2  0.   5.61] Loss_P: [ 3.78  2.44  2.77  1.33 10.32]\n",
      "Loss_Q: [2.18 2.24 1.19 0.   5.61] Loss_P: [ 3.91  2.45  2.77  1.28 10.41]\n",
      "Loss_Q: [2.23 2.28 1.2  0.   5.71] Loss_P: [ 3.89  2.41  2.76  1.32 10.39]\n",
      "Loss_Q: [2.18 2.21 1.23 0.   5.62] Loss_P: [ 3.82  2.51  2.75  1.31 10.39]\n",
      "Loss_Q: [2.19 2.28 1.19 0.   5.66] Loss_P: [ 3.85  2.41  2.76  1.29 10.32]\n",
      "Loss_Q: [2.25 2.23 1.19 0.   5.67] Loss_P: [ 3.96  2.41  2.78  1.29 10.44]\n",
      "Loss_Q: [2.25 2.25 1.19 0.   5.69] Loss_P: [ 3.84  2.47  2.82  1.27 10.4 ]\n",
      "Loss_Q: [2.26 2.23 1.24 0.   5.72] Loss_P: [ 3.81  2.52  2.81  1.32 10.46]\n",
      "Loss_Q: [2.23 2.29 1.27 0.   5.79] Loss_P: [ 3.84  2.44  2.82  1.32 10.42]\n",
      "Loss_Q: [2.2  2.24 1.3  0.   5.74] Loss_P: [ 3.77  2.55  2.78  1.39 10.49]\n",
      "Loss_Q: [2.22 2.27 1.28 0.   5.77] Loss_P: [ 3.79  2.55  2.81  1.38 10.53]\n",
      "Loss_Q: [2.2  2.22 1.3  0.   5.72] Loss_P: [ 3.76  2.53  2.8   1.41 10.49]\n",
      "Loss_Q: [2.23 2.25 1.31 0.   5.79] Loss_P: [ 3.83  2.47  2.8   1.41 10.5 ]\n",
      "Loss_Q: [2.24 2.21 1.26 0.   5.72] Loss_P: [ 3.87  2.43  2.76  1.38 10.44]\n",
      "Loss_Q: [2.16 2.22 1.3  0.   5.68] Loss_P: [ 3.79  2.47  2.76  1.36 10.38]\n",
      "Loss_Q: [2.13 2.2  1.31 0.   5.64] Loss_P: [ 3.8   2.54  2.75  1.4  10.49]\n",
      "Loss_Q: [2.25 2.21 1.28 0.   5.75] Loss_P: [ 3.83  2.48  2.72  1.38 10.42]\n",
      "Loss_Q: [2.14 2.2  1.28 0.   5.62] Loss_P: [ 3.71  2.46  2.77  1.37 10.3 ]\n",
      "Loss_Q: [2.22 2.28 1.3  0.   5.8 ] Loss_P: [ 3.85  2.42  2.76  1.37 10.4 ]\n",
      "Loss_Q: [2.13 2.21 1.28 0.   5.62] Loss_P: [ 3.75  2.48  2.74  1.38 10.35]\n",
      "Loss_Q: [2.13 2.24 1.26 0.   5.64] Loss_P: [ 3.85  2.43  2.78  1.38 10.43]\n",
      "Loss_Q: [2.16 2.22 1.25 0.   5.62] Loss_P: [ 3.82  2.4   2.73  1.39 10.34]\n",
      "Loss_Q: [2.06 2.22 1.34 0.   5.62] Loss_P: [ 3.79  2.45  2.78  1.43 10.45]\n",
      "Loss_Q: [2.1  2.25 1.31 0.   5.66] Loss_P: [ 3.85  2.39  2.74  1.42 10.41]\n",
      "Loss_Q: [2.08 2.21 1.34 0.   5.64] Loss_P: [ 3.85  2.39  2.78  1.43 10.44]\n",
      "Loss_Q: [2.13 2.16 1.39 0.   5.68] Loss_P: [ 3.77  2.43  2.77  1.43 10.4 ]\n",
      "Loss_Q: [2.15 2.2  1.37 0.   5.72] Loss_P: [ 3.85  2.42  2.74  1.44 10.44]\n",
      "Loss_Q: [2.13 2.26 1.35 0.   5.75] Loss_P: [ 3.78  2.4   2.77  1.45 10.4 ]\n",
      "Loss_Q: [2.19 2.21 1.37 0.   5.76] Loss_P: [ 3.83  2.43  2.8   1.43 10.5 ]\n",
      "Loss_Q: [2.17 2.23 1.33 0.   5.73] Loss_P: [ 3.78  2.41  2.8   1.43 10.42]\n",
      "Loss_Q: [2.22 2.24 1.35 0.   5.81] Loss_P: [ 3.77  2.41  2.81  1.4  10.4 ]\n",
      "Loss_Q: [2.26 2.21 1.32 0.   5.79] Loss_P: [ 3.87  2.53  2.86  1.41 10.67]\n",
      "Loss_Q: [2.2  2.26 1.34 0.   5.81] Loss_P: [ 3.66  2.43  2.84  1.44 10.37]\n",
      "Loss_Q: [2.25 2.23 1.35 0.   5.83] Loss_P: [ 3.77  2.44  2.81  1.42 10.44]\n",
      "Loss_Q: [2.24 2.27 1.31 0.   5.82] Loss_P: [ 3.8   2.51  2.82  1.4  10.53]\n",
      "Loss_Q: [2.28 2.2  1.33 0.   5.81] Loss_P: [ 3.81  2.49  2.76  1.42 10.48]\n",
      "Loss_Q: [2.25 2.19 1.35 0.   5.79] Loss_P: [ 3.81  2.47  2.8   1.43 10.51]\n",
      "Loss_Q: [2.25 2.24 1.31 0.   5.81] Loss_P: [ 3.8   2.45  2.81  1.4  10.46]\n",
      "Loss_Q: [2.18 2.24 1.32 0.   5.75] Loss_P: [ 3.79  2.48  2.8   1.41 10.49]\n",
      "Loss_Q: [2.16 2.25 1.29 0.   5.7 ] Loss_P: [ 3.8   2.47  2.8   1.41 10.47]\n",
      "Loss_Q: [2.15 2.21 1.32 0.   5.67] Loss_P: [ 3.8   2.47  2.79  1.41 10.47]\n",
      "Loss_Q: [2.24 2.22 1.32 0.   5.78] Loss_P: [ 3.81  2.42  2.82  1.42 10.48]\n",
      "Loss_Q: [2.25 2.22 1.33 0.   5.8 ] Loss_P: [ 3.85  2.43  2.81  1.43 10.52]\n",
      "Loss_Q: [2.19 2.23 1.32 0.   5.74] Loss_P: [ 3.88  2.43  2.82  1.42 10.54]\n",
      "Loss_Q: [2.28 2.19 1.34 0.   5.81] Loss_P: [ 3.87  2.42  2.79  1.44 10.52]\n",
      "Loss_Q: [2.28 2.18 1.32 0.   5.78] Loss_P: [ 3.79  2.45  2.76  1.4  10.4 ]\n",
      "Loss_Q: [2.26 2.16 1.32 0.   5.74] Loss_P: [ 3.86  2.39  2.74  1.4  10.39]\n",
      "Loss_Q: [2.16 2.18 1.31 0.   5.65] Loss_P: [ 3.83  2.42  2.75  1.4  10.4 ]\n",
      "Loss_Q: [2.23 2.15 1.35 0.   5.73] Loss_P: [ 3.81  2.45  2.74  1.39 10.4 ]\n",
      "Loss_Q: [2.1  2.24 1.31 0.   5.66] Loss_P: [ 3.82  2.43  2.76  1.38 10.39]\n",
      "Loss_Q: [2.15 2.16 1.32 0.   5.63] Loss_P: [ 3.9   2.43  2.77  1.36 10.45]\n",
      "Loss_Q: [2.21 2.2  1.31 0.   5.73] Loss_P: [ 3.85  2.36  2.78  1.39 10.38]\n",
      "Loss_Q: [2.05 2.25 1.29 0.   5.59] Loss_P: [ 3.8   2.43  2.77  1.43 10.42]\n",
      "Loss_Q: [2.11 2.27 1.31 0.   5.69] Loss_P: [ 3.87  2.42  2.74  1.41 10.44]\n",
      "Loss_Q: [2.15 2.22 1.34 0.   5.71] Loss_P: [ 3.87  2.4   2.78  1.39 10.44]\n",
      "Loss_Q: [2.15 2.22 1.28 0.   5.66] Loss_P: [ 3.82  2.4   2.8   1.38 10.4 ]\n",
      "Loss_Q: [2.13 2.2  1.29 0.   5.61] Loss_P: [ 3.82  2.41  2.78  1.36 10.36]\n",
      "Loss_Q: [2.19 2.27 1.28 0.   5.75] Loss_P: [ 3.82  2.48  2.82  1.39 10.51]\n",
      "Loss_Q: [2.09 2.26 1.25 0.   5.6 ] Loss_P: [ 3.81  2.47  2.77  1.37 10.43]\n",
      "Loss_Q: [2.16 2.19 1.27 0.   5.61] Loss_P: [ 3.88  2.4   2.74  1.34 10.35]\n",
      "Loss_Q: [2.05 2.18 1.25 0.   5.47] Loss_P: [ 3.84  2.39  2.77  1.32 10.33]\n",
      "Loss_Q: [2.2  2.23 1.28 0.   5.71] Loss_P: [ 3.86  2.4   2.8   1.34 10.4 ]\n",
      "Loss_Q: [2.13 2.22 1.2  0.   5.56] Loss_P: [ 3.86  2.4   2.77  1.34 10.37]\n",
      "Loss_Q: [2.06 2.25 1.19 0.   5.51] Loss_P: [ 3.93  2.23  2.8   1.34 10.31]\n",
      "Loss_Q: [2.05 2.22 1.21 0.   5.49] Loss_P: [ 3.86  2.4   2.77  1.28 10.31]\n",
      "Loss_Q: [2.07 2.21 1.2  0.   5.48] Loss_P: [ 3.89  2.39  2.79  1.29 10.36]\n",
      "Loss_Q: [2.14 2.19 1.23 0.   5.56] Loss_P: [ 3.87  2.38  2.78  1.29 10.33]\n",
      "Loss_Q: [2.17 2.2  1.17 0.   5.54] Loss_P: [ 3.81  2.39  2.76  1.28 10.24]\n",
      "Loss_Q: [2.12 2.24 1.2  0.   5.57] Loss_P: [ 3.79  2.42  2.75  1.26 10.22]\n",
      "Loss_Q: [2.16 2.23 1.2  0.   5.59] Loss_P: [ 3.81  2.39  2.73  1.28 10.21]\n",
      "Loss_Q: [2.14 2.18 1.21 0.   5.53] Loss_P: [ 3.9   2.45  2.79  1.28 10.41]\n",
      "Loss_Q: [2.15 2.23 1.21 0.   5.6 ] Loss_P: [ 3.89  2.47  2.76  1.3  10.43]\n",
      "Loss_Q: [2.19 2.24 1.24 0.   5.67] Loss_P: [ 3.86  2.5   2.73  1.3  10.4 ]\n",
      "Loss_Q: [2.1  2.25 1.24 0.   5.6 ] Loss_P: [ 3.82  2.45  2.8   1.29 10.36]\n",
      "Loss_Q: [2.07 2.32 1.22 0.   5.61] Loss_P: [ 3.83  2.45  2.76  1.29 10.33]\n",
      "Loss_Q: [2.16 2.18 1.19 0.   5.53] Loss_P: [ 3.76  2.46  2.82  1.3  10.34]\n",
      "Loss_Q: [2.15 2.19 1.23 0.   5.58] Loss_P: [ 3.84  2.49  2.8   1.29 10.41]\n",
      "Loss_Q: [2.16 2.24 1.18 0.   5.58] Loss_P: [ 3.85  2.51  2.78  1.24 10.38]\n",
      "Loss_Q: [2.24 2.27 1.25 0.   5.76] Loss_P: [ 3.9   2.42  2.76  1.29 10.37]\n",
      "Loss_Q: [2.17 2.25 1.24 0.   5.66] Loss_P: [ 3.88  2.44  2.79  1.31 10.41]\n",
      "Loss_Q: [2.16 2.25 1.23 0.   5.63] Loss_P: [ 3.89  2.42  2.76  1.34 10.41]\n",
      "Loss_Q: [2.16 2.24 1.24 0.   5.64] Loss_P: [ 3.96  2.38  2.76  1.33 10.42]\n",
      "Loss_Q: [2.2  2.22 1.28 0.   5.7 ] Loss_P: [ 3.84  2.5   2.75  1.36 10.45]\n",
      "Loss_Q: [2.18 2.27 1.28 0.   5.73] Loss_P: [ 3.85  2.43  2.82  1.37 10.46]\n",
      "Loss_Q: [2.14 2.19 1.24 0.   5.57] Loss_P: [ 3.9   2.36  2.79  1.36 10.41]\n",
      "Loss_Q: [2.17 2.2  1.23 0.   5.6 ] Loss_P: [ 3.86  2.36  2.75  1.32 10.29]\n",
      "Loss_Q: [2.13 2.28 1.26 0.   5.66] Loss_P: [ 3.97  2.39  2.74  1.37 10.47]\n",
      "Loss_Q: [2.11 2.29 1.18 0.   5.57] Loss_P: [ 3.91  2.32  2.79  1.29 10.32]\n",
      "Loss_Q: [2.16 2.28 1.21 0.   5.64] Loss_P: [ 3.99  2.39  2.77  1.31 10.46]\n",
      "Loss_Q: [2.09 2.24 1.24 0.   5.57] Loss_P: [ 3.87  2.41  2.78  1.3  10.35]\n",
      "Loss_Q: [2.07 2.28 1.21 0.   5.57] Loss_P: [ 3.91  2.36  2.79  1.32 10.38]\n",
      "Loss_Q: [2.08 2.26 1.28 0.   5.62] Loss_P: [ 3.87  2.33  2.8   1.32 10.33]\n",
      "Loss_Q: [2.21 2.22 1.26 0.   5.68] Loss_P: [ 3.95  2.37  2.78  1.34 10.44]\n",
      "Loss_Q: [2.12 2.21 1.2  0.   5.53] Loss_P: [ 3.86  2.45  2.8   1.3  10.41]\n",
      "Loss_Q: [2.16 2.23 1.27 0.   5.67] Loss_P: [ 3.94  2.39  2.8   1.36 10.49]\n",
      "Loss_Q: [2.14 2.26 1.25 0.   5.65] Loss_P: [ 3.9   2.43  2.81  1.33 10.47]\n",
      "Loss_Q: [2.21 2.28 1.26 0.   5.74] Loss_P: [ 3.86  2.48  2.8   1.3  10.43]\n",
      "Loss_Q: [2.16 2.29 1.23 0.   5.68] Loss_P: [ 3.86  2.35  2.78  1.36 10.35]\n",
      "Loss_Q: [2.17 2.22 1.25 0.   5.64] Loss_P: [ 3.86  2.4   2.78  1.32 10.36]\n",
      "Loss_Q: [2.2  2.19 1.24 0.   5.63] Loss_P: [ 3.91  2.33  2.75  1.37 10.35]\n",
      "Loss_Q: [2.18 2.3  1.22 0.   5.69] Loss_P: [ 3.88  2.45  2.81  1.32 10.45]\n",
      "Loss_Q: [2.26 2.21 1.23 0.   5.7 ] Loss_P: [ 3.76  2.44  2.81  1.3  10.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.22 2.21 1.22 0.   5.65] Loss_P: [ 3.84  2.36  2.82  1.31 10.33]\n",
      "Loss_Q: [2.19 2.23 1.19 0.   5.6 ] Loss_P: [ 3.94  2.41  2.82  1.3  10.46]\n",
      "Loss_Q: [2.19 2.2  1.22 0.   5.61] Loss_P: [ 3.83  2.47  2.79  1.3  10.39]\n",
      "Loss_Q: [2.18 2.23 1.25 0.   5.67] Loss_P: [ 3.87  2.33  2.81  1.3  10.31]\n",
      "Loss_Q: [2.19 2.27 1.23 0.   5.69] Loss_P: [ 3.86  2.42  2.86  1.31 10.44]\n",
      "Loss_Q: [2.14 2.22 1.22 0.   5.58] Loss_P: [ 3.88  2.37  2.8   1.31 10.36]\n",
      "Loss_Q: [2.17 2.21 1.24 0.   5.62] Loss_P: [ 3.84  2.45  2.83  1.3  10.43]\n",
      "Loss_Q: [2.22 2.26 1.17 0.   5.65] Loss_P: [ 3.84  2.44  2.79  1.3  10.36]\n",
      "Loss_Q: [2.24 2.24 1.22 0.   5.7 ] Loss_P: [ 3.8   2.44  2.85  1.28 10.38]\n",
      "Loss_Q: [2.18 2.27 1.19 0.   5.64] Loss_P: [ 3.81  2.47  2.83  1.26 10.36]\n",
      "Loss_Q: [2.17 2.3  1.2  0.   5.67] Loss_P: [ 3.86  2.42  2.85  1.3  10.43]\n",
      "Loss_Q: [2.19 2.26 1.18 0.   5.63] Loss_P: [ 3.93  2.41  2.86  1.27 10.47]\n",
      "Loss_Q: [2.23 2.22 1.19 0.   5.63] Loss_P: [ 3.85  2.46  2.82  1.31 10.45]\n",
      "Loss_Q: [2.27 2.27 1.24 0.   5.78] Loss_P: [ 3.84  2.45  2.86  1.25 10.41]\n",
      "Loss_Q: [2.17 2.3  1.17 0.   5.64] Loss_P: [ 3.91  2.46  2.83  1.29 10.49]\n",
      "Loss_Q: [2.23 2.29 1.21 0.   5.72] Loss_P: [ 3.79  2.36  2.83  1.26 10.24]\n",
      "Loss_Q: [2.17 2.22 1.23 0.   5.62] Loss_P: [ 3.85  2.4   2.81  1.29 10.35]\n",
      "Loss_Q: [2.14 2.32 1.18 0.   5.64] Loss_P: [ 3.84  2.42  2.83  1.29 10.37]\n",
      "Loss_Q: [2.19 2.21 1.23 0.   5.64] Loss_P: [ 3.81  2.43  2.8   1.3  10.33]\n",
      "Loss_Q: [2.13 2.31 1.22 0.   5.65] Loss_P: [ 3.87  2.44  2.84  1.32 10.48]\n",
      "Loss_Q: [2.23 2.25 1.25 0.   5.73] Loss_P: [ 3.85  2.44  2.8   1.32 10.42]\n",
      "Loss_Q: [2.18 2.26 1.25 0.   5.69] Loss_P: [ 3.87  2.47  2.8   1.32 10.46]\n",
      "Loss_Q: [2.2  2.2  1.25 0.   5.65] Loss_P: [ 3.87  2.37  2.83  1.35 10.43]\n",
      "Loss_Q: [2.19 2.22 1.23 0.   5.65] Loss_P: [ 3.89  2.43  2.82  1.31 10.45]\n",
      "Loss_Q: [2.13 2.29 1.24 0.   5.66] Loss_P: [ 3.95  2.42  2.81  1.35 10.53]\n",
      "Loss_Q: [2.25 2.24 1.24 0.   5.74] Loss_P: [ 3.85  2.45  2.84  1.31 10.46]\n",
      "Loss_Q: [2.27 2.26 1.2  0.   5.72] Loss_P: [ 3.86  2.4   2.79  1.34 10.4 ]\n",
      "Loss_Q: [2.21 2.24 1.21 0.   5.66] Loss_P: [ 3.95  2.42  2.84  1.33 10.54]\n",
      "Loss_Q: [2.22 2.22 1.24 0.   5.69] Loss_P: [ 3.94  2.39  2.83  1.35 10.5 ]\n",
      "Loss_Q: [2.19 2.22 1.21 0.   5.61] Loss_P: [ 3.84  2.5   2.86  1.32 10.51]\n",
      "Loss_Q: [2.16 2.25 1.24 0.   5.66] Loss_P: [ 3.81  2.43  2.86  1.3  10.4 ]\n",
      "Loss_Q: [2.17 2.24 1.2  0.   5.61] Loss_P: [ 3.89  2.38  2.86  1.32 10.45]\n",
      "Loss_Q: [2.19 2.25 1.21 0.   5.65] Loss_P: [ 3.87  2.37  2.87  1.29 10.41]\n",
      "Loss_Q: [2.17 2.24 1.22 0.   5.62] Loss_P: [ 3.85  2.35  2.82  1.32 10.34]\n",
      "Loss_Q: [2.18 2.19 1.22 0.   5.59] Loss_P: [ 3.85  2.38  2.83  1.3  10.36]\n",
      "Loss_Q: [2.16 2.23 1.2  0.   5.58] Loss_P: [ 3.8   2.46  2.84  1.27 10.37]\n",
      "Loss_Q: [2.25 2.23 1.22 0.   5.71] Loss_P: [ 3.83  2.42  2.8   1.33 10.37]\n",
      "Loss_Q: [2.25 2.19 1.26 0.   5.7 ] Loss_P: [ 3.9   2.37  2.82  1.32 10.4 ]\n",
      "Loss_Q: [2.29 2.24 1.25 0.   5.78] Loss_P: [ 3.92  2.42  2.81  1.32 10.47]\n",
      "Loss_Q: [2.21 2.21 1.25 0.   5.67] Loss_P: [ 3.85  2.35  2.79  1.34 10.34]\n",
      "Loss_Q: [2.25 2.25 1.22 0.   5.72] Loss_P: [ 3.81  2.44  2.83  1.33 10.41]\n",
      "Loss_Q: [2.19 2.23 1.23 0.   5.65] Loss_P: [ 3.9   2.38  2.8   1.34 10.42]\n",
      "Loss_Q: [2.29 2.24 1.26 0.   5.79] Loss_P: [ 3.9   2.44  2.81  1.33 10.49]\n",
      "Loss_Q: [2.21 2.29 1.21 0.   5.7 ] Loss_P: [ 3.82  2.47  2.79  1.33 10.4 ]\n",
      "Loss_Q: [2.25 2.22 1.23 0.   5.7 ] Loss_P: [ 3.88  2.45  2.81  1.32 10.45]\n",
      "Loss_Q: [2.26 2.17 1.21 0.   5.65] Loss_P: [ 3.89  2.45  2.8   1.32 10.47]\n",
      "Loss_Q: [2.2  2.24 1.23 0.   5.67] Loss_P: [ 3.85  2.45  2.78  1.34 10.41]\n",
      "Loss_Q: [2.2  2.18 1.24 0.   5.61] Loss_P: [ 3.93  2.43  2.81  1.31 10.48]\n",
      "Loss_Q: [2.18 2.27 1.24 0.   5.69] Loss_P: [ 3.86  2.36  2.8   1.32 10.34]\n",
      "Loss_Q: [2.2  2.27 1.2  0.   5.66] Loss_P: [ 3.85  2.45  2.81  1.33 10.44]\n",
      "Loss_Q: [2.24 2.26 1.26 0.   5.77] Loss_P: [ 3.88  2.38  2.78  1.35 10.4 ]\n",
      "Loss_Q: [2.23 2.21 1.24 0.   5.68] Loss_P: [ 3.9   2.37  2.8   1.34 10.4 ]\n",
      "Loss_Q: [2.15 2.3  1.21 0.   5.66] Loss_P: [ 3.84  2.44  2.84  1.33 10.45]\n",
      "Loss_Q: [2.23 2.28 1.25 0.   5.76] Loss_P: [ 3.98  2.4   2.83  1.32 10.53]\n",
      "Loss_Q: [2.19 2.26 1.27 0.   5.72] Loss_P: [ 3.86  2.41  2.83  1.33 10.43]\n",
      "Loss_Q: [2.18 2.24 1.2  0.   5.62] Loss_P: [ 3.92  2.35  2.82  1.33 10.42]\n",
      "Loss_Q: [2.15 2.29 1.24 0.   5.68] Loss_P: [ 3.8   2.43  2.85  1.32 10.41]\n",
      "Loss_Q: [2.19 2.29 1.25 0.   5.72] Loss_P: [ 3.92  2.43  2.82  1.34 10.51]\n",
      "Loss_Q: [2.24 2.26 1.27 0.   5.77] Loss_P: [ 3.85  2.45  2.85  1.33 10.48]\n",
      "Loss_Q: [2.21 2.21 1.26 0.   5.68] Loss_P: [ 3.9   2.43  2.84  1.32 10.49]\n",
      "Loss_Q: [2.26 2.24 1.26 0.   5.77] Loss_P: [ 3.88  2.48  2.79  1.38 10.54]\n",
      "Loss_Q: [2.25 2.27 1.29 0.   5.81] Loss_P: [ 3.86  2.46  2.82  1.32 10.46]\n",
      "Loss_Q: [2.23 2.28 1.24 0.   5.76] Loss_P: [ 3.83  2.41  2.84  1.31 10.4 ]\n",
      "Loss_Q: [2.19 2.26 1.23 0.   5.68] Loss_P: [ 3.84  2.52  2.8   1.29 10.45]\n",
      "Loss_Q: [2.28 2.31 1.2  0.   5.79] Loss_P: [ 3.87  2.4   2.81  1.31 10.4 ]\n",
      "Loss_Q: [2.27 2.23 1.24 0.   5.74] Loss_P: [ 3.8   2.51  2.8   1.35 10.46]\n",
      "Loss_Q: [2.16 2.27 1.22 0.   5.65] Loss_P: [ 3.89  2.48  2.79  1.32 10.47]\n",
      "Loss_Q: [2.19 2.26 1.24 0.   5.69] Loss_P: [ 3.91  2.5   2.79  1.32 10.52]\n",
      "Loss_Q: [2.15 2.3  1.22 0.   5.68] Loss_P: [ 3.89  2.39  2.81  1.32 10.42]\n",
      "Loss_Q: [2.11 2.24 1.22 0.   5.57] Loss_P: [ 3.91  2.44  2.8   1.3  10.45]\n",
      "Loss_Q: [2.23 2.31 1.19 0.   5.74] Loss_P: [ 3.85  2.45  2.82  1.34 10.46]\n",
      "Loss_Q: [2.2  2.27 1.24 0.   5.71] Loss_P: [ 3.93  2.38  2.79  1.32 10.42]\n",
      "Loss_Q: [2.27 2.27 1.27 0.   5.8 ] Loss_P: [ 3.95  2.4   2.82  1.35 10.51]\n",
      "Loss_Q: [2.22 2.29 1.21 0.   5.73] Loss_P: [ 3.84  2.41  2.83  1.3  10.39]\n",
      "Loss_Q: [2.11 2.24 1.23 0.   5.58] Loss_P: [ 3.94  2.34  2.82  1.32 10.43]\n",
      "Loss_Q: [2.17 2.28 1.27 0.   5.73] Loss_P: [ 3.93  2.39  2.82  1.33 10.47]\n",
      "Loss_Q: [2.07 2.28 1.23 0.   5.58] Loss_P: [ 3.86  2.35  2.83  1.34 10.37]\n",
      "Loss_Q: [2.23 2.24 1.26 0.   5.72] Loss_P: [ 3.84  2.4   2.83  1.37 10.44]\n",
      "Loss_Q: [2.09 2.28 1.28 0.   5.65] Loss_P: [ 3.93  2.34  2.83  1.38 10.47]\n",
      "Loss_Q: [2.23 2.24 1.28 0.   5.74] Loss_P: [ 3.88  2.32  2.83  1.31 10.34]\n",
      "Loss_Q: [2.15 2.19 1.26 0.   5.6 ] Loss_P: [ 3.89  2.33  2.78  1.34 10.34]\n",
      "Loss_Q: [2.13 2.21 1.26 0.   5.59] Loss_P: [ 3.82  2.36  2.79  1.32 10.28]\n",
      "Loss_Q: [2.17 2.26 1.27 0.   5.7 ] Loss_P: [ 3.82  2.3   2.8   1.35 10.26]\n",
      "Loss_Q: [2.09 2.25 1.23 0.   5.57] Loss_P: [ 3.88  2.32  2.83  1.35 10.38]\n",
      "Loss_Q: [2.12 2.3  1.23 0.   5.65] Loss_P: [ 3.9   2.3   2.83  1.33 10.36]\n",
      "Loss_Q: [2.1  2.19 1.27 0.   5.56] Loss_P: [ 3.83  2.38  2.8   1.35 10.36]\n",
      "Loss_Q: [2.14 2.23 1.26 0.   5.63] Loss_P: [ 3.86  2.32  2.83  1.33 10.35]\n",
      "Loss_Q: [2.16 2.22 1.25 0.   5.62] Loss_P: [ 3.85  2.31  2.87  1.32 10.34]\n",
      "Loss_Q: [2.09 2.23 1.22 0.   5.54] Loss_P: [ 3.84  2.37  2.83  1.33 10.37]\n",
      "Loss_Q: [2.12 2.3  1.23 0.   5.66] Loss_P: [ 3.9   2.39  2.8   1.36 10.46]\n",
      "Loss_Q: [2.1  2.22 1.25 0.   5.56] Loss_P: [ 3.8   2.39  2.82  1.31 10.33]\n",
      "Loss_Q: [2.09 2.27 1.22 0.   5.58] Loss_P: [ 3.81  2.34  2.81  1.33 10.29]\n",
      "Loss_Q: [2.17 2.26 1.23 0.   5.66] Loss_P: [ 3.83  2.35  2.8   1.34 10.32]\n",
      "Loss_Q: [2.16 2.23 1.23 0.   5.62] Loss_P: [ 3.89  2.37  2.79  1.35 10.41]\n",
      "Loss_Q: [2.11 2.18 1.27 0.   5.57] Loss_P: [ 3.9   2.37  2.82  1.33 10.42]\n",
      "Loss_Q: [2.07 2.27 1.26 0.   5.6 ] Loss_P: [ 3.85  2.39  2.82  1.34 10.39]\n",
      "Loss_Q: [2.09 2.26 1.25 0.   5.6 ] Loss_P: [ 3.85  2.4   2.8   1.37 10.42]\n",
      "Loss_Q: [2.17 2.31 1.27 0.   5.75] Loss_P: [ 3.86  2.44  2.81  1.37 10.49]\n",
      "Loss_Q: [2.21 2.26 1.2  0.   5.67] Loss_P: [ 3.9   2.42  2.8   1.34 10.46]\n",
      "Loss_Q: [2.22 2.22 1.26 0.   5.71] Loss_P: [ 3.85  2.45  2.81  1.35 10.46]\n",
      "Loss_Q: [2.26 2.18 1.28 0.   5.72] Loss_P: [ 3.87  2.35  2.81  1.33 10.36]\n",
      "Loss_Q: [2.21 2.28 1.16 0.   5.66] Loss_P: [ 3.88  2.43  2.8   1.33 10.43]\n",
      "Loss_Q: [2.23 2.22 1.24 0.   5.69] Loss_P: [ 3.86  2.45  2.82  1.33 10.45]\n",
      "Loss_Q: [2.26 2.26 1.22 0.   5.75] Loss_P: [ 3.88  2.43  2.82  1.33 10.46]\n",
      "Loss_Q: [2.24 2.2  1.22 0.   5.66] Loss_P: [ 3.82  2.36  2.79  1.31 10.28]\n",
      "Loss_Q: [2.2  2.21 1.23 0.   5.64] Loss_P: [ 3.92  2.38  2.84  1.31 10.45]\n",
      "Loss_Q: [2.21 2.18 1.21 0.   5.6 ] Loss_P: [ 3.79  2.4   2.76  1.33 10.27]\n",
      "Loss_Q: [2.14 2.11 1.21 0.   5.46] Loss_P: [ 3.81  2.44  2.81  1.31 10.38]\n",
      "Loss_Q: [2.31 2.23 1.26 0.   5.8 ] Loss_P: [ 3.84  2.46  2.81  1.31 10.41]\n",
      "Loss_Q: [2.2  2.24 1.19 0.   5.63] Loss_P: [ 3.8   2.42  2.8   1.31 10.32]\n",
      "Loss_Q: [2.24 2.26 1.16 0.   5.65] Loss_P: [ 3.87  2.38  2.81  1.28 10.34]\n",
      "Loss_Q: [2.18 2.25 1.22 0.   5.64] Loss_P: [ 3.89  2.44  2.78  1.3  10.41]\n",
      "Loss_Q: [2.25 2.2  1.22 0.   5.67] Loss_P: [ 3.85  2.44  2.79  1.33 10.41]\n",
      "Loss_Q: [2.26 2.19 1.2  0.   5.65] Loss_P: [ 3.88  2.45  2.81  1.3  10.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.19 2.23 1.2  0.   5.61] Loss_P: [ 3.8   2.35  2.81  1.3  10.25]\n",
      "Loss_Q: [2.17 2.19 1.18 0.   5.55] Loss_P: [ 3.89  2.44  2.79  1.31 10.43]\n",
      "Loss_Q: [2.21 2.19 1.19 0.   5.59] Loss_P: [ 3.85  2.47  2.77  1.3  10.4 ]\n",
      "Loss_Q: [2.1  2.22 1.18 0.   5.51] Loss_P: [ 3.82  2.37  2.78  1.3  10.26]\n",
      "Loss_Q: [2.08 2.2  1.19 0.   5.47] Loss_P: [ 3.87  2.36  2.83  1.3  10.36]\n",
      "Loss_Q: [2.22 2.21 1.15 0.   5.58] Loss_P: [ 3.82  2.39  2.8   1.24 10.25]\n",
      "Loss_Q: [2.19 2.21 1.17 0.   5.56] Loss_P: [ 3.89  2.35  2.77  1.28 10.29]\n",
      "Loss_Q: [2.2  2.21 1.13 0.   5.54] Loss_P: [ 3.86  2.35  2.78  1.27 10.26]\n",
      "Loss_Q: [2.19 2.24 1.13 0.   5.56] Loss_P: [ 3.83  2.38  2.79  1.25 10.24]\n",
      "Loss_Q: [2.13 2.19 1.15 0.   5.47] Loss_P: [ 3.9   2.31  2.75  1.24 10.21]\n",
      "Loss_Q: [2.19 2.16 1.15 0.   5.5 ] Loss_P: [ 3.92  2.31  2.77  1.24 10.24]\n",
      "Loss_Q: [2.15 2.22 1.07 0.   5.45] Loss_P: [ 3.93  2.25  2.74  1.24 10.16]\n",
      "Loss_Q: [2.15 2.22 1.12 0.   5.5 ] Loss_P: [ 3.93  2.24  2.76  1.24 10.16]\n",
      "Loss_Q: [2.19 2.17 1.2  0.   5.56] Loss_P: [ 3.83  2.37  2.78  1.26 10.24]\n",
      "Loss_Q: [2.18 2.17 1.14 0.   5.49] Loss_P: [ 3.92  2.32  2.71  1.25 10.19]\n",
      "Loss_Q: [2.13 2.19 1.17 0.   5.49] Loss_P: [ 3.95  2.29  2.75  1.2  10.2 ]\n",
      "Loss_Q: [2.17 2.16 1.2  0.   5.54] Loss_P: [ 3.83  2.32  2.74  1.26 10.16]\n",
      "Loss_Q: [2.13 2.19 1.18 0.   5.5 ] Loss_P: [ 3.86  2.3   2.73  1.25 10.15]\n",
      "Loss_Q: [2.14 2.17 1.12 0.   5.43] Loss_P: [ 3.94  2.29  2.73  1.31 10.26]\n",
      "Loss_Q: [2.1  2.21 1.17 0.   5.48] Loss_P: [ 3.83  2.31  2.74  1.28 10.17]\n",
      "Loss_Q: [2.13 2.17 1.15 0.   5.45] Loss_P: [ 3.88  2.34  2.75  1.24 10.21]\n",
      "Loss_Q: [2.11 2.16 1.18 0.   5.45] Loss_P: [ 3.91  2.32  2.75  1.27 10.25]\n",
      "Loss_Q: [2.11 2.13 1.21 0.   5.44] Loss_P: [ 3.89  2.26  2.74  1.27 10.17]\n",
      "Loss_Q: [2.26 2.18 1.18 0.   5.61] Loss_P: [ 3.86  2.35  2.74  1.27 10.22]\n",
      "Loss_Q: [2.12 2.17 1.14 0.   5.43] Loss_P: [ 3.8   2.37  2.75  1.28 10.19]\n",
      "Loss_Q: [2.1  2.19 1.18 0.   5.46] Loss_P: [ 3.81  2.35  2.75  1.27 10.18]\n",
      "Loss_Q: [2.11 2.17 1.14 0.   5.41] Loss_P: [ 3.85  2.3   2.74  1.27 10.16]\n",
      "Loss_Q: [2.12 2.2  1.15 0.   5.47] Loss_P: [ 3.86  2.25  2.75  1.23 10.09]\n",
      "Loss_Q: [2.09 2.15 1.14 0.   5.38] Loss_P: [ 3.88  2.25  2.76  1.25 10.13]\n",
      "Loss_Q: [2.17 2.14 1.13 0.   5.44] Loss_P: [ 3.81  2.34  2.73  1.25 10.14]\n",
      "Loss_Q: [2.17 2.16 1.09 0.   5.43] Loss_P: [ 3.86  2.34  2.69  1.27 10.16]\n",
      "Loss_Q: [2.15 2.15 1.14 0.   5.44] Loss_P: [ 3.76  2.45  2.69  1.26 10.16]\n",
      "Loss_Q: [2.13 2.12 1.1  0.   5.36] Loss_P: [ 3.89  2.35  2.71  1.26 10.22]\n",
      "Loss_Q: [2.19 2.17 1.09 0.   5.44] Loss_P: [ 3.94  2.27  2.67  1.23 10.11]\n",
      "Loss_Q: [2.05 2.19 1.09 0.   5.33] Loss_P: [ 3.8   2.35  2.68  1.22 10.06]\n",
      "Loss_Q: [2.14 2.18 1.12 0.   5.44] Loss_P: [ 3.84  2.31  2.7   1.22 10.07]\n",
      "Loss_Q: [2.15 2.19 1.14 0.   5.47] Loss_P: [ 3.84  2.25  2.73  1.2  10.01]\n",
      "Loss_Q: [2.17 2.14 1.08 0.   5.4 ] Loss_P: [ 3.82  2.34  2.75  1.18 10.1 ]\n",
      "Loss_Q: [2.09 2.17 1.06 0.   5.32] Loss_P: [ 3.79  2.46  2.72  1.21 10.18]\n",
      "Loss_Q: [2.12 2.15 1.11 0.   5.37] Loss_P: [ 3.82  2.33  2.75  1.15 10.05]\n",
      "Loss_Q: [2.1  2.13 1.04 0.   5.27] Loss_P: [ 3.86  2.32  2.72  1.2  10.11]\n",
      "Loss_Q: [2.1  2.13 1.05 0.   5.28] Loss_P: [ 3.77  2.32  2.74  1.18 10.01]\n",
      "Loss_Q: [2.2  2.16 1.09 0.   5.46] Loss_P: [ 3.73  2.38  2.77  1.21 10.09]\n",
      "Loss_Q: [2.19 2.14 1.12 0.   5.45] Loss_P: [ 3.84  2.36  2.77  1.18 10.15]\n",
      "Loss_Q: [2.15 2.13 1.09 0.   5.38] Loss_P: [ 3.79  2.33  2.74  1.22 10.08]\n",
      "Loss_Q: [2.08 2.14 1.05 0.   5.27] Loss_P: [ 3.83  2.3   2.73  1.18 10.04]\n",
      "Loss_Q: [2.11 2.13 1.05 0.   5.29] Loss_P: [3.85 2.26 2.72 1.15 9.99]\n",
      "Loss_Q: [2.16 2.1  1.04 0.   5.29] Loss_P: [3.85 2.28 2.73 1.13 9.99]\n",
      "Loss_Q: [2.09 2.16 1.05 0.   5.29] Loss_P: [3.8  2.29 2.73 1.15 9.98]\n",
      "Loss_Q: [2.07 2.12 1.06 0.   5.26] Loss_P: [ 3.83  2.3   2.69  1.18 10.01]\n",
      "Loss_Q: [2.23 2.06 1.06 0.   5.34] Loss_P: [ 3.89  2.28  2.72  1.19 10.07]\n",
      "Loss_Q: [2.11 2.08 1.02 0.   5.21] Loss_P: [ 3.81  2.28  2.71  1.21 10.01]\n",
      "Loss_Q: [2.09 2.11 1.09 0.   5.29] Loss_P: [ 3.78  2.42  2.68  1.14 10.02]\n",
      "Loss_Q: [2.05 2.16 0.97 0.   5.18] Loss_P: [ 3.88  2.27  2.75  1.16 10.05]\n",
      "Loss_Q: [2.11 2.12 1.03 0.   5.26] Loss_P: [3.83 2.24 2.67 1.16 9.91]\n",
      "Loss_Q: [2.09 2.12 1.07 0.   5.28] Loss_P: [3.82 2.26 2.68 1.13 9.89]\n",
      "Loss_Q: [2.07 2.09 1.04 0.   5.19] Loss_P: [3.88 2.28 2.68 1.11 9.94]\n",
      "Loss_Q: [2.07 2.09 1.07 0.   5.23] Loss_P: [3.92 2.26 2.68 1.1  9.95]\n",
      "Loss_Q: [2.01 2.14 1.03 0.   5.18] Loss_P: [3.78 2.22 2.7  1.12 9.82]\n",
      "Loss_Q: [2.02 2.06 1.02 0.   5.11] Loss_P: [3.78 2.26 2.69 1.12 9.85]\n",
      "Loss_Q: [2.11 2.16 1.03 0.   5.3 ] Loss_P: [3.82 2.26 2.7  1.11 9.89]\n",
      "Loss_Q: [2.01 2.11 1.06 0.   5.18] Loss_P: [3.78 2.22 2.72 1.11 9.83]\n",
      "Loss_Q: [2.05 2.15 1.06 0.   5.27] Loss_P: [3.76 2.27 2.73 1.15 9.91]\n",
      "Loss_Q: [2.05 2.15 1.03 0.   5.24] Loss_P: [3.76 2.22 2.71 1.08 9.78]\n",
      "Loss_Q: [1.92 2.15 1.05 0.   5.12] Loss_P: [3.82 2.26 2.73 1.12 9.92]\n",
      "Loss_Q: [2.09 2.09 0.99 0.   5.17] Loss_P: [3.83 2.23 2.7  1.09 9.84]\n",
      "Loss_Q: [2.02 2.18 1.   0.   5.2 ] Loss_P: [3.89 2.2  2.72 1.06 9.87]\n",
      "Loss_Q: [2.03 2.11 1.01 0.   5.16] Loss_P: [3.84 2.2  2.71 1.08 9.82]\n",
      "Loss_Q: [2.08 2.13 1.04 0.   5.25] Loss_P: [3.83 2.23 2.74 1.13 9.93]\n",
      "Loss_Q: [2.05 2.11 0.98 0.   5.13] Loss_P: [ 3.89  2.21  2.77  1.13 10.  ]\n",
      "Loss_Q: [2.13 2.12 1.03 0.   5.28] Loss_P: [3.8  2.25 2.76 1.1  9.91]\n",
      "Loss_Q: [2.04 2.1  1.02 0.   5.16] Loss_P: [3.86 2.24 2.74 1.11 9.95]\n",
      "Loss_Q: [2.1  2.14 1.03 0.   5.27] Loss_P: [ 3.85  2.27  2.78  1.14 10.03]\n",
      "Loss_Q: [2.1  2.16 1.03 0.   5.3 ] Loss_P: [3.79 2.3  2.76 1.15 9.99]\n",
      "Loss_Q: [2.09 2.13 1.1  0.   5.31] Loss_P: [ 3.87  2.27  2.71  1.15 10.01]\n",
      "Loss_Q: [2.11 2.13 1.12 0.   5.36] Loss_P: [3.79 2.23 2.77 1.19 9.98]\n",
      "Loss_Q: [2.07 2.14 1.08 0.   5.29] Loss_P: [ 3.89  2.25  2.7   1.16 10.01]\n",
      "Loss_Q: [2.09 2.14 1.05 0.   5.28] Loss_P: [ 3.87  2.24  2.74  1.17 10.02]\n",
      "Loss_Q: [2.13 2.04 1.05 0.   5.21] Loss_P: [3.81 2.32 2.66 1.12 9.92]\n",
      "Loss_Q: [2.11 2.03 1.05 0.   5.18] Loss_P: [3.9  2.19 2.7  1.12 9.92]\n",
      "Loss_Q: [2.1  2.06 1.06 0.   5.22] Loss_P: [3.82 2.27 2.69 1.13 9.91]\n",
      "Loss_Q: [2.08 2.05 1.08 0.   5.2 ] Loss_P: [3.8  2.2  2.67 1.15 9.81]\n",
      "Loss_Q: [2.1  2.09 1.07 0.   5.26] Loss_P: [3.79 2.26 2.65 1.12 9.82]\n",
      "Loss_Q: [2.05 2.11 1.03 0.   5.19] Loss_P: [3.8  2.26 2.69 1.13 9.87]\n",
      "Loss_Q: [2.01 2.17 1.02 0.   5.2 ] Loss_P: [3.8  2.19 2.69 1.08 9.77]\n",
      "Loss_Q: [2.05 2.12 0.99 0.   5.16] Loss_P: [3.82 2.21 2.71 1.11 9.86]\n",
      "Loss_Q: [2.06 2.07 1.01 0.   5.14] Loss_P: [3.88 2.21 2.72 1.1  9.92]\n",
      "Loss_Q: [2.12 2.14 1.   0.   5.26] Loss_P: [3.85 2.28 2.68 1.13 9.93]\n",
      "Loss_Q: [2.02 2.14 1.   0.   5.15] Loss_P: [3.84 2.26 2.72 1.12 9.94]\n",
      "Loss_Q: [2.02 2.19 1.   0.   5.21] Loss_P: [3.87 2.14 2.75 1.12 9.88]\n",
      "Loss_Q: [2.11 2.13 0.98 0.   5.22] Loss_P: [3.84 2.2  2.73 1.08 9.85]\n",
      "Loss_Q: [2.02 2.13 0.99 0.   5.13] Loss_P: [3.9  2.19 2.75 1.08 9.92]\n",
      "Loss_Q: [2.1  2.08 0.98 0.   5.17] Loss_P: [3.81 2.19 2.67 1.06 9.73]\n",
      "Loss_Q: [2.02 2.11 1.02 0.   5.15] Loss_P: [3.88 2.22 2.7  1.06 9.86]\n",
      "Loss_Q: [2.01 2.17 1.02 0.   5.2 ] Loss_P: [3.85 2.25 2.74 1.11 9.95]\n",
      "Loss_Q: [1.95 2.17 1.08 0.   5.2 ] Loss_P: [3.85 2.24 2.72 1.13 9.94]\n",
      "Loss_Q: [1.97 2.15 1.05 0.   5.16] Loss_P: [3.8  2.24 2.7  1.12 9.86]\n",
      "Loss_Q: [2.02 2.09 1.05 0.   5.17] Loss_P: [3.81 2.26 2.69 1.12 9.87]\n",
      "Loss_Q: [2.12 2.07 1.07 0.   5.27] Loss_P: [3.85 2.24 2.73 1.17 9.99]\n",
      "Loss_Q: [2.08 2.16 1.11 0.   5.35] Loss_P: [ 3.83  2.32  2.77  1.17 10.1 ]\n",
      "Loss_Q: [2.12 2.12 1.09 0.   5.34] Loss_P: [3.75 2.27 2.74 1.15 9.91]\n",
      "Loss_Q: [2.21 2.16 1.13 0.   5.5 ] Loss_P: [ 3.87  2.25  2.76  1.16 10.04]\n",
      "Loss_Q: [2.1  2.13 1.11 0.   5.34] Loss_P: [ 3.86  2.24  2.84  1.23 10.17]\n",
      "Loss_Q: [2.01 2.2  1.14 0.   5.35] Loss_P: [ 3.9   2.14  2.81  1.24 10.09]\n",
      "Loss_Q: [2.03 2.11 1.07 0.   5.2 ] Loss_P: [ 3.82  2.25  2.78  1.15 10.01]\n",
      "Loss_Q: [2.14 2.2  1.11 0.   5.46] Loss_P: [ 3.94  2.22  2.78  1.16 10.11]\n",
      "Loss_Q: [2.07 2.14 1.07 0.   5.28] Loss_P: [ 3.87  2.23  2.75  1.2  10.04]\n",
      "Loss_Q: [1.98 2.28 1.11 0.   5.37] Loss_P: [ 3.89  2.29  2.78  1.22 10.17]\n",
      "Loss_Q: [2.06 2.13 1.17 0.   5.36] Loss_P: [3.86 2.18 2.76 1.18 9.98]\n",
      "Loss_Q: [2.07 2.21 1.13 0.   5.41] Loss_P: [ 3.85  2.21  2.78  1.21 10.06]\n",
      "Loss_Q: [2.05 2.15 1.19 0.   5.39] Loss_P: [ 3.89  2.21  2.77  1.27 10.14]\n",
      "Loss_Q: [2.13 2.14 1.17 0.   5.44] Loss_P: [ 3.83  2.27  2.78  1.28 10.16]\n",
      "Loss_Q: [2.13 2.14 1.17 0.   5.44] Loss_P: [ 3.89  2.3   2.75  1.29 10.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.1  2.09 1.2  0.   5.39] Loss_P: [3.82 2.19 2.72 1.26 9.99]\n",
      "Loss_Q: [2.13 2.13 1.16 0.   5.41] Loss_P: [ 3.84  2.23  2.77  1.26 10.1 ]\n",
      "Loss_Q: [2.1  2.17 1.16 0.   5.43] Loss_P: [ 3.83  2.3   2.74  1.27 10.12]\n",
      "Loss_Q: [2.17 2.13 1.14 0.   5.43] Loss_P: [ 3.79  2.3   2.74  1.25 10.07]\n",
      "Loss_Q: [2.05 2.13 1.15 0.   5.33] Loss_P: [ 3.84  2.29  2.73  1.27 10.13]\n",
      "Loss_Q: [2.12 2.23 1.21 0.   5.56] Loss_P: [ 3.8   2.26  2.79  1.26 10.11]\n",
      "Loss_Q: [2.01 2.2  1.16 0.   5.37] Loss_P: [ 3.78  2.32  2.81  1.29 10.21]\n",
      "Loss_Q: [2.13 2.2  1.22 0.   5.54] Loss_P: [ 3.73  2.31  2.83  1.33 10.21]\n",
      "Loss_Q: [2.13 2.17 1.27 0.   5.57] Loss_P: [ 3.76  2.34  2.79  1.34 10.23]\n",
      "Loss_Q: [2.22 2.2  1.25 0.   5.67] Loss_P: [ 3.8   2.35  2.79  1.34 10.28]\n",
      "Loss_Q: [2.12 2.22 1.19 0.   5.52] Loss_P: [ 3.82  2.3   2.82  1.35 10.28]\n",
      "Loss_Q: [2.1  2.13 1.23 0.   5.46] Loss_P: [ 3.8   2.29  2.79  1.34 10.22]\n",
      "Loss_Q: [2.15 2.2  1.23 0.   5.58] Loss_P: [ 3.78  2.26  2.79  1.3  10.13]\n",
      "Loss_Q: [2.2  2.2  1.2  0.   5.59] Loss_P: [ 3.78  2.3   2.8   1.33 10.22]\n",
      "Loss_Q: [2.12 2.21 1.2  0.   5.53] Loss_P: [ 3.88  2.25  2.78  1.33 10.23]\n",
      "Loss_Q: [2.06 2.18 1.26 0.   5.5 ] Loss_P: [ 3.9   2.3   2.79  1.32 10.3 ]\n",
      "Loss_Q: [2.1  2.19 1.28 0.   5.58] Loss_P: [ 3.84  2.31  2.81  1.35 10.3 ]\n",
      "Loss_Q: [2.08 2.13 1.28 0.   5.49] Loss_P: [ 3.81  2.3   2.74  1.37 10.22]\n",
      "Loss_Q: [2.09 2.16 1.26 0.   5.51] Loss_P: [ 3.79  2.29  2.75  1.33 10.17]\n",
      "Loss_Q: [2.09 2.18 1.27 0.   5.54] Loss_P: [ 3.88  2.25  2.8   1.37 10.31]\n",
      "Loss_Q: [2.11 2.15 1.23 0.   5.49] Loss_P: [ 3.89  2.28  2.75  1.34 10.25]\n",
      "Loss_Q: [2.09 2.18 1.26 0.   5.53] Loss_P: [ 3.86  2.29  2.75  1.35 10.24]\n",
      "Loss_Q: [2.05 2.18 1.22 0.   5.45] Loss_P: [ 3.84  2.31  2.75  1.32 10.22]\n",
      "Loss_Q: [2.08 2.16 1.23 0.   5.47] Loss_P: [ 3.81  2.29  2.76  1.35 10.22]\n",
      "Loss_Q: [2.16 2.19 1.23 0.   5.58] Loss_P: [ 3.84  2.35  2.82  1.31 10.31]\n",
      "Loss_Q: [2.04 2.15 1.2  0.   5.39] Loss_P: [ 3.88  2.28  2.82  1.32 10.29]\n",
      "Loss_Q: [2.14 2.16 1.17 0.   5.47] Loss_P: [ 3.86  2.26  2.74  1.28 10.14]\n",
      "Loss_Q: [2.2  2.17 1.2  0.   5.57] Loss_P: [ 3.83  2.35  2.76  1.28 10.23]\n",
      "Loss_Q: [2.17 2.24 1.19 0.   5.59] Loss_P: [ 3.86  2.32  2.78  1.29 10.25]\n",
      "Loss_Q: [2.1  2.15 1.2  0.   5.44] Loss_P: [ 3.88  2.24  2.77  1.3  10.19]\n",
      "Loss_Q: [2.1  2.24 1.19 0.   5.53] Loss_P: [ 3.81  2.3   2.84  1.31 10.25]\n",
      "Loss_Q: [2.1 2.2 1.2 0.  5.5] Loss_P: [ 3.88  2.28  2.8   1.28 10.24]\n",
      "Loss_Q: [2.08 2.18 1.18 0.   5.44] Loss_P: [ 3.8   2.33  2.8   1.29 10.22]\n",
      "Loss_Q: [2.06 2.17 1.17 0.   5.41] Loss_P: [ 3.87  2.23  2.78  1.25 10.13]\n",
      "Loss_Q: [2.11 2.15 1.14 0.   5.4 ] Loss_P: [ 3.8   2.32  2.81  1.26 10.19]\n",
      "Loss_Q: [2.09 2.22 1.18 0.   5.49] Loss_P: [ 3.85  2.26  2.74  1.24 10.09]\n",
      "Loss_Q: [2.1  2.22 1.15 0.   5.47] Loss_P: [ 3.76  2.33  2.8   1.25 10.14]\n",
      "Loss_Q: [2.11 2.15 1.21 0.   5.47] Loss_P: [ 3.85  2.34  2.79  1.27 10.24]\n",
      "Loss_Q: [2.07 2.24 1.19 0.   5.5 ] Loss_P: [ 3.78  2.27  2.79  1.27 10.12]\n",
      "Loss_Q: [2.03 2.21 1.19 0.   5.43] Loss_P: [ 3.78  2.39  2.77  1.25 10.2 ]\n",
      "Loss_Q: [2.15 2.18 1.22 0.   5.55] Loss_P: [ 3.78  2.34  2.8   1.28 10.2 ]\n",
      "Loss_Q: [2.15 2.19 1.22 0.   5.57] Loss_P: [ 3.77  2.34  2.81  1.31 10.22]\n",
      "Loss_Q: [2.09 2.21 1.2  0.   5.51] Loss_P: [ 3.8   2.31  2.79  1.33 10.23]\n",
      "Loss_Q: [2.07 2.2  1.21 0.   5.48] Loss_P: [ 3.76  2.39  2.8   1.31 10.26]\n",
      "Loss_Q: [2.09 2.17 1.18 0.   5.44] Loss_P: [ 3.78  2.31  2.77  1.31 10.17]\n",
      "Loss_Q: [2.03 2.17 1.14 0.   5.34] Loss_P: [ 3.77  2.33  2.79  1.26 10.16]\n",
      "Loss_Q: [2.05 2.25 1.17 0.   5.46] Loss_P: [ 3.74  2.32  2.83  1.26 10.15]\n",
      "Loss_Q: [2.05 2.26 1.18 0.   5.49] Loss_P: [ 3.78  2.2   2.8   1.24 10.02]\n",
      "Loss_Q: [2.12 2.15 1.15 0.   5.42] Loss_P: [ 3.82  2.26  2.8   1.24 10.12]\n",
      "Loss_Q: [2.03 2.21 1.14 0.   5.38] Loss_P: [ 3.77  2.25  2.81  1.24 10.06]\n",
      "Loss_Q: [2.14 2.15 1.14 0.   5.43] Loss_P: [ 3.81  2.32  2.75  1.28 10.15]\n",
      "Loss_Q: [2.04 2.16 1.14 0.   5.35] Loss_P: [ 3.82  2.3   2.77  1.25 10.14]\n",
      "Loss_Q: [2.09 2.15 1.12 0.   5.36] Loss_P: [ 3.78  2.29  2.79  1.25 10.11]\n",
      "Loss_Q: [2.08 2.15 1.1  0.   5.32] Loss_P: [ 3.74  2.31  2.71  1.27 10.03]\n",
      "Loss_Q: [2.01 2.15 1.13 0.   5.3 ] Loss_P: [ 3.84  2.28  2.73  1.23 10.08]\n",
      "Loss_Q: [2.17 2.16 1.13 0.   5.46] Loss_P: [ 3.76  2.26  2.75  1.24 10.  ]\n",
      "Loss_Q: [2.11 2.14 1.13 0.   5.38] Loss_P: [ 3.77  2.38  2.76  1.21 10.12]\n",
      "Loss_Q: [2.08 2.15 1.12 0.   5.35] Loss_P: [ 3.79  2.34  2.74  1.25 10.12]\n",
      "Loss_Q: [2.12 2.15 1.16 0.   5.44] Loss_P: [ 3.77  2.29  2.77  1.24 10.07]\n",
      "Loss_Q: [2.09 2.16 1.07 0.   5.32] Loss_P: [ 3.78  2.26  2.78  1.2  10.02]\n",
      "Loss_Q: [2.11 2.16 1.09 0.   5.37] Loss_P: [ 3.8   2.31  2.76  1.2  10.07]\n",
      "Loss_Q: [1.97 2.15 1.11 0.   5.23] Loss_P: [3.78 2.26 2.72 1.21 9.98]\n",
      "Loss_Q: [2.04 2.17 1.08 0.   5.29] Loss_P: [3.82 2.24 2.7  1.15 9.92]\n",
      "Loss_Q: [1.95 2.13 1.1  0.   5.18] Loss_P: [3.84 2.24 2.7  1.17 9.96]\n",
      "Loss_Q: [2.   2.14 1.09 0.   5.23] Loss_P: [3.74 2.26 2.65 1.13 9.78]\n",
      "Loss_Q: [2.02 2.07 1.04 0.   5.13] Loss_P: [3.9  2.16 2.68 1.16 9.9 ]\n",
      "Loss_Q: [1.96 2.11 1.02 0.   5.1 ] Loss_P: [3.83 2.21 2.64 1.18 9.86]\n",
      "Loss_Q: [2.04 2.14 1.05 0.   5.23] Loss_P: [3.87 2.26 2.68 1.12 9.93]\n",
      "Loss_Q: [2.   2.1  1.06 0.   5.16] Loss_P: [3.86 2.22 2.69 1.2  9.97]\n",
      "Loss_Q: [2.17 2.07 1.02 0.   5.26] Loss_P: [3.82 2.28 2.74 1.16 9.99]\n",
      "Loss_Q: [2.04 2.08 1.07 0.   5.2 ] Loss_P: [3.83 2.21 2.71 1.14 9.9 ]\n",
      "Loss_Q: [2.   2.14 1.02 0.   5.16] Loss_P: [3.75 2.32 2.74 1.15 9.97]\n",
      "Loss_Q: [2.03 2.1  1.07 0.   5.2 ] Loss_P: [3.75 2.32 2.74 1.16 9.96]\n",
      "Loss_Q: [2.05 2.05 1.09 0.   5.2 ] Loss_P: [3.76 2.26 2.69 1.18 9.89]\n",
      "Loss_Q: [2.05 2.11 1.07 0.   5.23] Loss_P: [3.74 2.29 2.71 1.15 9.9 ]\n",
      "Loss_Q: [2.07 2.03 1.13 0.   5.23] Loss_P: [3.78 2.29 2.7  1.21 9.97]\n",
      "Loss_Q: [2.19 2.1  1.07 0.   5.36] Loss_P: [ 3.8   2.37  2.72  1.21 10.11]\n",
      "Loss_Q: [2.1  2.07 1.05 0.   5.22] Loss_P: [3.78 2.3  2.7  1.17 9.95]\n",
      "Loss_Q: [2.05 2.1  1.04 0.   5.19] Loss_P: [3.76 2.21 2.66 1.17 9.8 ]\n",
      "Loss_Q: [2.1  2.08 1.05 0.   5.23] Loss_P: [3.86 2.28 2.69 1.12 9.96]\n",
      "Loss_Q: [2.09 2.09 1.02 0.   5.19] Loss_P: [3.75 2.34 2.68 1.14 9.91]\n",
      "Loss_Q: [2.14 2.05 1.01 0.   5.19] Loss_P: [3.73 2.38 2.68 1.11 9.9 ]\n",
      "Loss_Q: [2.05 2.08 1.   0.   5.12] Loss_P: [3.75 2.34 2.67 1.06 9.82]\n",
      "Loss_Q: [2.1  2.11 1.04 0.   5.24] Loss_P: [3.8  2.26 2.72 1.14 9.91]\n",
      "Loss_Q: [1.94 2.1  1.01 0.   5.06] Loss_P: [3.78 2.22 2.71 1.11 9.83]\n",
      "Loss_Q: [1.98 2.15 1.06 0.   5.19] Loss_P: [3.79 2.28 2.72 1.16 9.95]\n",
      "Loss_Q: [1.96 2.14 1.01 0.   5.11] Loss_P: [3.81 2.26 2.68 1.1  9.85]\n",
      "Loss_Q: [2.07 2.2  1.01 0.   5.28] Loss_P: [3.87 2.27 2.71 1.14 9.99]\n",
      "Loss_Q: [2.03 2.12 1.03 0.   5.18] Loss_P: [3.79 2.24 2.68 1.17 9.88]\n",
      "Loss_Q: [2.02 2.09 1.04 0.   5.15] Loss_P: [3.84 2.11 2.71 1.13 9.79]\n",
      "Loss_Q: [2.05 2.06 1.   0.   5.11] Loss_P: [3.79 2.17 2.68 1.08 9.73]\n",
      "Loss_Q: [2.03 2.09 0.98 0.   5.1 ] Loss_P: [3.81 2.09 2.66 1.12 9.68]\n",
      "Loss_Q: [1.96 2.03 1.   0.   4.99] Loss_P: [3.84 2.16 2.71 1.1  9.82]\n",
      "Loss_Q: [1.99 2.12 1.05 0.   5.16] Loss_P: [3.79 2.19 2.76 1.12 9.86]\n",
      "Loss_Q: [1.99 2.13 1.01 0.   5.14] Loss_P: [3.78 2.11 2.69 1.16 9.73]\n",
      "Loss_Q: [1.98 2.09 1.05 0.   5.12] Loss_P: [3.82 2.2  2.7  1.15 9.88]\n",
      "Loss_Q: [2.   2.1  1.08 0.   5.18] Loss_P: [3.7  2.23 2.69 1.11 9.73]\n",
      "Loss_Q: [1.9  2.15 1.04 0.   5.09] Loss_P: [3.85 2.14 2.72 1.14 9.86]\n",
      "Loss_Q: [2.   2.14 1.04 0.   5.18] Loss_P: [3.89 2.18 2.71 1.12 9.9 ]\n",
      "Loss_Q: [1.9  2.09 1.01 0.   5.01] Loss_P: [3.83 2.17 2.7  1.11 9.81]\n",
      "Loss_Q: [1.92 2.14 1.02 0.   5.08] Loss_P: [3.85 2.2  2.68 1.08 9.82]\n",
      "Loss_Q: [1.97 2.06 0.93 0.   4.97] Loss_P: [3.87 2.12 2.65 1.06 9.7 ]\n",
      "Loss_Q: [2.03 2.04 0.94 0.   5.01] Loss_P: [3.87 2.12 2.65 1.04 9.67]\n",
      "Loss_Q: [1.95 2.13 0.94 0.   5.02] Loss_P: [3.84 2.15 2.64 1.04 9.67]\n",
      "Loss_Q: [1.97 2.06 0.95 0.   4.98] Loss_P: [3.88 2.17 2.65 1.02 9.72]\n",
      "Loss_Q: [1.94 2.12 0.98 0.   5.04] Loss_P: [3.82 2.2  2.65 1.1  9.76]\n",
      "Loss_Q: [2.03 2.11 1.   0.   5.14] Loss_P: [3.82 2.16 2.69 1.06 9.73]\n",
      "Loss_Q: [1.97 2.06 0.94 0.   4.97] Loss_P: [3.76 2.14 2.66 1.06 9.62]\n",
      "Loss_Q: [1.99 2.13 0.97 0.   5.08] Loss_P: [3.81 2.14 2.69 1.06 9.71]\n",
      "Loss_Q: [2.02 2.13 0.95 0.   5.1 ] Loss_P: [3.82 2.26 2.73 1.07 9.88]\n",
      "Loss_Q: [2.   2.15 1.01 0.   5.16] Loss_P: [3.79 2.23 2.73 1.06 9.81]\n",
      "Loss_Q: [2.04 2.2  0.99 0.   5.23] Loss_P: [3.75 2.25 2.73 1.08 9.81]\n",
      "Loss_Q: [2.05 2.12 1.03 0.   5.2 ] Loss_P: [3.85 2.28 2.71 1.14 9.97]\n",
      "Loss_Q: [1.97 2.13 0.99 0.   5.1 ] Loss_P: [3.77 2.22 2.72 1.14 9.84]\n",
      "Loss_Q: [2.01 2.17 1.01 0.   5.19] Loss_P: [3.8  2.27 2.74 1.16 9.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.08 2.12 1.   0.   5.2 ] Loss_P: [3.76 2.3  2.72 1.12 9.89]\n",
      "Loss_Q: [2.1  2.15 1.   0.   5.25] Loss_P: [3.88 2.23 2.71 1.11 9.93]\n",
      "Loss_Q: [2.03 2.12 1.01 0.   5.16] Loss_P: [3.81 2.22 2.73 1.16 9.91]\n",
      "Loss_Q: [2.07 2.13 1.04 0.   5.24] Loss_P: [3.7  2.32 2.77 1.09 9.87]\n",
      "Loss_Q: [2.11 2.15 1.03 0.   5.29] Loss_P: [3.78 2.3  2.7  1.15 9.93]\n",
      "Loss_Q: [1.99 2.2  1.1  0.   5.28] Loss_P: [3.69 2.26 2.72 1.13 9.8 ]\n",
      "Loss_Q: [1.99 2.19 1.04 0.   5.22] Loss_P: [3.77 2.28 2.76 1.09 9.9 ]\n",
      "Loss_Q: [1.94 2.17 0.97 0.   5.09] Loss_P: [3.85 2.27 2.71 1.13 9.96]\n",
      "Loss_Q: [1.99 2.15 1.03 0.   5.17] Loss_P: [3.78 2.22 2.75 1.15 9.91]\n",
      "Loss_Q: [1.99 2.17 1.   0.   5.15] Loss_P: [3.75 2.18 2.69 1.14 9.76]\n",
      "Loss_Q: [1.99 2.15 1.05 0.   5.19] Loss_P: [3.71 2.27 2.72 1.12 9.82]\n",
      "Loss_Q: [1.96 2.18 0.95 0.   5.1 ] Loss_P: [3.77 2.3  2.75 1.11 9.93]\n",
      "Loss_Q: [2.02 2.15 0.98 0.   5.15] Loss_P: [3.77 2.25 2.67 1.05 9.74]\n",
      "Loss_Q: [1.95 2.11 0.99 0.   5.05] Loss_P: [3.75 2.24 2.69 1.11 9.79]\n",
      "Loss_Q: [1.96 2.04 0.96 0.   4.97] Loss_P: [3.73 2.29 2.7  1.06 9.78]\n",
      "Loss_Q: [1.96 2.14 0.95 0.   5.05] Loss_P: [3.77 2.25 2.69 1.09 9.8 ]\n",
      "Loss_Q: [1.97 2.15 0.98 0.   5.1 ] Loss_P: [3.71 2.26 2.7  1.08 9.75]\n",
      "Loss_Q: [1.97 2.15 0.95 0.   5.07] Loss_P: [3.74 2.23 2.7  1.06 9.73]\n",
      "Loss_Q: [2.05 2.16 0.97 0.   5.19] Loss_P: [3.79 2.2  2.7  1.03 9.73]\n",
      "Loss_Q: [2.08 2.15 0.97 0.   5.2 ] Loss_P: [3.8  2.27 2.74 1.1  9.91]\n",
      "Loss_Q: [2.01 2.11 1.01 0.   5.13] Loss_P: [3.82 2.27 2.76 1.11 9.96]\n",
      "Loss_Q: [2.05 2.13 1.04 0.   5.21] Loss_P: [3.87 2.2  2.76 1.1  9.93]\n",
      "Loss_Q: [2.05 2.21 1.03 0.   5.29] Loss_P: [3.77 2.21 2.75 1.14 9.88]\n",
      "Loss_Q: [2.   2.23 0.99 0.   5.22] Loss_P: [3.72 2.29 2.77 1.15 9.93]\n",
      "Loss_Q: [2.08 2.13 0.97 0.   5.18] Loss_P: [3.76 2.3  2.75 1.06 9.87]\n",
      "Loss_Q: [1.96 2.15 0.98 0.   5.09] Loss_P: [3.75 2.25 2.75 1.11 9.87]\n",
      "Loss_Q: [1.99 2.16 1.06 0.   5.21] Loss_P: [3.76 2.29 2.73 1.14 9.92]\n",
      "Loss_Q: [2.02 2.22 0.99 0.   5.23] Loss_P: [3.82 2.27 2.74 1.12 9.95]\n",
      "Loss_Q: [1.92 2.13 0.93 0.   4.98] Loss_P: [3.72 2.25 2.73 1.08 9.77]\n",
      "Loss_Q: [1.94 2.13 0.95 0.   5.02] Loss_P: [3.86 2.25 2.72 1.1  9.93]\n",
      "Loss_Q: [2.06 2.11 1.03 0.   5.2 ] Loss_P: [3.73 2.22 2.69 1.08 9.71]\n",
      "Loss_Q: [2.05 2.12 0.98 0.   5.16] Loss_P: [3.84 2.23 2.74 1.07 9.88]\n",
      "Loss_Q: [1.97 2.1  0.95 0.   5.02] Loss_P: [3.81 2.23 2.75 1.09 9.87]\n",
      "Loss_Q: [1.99 2.09 0.94 0.   5.02] Loss_P: [3.78 2.21 2.73 1.09 9.82]\n",
      "Loss_Q: [2.01 2.11 1.02 0.   5.14] Loss_P: [3.79 2.25 2.72 1.1  9.85]\n",
      "Loss_Q: [1.93 2.14 0.97 0.   5.04] Loss_P: [3.79 2.29 2.72 1.12 9.91]\n",
      "Loss_Q: [2.04 2.09 1.01 0.   5.14] Loss_P: [3.75 2.25 2.72 1.11 9.83]\n",
      "Loss_Q: [1.95 2.16 0.97 0.   5.08] Loss_P: [3.8  2.23 2.68 1.11 9.83]\n",
      "Loss_Q: [2.08 2.1  1.03 0.   5.21] Loss_P: [3.77 2.27 2.72 1.07 9.82]\n",
      "Loss_Q: [2.05 2.22 1.   0.   5.26] Loss_P: [3.75 2.27 2.71 1.08 9.81]\n",
      "Loss_Q: [2.05 2.12 0.94 0.   5.11] Loss_P: [3.77 2.27 2.68 1.04 9.75]\n",
      "Loss_Q: [1.97 2.07 0.94 0.   4.98] Loss_P: [3.78 2.22 2.74 1.06 9.8 ]\n",
      "Loss_Q: [1.94 2.14 0.9  0.   4.98] Loss_P: [3.8  2.19 2.68 1.04 9.7 ]\n",
      "Loss_Q: [1.87 2.07 0.92 0.   4.86] Loss_P: [3.74 2.19 2.68 1.   9.61]\n",
      "Loss_Q: [1.89 2.11 0.93 0.   4.93] Loss_P: [3.79 2.19 2.66 0.99 9.62]\n",
      "Loss_Q: [1.99 2.12 0.95 0.   5.05] Loss_P: [3.74 2.23 2.69 1.08 9.74]\n",
      "Loss_Q: [1.98 2.14 0.93 0.   5.05] Loss_P: [3.75 2.25 2.66 1.03 9.69]\n",
      "Loss_Q: [1.97 2.21 0.96 0.   5.14] Loss_P: [3.69 2.38 2.68 1.06 9.8 ]\n",
      "Loss_Q: [2.03 2.09 1.   0.   5.12] Loss_P: [3.76 2.37 2.73 1.09 9.95]\n",
      "Loss_Q: [1.99 2.17 0.95 0.   5.1 ] Loss_P: [3.73 2.29 2.71 1.09 9.82]\n",
      "Loss_Q: [1.98 2.16 1.02 0.   5.16] Loss_P: [3.83 2.18 2.72 1.15 9.88]\n",
      "Loss_Q: [2.07 2.16 0.96 0.   5.19] Loss_P: [3.8  2.2  2.67 1.08 9.75]\n",
      "Loss_Q: [1.91 2.15 1.05 0.   5.11] Loss_P: [3.85 2.21 2.76 1.12 9.94]\n",
      "Loss_Q: [1.97 2.14 1.   0.   5.11] Loss_P: [3.78 2.25 2.75 1.14 9.92]\n",
      "Loss_Q: [1.97 2.17 1.   0.   5.13] Loss_P: [3.78 2.2  2.74 1.09 9.82]\n",
      "Loss_Q: [1.96 2.18 0.99 0.   5.14] Loss_P: [3.8  2.23 2.72 1.12 9.86]\n",
      "Loss_Q: [2.03 2.15 1.01 0.   5.18] Loss_P: [3.76 2.34 2.73 1.1  9.93]\n",
      "Loss_Q: [1.93 2.15 0.97 0.   5.05] Loss_P: [3.77 2.18 2.68 1.09 9.73]\n",
      "Loss_Q: [1.97 2.09 0.98 0.   5.04] Loss_P: [3.8  2.16 2.7  1.09 9.76]\n",
      "Loss_Q: [1.97 2.15 1.   0.   5.12] Loss_P: [3.84 2.18 2.7  1.12 9.84]\n",
      "Loss_Q: [1.97 2.12 1.   0.   5.09] Loss_P: [3.77 2.15 2.68 1.09 9.68]\n",
      "Loss_Q: [2.   2.11 1.01 0.   5.12] Loss_P: [3.84 2.19 2.72 1.08 9.83]\n",
      "Loss_Q: [1.99 2.17 1.02 0.   5.19] Loss_P: [3.75 2.21 2.68 1.13 9.77]\n",
      "Loss_Q: [1.98 2.18 1.08 0.   5.24] Loss_P: [3.76 2.25 2.65 1.15 9.81]\n",
      "Loss_Q: [1.91 2.1  0.99 0.   5.01] Loss_P: [3.81 2.23 2.66 1.12 9.81]\n",
      "Loss_Q: [1.94 2.15 1.02 0.   5.12] Loss_P: [3.83 2.25 2.7  1.11 9.88]\n",
      "Loss_Q: [2.08 2.11 1.01 0.   5.2 ] Loss_P: [3.76 2.24 2.68 1.13 9.81]\n",
      "Loss_Q: [1.94 2.13 1.06 0.   5.13] Loss_P: [3.79 2.23 2.67 1.11 9.8 ]\n",
      "Loss_Q: [2.   2.13 1.05 0.   5.19] Loss_P: [3.78 2.19 2.65 1.14 9.77]\n",
      "Loss_Q: [1.95 2.17 1.1  0.   5.21] Loss_P: [3.79 2.19 2.76 1.15 9.89]\n",
      "Loss_Q: [2.1  2.14 1.07 0.   5.3 ] Loss_P: [3.87 2.23 2.69 1.13 9.92]\n",
      "Loss_Q: [2.   2.15 1.04 0.   5.19] Loss_P: [3.77 2.2  2.67 1.12 9.76]\n",
      "Loss_Q: [2.   2.14 1.   0.   5.15] Loss_P: [3.79 2.3  2.71 1.16 9.95]\n",
      "Loss_Q: [2.08 2.09 1.01 0.   5.19] Loss_P: [3.79 2.26 2.66 1.11 9.83]\n",
      "Loss_Q: [2.01 2.07 0.99 0.   5.06] Loss_P: [3.83 2.26 2.74 1.11 9.94]\n",
      "Loss_Q: [1.98 2.16 1.02 0.   5.15] Loss_P: [3.81 2.23 2.73 1.07 9.83]\n",
      "Loss_Q: [1.99 2.11 1.   0.   5.1 ] Loss_P: [3.77 2.2  2.72 1.1  9.79]\n",
      "Loss_Q: [1.97 2.14 1.01 0.   5.12] Loss_P: [3.8  2.24 2.73 1.16 9.93]\n",
      "Loss_Q: [1.96 2.12 1.   0.   5.09] Loss_P: [3.89 2.2  2.74 1.12 9.95]\n",
      "Loss_Q: [1.96 2.17 0.95 0.   5.08] Loss_P: [3.83 2.18 2.76 1.04 9.82]\n",
      "Loss_Q: [1.96 2.17 0.99 0.   5.12] Loss_P: [3.75 2.18 2.72 1.11 9.76]\n",
      "Loss_Q: [1.99 2.14 0.99 0.   5.13] Loss_P: [3.81 2.17 2.73 1.06 9.77]\n",
      "Loss_Q: [1.92 2.14 0.99 0.   5.04] Loss_P: [3.8  2.06 2.72 1.04 9.62]\n",
      "Loss_Q: [2.01 2.2  0.95 0.   5.16] Loss_P: [3.88 2.19 2.69 1.04 9.81]\n",
      "Loss_Q: [2.03 2.19 0.97 0.   5.19] Loss_P: [3.79 2.23 2.71 1.08 9.81]\n",
      "Loss_Q: [1.99 2.19 1.01 0.   5.18] Loss_P: [3.8  2.17 2.79 1.04 9.8 ]\n",
      "Loss_Q: [1.91 2.17 0.99 0.   5.08] Loss_P: [3.76 2.22 2.77 1.06 9.81]\n",
      "Loss_Q: [2.   2.22 1.01 0.   5.23] Loss_P: [3.81 2.28 2.79 1.09 9.96]\n",
      "Loss_Q: [1.96 2.15 0.91 0.   5.01] Loss_P: [3.85 2.16 2.76 1.02 9.79]\n",
      "Loss_Q: [1.99 2.19 1.01 0.   5.19] Loss_P: [3.73 2.24 2.8  1.09 9.86]\n",
      "Loss_Q: [2.   2.19 0.99 0.   5.17] Loss_P: [3.78 2.21 2.74 1.07 9.79]\n",
      "Loss_Q: [2.01 2.12 1.   0.   5.13] Loss_P: [3.76 2.16 2.73 1.07 9.72]\n",
      "Loss_Q: [1.98 2.15 0.96 0.   5.09] Loss_P: [3.82 2.26 2.74 1.06 9.87]\n",
      "Loss_Q: [2.01 2.15 0.95 0.   5.11] Loss_P: [3.83 2.18 2.72 1.11 9.84]\n",
      "Loss_Q: [2.02 2.17 1.01 0.   5.2 ] Loss_P: [3.77 2.22 2.78 1.07 9.84]\n",
      "Loss_Q: [2.   2.22 1.   0.   5.22] Loss_P: [3.78 2.27 2.77 1.05 9.87]\n",
      "Loss_Q: [1.96 2.2  1.   0.   5.16] Loss_P: [3.82 2.2  2.75 1.07 9.83]\n",
      "Loss_Q: [1.99 2.14 1.   0.   5.14] Loss_P: [3.76 2.24 2.77 1.1  9.86]\n",
      "Loss_Q: [1.98 2.18 1.06 0.   5.22] Loss_P: [ 3.81  2.3   2.77  1.12 10.  ]\n",
      "Loss_Q: [2.08 2.19 1.   0.   5.28] Loss_P: [3.78 2.25 2.75 1.13 9.9 ]\n",
      "Loss_Q: [2.01 2.15 1.   0.   5.16] Loss_P: [3.73 2.28 2.76 1.13 9.9 ]\n",
      "Loss_Q: [2.11 2.19 1.04 0.   5.34] Loss_P: [3.82 2.3  2.72 1.16 9.99]\n",
      "Loss_Q: [2.03 2.2  1.06 0.   5.29] Loss_P: [ 3.81  2.29  2.72  1.17 10.  ]\n",
      "Loss_Q: [1.97 2.15 1.06 0.   5.18] Loss_P: [ 3.77  2.34  2.76  1.18 10.06]\n",
      "Loss_Q: [2.03 2.2  1.09 0.   5.32] Loss_P: [ 3.78  2.32  2.73  1.16 10.  ]\n",
      "Loss_Q: [2.08 2.15 1.09 0.   5.32] Loss_P: [3.79 2.3  2.68 1.21 9.98]\n",
      "Loss_Q: [2.1  2.18 1.08 0.   5.37] Loss_P: [ 3.81  2.28  2.77  1.15 10.01]\n",
      "Loss_Q: [2.04 2.17 1.05 0.   5.26] Loss_P: [ 3.79  2.31  2.75  1.21 10.06]\n",
      "Loss_Q: [2.01 2.22 1.04 0.   5.27] Loss_P: [3.8  2.25 2.75 1.18 9.99]\n",
      "Loss_Q: [1.99 2.23 1.1  0.   5.32] Loss_P: [ 3.82  2.28  2.76  1.16 10.02]\n",
      "Loss_Q: [2.   2.23 0.99 0.   5.22] Loss_P: [ 3.76  2.33  2.76  1.16 10.01]\n",
      "Loss_Q: [2.08 2.2  1.04 0.   5.32] Loss_P: [3.77 2.25 2.71 1.15 9.88]\n",
      "Loss_Q: [2.01 2.14 1.07 0.   5.21] Loss_P: [ 3.8   2.35  2.73  1.16 10.03]\n",
      "Loss_Q: [2.04 2.08 1.04 0.   5.17] Loss_P: [3.77 2.32 2.74 1.12 9.94]\n",
      "Loss_Q: [1.89 2.15 1.04 0.   5.08] Loss_P: [3.73 2.27 2.7  1.12 9.82]\n",
      "Loss_Q: [2.09 2.16 1.05 0.   5.3 ] Loss_P: [3.78 2.26 2.69 1.17 9.9 ]\n",
      "Loss_Q: [1.99 2.1  1.01 0.   5.1 ] Loss_P: [ 3.82  2.34  2.69  1.17 10.02]\n",
      "Loss_Q: [1.91 2.14 1.07 0.   5.12] Loss_P: [3.81 2.27 2.76 1.13 9.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.97 2.13 1.03 0.   5.13] Loss_P: [3.75 2.31 2.72 1.16 9.94]\n",
      "Loss_Q: [1.94 2.12 1.01 0.   5.07] Loss_P: [3.71 2.33 2.7  1.1  9.83]\n",
      "Loss_Q: [2.   2.15 1.03 0.   5.18] Loss_P: [3.84 2.26 2.68 1.1  9.89]\n",
      "Loss_Q: [2.07 2.09 1.08 0.   5.24] Loss_P: [3.76 2.33 2.67 1.14 9.91]\n",
      "Loss_Q: [1.96 2.12 1.07 0.   5.15] Loss_P: [3.74 2.3  2.69 1.17 9.89]\n",
      "Loss_Q: [1.93 2.12 0.98 0.   5.03] Loss_P: [3.79 2.21 2.68 1.14 9.82]\n",
      "Loss_Q: [2.07 2.11 1.04 0.   5.22] Loss_P: [3.78 2.3  2.64 1.12 9.84]\n",
      "Loss_Q: [1.93 2.14 1.07 0.   5.15] Loss_P: [ 3.83  2.3   2.7   1.17 10.01]\n",
      "Loss_Q: [1.95 2.16 1.07 0.   5.17] Loss_P: [3.82 2.25 2.7  1.13 9.91]\n",
      "Loss_Q: [1.99 2.13 1.09 0.   5.21] Loss_P: [3.81 2.2  2.73 1.19 9.94]\n",
      "Loss_Q: [1.95 2.15 1.1  0.   5.2 ] Loss_P: [3.86 2.22 2.67 1.17 9.92]\n",
      "Loss_Q: [1.92 2.15 1.15 0.   5.21] Loss_P: [3.71 2.29 2.7  1.24 9.93]\n",
      "Loss_Q: [1.97 2.19 1.16 0.   5.32] Loss_P: [3.76 2.21 2.75 1.22 9.93]\n",
      "Loss_Q: [2.06 2.1  1.19 0.   5.35] Loss_P: [ 3.82  2.26  2.77  1.24 10.08]\n",
      "Loss_Q: [2.02 2.17 1.16 0.   5.34] Loss_P: [ 3.82  2.29  2.71  1.25 10.07]\n",
      "Loss_Q: [2.01 2.15 1.14 0.   5.3 ] Loss_P: [3.84 2.18 2.68 1.23 9.92]\n",
      "Loss_Q: [2.01 2.08 1.12 0.   5.21] Loss_P: [ 3.84  2.28  2.72  1.22 10.07]\n",
      "Loss_Q: [2.08 2.06 1.16 0.   5.3 ] Loss_P: [ 3.86  2.22  2.67  1.25 10.  ]\n",
      "Loss_Q: [1.99 2.08 1.17 0.   5.23] Loss_P: [ 3.83  2.24  2.69  1.26 10.02]\n",
      "Loss_Q: [2.03 2.06 1.17 0.   5.26] Loss_P: [ 3.79  2.23  2.72  1.28 10.02]\n",
      "Loss_Q: [2.04 2.02 1.22 0.   5.28] Loss_P: [ 3.81  2.26  2.68  1.28 10.03]\n",
      "Loss_Q: [2.11 2.12 1.17 0.   5.4 ] Loss_P: [3.8  2.26 2.65 1.24 9.95]\n",
      "Loss_Q: [2.04 2.14 1.18 0.   5.36] Loss_P: [3.84 2.21 2.69 1.24 9.99]\n",
      "Loss_Q: [2.02 2.16 1.14 0.   5.32] Loss_P: [3.79 2.2  2.66 1.27 9.91]\n",
      "Loss_Q: [2.07 2.04 1.16 0.   5.26] Loss_P: [3.82 2.2  2.69 1.25 9.95]\n",
      "Loss_Q: [1.92 2.16 1.2  0.   5.28] Loss_P: [ 3.81  2.22  2.72  1.29 10.04]\n",
      "Loss_Q: [1.98 2.09 1.16 0.   5.24] Loss_P: [3.81 2.21 2.65 1.25 9.93]\n",
      "Loss_Q: [2.07 2.12 1.15 0.   5.34] Loss_P: [3.88 2.15 2.65 1.28 9.97]\n",
      "Loss_Q: [2.06 2.08 1.2  0.   5.34] Loss_P: [3.81 2.19 2.7  1.29 9.99]\n",
      "Loss_Q: [1.92 2.17 1.17 0.   5.26] Loss_P: [ 3.85  2.27  2.7   1.29 10.11]\n",
      "Loss_Q: [1.99 2.22 1.19 0.   5.4 ] Loss_P: [3.8  2.17 2.72 1.23 9.91]\n",
      "Loss_Q: [1.96 2.15 1.15 0.   5.26] Loss_P: [3.85 2.15 2.76 1.24 9.99]\n",
      "Loss_Q: [2.02 2.22 1.18 0.   5.42] Loss_P: [ 3.81  2.2   2.73  1.28 10.02]\n",
      "Loss_Q: [1.96 2.17 1.19 0.   5.31] Loss_P: [3.8  2.19 2.71 1.3  9.99]\n",
      "Loss_Q: [1.98 2.09 1.27 0.   5.34] Loss_P: [3.76 2.17 2.72 1.26 9.91]\n",
      "Loss_Q: [1.99 2.17 1.22 0.   5.39] Loss_P: [3.79 2.22 2.68 1.29 9.98]\n",
      "Loss_Q: [1.97 2.25 1.21 0.   5.43] Loss_P: [ 3.83  2.2   2.73  1.28 10.04]\n",
      "Loss_Q: [2.   2.15 1.22 0.   5.37] Loss_P: [ 3.76  2.23  2.75  1.29 10.04]\n",
      "Loss_Q: [1.93 2.12 1.23 0.   5.28] Loss_P: [ 3.77  2.27  2.73  1.29 10.06]\n",
      "Loss_Q: [1.98 2.15 1.21 0.   5.34] Loss_P: [ 3.79  2.2   2.72  1.31 10.02]\n",
      "Loss_Q: [1.87 2.22 1.23 0.   5.33] Loss_P: [ 3.79  2.2   2.77  1.33 10.09]\n",
      "Loss_Q: [1.95 2.13 1.21 0.   5.29] Loss_P: [ 3.86  2.19  2.73  1.31 10.08]\n",
      "Loss_Q: [1.97 2.19 1.23 0.   5.39] Loss_P: [ 3.79  2.22  2.76  1.3  10.06]\n",
      "Loss_Q: [1.92 2.17 1.2  0.   5.28] Loss_P: [ 3.79  2.21  2.73  1.28 10.01]\n",
      "Loss_Q: [2.05 2.15 1.23 0.   5.43] Loss_P: [ 3.75  2.24  2.72  1.31 10.02]\n",
      "Loss_Q: [1.97 2.16 1.19 0.   5.32] Loss_P: [ 3.82  2.23  2.69  1.3  10.03]\n",
      "Loss_Q: [1.98 2.14 1.18 0.   5.29] Loss_P: [ 3.85  2.17  2.69  1.34 10.05]\n",
      "Loss_Q: [1.99 2.18 1.17 0.   5.34] Loss_P: [ 3.78  2.25  2.7   1.27 10.  ]\n",
      "Loss_Q: [2.   2.17 1.23 0.   5.4 ] Loss_P: [ 3.77  2.24  2.74  1.28 10.02]\n",
      "Loss_Q: [1.93 2.19 1.2  0.   5.32] Loss_P: [ 3.75  2.22  2.75  1.31 10.04]\n",
      "Loss_Q: [1.96 2.2  1.23 0.   5.39] Loss_P: [ 3.86  2.16  2.78  1.29 10.08]\n",
      "Loss_Q: [1.97 2.17 1.23 0.   5.37] Loss_P: [ 3.74  2.22  2.78  1.3  10.05]\n",
      "Loss_Q: [1.96 2.15 1.24 0.   5.36] Loss_P: [ 3.76  2.28  2.79  1.37 10.19]\n",
      "Loss_Q: [1.95 2.16 1.26 0.   5.37] Loss_P: [ 3.79  2.21  2.8   1.3  10.1 ]\n",
      "Loss_Q: [1.97 2.15 1.25 0.   5.37] Loss_P: [ 3.8   2.23  2.75  1.29 10.07]\n",
      "Loss_Q: [1.98 2.19 1.23 0.   5.41] Loss_P: [ 3.73  2.3   2.77  1.35 10.14]\n",
      "Loss_Q: [1.96 2.16 1.22 0.   5.34] Loss_P: [ 3.75  2.24  2.82  1.32 10.14]\n",
      "Loss_Q: [1.95 2.18 1.21 0.   5.34] Loss_P: [ 3.8   2.26  2.78  1.3  10.15]\n",
      "Loss_Q: [2.   2.18 1.26 0.   5.44] Loss_P: [ 3.79  2.23  2.77  1.3  10.09]\n",
      "Loss_Q: [1.92 2.17 1.22 0.   5.31] Loss_P: [ 3.82  2.17  2.79  1.34 10.13]\n",
      "Loss_Q: [1.91 2.2  1.2  0.   5.31] Loss_P: [ 3.78  2.23  2.78  1.35 10.14]\n",
      "Loss_Q: [1.97 2.18 1.21 0.   5.36] Loss_P: [ 3.79  2.19  2.74  1.32 10.05]\n",
      "Loss_Q: [1.98 2.18 1.21 0.   5.38] Loss_P: [ 3.79  2.22  2.8   1.31 10.12]\n",
      "Loss_Q: [2.01 2.18 1.23 0.   5.42] Loss_P: [ 3.76  2.22  2.8   1.29 10.07]\n",
      "Loss_Q: [1.93 2.18 1.25 0.   5.36] Loss_P: [ 3.79  2.25  2.77  1.33 10.13]\n",
      "Loss_Q: [1.99 2.19 1.22 0.   5.41] Loss_P: [ 3.82  2.26  2.74  1.36 10.19]\n",
      "Loss_Q: [1.95 2.15 1.27 0.   5.37] Loss_P: [ 3.76  2.23  2.7   1.35 10.04]\n",
      "Loss_Q: [1.91 2.15 1.27 0.   5.33] Loss_P: [ 3.83  2.24  2.77  1.37 10.21]\n",
      "Loss_Q: [2.   2.13 1.28 0.   5.41] Loss_P: [ 3.77  2.25  2.68  1.39 10.08]\n",
      "Loss_Q: [2.03 2.08 1.25 0.   5.37] Loss_P: [ 3.74  2.24  2.68  1.36 10.02]\n",
      "Loss_Q: [1.91 2.13 1.25 0.   5.3 ] Loss_P: [ 3.78  2.18  2.71  1.39 10.07]\n",
      "Loss_Q: [1.87 2.17 1.3  0.   5.34] Loss_P: [ 3.74  2.21  2.74  1.37 10.07]\n",
      "Loss_Q: [1.87 2.22 1.27 0.   5.37] Loss_P: [ 3.82  2.15  2.7   1.36 10.03]\n",
      "Loss_Q: [1.96 2.18 1.24 0.   5.37] Loss_P: [ 3.9   2.17  2.72  1.32 10.12]\n",
      "Loss_Q: [1.89 2.12 1.29 0.   5.3 ] Loss_P: [ 3.81  2.16  2.72  1.35 10.03]\n",
      "Loss_Q: [2.01 2.13 1.2  0.   5.33] Loss_P: [ 3.81  2.23  2.73  1.36 10.14]\n",
      "Loss_Q: [1.95 2.13 1.27 0.   5.36] Loss_P: [ 3.77  2.28  2.74  1.38 10.17]\n",
      "Loss_Q: [2.01 2.21 1.27 0.   5.49] Loss_P: [ 3.79  2.3   2.76  1.37 10.22]\n",
      "Loss_Q: [1.93 2.27 1.27 0.   5.47] Loss_P: [ 3.8   2.24  2.77  1.38 10.19]\n",
      "Loss_Q: [2.   2.2  1.27 0.   5.47] Loss_P: [ 3.83  2.23  2.77  1.39 10.23]\n",
      "Loss_Q: [1.95 2.2  1.28 0.   5.43] Loss_P: [ 3.75  2.27  2.79  1.39 10.21]\n",
      "Loss_Q: [2.11 2.18 1.26 0.   5.55] Loss_P: [ 3.83  2.26  2.74  1.37 10.21]\n",
      "Loss_Q: [2.03 2.17 1.31 0.   5.5 ] Loss_P: [ 3.76  2.28  2.75  1.41 10.2 ]\n",
      "Loss_Q: [1.96 2.15 1.28 0.   5.4 ] Loss_P: [ 3.74  2.32  2.71  1.4  10.17]\n",
      "Loss_Q: [2.02 2.2  1.31 0.   5.53] Loss_P: [ 3.77  2.32  2.76  1.42 10.28]\n",
      "Loss_Q: [2.08 2.16 1.31 0.   5.54] Loss_P: [ 3.78  2.29  2.76  1.41 10.23]\n",
      "Loss_Q: [2.09 2.13 1.29 0.   5.51] Loss_P: [ 3.78  2.33  2.77  1.43 10.31]\n",
      "Loss_Q: [2.2  2.18 1.33 0.   5.71] Loss_P: [ 3.83  2.25  2.76  1.4  10.24]\n",
      "Loss_Q: [1.97 2.13 1.3  0.   5.39] Loss_P: [ 3.81  2.25  2.74  1.38 10.18]\n",
      "Loss_Q: [2.01 2.21 1.23 0.   5.45] Loss_P: [ 3.73  2.26  2.78  1.33 10.1 ]\n",
      "Loss_Q: [2.05 2.2  1.28 0.   5.53] Loss_P: [ 3.78  2.33  2.82  1.41 10.34]\n",
      "Loss_Q: [1.94 2.19 1.3  0.   5.43] Loss_P: [ 3.73  2.21  2.78  1.37 10.1 ]\n",
      "Loss_Q: [2.05 2.18 1.29 0.   5.51] Loss_P: [ 3.78  2.21  2.81  1.38 10.18]\n",
      "Loss_Q: [1.99 2.13 1.31 0.   5.44] Loss_P: [ 3.76  2.3   2.77  1.41 10.25]\n",
      "Loss_Q: [1.94 2.22 1.3  0.   5.46] Loss_P: [ 3.83  2.23  2.81  1.39 10.26]\n",
      "Loss_Q: [2.03 2.16 1.25 0.   5.44] Loss_P: [ 3.78  2.26  2.77  1.37 10.18]\n",
      "Loss_Q: [1.97 2.19 1.27 0.   5.43] Loss_P: [ 3.79  2.25  2.79  1.37 10.2 ]\n",
      "Loss_Q: [1.95 2.17 1.23 0.   5.35] Loss_P: [ 3.81  2.25  2.75  1.32 10.14]\n",
      "Loss_Q: [1.96 2.23 1.23 0.   5.42] Loss_P: [ 3.75  2.31  2.76  1.35 10.17]\n",
      "Loss_Q: [1.94 2.14 1.21 0.   5.29] Loss_P: [ 3.79  2.31  2.77  1.34 10.21]\n",
      "Loss_Q: [1.93 2.16 1.23 0.   5.33] Loss_P: [ 3.88  2.29  2.75  1.35 10.27]\n",
      "Loss_Q: [1.93 2.16 1.22 0.   5.3 ] Loss_P: [ 3.85  2.3   2.74  1.34 10.23]\n",
      "Loss_Q: [2.02 2.19 1.27 0.   5.48] Loss_P: [ 3.78  2.19  2.72  1.34 10.04]\n",
      "Loss_Q: [1.92 2.16 1.19 0.   5.27] Loss_P: [ 3.88  2.2   2.77  1.31 10.16]\n",
      "Loss_Q: [1.95 2.17 1.24 0.   5.37] Loss_P: [ 3.8   2.21  2.72  1.32 10.06]\n",
      "Loss_Q: [2.08 2.18 1.27 0.   5.53] Loss_P: [ 3.74  2.25  2.74  1.35 10.08]\n",
      "Loss_Q: [1.91 2.19 1.27 0.   5.38] Loss_P: [ 3.85  2.22  2.79  1.34 10.21]\n",
      "Loss_Q: [1.97 2.2  1.27 0.   5.43] Loss_P: [ 3.84  2.2   2.75  1.34 10.13]\n",
      "Loss_Q: [1.97 2.22 1.27 0.   5.45] Loss_P: [ 3.78  2.21  2.75  1.36 10.1 ]\n",
      "Loss_Q: [1.9  2.24 1.26 0.   5.4 ] Loss_P: [ 3.8   2.23  2.73  1.33 10.08]\n",
      "Loss_Q: [1.88 2.21 1.25 0.   5.34] Loss_P: [ 3.82  2.15  2.79  1.32 10.08]\n",
      "Loss_Q: [1.93 2.21 1.26 0.   5.4 ] Loss_P: [ 3.78  2.22  2.8   1.37 10.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.98 2.25 1.31 0.   5.55] Loss_P: [ 3.81  2.21  2.83  1.37 10.23]\n",
      "Loss_Q: [1.87 2.27 1.28 0.   5.41] Loss_P: [ 3.83  2.25  2.81  1.4  10.29]\n",
      "Loss_Q: [2.01 2.22 1.31 0.   5.54] Loss_P: [ 3.84  2.27  2.8   1.37 10.28]\n",
      "Loss_Q: [2.02 2.2  1.28 0.   5.5 ] Loss_P: [ 3.83  2.25  2.77  1.4  10.26]\n",
      "Loss_Q: [1.98 2.21 1.25 0.   5.44] Loss_P: [ 3.8   2.26  2.73  1.37 10.16]\n",
      "Loss_Q: [1.89 2.19 1.28 0.   5.35] Loss_P: [ 3.85  2.16  2.78  1.38 10.16]\n",
      "Loss_Q: [1.91 2.18 1.28 0.   5.37] Loss_P: [ 3.77  2.24  2.72  1.38 10.1 ]\n",
      "Loss_Q: [1.95 2.14 1.21 0.   5.29] Loss_P: [ 3.82  2.21  2.72  1.33 10.09]\n",
      "Loss_Q: [1.95 2.16 1.26 0.   5.37] Loss_P: [ 3.84  2.24  2.71  1.33 10.12]\n",
      "Loss_Q: [1.92 2.17 1.26 0.   5.36] Loss_P: [ 3.84  2.17  2.72  1.33 10.05]\n",
      "Loss_Q: [1.96 2.09 1.22 0.   5.27] Loss_P: [ 3.76  2.26  2.65  1.38 10.05]\n",
      "Loss_Q: [2.   2.2  1.32 0.   5.52] Loss_P: [ 3.83  2.28  2.77  1.39 10.27]\n",
      "Loss_Q: [1.97 2.15 1.31 0.   5.44] Loss_P: [ 3.81  2.3   2.7   1.37 10.17]\n",
      "Loss_Q: [2.01 2.12 1.24 0.   5.36] Loss_P: [ 3.83  2.23  2.68  1.37 10.11]\n",
      "Loss_Q: [1.93 2.13 1.27 0.   5.32] Loss_P: [ 3.75  2.27  2.69  1.36 10.07]\n",
      "Loss_Q: [1.99 2.14 1.28 0.   5.4 ] Loss_P: [ 3.84  2.23  2.69  1.36 10.11]\n",
      "Loss_Q: [1.94 2.11 1.22 0.   5.27] Loss_P: [ 3.8   2.22  2.7   1.29 10.01]\n",
      "Loss_Q: [1.95 2.17 1.18 0.   5.29] Loss_P: [ 3.84  2.25  2.74  1.33 10.15]\n",
      "Loss_Q: [1.97 2.21 1.22 0.   5.39] Loss_P: [3.75 2.23 2.72 1.27 9.98]\n",
      "Loss_Q: [1.97 2.15 1.18 0.   5.3 ] Loss_P: [3.83 2.21 2.69 1.25 9.99]\n",
      "Loss_Q: [1.84 2.09 1.15 0.   5.08] Loss_P: [3.74 2.2  2.62 1.27 9.85]\n",
      "Loss_Q: [1.93 2.08 1.18 0.   5.19] Loss_P: [3.8  2.23 2.65 1.3  9.98]\n",
      "Loss_Q: [1.92 2.13 1.22 0.   5.27] Loss_P: [3.8  2.14 2.62 1.31 9.87]\n",
      "Loss_Q: [1.93 2.11 1.26 0.   5.3 ] Loss_P: [3.85 2.18 2.57 1.3  9.9 ]\n",
      "Loss_Q: [1.83 2.08 1.18 0.   5.09] Loss_P: [3.78 2.19 2.59 1.29 9.85]\n",
      "Loss_Q: [1.88 2.13 1.15 0.   5.16] Loss_P: [3.84 2.17 2.6  1.25 9.86]\n",
      "Loss_Q: [1.86 2.07 1.14 0.   5.07] Loss_P: [3.74 2.21 2.61 1.23 9.8 ]\n",
      "Loss_Q: [1.9  1.97 1.1  0.   4.97] Loss_P: [3.86 2.24 2.56 1.2  9.87]\n",
      "Loss_Q: [1.91 2.08 1.15 0.   5.14] Loss_P: [3.81 2.18 2.58 1.19 9.77]\n",
      "Loss_Q: [1.95 2.03 1.18 0.   5.16] Loss_P: [3.79 2.23 2.57 1.24 9.84]\n",
      "Loss_Q: [1.88 2.1  1.15 0.   5.12] Loss_P: [3.89 2.17 2.55 1.22 9.83]\n",
      "Loss_Q: [1.94 2.06 1.07 0.   5.07] Loss_P: [3.83 2.26 2.57 1.24 9.91]\n",
      "Loss_Q: [1.93 2.12 1.13 0.   5.17] Loss_P: [3.83 2.16 2.57 1.21 9.77]\n",
      "Loss_Q: [1.98 2.11 1.12 0.   5.21] Loss_P: [3.83 2.19 2.59 1.21 9.83]\n",
      "Loss_Q: [1.95 2.13 1.06 0.   5.14] Loss_P: [3.81 2.23 2.64 1.21 9.89]\n",
      "Loss_Q: [1.95 2.08 1.13 0.   5.17] Loss_P: [3.76 2.23 2.6  1.17 9.75]\n",
      "Loss_Q: [1.82 2.17 1.09 0.   5.08] Loss_P: [3.85 2.26 2.67 1.16 9.94]\n",
      "Loss_Q: [1.94 2.09 1.1  0.   5.13] Loss_P: [3.82 2.17 2.71 1.19 9.89]\n",
      "Loss_Q: [1.93 2.13 1.08 0.   5.14] Loss_P: [3.84 2.18 2.64 1.16 9.82]\n",
      "Loss_Q: [2.01 2.12 1.07 0.   5.2 ] Loss_P: [3.88 2.18 2.65 1.16 9.88]\n",
      "Loss_Q: [1.94 2.14 1.13 0.   5.22] Loss_P: [3.83 2.27 2.67 1.16 9.94]\n",
      "Loss_Q: [1.87 2.14 1.08 0.   5.09] Loss_P: [ 3.8   2.26  2.77  1.19 10.02]\n",
      "Loss_Q: [1.92 2.17 1.04 0.   5.13] Loss_P: [ 3.85  2.27  2.73  1.18 10.02]\n",
      "Loss_Q: [1.95 2.2  1.08 0.   5.23] Loss_P: [ 3.85  2.26  2.72  1.17 10.  ]\n",
      "Loss_Q: [2.   2.2  1.07 0.   5.27] Loss_P: [ 3.85  2.27  2.71  1.18 10.01]\n",
      "Loss_Q: [1.87 2.11 1.15 0.   5.13] Loss_P: [3.82 2.23 2.66 1.22 9.93]\n",
      "Loss_Q: [1.91 2.12 1.09 0.   5.12] Loss_P: [3.88 2.17 2.68 1.18 9.92]\n",
      "Loss_Q: [2.   2.14 1.09 0.   5.23] Loss_P: [3.81 2.22 2.71 1.21 9.95]\n",
      "Loss_Q: [2.01 2.14 1.08 0.   5.23] Loss_P: [3.81 2.18 2.72 1.19 9.91]\n",
      "Loss_Q: [2.07 2.18 1.12 0.   5.36] Loss_P: [3.79 2.27 2.67 1.17 9.9 ]\n",
      "Loss_Q: [2.02 2.14 1.1  0.   5.26] Loss_P: [3.84 2.26 2.74 1.16 9.99]\n",
      "Loss_Q: [1.84 2.15 1.12 0.   5.1 ] Loss_P: [3.75 2.24 2.69 1.23 9.92]\n",
      "Loss_Q: [2.02 2.14 1.12 0.   5.28] Loss_P: [3.86 2.22 2.67 1.21 9.97]\n",
      "Loss_Q: [2.   2.13 1.1  0.   5.23] Loss_P: [3.73 2.22 2.64 1.25 9.84]\n",
      "Loss_Q: [1.96 2.11 1.15 0.   5.23] Loss_P: [ 3.89  2.21  2.66  1.27 10.02]\n",
      "Loss_Q: [1.96 2.14 1.17 0.   5.26] Loss_P: [ 3.85  2.25  2.64  1.27 10.01]\n",
      "Loss_Q: [1.99 2.14 1.16 0.   5.28] Loss_P: [3.84 2.15 2.65 1.26 9.91]\n",
      "Loss_Q: [1.96 2.14 1.2  0.   5.3 ] Loss_P: [ 3.85  2.27  2.71  1.25 10.07]\n",
      "Loss_Q: [1.92 2.08 1.14 0.   5.13] Loss_P: [ 3.9   2.27  2.66  1.29 10.11]\n",
      "Loss_Q: [2.   2.13 1.18 0.   5.31] Loss_P: [ 3.94  2.26  2.64  1.31 10.16]\n",
      "Loss_Q: [2.   2.14 1.21 0.   5.35] Loss_P: [ 3.82  2.25  2.63  1.32 10.02]\n",
      "Loss_Q: [2.07 2.19 1.18 0.   5.45] Loss_P: [ 3.83  2.26  2.64  1.3  10.03]\n",
      "Loss_Q: [1.95 2.13 1.15 0.   5.23] Loss_P: [ 3.86  2.31  2.63  1.25 10.05]\n",
      "Loss_Q: [2.09 2.14 1.17 0.   5.4 ] Loss_P: [ 3.89  2.21  2.67  1.29 10.06]\n",
      "Loss_Q: [2.07 2.1  1.15 0.   5.32] Loss_P: [ 3.86  2.26  2.7   1.27 10.09]\n",
      "Loss_Q: [2.   2.14 1.14 0.   5.28] Loss_P: [3.8  2.24 2.69 1.24 9.98]\n",
      "Loss_Q: [2.09 2.08 1.21 0.   5.39] Loss_P: [ 3.91  2.23  2.68  1.21 10.02]\n",
      "Loss_Q: [2.05 2.17 1.21 0.   5.42] Loss_P: [ 3.75  2.28  2.68  1.31 10.01]\n",
      "Loss_Q: [1.95 2.1  1.18 0.   5.23] Loss_P: [ 3.82  2.25  2.69  1.3  10.06]\n",
      "Loss_Q: [2.07 2.12 1.14 0.   5.33] Loss_P: [3.83 2.25 2.64 1.27 9.99]\n",
      "Loss_Q: [2.02 2.1  1.24 0.   5.36] Loss_P: [ 3.82  2.27  2.68  1.28 10.06]\n",
      "Loss_Q: [2.01 2.16 1.19 0.   5.36] Loss_P: [ 3.74  2.33  2.68  1.32 10.08]\n",
      "Loss_Q: [2.03 2.09 1.2  0.   5.32] Loss_P: [ 3.78  2.32  2.68  1.31 10.09]\n",
      "Loss_Q: [2.04 2.1  1.25 0.   5.39] Loss_P: [ 3.81  2.3   2.68  1.33 10.12]\n",
      "Loss_Q: [2.04 2.07 1.25 0.   5.36] Loss_P: [ 3.83  2.27  2.62  1.29 10.  ]\n",
      "Loss_Q: [2.   2.19 1.2  0.   5.39] Loss_P: [ 3.87  2.25  2.73  1.31 10.16]\n",
      "Loss_Q: [1.95 2.07 1.16 0.   5.17] Loss_P: [3.74 2.25 2.66 1.27 9.92]\n",
      "Loss_Q: [1.95 2.15 1.22 0.   5.32] Loss_P: [3.84 2.19 2.67 1.28 9.98]\n",
      "Loss_Q: [2.06 2.15 1.19 0.   5.4 ] Loss_P: [ 3.8   2.27  2.69  1.3  10.06]\n",
      "Loss_Q: [1.87 2.13 1.22 0.   5.22] Loss_P: [ 3.79  2.23  2.72  1.27 10.01]\n",
      "Loss_Q: [2.   2.14 1.21 0.   5.34] Loss_P: [ 3.79  2.26  2.75  1.29 10.09]\n",
      "Loss_Q: [2.04 2.17 1.16 0.   5.37] Loss_P: [ 3.81  2.25  2.75  1.28 10.09]\n",
      "Loss_Q: [2.06 2.22 1.23 0.   5.51] Loss_P: [ 3.81  2.23  2.71  1.28 10.02]\n",
      "Loss_Q: [1.97 2.22 1.21 0.   5.4 ] Loss_P: [ 3.79  2.19  2.79  1.32 10.08]\n",
      "Loss_Q: [1.92 2.17 1.21 0.   5.3 ] Loss_P: [ 3.75  2.31  2.77  1.32 10.15]\n",
      "Loss_Q: [1.95 2.14 1.19 0.   5.28] Loss_P: [ 3.82  2.18  2.76  1.29 10.06]\n",
      "Loss_Q: [1.94 2.13 1.22 0.   5.29] Loss_P: [ 3.81  2.21  2.73  1.28 10.02]\n",
      "Loss_Q: [1.95 2.17 1.21 0.   5.33] Loss_P: [ 3.75  2.25  2.72  1.31 10.02]\n",
      "Loss_Q: [1.91 2.2  1.24 0.   5.35] Loss_P: [ 3.87  2.19  2.74  1.36 10.16]\n",
      "Loss_Q: [1.99 2.22 1.21 0.   5.41] Loss_P: [ 3.8   2.23  2.73  1.28 10.04]\n",
      "Loss_Q: [2.03 2.18 1.17 0.   5.39] Loss_P: [ 3.75  2.24  2.76  1.27 10.02]\n",
      "Loss_Q: [1.91 2.18 1.2  0.   5.29] Loss_P: [3.81 2.14 2.73 1.28 9.97]\n",
      "Loss_Q: [1.99 2.19 1.25 0.   5.42] Loss_P: [ 3.82  2.22  2.77  1.29 10.11]\n",
      "Loss_Q: [1.9  2.19 1.25 0.   5.34] Loss_P: [ 3.82  2.21  2.72  1.29 10.04]\n",
      "Loss_Q: [1.92 2.21 1.25 0.   5.38] Loss_P: [ 3.76  2.18  2.76  1.32 10.03]\n",
      "Loss_Q: [1.94 2.24 1.22 0.   5.39] Loss_P: [ 3.76  2.26  2.72  1.26 10.01]\n",
      "Loss_Q: [1.99 2.16 1.18 0.   5.33] Loss_P: [ 3.76  2.32  2.75  1.3  10.14]\n",
      "Loss_Q: [2.   2.14 1.21 0.   5.35] Loss_P: [ 3.79  2.28  2.76  1.3  10.12]\n",
      "Loss_Q: [2.02 2.17 1.25 0.   5.45] Loss_P: [ 3.78  2.27  2.77  1.33 10.15]\n",
      "Loss_Q: [2.07 2.13 1.2  0.   5.4 ] Loss_P: [ 3.77  2.28  2.76  1.3  10.11]\n",
      "Loss_Q: [1.88 2.19 1.16 0.   5.24] Loss_P: [ 3.77  2.23  2.78  1.24 10.03]\n",
      "Loss_Q: [2.04 2.19 1.2  0.   5.43] Loss_P: [ 3.78  2.3   2.81  1.26 10.15]\n",
      "Loss_Q: [1.99 2.17 1.23 0.   5.4 ] Loss_P: [ 3.66  2.28  2.77  1.31 10.02]\n",
      "Loss_Q: [2.   2.22 1.23 0.   5.45] Loss_P: [ 3.85  2.21  2.78  1.28 10.12]\n",
      "Loss_Q: [1.98 2.19 1.16 0.   5.32] Loss_P: [ 3.8   2.26  2.8   1.26 10.13]\n",
      "Loss_Q: [1.95 2.16 1.14 0.   5.26] Loss_P: [ 3.74  2.31  2.76  1.25 10.06]\n",
      "Loss_Q: [1.96 2.16 1.13 0.   5.26] Loss_P: [ 3.83  2.19  2.76  1.22 10.  ]\n",
      "Loss_Q: [1.84 2.22 1.15 0.   5.21] Loss_P: [3.78 2.2  2.74 1.2  9.92]\n",
      "Loss_Q: [1.98 2.17 1.12 0.   5.27] Loss_P: [3.75 2.25 2.77 1.21 9.98]\n",
      "Loss_Q: [1.96 2.15 1.11 0.   5.23] Loss_P: [ 3.83  2.25  2.74  1.2  10.02]\n",
      "Loss_Q: [1.98 2.21 1.15 0.   5.34] Loss_P: [ 3.76  2.26  2.77  1.21 10.01]\n",
      "Loss_Q: [2.01 2.21 1.15 0.   5.36] Loss_P: [ 3.89  2.19  2.71  1.23 10.03]\n",
      "Loss_Q: [1.98 2.17 1.11 0.   5.25] Loss_P: [ 3.83  2.21  2.73  1.26 10.03]\n",
      "Loss_Q: [1.99 2.15 1.12 0.   5.25] Loss_P: [3.85 2.15 2.73 1.19 9.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.06 2.13 1.14 0.   5.33] Loss_P: [3.79 2.18 2.73 1.2  9.9 ]\n",
      "Loss_Q: [1.98 2.11 1.16 0.   5.26] Loss_P: [3.79 2.26 2.69 1.2  9.94]\n",
      "Loss_Q: [2.02 2.13 1.12 0.   5.27] Loss_P: [3.77 2.29 2.67 1.15 9.88]\n",
      "Loss_Q: [1.94 2.13 1.15 0.   5.23] Loss_P: [3.81 2.18 2.75 1.22 9.96]\n",
      "Loss_Q: [1.94 2.18 1.15 0.   5.27] Loss_P: [ 3.81  2.21  2.75  1.24 10.01]\n",
      "Loss_Q: [1.94 2.16 1.15 0.   5.26] Loss_P: [3.85 2.16 2.73 1.23 9.96]\n",
      "Loss_Q: [1.93 2.11 1.15 0.   5.19] Loss_P: [ 3.89  2.2   2.72  1.22 10.03]\n",
      "Loss_Q: [1.99 2.11 1.1  0.   5.2 ] Loss_P: [ 3.84  2.29  2.73  1.2  10.07]\n",
      "Loss_Q: [2.04 2.13 1.17 0.   5.35] Loss_P: [ 3.71  2.35  2.76  1.24 10.05]\n",
      "Loss_Q: [2.02 2.11 1.16 0.   5.29] Loss_P: [ 3.8   2.27  2.77  1.23 10.07]\n",
      "Loss_Q: [2.05 2.16 1.13 0.   5.34] Loss_P: [3.76 2.26 2.77 1.19 9.99]\n",
      "Loss_Q: [2.05 2.21 1.14 0.   5.39] Loss_P: [ 3.74  2.29  2.76  1.24 10.02]\n",
      "Loss_Q: [1.95 2.14 1.09 0.   5.18] Loss_P: [ 3.76  2.3   2.79  1.2  10.05]\n",
      "Loss_Q: [2.03 2.09 1.12 0.   5.24] Loss_P: [3.7  2.33 2.75 1.18 9.96]\n",
      "Loss_Q: [2.13 2.19 1.17 0.   5.48] Loss_P: [ 3.72  2.37  2.81  1.19 10.08]\n",
      "Loss_Q: [2.1  2.19 1.19 0.   5.48] Loss_P: [ 3.69  2.35  2.76  1.22 10.03]\n",
      "Loss_Q: [2.06 2.19 1.2  0.   5.46] Loss_P: [ 3.78  2.29  2.83  1.24 10.13]\n",
      "Loss_Q: [2.   2.13 1.18 0.   5.32] Loss_P: [ 3.66  2.35  2.78  1.28 10.08]\n",
      "Loss_Q: [1.96 2.21 1.18 0.   5.36] Loss_P: [ 3.76  2.27  2.82  1.25 10.1 ]\n",
      "Loss_Q: [2.08 2.25 1.19 0.   5.52] Loss_P: [ 3.71  2.39  2.81  1.25 10.16]\n",
      "Loss_Q: [2.09 2.16 1.19 0.   5.45] Loss_P: [ 3.68  2.39  2.76  1.32 10.13]\n",
      "Loss_Q: [2.06 2.21 1.25 0.   5.51] Loss_P: [ 3.71  2.44  2.79  1.31 10.25]\n",
      "Loss_Q: [1.97 2.17 1.2  0.   5.34] Loss_P: [ 3.65  2.32  2.8   1.29 10.07]\n",
      "Loss_Q: [2.05 2.12 1.25 0.   5.41] Loss_P: [ 3.69  2.45  2.8   1.28 10.23]\n",
      "Loss_Q: [2.02 2.17 1.14 0.   5.33] Loss_P: [ 3.74  2.29  2.8   1.25 10.09]\n",
      "Loss_Q: [2.   2.2  1.17 0.   5.38] Loss_P: [ 3.76  2.31  2.81  1.26 10.14]\n",
      "Loss_Q: [2.01 2.21 1.16 0.   5.37] Loss_P: [ 3.74  2.36  2.83  1.26 10.19]\n",
      "Loss_Q: [2.05 2.16 1.17 0.   5.38] Loss_P: [ 3.75  2.33  2.78  1.21 10.07]\n",
      "Loss_Q: [1.99 2.16 1.12 0.   5.27] Loss_P: [ 3.71  2.29  2.78  1.22 10.  ]\n",
      "Loss_Q: [2.02 2.16 1.16 0.   5.34] Loss_P: [ 3.66  2.3   2.81  1.23 10.  ]\n",
      "Loss_Q: [2.02 2.19 1.17 0.   5.38] Loss_P: [ 3.72  2.3   2.81  1.21 10.03]\n",
      "Loss_Q: [1.98 2.16 1.15 0.   5.28] Loss_P: [ 3.78  2.28  2.81  1.27 10.13]\n",
      "Loss_Q: [1.97 2.21 1.15 0.   5.33] Loss_P: [ 3.77  2.34  2.78  1.25 10.14]\n",
      "Loss_Q: [2.03 2.15 1.19 0.   5.36] Loss_P: [ 3.71  2.37  2.78  1.25 10.11]\n",
      "Loss_Q: [1.97 2.16 1.15 0.   5.29] Loss_P: [ 3.72  2.37  2.74  1.25 10.09]\n",
      "Loss_Q: [2.06 2.22 1.16 0.   5.44] Loss_P: [ 3.68  2.42  2.78  1.24 10.13]\n",
      "Loss_Q: [2.07 2.15 1.13 0.   5.34] Loss_P: [ 3.7   2.32  2.79  1.21 10.02]\n",
      "Loss_Q: [1.95 2.19 1.16 0.   5.29] Loss_P: [ 3.7   2.36  2.82  1.24 10.12]\n",
      "Loss_Q: [2.11 2.19 1.16 0.   5.46] Loss_P: [ 3.74  2.41  2.79  1.24 10.17]\n",
      "Loss_Q: [2.05 2.2  1.19 0.   5.43] Loss_P: [ 3.78  2.3   2.79  1.22 10.08]\n",
      "Loss_Q: [2.05 2.14 1.14 0.   5.33] Loss_P: [ 3.84  2.29  2.77  1.21 10.12]\n",
      "Loss_Q: [2.03 2.23 1.16 0.   5.42] Loss_P: [ 3.65  2.4   2.8   1.27 10.12]\n",
      "Loss_Q: [2.07 2.11 1.22 0.   5.41] Loss_P: [ 3.79  2.27  2.78  1.26 10.11]\n",
      "Loss_Q: [2.04 2.17 1.22 0.   5.43] Loss_P: [ 3.73  2.34  2.79  1.24 10.1 ]\n",
      "Loss_Q: [2.15 2.15 1.24 0.   5.54] Loss_P: [ 3.74  2.3   2.81  1.27 10.12]\n",
      "Loss_Q: [2.02 2.21 1.23 0.   5.46] Loss_P: [ 3.69  2.31  2.77  1.32 10.09]\n",
      "Loss_Q: [2.07 2.15 1.22 0.   5.44] Loss_P: [ 3.74  2.34  2.78  1.3  10.16]\n",
      "Loss_Q: [2.13 2.18 1.28 0.   5.58] Loss_P: [ 3.68  2.39  2.8   1.31 10.18]\n",
      "Loss_Q: [2.06 2.15 1.27 0.   5.48] Loss_P: [ 3.79  2.37  2.79  1.32 10.28]\n",
      "Loss_Q: [2.1  2.24 1.26 0.   5.6 ] Loss_P: [ 3.76  2.36  2.79  1.34 10.25]\n",
      "Loss_Q: [2.01 2.14 1.28 0.   5.43] Loss_P: [ 3.75  2.39  2.78  1.32 10.24]\n",
      "Loss_Q: [1.99 2.19 1.22 0.   5.4 ] Loss_P: [ 3.76  2.35  2.75  1.31 10.17]\n",
      "Loss_Q: [2.06 2.13 1.25 0.   5.44] Loss_P: [ 3.78  2.37  2.76  1.34 10.26]\n",
      "Loss_Q: [2.07 2.19 1.23 0.   5.49] Loss_P: [ 3.81  2.27  2.77  1.28 10.12]\n",
      "Loss_Q: [1.97 2.17 1.26 0.   5.4 ] Loss_P: [ 3.84  2.27  2.7   1.28 10.09]\n",
      "Loss_Q: [2.03 2.11 1.21 0.   5.34] Loss_P: [ 3.74  2.35  2.74  1.31 10.14]\n",
      "Loss_Q: [2.08 2.16 1.2  0.   5.45] Loss_P: [ 3.87  2.29  2.78  1.28 10.23]\n",
      "Loss_Q: [2.09 2.14 1.24 0.   5.47] Loss_P: [ 3.75  2.29  2.79  1.29 10.13]\n",
      "Loss_Q: [2.07 2.18 1.27 0.   5.53] Loss_P: [ 3.67  2.35  2.8   1.33 10.15]\n",
      "Loss_Q: [2.03 2.22 1.25 0.   5.5 ] Loss_P: [ 3.73  2.31  2.82  1.33 10.19]\n",
      "Loss_Q: [1.96 2.23 1.27 0.   5.46] Loss_P: [ 3.72  2.33  2.81  1.32 10.18]\n",
      "Loss_Q: [2.09 2.13 1.3  0.   5.52] Loss_P: [ 3.74  2.27  2.79  1.34 10.14]\n",
      "Loss_Q: [2.07 2.22 1.24 0.   5.52] Loss_P: [ 3.69  2.31  2.82  1.32 10.14]\n",
      "Loss_Q: [2.05 2.18 1.2  0.   5.43] Loss_P: [ 3.75  2.31  2.83  1.31 10.21]\n",
      "Loss_Q: [2.07 2.13 1.26 0.   5.46] Loss_P: [ 3.71  2.32  2.86  1.29 10.19]\n",
      "Loss_Q: [1.99 2.23 1.22 0.   5.45] Loss_P: [ 3.65  2.26  2.82  1.31 10.04]\n",
      "Loss_Q: [1.99 2.18 1.23 0.   5.39] Loss_P: [ 3.76  2.27  2.85  1.3  10.17]\n",
      "Loss_Q: [1.97 2.23 1.25 0.   5.45] Loss_P: [ 3.77  2.27  2.79  1.3  10.13]\n",
      "Loss_Q: [1.95 2.23 1.24 0.   5.43] Loss_P: [3.69 2.24 2.77 1.28 9.98]\n",
      "Loss_Q: [2.06 2.12 1.24 0.   5.42] Loss_P: [ 3.76  2.29  2.8   1.27 10.11]\n",
      "Loss_Q: [1.99 2.15 1.27 0.   5.42] Loss_P: [ 3.8   2.34  2.8   1.31 10.25]\n",
      "Loss_Q: [1.94 2.19 1.24 0.   5.37] Loss_P: [ 3.75  2.35  2.82  1.31 10.24]\n",
      "Loss_Q: [1.99 2.2  1.23 0.   5.42] Loss_P: [ 3.69  2.29  2.83  1.3  10.1 ]\n",
      "Loss_Q: [1.98 2.21 1.25 0.   5.44] Loss_P: [ 3.73  2.29  2.81  1.32 10.14]\n",
      "Loss_Q: [2.04 2.21 1.27 0.   5.52] Loss_P: [ 3.78  2.29  2.81  1.31 10.2 ]\n",
      "Loss_Q: [1.99 2.24 1.27 0.   5.51] Loss_P: [ 3.69  2.3   2.82  1.33 10.15]\n",
      "Loss_Q: [2.03 2.2  1.28 0.   5.51] Loss_P: [ 3.76  2.27  2.88  1.37 10.28]\n",
      "Loss_Q: [1.98 2.25 1.25 0.   5.48] Loss_P: [ 3.76  2.3   2.81  1.32 10.19]\n",
      "Loss_Q: [2.03 2.19 1.28 0.   5.49] Loss_P: [ 3.79  2.3   2.84  1.31 10.24]\n",
      "Loss_Q: [2.01 2.18 1.26 0.   5.44] Loss_P: [ 3.75  2.37  2.88  1.36 10.35]\n",
      "Loss_Q: [2.07 2.21 1.26 0.   5.54] Loss_P: [ 3.71  2.34  2.83  1.33 10.21]\n",
      "Loss_Q: [2.07 2.26 1.28 0.   5.61] Loss_P: [ 3.71  2.32  2.83  1.36 10.22]\n",
      "Loss_Q: [2.06 2.24 1.29 0.   5.59] Loss_P: [ 3.78  2.34  2.85  1.39 10.36]\n",
      "Loss_Q: [2.02 2.18 1.3  0.   5.49] Loss_P: [ 3.74  2.35  2.82  1.37 10.29]\n",
      "Loss_Q: [2.05 2.2  1.27 0.   5.53] Loss_P: [ 3.78  2.32  2.84  1.4  10.33]\n",
      "Loss_Q: [2.04 2.23 1.34 0.   5.61] Loss_P: [ 3.69  2.35  2.83  1.4  10.28]\n",
      "Loss_Q: [2.05 2.21 1.34 0.   5.6 ] Loss_P: [ 3.72  2.38  2.82  1.38 10.31]\n",
      "Loss_Q: [2.   2.21 1.29 0.   5.5 ] Loss_P: [ 3.79  2.21  2.83  1.39 10.23]\n",
      "Loss_Q: [1.97 2.2  1.32 0.   5.5 ] Loss_P: [ 3.74  2.19  2.82  1.39 10.14]\n",
      "Loss_Q: [1.93 2.21 1.31 0.   5.45] Loss_P: [ 3.74  2.29  2.81  1.38 10.21]\n",
      "Loss_Q: [2.02 2.14 1.33 0.   5.5 ] Loss_P: [ 3.84  2.21  2.78  1.4  10.23]\n",
      "Loss_Q: [2.04 2.22 1.31 0.   5.57] Loss_P: [ 3.77  2.32  2.79  1.37 10.25]\n",
      "Loss_Q: [2.01 2.18 1.31 0.   5.5 ] Loss_P: [ 3.81  2.3   2.79  1.41 10.31]\n",
      "Loss_Q: [1.98 2.18 1.3  0.   5.45] Loss_P: [ 3.84  2.23  2.78  1.38 10.23]\n",
      "Loss_Q: [2.01 2.22 1.27 0.   5.5 ] Loss_P: [ 3.72  2.38  2.79  1.4  10.29]\n",
      "Loss_Q: [1.97 2.21 1.28 0.   5.47] Loss_P: [ 3.67  2.26  2.76  1.37 10.05]\n",
      "Loss_Q: [1.97 2.15 1.35 0.   5.47] Loss_P: [ 3.7   2.36  2.76  1.41 10.22]\n",
      "Loss_Q: [1.92 2.15 1.31 0.   5.38] Loss_P: [ 3.71  2.31  2.79  1.4  10.2 ]\n",
      "Loss_Q: [1.97 2.22 1.33 0.   5.52] Loss_P: [ 3.75  2.29  2.8   1.41 10.24]\n",
      "Loss_Q: [1.99 2.24 1.37 0.   5.6 ] Loss_P: [ 3.73  2.24  2.82  1.42 10.21]\n",
      "Loss_Q: [2.04 2.18 1.3  0.   5.52] Loss_P: [ 3.71  2.33  2.81  1.45 10.3 ]\n",
      "Loss_Q: [2.03 2.22 1.32 0.   5.57] Loss_P: [ 3.71  2.27  2.79  1.4  10.18]\n",
      "Loss_Q: [1.98 2.2  1.31 0.   5.49] Loss_P: [ 3.7   2.35  2.81  1.39 10.26]\n",
      "Loss_Q: [2.1  2.13 1.28 0.   5.51] Loss_P: [ 3.71  2.38  2.8   1.38 10.26]\n",
      "Loss_Q: [1.95 2.18 1.24 0.   5.37] Loss_P: [ 3.77  2.26  2.78  1.35 10.17]\n",
      "Loss_Q: [1.96 2.21 1.31 0.   5.48] Loss_P: [ 3.75  2.3   2.83  1.36 10.25]\n",
      "Loss_Q: [1.98 2.16 1.25 0.   5.39] Loss_P: [ 3.78  2.25  2.79  1.35 10.18]\n",
      "Loss_Q: [1.96 2.12 1.31 0.   5.4 ] Loss_P: [ 3.8   2.19  2.79  1.38 10.15]\n",
      "Loss_Q: [2.01 2.09 1.29 0.   5.39] Loss_P: [ 3.74  2.24  2.78  1.35 10.11]\n",
      "Loss_Q: [2.03 2.17 1.3  0.   5.5 ] Loss_P: [ 3.68  2.26  2.76  1.35 10.05]\n",
      "Loss_Q: [2.04 2.17 1.25 0.   5.46] Loss_P: [ 3.73  2.27  2.79  1.32 10.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.01 2.08 1.28 0.   5.37] Loss_P: [ 3.82  2.23  2.76  1.34 10.16]\n",
      "Loss_Q: [1.99 2.2  1.3  0.   5.49] Loss_P: [ 3.8   2.23  2.82  1.37 10.22]\n",
      "Loss_Q: [2.05 2.17 1.3  0.   5.52] Loss_P: [ 3.71  2.27  2.78  1.38 10.14]\n",
      "Loss_Q: [1.9  2.19 1.28 0.   5.37] Loss_P: [ 3.75  2.29  2.8   1.4  10.24]\n",
      "Loss_Q: [1.98 2.14 1.35 0.   5.47] Loss_P: [ 3.69  2.22  2.8   1.42 10.13]\n",
      "Loss_Q: [1.94 2.2  1.32 0.   5.46] Loss_P: [ 3.78  2.25  2.81  1.41 10.25]\n",
      "Loss_Q: [1.97 2.17 1.3  0.   5.44] Loss_P: [ 3.71  2.31  2.81  1.41 10.24]\n",
      "Loss_Q: [1.97 2.11 1.33 0.   5.42] Loss_P: [ 3.75  2.26  2.83  1.41 10.24]\n",
      "Loss_Q: [2.03 2.16 1.33 0.   5.52] Loss_P: [ 3.73  2.33  2.81  1.41 10.28]\n",
      "Loss_Q: [2.1  2.11 1.32 0.   5.53] Loss_P: [ 3.78  2.24  2.82  1.4  10.23]\n",
      "Loss_Q: [2.06 2.24 1.32 0.   5.61] Loss_P: [ 3.74  2.4   2.8   1.41 10.35]\n",
      "Loss_Q: [1.99 2.2  1.37 0.   5.55] Loss_P: [ 3.81  2.32  2.85  1.41 10.4 ]\n",
      "Loss_Q: [2.03 2.17 1.34 0.   5.54] Loss_P: [ 3.72  2.32  2.85  1.43 10.32]\n",
      "Loss_Q: [1.96 2.23 1.37 0.   5.57] Loss_P: [ 3.73  2.33  2.85  1.4  10.31]\n",
      "Loss_Q: [1.97 2.24 1.32 0.   5.53] Loss_P: [ 3.77  2.33  2.81  1.41 10.32]\n",
      "Loss_Q: [2.06 2.24 1.33 0.   5.63] Loss_P: [ 3.76  2.24  2.88  1.38 10.26]\n",
      "Loss_Q: [1.9  2.23 1.29 0.   5.42] Loss_P: [ 3.75  2.27  2.83  1.37 10.21]\n",
      "Loss_Q: [2.   2.22 1.32 0.   5.53] Loss_P: [ 3.78  2.3   2.85  1.39 10.32]\n",
      "Loss_Q: [2.04 2.18 1.33 0.   5.55] Loss_P: [ 3.7   2.32  2.82  1.39 10.24]\n",
      "Loss_Q: [1.98 2.19 1.32 0.   5.49] Loss_P: [ 3.71  2.25  2.9   1.37 10.23]\n",
      "Loss_Q: [1.99 2.22 1.31 0.   5.51] Loss_P: [ 3.76  2.27  2.87  1.34 10.24]\n",
      "Loss_Q: [2.08 2.19 1.34 0.   5.6 ] Loss_P: [ 3.75  2.32  2.87  1.37 10.31]\n",
      "Loss_Q: [2.01 2.24 1.32 0.   5.57] Loss_P: [ 3.74  2.3   2.87  1.35 10.26]\n",
      "Loss_Q: [1.99 2.24 1.29 0.   5.53] Loss_P: [ 3.74  2.24  2.85  1.37 10.2 ]\n",
      "Loss_Q: [1.89 2.24 1.35 0.   5.48] Loss_P: [ 3.73  2.26  2.86  1.4  10.24]\n",
      "Loss_Q: [1.93 2.2  1.32 0.   5.45] Loss_P: [ 3.78  2.21  2.85  1.38 10.22]\n",
      "Loss_Q: [1.94 2.25 1.34 0.   5.53] Loss_P: [ 3.83  2.18  2.83  1.42 10.26]\n",
      "Loss_Q: [1.95 2.25 1.32 0.   5.53] Loss_P: [ 3.76  2.22  2.82  1.4  10.2 ]\n",
      "Loss_Q: [1.98 2.17 1.37 0.   5.52] Loss_P: [ 3.75  2.21  2.86  1.42 10.24]\n",
      "Loss_Q: [2.06 2.27 1.37 0.   5.69] Loss_P: [ 3.76  2.24  2.82  1.41 10.23]\n",
      "Loss_Q: [2.03 2.16 1.35 0.   5.54] Loss_P: [ 3.73  2.26  2.81  1.42 10.23]\n",
      "Loss_Q: [1.94 2.26 1.31 0.   5.51] Loss_P: [ 3.75  2.24  2.82  1.44 10.24]\n",
      "Loss_Q: [2.   2.23 1.36 0.   5.59] Loss_P: [ 3.74  2.25  2.76  1.43 10.18]\n",
      "Loss_Q: [1.93 2.17 1.37 0.   5.48] Loss_P: [ 3.71  2.28  2.79  1.44 10.22]\n",
      "Loss_Q: [2.09 2.15 1.34 0.   5.58] Loss_P: [ 3.78  2.31  2.79  1.46 10.34]\n",
      "Loss_Q: [2.06 2.18 1.38 0.   5.62] Loss_P: [ 3.76  2.25  2.8   1.44 10.26]\n",
      "Loss_Q: [2.03 2.14 1.36 0.   5.53] Loss_P: [ 3.76  2.34  2.76  1.45 10.31]\n",
      "Loss_Q: [2.03 2.2  1.37 0.   5.6 ] Loss_P: [ 3.68  2.26  2.73  1.43 10.1 ]\n",
      "Loss_Q: [1.99 2.17 1.36 0.   5.53] Loss_P: [ 3.74  2.32  2.72  1.45 10.23]\n",
      "Loss_Q: [1.96 2.11 1.36 0.   5.44] Loss_P: [ 3.82  2.27  2.77  1.45 10.31]\n",
      "Loss_Q: [1.94 2.15 1.34 0.   5.43] Loss_P: [ 3.65  2.25  2.74  1.44 10.08]\n",
      "Loss_Q: [1.95 2.17 1.33 0.   5.45] Loss_P: [ 3.71  2.25  2.75  1.42 10.13]\n",
      "Loss_Q: [1.94 2.15 1.36 0.   5.45] Loss_P: [ 3.8   2.25  2.83  1.41 10.29]\n",
      "Loss_Q: [2.01 2.24 1.34 0.   5.58] Loss_P: [ 3.73  2.29  2.85  1.42 10.29]\n",
      "Loss_Q: [2.05 2.24 1.34 0.   5.63] Loss_P: [ 3.77  2.32  2.78  1.4  10.27]\n",
      "Loss_Q: [2.03 2.18 1.35 0.   5.56] Loss_P: [ 3.72  2.41  2.87  1.38 10.37]\n",
      "Loss_Q: [1.99 2.2  1.35 0.   5.54] Loss_P: [ 3.77  2.33  2.86  1.42 10.38]\n",
      "Loss_Q: [2.05 2.23 1.32 0.   5.61] Loss_P: [ 3.77  2.21  2.81  1.39 10.19]\n",
      "Loss_Q: [1.97 2.25 1.31 0.   5.53] Loss_P: [ 3.86  2.21  2.83  1.38 10.28]\n",
      "Loss_Q: [2.07 2.18 1.29 0.   5.53] Loss_P: [ 3.79  2.26  2.78  1.35 10.18]\n",
      "Loss_Q: [1.96 2.24 1.26 0.   5.46] Loss_P: [ 3.81  2.18  2.79  1.31 10.1 ]\n",
      "Loss_Q: [1.98 2.24 1.24 0.   5.46] Loss_P: [ 3.78  2.21  2.8   1.29 10.08]\n",
      "Loss_Q: [1.92 2.24 1.25 0.   5.41] Loss_P: [ 3.73  2.31  2.81  1.31 10.16]\n",
      "Loss_Q: [1.96 2.2  1.26 0.   5.42] Loss_P: [ 3.92  2.19  2.84  1.34 10.3 ]\n",
      "Loss_Q: [1.92 2.23 1.28 0.   5.43] Loss_P: [ 3.84  2.19  2.77  1.34 10.14]\n",
      "Loss_Q: [1.95 2.23 1.33 0.   5.51] Loss_P: [ 3.77  2.28  2.81  1.35 10.21]\n",
      "Loss_Q: [1.9  2.2  1.27 0.   5.37] Loss_P: [ 3.86  2.23  2.77  1.33 10.18]\n",
      "Loss_Q: [1.92 2.16 1.18 0.   5.27] Loss_P: [ 3.81  2.26  2.77  1.31 10.14]\n",
      "Loss_Q: [1.97 2.11 1.25 0.   5.33] Loss_P: [ 3.79  2.26  2.79  1.34 10.17]\n",
      "Loss_Q: [2.   2.18 1.25 0.   5.43] Loss_P: [3.74 2.19 2.74 1.3  9.96]\n",
      "Loss_Q: [2.03 2.13 1.22 0.   5.38] Loss_P: [ 3.74  2.31  2.73  1.32 10.09]\n",
      "Loss_Q: [2.01 2.15 1.21 0.   5.36] Loss_P: [ 3.71  2.3   2.73  1.34 10.08]\n",
      "Loss_Q: [1.91 2.21 1.25 0.   5.36] Loss_P: [ 3.8   2.24  2.71  1.34 10.09]\n",
      "Loss_Q: [1.9  2.2  1.22 0.   5.32] Loss_P: [ 3.79  2.25  2.71  1.33 10.08]\n",
      "Loss_Q: [1.98 2.11 1.24 0.   5.33] Loss_P: [3.84 2.22 2.63 1.3  9.99]\n",
      "Loss_Q: [1.86 2.12 1.23 0.   5.22] Loss_P: [ 3.72  2.28  2.76  1.33 10.1 ]\n",
      "Loss_Q: [2.   2.1  1.19 0.   5.29] Loss_P: [ 3.78  2.3   2.72  1.3  10.1 ]\n",
      "Loss_Q: [1.96 2.2  1.29 0.   5.45] Loss_P: [ 3.76  2.31  2.77  1.37 10.21]\n",
      "Loss_Q: [1.97 2.14 1.27 0.   5.38] Loss_P: [ 3.77  2.28  2.74  1.34 10.12]\n",
      "Loss_Q: [2.09 2.11 1.17 0.   5.37] Loss_P: [ 3.83  2.28  2.74  1.28 10.12]\n",
      "Loss_Q: [2.   2.19 1.24 0.   5.44] Loss_P: [ 3.78  2.26  2.79  1.35 10.18]\n",
      "Loss_Q: [2.05 2.14 1.24 0.   5.44] Loss_P: [ 3.76  2.29  2.79  1.31 10.15]\n",
      "Loss_Q: [1.93 2.17 1.24 0.   5.35] Loss_P: [ 3.78  2.26  2.8   1.33 10.17]\n",
      "Loss_Q: [2.09 2.18 1.23 0.   5.5 ] Loss_P: [ 3.84  2.26  2.78  1.28 10.16]\n",
      "Loss_Q: [2.02 2.18 1.21 0.   5.42] Loss_P: [ 3.86  2.25  2.78  1.29 10.18]\n",
      "Loss_Q: [2.01 2.17 1.21 0.   5.39] Loss_P: [ 3.91  2.17  2.77  1.29 10.14]\n",
      "Loss_Q: [1.95 2.22 1.19 0.   5.36] Loss_P: [ 3.85  2.18  2.75  1.27 10.06]\n",
      "Loss_Q: [2.   2.17 1.21 0.   5.38] Loss_P: [ 3.77  2.21  2.8   1.33 10.11]\n",
      "Loss_Q: [1.98 2.11 1.18 0.   5.27] Loss_P: [3.73 2.18 2.76 1.27 9.94]\n",
      "Loss_Q: [1.98 2.11 1.17 0.   5.26] Loss_P: [ 3.77  2.26  2.76  1.25 10.04]\n",
      "Loss_Q: [2.09 2.07 1.11 0.   5.27] Loss_P: [ 3.83  2.21  2.74  1.24 10.02]\n",
      "Loss_Q: [1.92 2.12 1.25 0.   5.28] Loss_P: [ 3.83  2.26  2.73  1.26 10.09]\n",
      "Loss_Q: [1.98 2.11 1.14 0.   5.22] Loss_P: [ 3.81  2.21  2.77  1.27 10.06]\n",
      "Loss_Q: [1.96 2.15 1.2  0.   5.31] Loss_P: [3.84 2.17 2.7  1.27 9.97]\n",
      "Loss_Q: [2.06 2.04 1.17 0.   5.28] Loss_P: [3.76 2.22 2.72 1.25 9.95]\n",
      "Loss_Q: [1.93 2.2  1.12 0.   5.24] Loss_P: [3.82 2.22 2.72 1.23 9.99]\n",
      "Loss_Q: [2.02 2.17 1.19 0.   5.39] Loss_P: [3.73 2.25 2.77 1.22 9.97]\n",
      "Loss_Q: [2.   2.13 1.2  0.   5.34] Loss_P: [ 3.78  2.35  2.77  1.25 10.15]\n",
      "Loss_Q: [2.01 2.08 1.16 0.   5.25] Loss_P: [ 3.79  2.27  2.76  1.22 10.04]\n",
      "Loss_Q: [2.02 2.13 1.17 0.   5.31] Loss_P: [ 3.8   2.23  2.77  1.27 10.08]\n",
      "Loss_Q: [2.08 2.15 1.18 0.   5.4 ] Loss_P: [ 3.73  2.29  2.76  1.29 10.06]\n",
      "Loss_Q: [2.11 2.15 1.24 0.   5.5 ] Loss_P: [ 3.71  2.33  2.78  1.27 10.09]\n",
      "Loss_Q: [2.12 2.1  1.21 0.   5.44] Loss_P: [ 3.75  2.33  2.78  1.29 10.15]\n",
      "Loss_Q: [2.   2.21 1.18 0.   5.39] Loss_P: [ 3.79  2.27  2.79  1.26 10.1 ]\n",
      "Loss_Q: [1.98 2.12 1.17 0.   5.27] Loss_P: [ 3.79  2.31  2.78  1.28 10.17]\n",
      "Loss_Q: [2.02 2.14 1.22 0.   5.37] Loss_P: [ 3.73  2.24  2.81  1.3  10.08]\n",
      "Loss_Q: [2.1  2.14 1.2  0.   5.43] Loss_P: [ 3.79  2.21  2.76  1.27 10.04]\n",
      "Loss_Q: [2.   2.2  1.24 0.   5.44] Loss_P: [ 3.79  2.26  2.77  1.28 10.1 ]\n",
      "Loss_Q: [2.11 2.19 1.25 0.   5.55] Loss_P: [ 3.78  2.26  2.78  1.29 10.11]\n",
      "Loss_Q: [2.07 2.14 1.19 0.   5.4 ] Loss_P: [ 3.69  2.29  2.78  1.33 10.1 ]\n",
      "Loss_Q: [2.06 2.15 1.23 0.   5.44] Loss_P: [ 3.79  2.25  2.82  1.33 10.2 ]\n",
      "Loss_Q: [2.09 2.22 1.23 0.   5.53] Loss_P: [ 3.81  2.18  2.75  1.31 10.05]\n",
      "Loss_Q: [2.1  2.18 1.21 0.   5.49] Loss_P: [ 3.77  2.34  2.83  1.28 10.22]\n",
      "Loss_Q: [1.97 2.16 1.17 0.   5.3 ] Loss_P: [ 3.78  2.29  2.82  1.29 10.19]\n",
      "Loss_Q: [2.04 2.14 1.12 0.   5.29] Loss_P: [ 3.85  2.18  2.83  1.24 10.1 ]\n",
      "Loss_Q: [2.03 2.2  1.17 0.   5.4 ] Loss_P: [ 3.74  2.3   2.83  1.28 10.14]\n",
      "Loss_Q: [2.03 2.22 1.18 0.   5.43] Loss_P: [ 3.76  2.24  2.83  1.26 10.08]\n",
      "Loss_Q: [2.01 2.16 1.2  0.   5.38] Loss_P: [ 3.82  2.26  2.83  1.28 10.19]\n",
      "Loss_Q: [1.96 2.09 1.15 0.   5.2 ] Loss_P: [ 3.76  2.25  2.78  1.29 10.09]\n",
      "Loss_Q: [2.   2.15 1.19 0.   5.34] Loss_P: [ 3.8   2.16  2.81  1.26 10.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.9  2.17 1.2  0.   5.27] Loss_P: [ 3.81  2.25  2.77  1.24 10.08]\n",
      "Loss_Q: [2.08 2.15 1.21 0.   5.44] Loss_P: [ 3.84  2.15  2.83  1.26 10.08]\n",
      "Loss_Q: [2.01 2.13 1.2  0.   5.34] Loss_P: [ 3.72  2.3   2.82  1.28 10.12]\n",
      "Loss_Q: [2.02 2.12 1.23 0.   5.37] Loss_P: [ 3.72  2.31  2.76  1.29 10.07]\n",
      "Loss_Q: [2.06 2.15 1.21 0.   5.42] Loss_P: [ 3.7   2.27  2.78  1.3  10.05]\n",
      "Loss_Q: [1.95 2.19 1.2  0.   5.35] Loss_P: [ 3.68  2.29  2.79  1.3  10.07]\n",
      "Loss_Q: [2.02 2.14 1.23 0.   5.39] Loss_P: [ 3.8   2.2   2.81  1.3  10.1 ]\n",
      "Loss_Q: [1.98 2.2  1.18 0.   5.36] Loss_P: [ 3.68  2.26  2.82  1.28 10.04]\n",
      "Loss_Q: [1.94 2.18 1.19 0.   5.31] Loss_P: [ 3.76  2.21  2.8   1.25 10.01]\n",
      "Loss_Q: [2.   2.2  1.17 0.   5.38] Loss_P: [ 3.7   2.2   2.85  1.27 10.02]\n",
      "Loss_Q: [1.99 2.14 1.16 0.   5.28] Loss_P: [3.81 2.11 2.82 1.22 9.97]\n",
      "Loss_Q: [1.89 2.23 1.15 0.   5.26] Loss_P: [3.72 2.2  2.81 1.19 9.92]\n",
      "Loss_Q: [1.93 2.16 1.11 0.   5.2 ] Loss_P: [3.75 2.2  2.8  1.19 9.94]\n",
      "Loss_Q: [2.01 2.19 1.14 0.   5.34] Loss_P: [ 3.79  2.31  2.84  1.21 10.15]\n",
      "Loss_Q: [1.91 2.19 1.13 0.   5.24] Loss_P: [3.72 2.22 2.81 1.2  9.95]\n",
      "Loss_Q: [2.08 2.16 1.18 0.   5.42] Loss_P: [3.77 2.22 2.81 1.19 9.99]\n",
      "Loss_Q: [1.99 2.09 1.15 0.   5.24] Loss_P: [ 3.81  2.26  2.79  1.23 10.08]\n",
      "Loss_Q: [2.04 2.13 1.22 0.   5.38] Loss_P: [ 3.78  2.25  2.82  1.28 10.14]\n",
      "Loss_Q: [2.07 2.18 1.15 0.   5.4 ] Loss_P: [ 3.77  2.27  2.85  1.26 10.15]\n",
      "Loss_Q: [1.94 2.12 1.18 0.   5.24] Loss_P: [ 3.81  2.2   2.8   1.25 10.07]\n",
      "Loss_Q: [2.04 2.15 1.2  0.   5.39] Loss_P: [ 3.84  2.21  2.8   1.25 10.1 ]\n",
      "Loss_Q: [1.95 2.15 1.17 0.   5.27] Loss_P: [ 3.83  2.14  2.79  1.25 10.  ]\n",
      "Loss_Q: [2.04 2.17 1.13 0.   5.34] Loss_P: [ 3.77  2.27  2.83  1.22 10.09]\n",
      "Loss_Q: [2.06 2.22 1.16 0.   5.45] Loss_P: [ 3.72  2.32  2.81  1.22 10.07]\n",
      "Loss_Q: [1.96 2.18 1.16 0.   5.31] Loss_P: [ 3.82  2.21  2.82  1.23 10.08]\n",
      "Loss_Q: [2.06 2.2  1.15 0.   5.41] Loss_P: [3.72 2.25 2.8  1.21 9.98]\n",
      "Loss_Q: [2.02 2.16 1.14 0.   5.32] Loss_P: [ 3.8   2.29  2.78  1.2  10.07]\n",
      "Loss_Q: [2.09 2.1  1.13 0.   5.33] Loss_P: [ 3.77  2.29  2.74  1.23 10.03]\n",
      "Loss_Q: [2.04 2.15 1.09 0.   5.28] Loss_P: [ 3.66  2.34  2.8   1.21 10.01]\n",
      "Loss_Q: [2.   2.14 1.15 0.   5.29] Loss_P: [ 3.78  2.27  2.76  1.23 10.04]\n",
      "Loss_Q: [1.93 2.19 1.16 0.   5.28] Loss_P: [ 3.85  2.26  2.81  1.21 10.13]\n",
      "Loss_Q: [2.07 2.16 1.15 0.   5.37] Loss_P: [ 3.75  2.3   2.79  1.22 10.07]\n",
      "Loss_Q: [1.97 2.17 1.15 0.   5.29] Loss_P: [3.76 2.24 2.81 1.18 9.99]\n",
      "Loss_Q: [2.03 2.22 1.11 0.   5.36] Loss_P: [ 3.81  2.26  2.79  1.2  10.05]\n",
      "Loss_Q: [2.07 2.12 1.09 0.   5.28] Loss_P: [3.82 2.19 2.8  1.16 9.97]\n",
      "Loss_Q: [2.02 2.2  1.07 0.   5.29] Loss_P: [3.8  2.21 2.81 1.16 9.98]\n",
      "Loss_Q: [2.03 2.21 1.1  0.   5.33] Loss_P: [3.79 2.22 2.79 1.19 9.99]\n",
      "Loss_Q: [2.04 2.15 1.06 0.   5.25] Loss_P: [3.77 2.19 2.84 1.12 9.91]\n",
      "Loss_Q: [1.95 2.18 1.06 0.   5.18] Loss_P: [3.81 2.23 2.8  1.13 9.97]\n",
      "Loss_Q: [2.02 2.16 1.06 0.   5.24] Loss_P: [3.75 2.25 2.82 1.16 9.98]\n",
      "Loss_Q: [2.   2.17 1.04 0.   5.21] Loss_P: [3.74 2.21 2.8  1.11 9.87]\n",
      "Loss_Q: [1.97 2.22 1.07 0.   5.25] Loss_P: [3.73 2.27 2.8  1.07 9.87]\n",
      "Loss_Q: [1.93 2.16 1.05 0.   5.14] Loss_P: [3.79 2.21 2.81 1.14 9.95]\n",
      "Loss_Q: [1.96 2.21 1.03 0.   5.2 ] Loss_P: [3.76 2.21 2.84 1.13 9.93]\n",
      "Loss_Q: [1.94 2.2  1.02 0.   5.16] Loss_P: [3.73 2.31 2.85 1.11 9.99]\n",
      "Loss_Q: [1.96 2.12 1.07 0.   5.15] Loss_P: [3.82 2.22 2.78 1.16 9.97]\n",
      "Loss_Q: [1.93 2.14 1.07 0.   5.14] Loss_P: [3.73 2.23 2.79 1.13 9.88]\n",
      "Loss_Q: [1.96 2.18 1.07 0.   5.21] Loss_P: [3.7  2.2  2.82 1.16 9.88]\n",
      "Loss_Q: [2.   2.14 1.01 0.   5.15] Loss_P: [3.82 2.27 2.78 1.11 9.98]\n",
      "Loss_Q: [2.02 2.14 1.09 0.   5.26] Loss_P: [3.79 2.26 2.81 1.11 9.97]\n",
      "Loss_Q: [1.98 2.21 1.07 0.   5.26] Loss_P: [3.82 2.22 2.79 1.11 9.94]\n",
      "Loss_Q: [1.94 2.17 1.08 0.   5.19] Loss_P: [3.8  2.19 2.78 1.1  9.87]\n",
      "Loss_Q: [2.05 2.12 1.07 0.   5.24] Loss_P: [3.77 2.18 2.76 1.11 9.82]\n",
      "Loss_Q: [1.99 2.13 0.99 0.   5.11] Loss_P: [3.74 2.23 2.79 1.12 9.88]\n",
      "Loss_Q: [1.92 2.12 1.   0.   5.04] Loss_P: [3.78 2.29 2.79 1.11 9.97]\n",
      "Loss_Q: [2.   2.17 1.03 0.   5.2 ] Loss_P: [3.78 2.26 2.76 1.13 9.92]\n",
      "Loss_Q: [2.07 2.21 1.12 0.   5.4 ] Loss_P: [ 3.81  2.22  2.82  1.16 10.01]\n",
      "Loss_Q: [2.03 2.13 1.12 0.   5.28] Loss_P: [3.77 2.18 2.78 1.17 9.9 ]\n",
      "Loss_Q: [2.03 2.18 1.11 0.   5.32] Loss_P: [3.71 2.24 2.81 1.16 9.93]\n",
      "Loss_Q: [1.98 2.17 1.06 0.   5.21] Loss_P: [3.78 2.19 2.82 1.13 9.91]\n",
      "Loss_Q: [1.96 2.15 1.07 0.   5.18] Loss_P: [3.77 2.19 2.8  1.12 9.88]\n",
      "Loss_Q: [2.   2.15 1.08 0.   5.23] Loss_P: [3.85 2.17 2.77 1.08 9.87]\n",
      "Loss_Q: [1.93 2.17 1.02 0.   5.13] Loss_P: [3.8  2.15 2.78 1.09 9.82]\n",
      "Loss_Q: [2.05 2.14 1.   0.   5.19] Loss_P: [3.73 2.17 2.77 1.05 9.72]\n",
      "Loss_Q: [1.93 2.19 1.   0.   5.12] Loss_P: [3.76 2.17 2.74 1.03 9.71]\n",
      "Loss_Q: [1.93 2.12 0.96 0.   5.01] Loss_P: [3.81 2.14 2.76 1.   9.7 ]\n",
      "Loss_Q: [1.95 2.14 0.93 0.   5.02] Loss_P: [3.82 2.13 2.77 1.01 9.73]\n",
      "Loss_Q: [1.87 2.18 0.98 0.   5.03] Loss_P: [3.78 2.14 2.76 1.04 9.71]\n",
      "Loss_Q: [1.85 2.15 1.   0.   5.  ] Loss_P: [3.73 2.12 2.78 1.04 9.66]\n",
      "Loss_Q: [1.9  2.15 0.98 0.   5.03] Loss_P: [3.85 2.15 2.8  1.07 9.87]\n",
      "Loss_Q: [1.9  2.21 1.01 0.   5.12] Loss_P: [3.86 2.18 2.83 1.09 9.97]\n",
      "Loss_Q: [1.92 2.15 1.01 0.   5.09] Loss_P: [3.81 2.22 2.82 1.02 9.87]\n",
      "Loss_Q: [2.06 2.17 1.01 0.   5.24] Loss_P: [3.88 2.16 2.78 1.06 9.88]\n",
      "Loss_Q: [1.93 2.14 1.03 0.   5.1 ] Loss_P: [3.78 2.14 2.8  1.08 9.81]\n",
      "Loss_Q: [1.93 2.18 1.   0.   5.12] Loss_P: [3.68 2.16 2.83 1.09 9.76]\n",
      "Loss_Q: [1.95 2.13 1.01 0.   5.08] Loss_P: [3.74 2.18 2.8  1.07 9.79]\n",
      "Loss_Q: [1.89 2.23 1.03 0.   5.15] Loss_P: [3.77 2.15 2.87 1.08 9.87]\n",
      "Loss_Q: [1.93 2.21 1.04 0.   5.18] Loss_P: [ 3.78  2.23  2.86  1.13 10.  ]\n",
      "Loss_Q: [1.97 2.17 1.07 0.   5.22] Loss_P: [3.76 2.2  2.84 1.07 9.88]\n",
      "Loss_Q: [1.94 2.17 1.01 0.   5.12] Loss_P: [3.75 2.22 2.81 1.07 9.85]\n",
      "Loss_Q: [1.98 2.23 1.01 0.   5.22] Loss_P: [3.72 2.22 2.83 1.08 9.85]\n",
      "Loss_Q: [1.94 2.19 0.98 0.   5.1 ] Loss_P: [3.88 2.11 2.8  1.04 9.83]\n",
      "Loss_Q: [1.94 2.18 0.99 0.   5.11] Loss_P: [3.81 2.16 2.84 1.09 9.9 ]\n",
      "Loss_Q: [1.92 2.2  1.02 0.   5.14] Loss_P: [3.77 2.12 2.79 1.09 9.76]\n",
      "Loss_Q: [1.91 2.11 1.04 0.   5.06] Loss_P: [3.8  2.14 2.8  1.08 9.82]\n",
      "Loss_Q: [1.95 2.13 0.98 0.   5.06] Loss_P: [3.84 2.18 2.8  1.02 9.84]\n",
      "Loss_Q: [1.95 2.15 0.98 0.   5.08] Loss_P: [3.79 2.15 2.79 1.05 9.77]\n",
      "Loss_Q: [1.91 2.16 0.99 0.   5.06] Loss_P: [3.8  2.15 2.79 1.04 9.78]\n",
      "Loss_Q: [1.92 2.14 0.99 0.   5.05] Loss_P: [3.77 2.18 2.82 1.   9.77]\n",
      "Loss_Q: [1.92 2.17 0.96 0.   5.05] Loss_P: [3.77 2.17 2.79 1.03 9.75]\n",
      "Loss_Q: [1.96 2.14 1.03 0.   5.13] Loss_P: [3.78 2.2  2.8  1.06 9.84]\n",
      "Loss_Q: [2.03 2.11 0.99 0.   5.12] Loss_P: [3.83 2.13 2.78 1.05 9.78]\n",
      "Loss_Q: [2.03 2.16 1.02 0.   5.22] Loss_P: [3.74 2.3  2.78 1.05 9.87]\n",
      "Loss_Q: [1.93 2.12 1.02 0.   5.07] Loss_P: [3.79 2.14 2.8  1.05 9.77]\n",
      "Loss_Q: [1.97 2.21 1.   0.   5.18] Loss_P: [3.85 2.15 2.81 1.04 9.85]\n",
      "Loss_Q: [1.94 2.1  1.02 0.   5.07] Loss_P: [3.81 2.22 2.82 1.13 9.98]\n",
      "Loss_Q: [1.93 2.18 1.   0.   5.11] Loss_P: [3.81 2.2  2.83 1.09 9.92]\n",
      "Loss_Q: [1.96 2.19 1.05 0.   5.2 ] Loss_P: [3.81 2.2  2.81 1.07 9.89]\n",
      "Loss_Q: [1.9  2.13 1.03 0.   5.05] Loss_P: [3.75 2.18 2.8  1.1  9.82]\n",
      "Loss_Q: [1.94 2.14 1.07 0.   5.15] Loss_P: [3.77 2.26 2.82 1.08 9.93]\n",
      "Loss_Q: [2.02 2.08 1.03 0.   5.14] Loss_P: [3.72 2.17 2.82 1.14 9.85]\n",
      "Loss_Q: [1.95 2.14 1.1  0.   5.19] Loss_P: [ 3.82  2.27  2.81  1.16 10.05]\n",
      "Loss_Q: [2.07 2.24 1.11 0.   5.42] Loss_P: [ 3.79  2.23  2.84  1.14 10.01]\n",
      "Loss_Q: [1.9  2.14 1.11 0.   5.15] Loss_P: [3.76 2.19 2.81 1.19 9.96]\n",
      "Loss_Q: [1.93 2.18 1.14 0.   5.25] Loss_P: [ 3.73  2.25  2.84  1.21 10.03]\n",
      "Loss_Q: [1.99 2.16 1.13 0.   5.27] Loss_P: [ 3.74  2.24  2.87  1.18 10.02]\n",
      "Loss_Q: [1.96 2.15 1.16 0.   5.26] Loss_P: [ 3.71  2.19  2.85  1.25 10.  ]\n",
      "Loss_Q: [2.02 2.12 1.17 0.   5.3 ] Loss_P: [ 3.71  2.27  2.84  1.24 10.07]\n",
      "Loss_Q: [1.98 2.2  1.17 0.   5.35] Loss_P: [ 3.77  2.25  2.86  1.21 10.1 ]\n",
      "Loss_Q: [2.05 2.22 1.2  0.   5.46] Loss_P: [ 3.71  2.24  2.84  1.24 10.03]\n",
      "Loss_Q: [2.03 2.19 1.18 0.   5.4 ] Loss_P: [ 3.74  2.26  2.84  1.3  10.15]\n",
      "Loss_Q: [1.97 2.2  1.19 0.   5.35] Loss_P: [ 3.73  2.29  2.84  1.28 10.15]\n",
      "Loss_Q: [2.   2.25 1.2  0.   5.45] Loss_P: [ 3.74  2.28  2.85  1.28 10.15]\n",
      "Loss_Q: [2.04 2.17 1.23 0.   5.44] Loss_P: [ 3.74  2.26  2.84  1.26 10.09]\n",
      "Loss_Q: [1.99 2.16 1.21 0.   5.37] Loss_P: [ 3.7   2.29  2.86  1.26 10.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.94 2.19 1.2  0.   5.33] Loss_P: [ 3.71  2.26  2.85  1.25 10.07]\n",
      "Loss_Q: [1.99 2.16 1.17 0.   5.31] Loss_P: [ 3.72  2.25  2.85  1.23 10.05]\n",
      "Loss_Q: [2.09 2.16 1.14 0.   5.39] Loss_P: [ 3.69  2.23  2.87  1.23 10.02]\n",
      "Loss_Q: [1.96 2.24 1.16 0.   5.37] Loss_P: [ 3.73  2.37  2.81  1.26 10.18]\n",
      "Loss_Q: [2.06 2.14 1.12 0.   5.32] Loss_P: [ 3.72  2.28  2.84  1.25 10.09]\n",
      "Loss_Q: [2.01 2.19 1.17 0.   5.37] Loss_P: [ 3.75  2.27  2.84  1.21 10.06]\n",
      "Loss_Q: [2.   2.21 1.14 0.   5.34] Loss_P: [ 3.71  2.37  2.83  1.25 10.16]\n",
      "Loss_Q: [1.98 2.15 1.14 0.   5.28] Loss_P: [ 3.73  2.3   2.83  1.19 10.05]\n",
      "Loss_Q: [1.94 2.15 1.12 0.   5.2 ] Loss_P: [ 3.75  2.28  2.82  1.18 10.04]\n",
      "Loss_Q: [1.97 2.2  1.1  0.   5.27] Loss_P: [ 3.74  2.26  2.82  1.18 10.01]\n",
      "Loss_Q: [1.97 2.15 1.14 0.   5.26] Loss_P: [ 3.79  2.23  2.79  1.2  10.01]\n",
      "Loss_Q: [2.01 2.19 1.13 0.   5.33] Loss_P: [ 3.77  2.26  2.81  1.2  10.03]\n",
      "Loss_Q: [1.98 2.15 1.13 0.   5.26] Loss_P: [3.79 2.1  2.79 1.2  9.89]\n",
      "Loss_Q: [1.93 2.23 1.1  0.   5.26] Loss_P: [ 3.74  2.27  2.83  1.2  10.03]\n",
      "Loss_Q: [1.97 2.17 1.17 0.   5.3 ] Loss_P: [ 3.84  2.26  2.81  1.24 10.14]\n",
      "Loss_Q: [1.93 2.1  1.12 0.   5.15] Loss_P: [ 3.76  2.24  2.79  1.22 10.01]\n",
      "Loss_Q: [1.96 2.16 1.14 0.   5.26] Loss_P: [3.8  2.15 2.83 1.21 9.98]\n",
      "Loss_Q: [2.   2.19 1.15 0.   5.34] Loss_P: [ 3.86  2.26  2.83  1.2  10.14]\n",
      "Loss_Q: [2.   2.2  1.14 0.   5.34] Loss_P: [3.85 2.11 2.79 1.19 9.95]\n",
      "Loss_Q: [1.94 2.16 1.12 0.   5.22] Loss_P: [ 3.82  2.18  2.81  1.24 10.05]\n",
      "Loss_Q: [1.92 2.17 1.12 0.   5.21] Loss_P: [ 3.78  2.21  2.79  1.24 10.02]\n",
      "Loss_Q: [1.97 2.16 1.13 0.   5.26] Loss_P: [ 3.81  2.21  2.77  1.26 10.05]\n",
      "Loss_Q: [1.98 2.18 1.18 0.   5.33] Loss_P: [ 3.85  2.29  2.79  1.25 10.18]\n",
      "Loss_Q: [2.06 2.21 1.16 0.   5.42] Loss_P: [ 3.84  2.22  2.81  1.24 10.1 ]\n",
      "Loss_Q: [2.01 2.14 1.15 0.   5.31] Loss_P: [3.7  2.23 2.82 1.23 9.99]\n",
      "Loss_Q: [1.99 2.25 1.17 0.   5.41] Loss_P: [ 3.71  2.27  2.79  1.23 10.  ]\n",
      "Loss_Q: [1.98 2.18 1.17 0.   5.33] Loss_P: [ 3.74  2.28  2.83  1.23 10.08]\n",
      "Loss_Q: [1.98 2.29 1.16 0.   5.43] Loss_P: [ 3.75  2.25  2.83  1.21 10.04]\n",
      "Loss_Q: [1.97 2.3  1.14 0.   5.41] Loss_P: [ 3.81  2.28  2.82  1.23 10.14]\n",
      "Loss_Q: [2.03 2.25 1.18 0.   5.45] Loss_P: [ 3.84  2.19  2.84  1.23 10.1 ]\n",
      "Loss_Q: [2.01 2.21 1.14 0.   5.36] Loss_P: [ 3.85  2.21  2.81  1.25 10.12]\n",
      "Loss_Q: [1.94 2.21 1.17 0.   5.32] Loss_P: [ 3.73  2.24  2.84  1.21 10.01]\n",
      "Loss_Q: [2.1  2.14 1.15 0.   5.39] Loss_P: [3.72 2.19 2.83 1.23 9.96]\n",
      "Loss_Q: [2.01 2.27 1.13 0.   5.41] Loss_P: [ 3.72  2.3   2.83  1.23 10.08]\n",
      "Loss_Q: [2.   2.23 1.18 0.   5.41] Loss_P: [ 3.76  2.29  2.86  1.23 10.14]\n",
      "Loss_Q: [1.94 2.2  1.12 0.   5.26] Loss_P: [3.7  2.23 2.85 1.2  9.98]\n",
      "Loss_Q: [1.98 2.2  1.13 0.   5.32] Loss_P: [ 3.81  2.22  2.84  1.21 10.08]\n",
      "Loss_Q: [2.03 2.12 1.14 0.   5.29] Loss_P: [ 3.81  2.27  2.79  1.23 10.11]\n",
      "Loss_Q: [2.08 2.18 1.13 0.   5.39] Loss_P: [ 3.81  2.26  2.83  1.2  10.1 ]\n",
      "Loss_Q: [2.04 2.2  1.13 0.   5.37] Loss_P: [ 3.76  2.26  2.83  1.19 10.04]\n",
      "Loss_Q: [2.05 2.17 1.13 0.   5.36] Loss_P: [ 3.74  2.29  2.82  1.2  10.06]\n",
      "Loss_Q: [2.01 2.22 1.14 0.   5.36] Loss_P: [ 3.68  2.32  2.85  1.2  10.06]\n",
      "Loss_Q: [2.01 2.19 1.12 0.   5.32] Loss_P: [ 3.72  2.31  2.83  1.23 10.09]\n",
      "Loss_Q: [2.   2.24 1.14 0.   5.38] Loss_P: [ 3.7   2.25  2.87  1.17 10.  ]\n",
      "Loss_Q: [2.09 2.2  1.16 0.   5.44] Loss_P: [ 3.66  2.34  2.88  1.21 10.08]\n",
      "Loss_Q: [1.97 2.2  1.16 0.   5.33] Loss_P: [ 3.78  2.28  2.82  1.25 10.13]\n",
      "Loss_Q: [2.09 2.2  1.11 0.   5.4 ] Loss_P: [ 3.75  2.32  2.83  1.2  10.1 ]\n",
      "Loss_Q: [2.01 2.2  1.16 0.   5.36] Loss_P: [3.71 2.21 2.84 1.18 9.95]\n",
      "Loss_Q: [2.03 2.26 1.14 0.   5.43] Loss_P: [ 3.74  2.31  2.8   1.2  10.05]\n",
      "Loss_Q: [2.03 2.15 1.08 0.   5.26] Loss_P: [ 3.76  2.25  2.8   1.21 10.01]\n",
      "Loss_Q: [1.93 2.15 1.07 0.   5.16] Loss_P: [ 3.8   2.3   2.8   1.17 10.07]\n",
      "Loss_Q: [2.   2.17 1.14 0.   5.31] Loss_P: [ 3.74  2.3   2.79  1.18 10.02]\n",
      "Loss_Q: [2.04 2.15 1.11 0.   5.3 ] Loss_P: [3.73 2.23 2.82 1.18 9.97]\n",
      "Loss_Q: [1.99 2.19 1.08 0.   5.25] Loss_P: [3.69 2.22 2.84 1.18 9.93]\n",
      "Loss_Q: [2.02 2.21 1.1  0.   5.34] Loss_P: [3.78 2.22 2.81 1.17 9.99]\n",
      "Loss_Q: [2.02 2.2  1.07 0.   5.29] Loss_P: [ 3.72  2.3   2.82  1.17 10.  ]\n",
      "Loss_Q: [2.   2.17 1.11 0.   5.28] Loss_P: [ 3.67  2.31  2.83  1.18 10.  ]\n",
      "Loss_Q: [2.02 2.2  1.07 0.   5.29] Loss_P: [ 3.74  2.3   2.86  1.17 10.07]\n",
      "Loss_Q: [2.   2.24 1.08 0.   5.32] Loss_P: [ 3.68  2.32  2.83  1.21 10.03]\n",
      "Loss_Q: [1.94 2.22 1.09 0.   5.26] Loss_P: [ 3.72  2.3   2.86  1.17 10.05]\n",
      "Loss_Q: [2.02 2.22 1.07 0.   5.31] Loss_P: [ 3.75  2.36  2.85  1.17 10.13]\n",
      "Loss_Q: [1.92 2.18 1.04 0.   5.15] Loss_P: [ 3.77  2.26  2.85  1.16 10.03]\n",
      "Loss_Q: [2.   2.21 1.05 0.   5.27] Loss_P: [3.71 2.31 2.81 1.15 9.98]\n",
      "Loss_Q: [2.   2.13 1.04 0.   5.16] Loss_P: [3.77 2.24 2.82 1.11 9.94]\n",
      "Loss_Q: [1.98 2.15 1.03 0.   5.16] Loss_P: [3.76 2.22 2.8  1.12 9.9 ]\n",
      "Loss_Q: [2.04 2.23 1.04 0.   5.31] Loss_P: [3.72 2.32 2.78 1.15 9.98]\n",
      "Loss_Q: [1.92 2.25 1.04 0.   5.22] Loss_P: [3.79 2.25 2.77 1.15 9.95]\n",
      "Loss_Q: [2.   2.19 1.06 0.   5.25] Loss_P: [3.76 2.25 2.76 1.12 9.89]\n",
      "Loss_Q: [2.03 2.14 1.04 0.   5.21] Loss_P: [3.8  2.27 2.75 1.13 9.95]\n",
      "Loss_Q: [1.97 2.17 1.03 0.   5.17] Loss_P: [3.74 2.25 2.77 1.11 9.88]\n",
      "Loss_Q: [2.07 2.17 1.03 0.   5.27] Loss_P: [3.73 2.26 2.79 1.1  9.88]\n",
      "Loss_Q: [1.92 2.13 1.01 0.   5.07] Loss_P: [ 3.74  2.35  2.8   1.14 10.01]\n",
      "Loss_Q: [1.94 2.21 1.06 0.   5.21] Loss_P: [3.71 2.2  2.82 1.14 9.87]\n",
      "Loss_Q: [1.88 2.12 1.02 0.   5.03] Loss_P: [3.73 2.27 2.8  1.13 9.92]\n",
      "Loss_Q: [1.92 2.24 1.05 0.   5.2 ] Loss_P: [3.79 2.18 2.8  1.12 9.89]\n",
      "Loss_Q: [2.03 2.16 1.08 0.   5.26] Loss_P: [3.69 2.23 2.81 1.16 9.88]\n",
      "Loss_Q: [1.89 2.27 1.11 0.   5.27] Loss_P: [ 3.89  2.21  2.77  1.13 10.  ]\n",
      "Loss_Q: [1.99 2.19 1.07 0.   5.25] Loss_P: [3.77 2.22 2.81 1.14 9.94]\n",
      "Loss_Q: [1.86 2.21 1.06 0.   5.13] Loss_P: [3.74 2.26 2.75 1.2  9.96]\n",
      "Loss_Q: [1.92 2.18 1.08 0.   5.18] Loss_P: [3.71 2.27 2.77 1.17 9.93]\n",
      "Loss_Q: [1.95 2.19 1.07 0.   5.22] Loss_P: [3.7  2.19 2.8  1.18 9.87]\n",
      "Loss_Q: [1.92 2.17 1.05 0.   5.13] Loss_P: [3.72 2.24 2.79 1.12 9.88]\n",
      "Loss_Q: [1.91 2.15 1.05 0.   5.1 ] Loss_P: [3.75 2.24 2.8  1.15 9.94]\n",
      "Loss_Q: [2.   2.11 1.07 0.   5.19] Loss_P: [3.75 2.27 2.77 1.15 9.94]\n",
      "Loss_Q: [1.92 2.18 1.05 0.   5.15] Loss_P: [3.75 2.24 2.8  1.14 9.93]\n",
      "Loss_Q: [1.93 2.19 1.09 0.   5.21] Loss_P: [3.66 2.26 2.8  1.13 9.85]\n",
      "Loss_Q: [1.91 2.2  1.03 0.   5.14] Loss_P: [3.8  2.22 2.78 1.12 9.92]\n",
      "Loss_Q: [1.92 2.17 1.1  0.   5.19] Loss_P: [3.74 2.27 2.79 1.15 9.95]\n",
      "Loss_Q: [1.95 2.15 1.09 0.   5.19] Loss_P: [3.73 2.25 2.76 1.15 9.89]\n",
      "Loss_Q: [1.95 2.14 1.09 0.   5.18] Loss_P: [3.76 2.26 2.77 1.16 9.95]\n",
      "Loss_Q: [1.97 2.18 1.1  0.   5.24] Loss_P: [3.7  2.24 2.71 1.15 9.8 ]\n",
      "Loss_Q: [1.98 2.13 1.09 0.   5.2 ] Loss_P: [3.69 2.28 2.75 1.14 9.85]\n",
      "Loss_Q: [1.91 2.17 1.09 0.   5.18] Loss_P: [3.75 2.21 2.79 1.14 9.88]\n",
      "Loss_Q: [1.99 2.11 1.02 0.   5.12] Loss_P: [3.66 2.27 2.79 1.16 9.88]\n",
      "Loss_Q: [1.97 2.2  1.13 0.   5.29] Loss_P: [3.71 2.31 2.79 1.16 9.97]\n",
      "Loss_Q: [1.99 2.23 1.06 0.   5.28] Loss_P: [3.75 2.17 2.84 1.15 9.92]\n",
      "Loss_Q: [1.93 2.23 1.1  0.   5.27] Loss_P: [3.73 2.24 2.79 1.17 9.93]\n",
      "Loss_Q: [2.01 2.23 1.14 0.   5.38] Loss_P: [ 3.72  2.29  2.81  1.19 10.01]\n",
      "Loss_Q: [1.87 2.19 1.17 0.   5.23] Loss_P: [3.74 2.23 2.83 1.2  9.99]\n",
      "Loss_Q: [1.94 2.28 1.13 0.   5.35] Loss_P: [ 3.71  2.28  2.82  1.23 10.03]\n",
      "Loss_Q: [1.96 2.21 1.15 0.   5.32] Loss_P: [ 3.72  2.22  2.86  1.2  10.  ]\n",
      "Loss_Q: [1.98 2.23 1.12 0.   5.33] Loss_P: [3.75 2.18 2.83 1.22 9.99]\n",
      "Loss_Q: [1.97 2.26 1.09 0.   5.32] Loss_P: [3.79 2.14 2.85 1.18 9.97]\n",
      "Loss_Q: [1.92 2.25 1.06 0.   5.23] Loss_P: [3.72 2.25 2.82 1.15 9.94]\n",
      "Loss_Q: [2.   2.23 1.1  0.   5.33] Loss_P: [ 3.76  2.23  2.82  1.19 10.  ]\n",
      "Loss_Q: [2.06 2.21 1.11 0.   5.39] Loss_P: [3.75 2.24 2.82 1.15 9.96]\n",
      "Loss_Q: [2.   2.24 1.11 0.   5.35] Loss_P: [3.73 2.25 2.83 1.15 9.96]\n",
      "Loss_Q: [2.02 2.26 1.11 0.   5.39] Loss_P: [ 3.77  2.29  2.85  1.2  10.11]\n",
      "Loss_Q: [1.99 2.26 1.12 0.   5.38] Loss_P: [ 3.78  2.24  2.85  1.18 10.05]\n",
      "Loss_Q: [1.94 2.24 1.09 0.   5.27] Loss_P: [ 3.8   2.3   2.86  1.16 10.12]\n",
      "Loss_Q: [1.96 2.17 1.08 0.   5.2 ] Loss_P: [ 3.77  2.3   2.82  1.11 10.  ]\n",
      "Loss_Q: [2.   2.22 1.03 0.   5.26] Loss_P: [3.7  2.24 2.84 1.13 9.9 ]\n",
      "Loss_Q: [2.   2.19 1.07 0.   5.25] Loss_P: [ 3.79  2.26  2.86  1.14 10.05]\n",
      "Loss_Q: [2.02 2.19 1.04 0.   5.25] Loss_P: [ 3.74  2.33  2.81  1.16 10.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.93 2.21 1.09 0.   5.23] Loss_P: [ 3.79  2.31  2.82  1.17 10.09]\n",
      "Loss_Q: [1.95 2.19 1.07 0.   5.21] Loss_P: [ 3.74  2.28  2.85  1.16 10.02]\n",
      "Loss_Q: [2.05 2.17 1.14 0.   5.36] Loss_P: [ 3.73  2.31  2.86  1.19 10.09]\n",
      "Loss_Q: [2.03 2.26 1.15 0.   5.44] Loss_P: [ 3.75  2.32  2.88  1.18 10.13]\n",
      "Loss_Q: [2.04 2.29 1.14 0.   5.46] Loss_P: [ 3.78  2.3   2.83  1.2  10.1 ]\n",
      "Loss_Q: [2.11 2.18 1.12 0.   5.41] Loss_P: [ 3.65  2.32  2.88  1.17 10.02]\n",
      "Loss_Q: [2.05 2.23 1.14 0.   5.41] Loss_P: [ 3.73  2.33  2.87  1.17 10.1 ]\n",
      "Loss_Q: [2.04 2.22 1.13 0.   5.4 ] Loss_P: [3.65 2.31 2.83 1.18 9.98]\n",
      "Loss_Q: [2.01 2.21 1.09 0.   5.3 ] Loss_P: [ 3.67  2.42  2.85  1.2  10.14]\n",
      "Loss_Q: [2.03 2.2  1.11 0.   5.34] Loss_P: [ 3.67  2.36  2.84  1.23 10.1 ]\n",
      "Loss_Q: [2.09 2.2  1.13 0.   5.41] Loss_P: [ 3.67  2.32  2.85  1.19 10.02]\n",
      "Loss_Q: [2.03 2.17 1.17 0.   5.37] Loss_P: [ 3.68  2.37  2.85  1.21 10.11]\n",
      "Loss_Q: [2.02 2.2  1.13 0.   5.34] Loss_P: [ 3.67  2.38  2.86  1.18 10.09]\n",
      "Loss_Q: [2.03 2.18 1.09 0.   5.3 ] Loss_P: [ 3.67  2.28  2.86  1.22 10.04]\n",
      "Loss_Q: [2.   2.25 1.09 0.   5.35] Loss_P: [ 3.72  2.35  2.86  1.19 10.11]\n",
      "Loss_Q: [2.06 2.15 1.09 0.   5.29] Loss_P: [ 3.68  2.3   2.85  1.19 10.01]\n",
      "Loss_Q: [2.06 2.23 1.1  0.   5.38] Loss_P: [ 3.75  2.25  2.84  1.17 10.01]\n",
      "Loss_Q: [2.08 2.22 1.09 0.   5.4 ] Loss_P: [ 3.71  2.39  2.82  1.18 10.1 ]\n",
      "Loss_Q: [2.18 2.17 1.1  0.   5.45] Loss_P: [ 3.66  2.36  2.79  1.18 10.  ]\n",
      "Loss_Q: [1.98 2.23 1.09 0.   5.29] Loss_P: [ 3.73  2.31  2.85  1.17 10.05]\n",
      "Loss_Q: [2.18 2.15 1.11 0.   5.44] Loss_P: [ 3.72  2.32  2.89  1.17 10.1 ]\n",
      "Loss_Q: [2.11 2.17 1.06 0.   5.35] Loss_P: [ 3.72  2.42  2.82  1.18 10.13]\n",
      "Loss_Q: [2.11 2.14 1.06 0.   5.31] Loss_P: [ 3.71  2.41  2.83  1.19 10.14]\n",
      "Loss_Q: [2.12 2.18 1.07 0.   5.38] Loss_P: [ 3.72  2.36  2.81  1.19 10.08]\n",
      "Loss_Q: [2.08 2.18 1.08 0.   5.34] Loss_P: [3.66 2.32 2.82 1.18 9.97]\n",
      "Loss_Q: [2.13 2.19 1.11 0.   5.43] Loss_P: [ 3.78  2.36  2.78  1.15 10.07]\n",
      "Loss_Q: [1.98 2.17 1.07 0.   5.22] Loss_P: [ 3.75  2.29  2.84  1.14 10.02]\n",
      "Loss_Q: [2.06 2.16 1.05 0.   5.27] Loss_P: [ 3.73  2.31  2.79  1.17 10.  ]\n",
      "Loss_Q: [2.01 2.18 1.1  0.   5.3 ] Loss_P: [3.68 2.31 2.79 1.18 9.97]\n",
      "Loss_Q: [2.05 2.16 1.13 0.   5.33] Loss_P: [ 3.78  2.32  2.79  1.18 10.06]\n",
      "Loss_Q: [1.97 2.21 1.12 0.   5.3 ] Loss_P: [ 3.77  2.32  2.8   1.21 10.09]\n",
      "Loss_Q: [1.97 2.27 1.09 0.   5.33] Loss_P: [ 3.69  2.38  2.8   1.21 10.08]\n",
      "Loss_Q: [2.03 2.13 1.11 0.   5.26] Loss_P: [3.7  2.29 2.78 1.2  9.97]\n",
      "Loss_Q: [2.05 2.14 1.11 0.   5.31] Loss_P: [ 3.68  2.36  2.83  1.22 10.08]\n",
      "Loss_Q: [2.17 2.14 1.15 0.   5.45] Loss_P: [ 3.64  2.4   2.81  1.21 10.06]\n",
      "Loss_Q: [2.1  2.17 1.14 0.   5.41] Loss_P: [ 3.71  2.35  2.83  1.24 10.13]\n",
      "Loss_Q: [2.03 2.22 1.14 0.   5.39] Loss_P: [ 3.67  2.29  2.83  1.23 10.01]\n",
      "Loss_Q: [2.05 2.2  1.14 0.   5.4 ] Loss_P: [ 3.68  2.33  2.84  1.22 10.08]\n",
      "Loss_Q: [2.08 2.11 1.13 0.   5.31] Loss_P: [ 3.67  2.32  2.81  1.21 10.01]\n",
      "Loss_Q: [2.06 2.11 1.17 0.   5.34] Loss_P: [ 3.71  2.4   2.78  1.19 10.07]\n",
      "Loss_Q: [2.   2.11 1.13 0.   5.24] Loss_P: [ 3.67  2.38  2.8   1.21 10.06]\n",
      "Loss_Q: [2.09 2.13 1.14 0.   5.36] Loss_P: [ 3.69  2.36  2.77  1.27 10.1 ]\n",
      "Loss_Q: [2.09 2.16 1.14 0.   5.39] Loss_P: [ 3.68  2.34  2.8   1.22 10.05]\n",
      "Loss_Q: [2.15 2.14 1.17 0.   5.46] Loss_P: [ 3.67  2.43  2.86  1.22 10.17]\n",
      "Loss_Q: [2.13 2.22 1.15 0.   5.5 ] Loss_P: [ 3.72  2.34  2.79  1.22 10.07]\n",
      "Loss_Q: [2.04 2.11 1.15 0.   5.3 ] Loss_P: [ 3.69  2.36  2.82  1.22 10.09]\n",
      "Loss_Q: [2.13 2.06 1.13 0.   5.32] Loss_P: [ 3.64  2.32  2.84  1.21 10.  ]\n",
      "Loss_Q: [1.97 2.18 1.15 0.   5.3 ] Loss_P: [ 3.63  2.32  2.83  1.22 10.  ]\n",
      "Loss_Q: [1.99 2.17 1.13 0.   5.29] Loss_P: [ 3.67  2.36  2.8   1.18 10.01]\n",
      "Loss_Q: [2.   2.1  1.1  0.   5.21] Loss_P: [ 3.71  2.31  2.8   1.2  10.01]\n",
      "Loss_Q: [2.01 2.12 1.12 0.   5.25] Loss_P: [ 3.72  2.33  2.8   1.2  10.05]\n",
      "Loss_Q: [2.03 2.15 1.1  0.   5.28] Loss_P: [3.65 2.32 2.78 1.18 9.94]\n",
      "Loss_Q: [1.94 2.17 1.1  0.   5.21] Loss_P: [ 3.76  2.29  2.81  1.17 10.03]\n",
      "Loss_Q: [2.04 2.15 1.16 0.   5.35] Loss_P: [3.69 2.29 2.79 1.17 9.94]\n",
      "Loss_Q: [2.   2.15 1.13 0.   5.28] Loss_P: [3.68 2.29 2.76 1.22 9.95]\n",
      "Loss_Q: [2.03 2.2  1.12 0.   5.35] Loss_P: [3.64 2.3  2.72 1.19 9.85]\n",
      "Loss_Q: [2.06 2.21 1.09 0.   5.36] Loss_P: [3.7  2.28 2.79 1.21 9.98]\n",
      "Loss_Q: [1.96 2.14 1.1  0.   5.2 ] Loss_P: [3.6  2.31 2.79 1.18 9.88]\n",
      "Loss_Q: [1.99 2.16 1.15 0.   5.3 ] Loss_P: [ 3.79  2.27  2.81  1.16 10.02]\n",
      "Loss_Q: [2.   2.17 1.09 0.   5.27] Loss_P: [ 3.71  2.34  2.79  1.18 10.01]\n",
      "Loss_Q: [1.98 2.09 1.1  0.   5.18] Loss_P: [3.66 2.3  2.79 1.13 9.89]\n",
      "Loss_Q: [1.98 2.16 1.09 0.   5.22] Loss_P: [3.65 2.23 2.8  1.17 9.85]\n",
      "Loss_Q: [2.03 2.16 1.12 0.   5.32] Loss_P: [3.73 2.26 2.8  1.19 9.98]\n",
      "Loss_Q: [2.02 2.14 1.12 0.   5.28] Loss_P: [ 3.71  2.3   2.81  1.2  10.01]\n",
      "Loss_Q: [2.1  2.14 1.11 0.   5.34] Loss_P: [ 3.69  2.38  2.84  1.16 10.07]\n",
      "Loss_Q: [2.   2.2  1.11 0.   5.3 ] Loss_P: [3.73 2.21 2.77 1.18 9.89]\n",
      "Loss_Q: [1.99 2.14 1.11 0.   5.24] Loss_P: [3.63 2.3  2.77 1.18 9.88]\n",
      "Loss_Q: [1.92 2.12 1.11 0.   5.15] Loss_P: [3.79 2.17 2.79 1.15 9.91]\n",
      "Loss_Q: [2.   2.15 1.08 0.   5.23] Loss_P: [3.79 2.21 2.8  1.16 9.96]\n",
      "Loss_Q: [1.97 2.2  1.11 0.   5.28] Loss_P: [3.66 2.28 2.8  1.19 9.93]\n",
      "Loss_Q: [1.99 2.15 1.15 0.   5.28] Loss_P: [ 3.72  2.33  2.81  1.2  10.06]\n",
      "Loss_Q: [2.03 2.18 1.1  0.   5.31] Loss_P: [ 3.72  2.29  2.84  1.19 10.05]\n",
      "Loss_Q: [2.02 2.18 1.15 0.   5.34] Loss_P: [3.63 2.3  2.83 1.19 9.95]\n",
      "Loss_Q: [2.03 2.24 1.15 0.   5.43] Loss_P: [ 3.71  2.37  2.79  1.21 10.09]\n",
      "Loss_Q: [2.05 2.17 1.16 0.   5.38] Loss_P: [ 3.75  2.31  2.8   1.23 10.1 ]\n",
      "Loss_Q: [2.11 2.21 1.16 0.   5.48] Loss_P: [ 3.71  2.29  2.78  1.25 10.03]\n",
      "Loss_Q: [2.04 2.1  1.17 0.   5.31] Loss_P: [ 3.7   2.31  2.85  1.21 10.07]\n",
      "Loss_Q: [2.01 2.18 1.17 0.   5.35] Loss_P: [ 3.67  2.38  2.78  1.24 10.07]\n",
      "Loss_Q: [2.06 2.15 1.14 0.   5.35] Loss_P: [ 3.68  2.34  2.87  1.22 10.1 ]\n",
      "Loss_Q: [2.01 2.2  1.14 0.   5.34] Loss_P: [ 3.64  2.38  2.86  1.21 10.1 ]\n",
      "Loss_Q: [2.08 2.17 1.14 0.   5.39] Loss_P: [ 3.69  2.37  2.83  1.25 10.15]\n",
      "Loss_Q: [2.08 2.1  1.14 0.   5.32] Loss_P: [ 3.7   2.31  2.86  1.2  10.07]\n",
      "Loss_Q: [1.98 2.25 1.15 0.   5.38] Loss_P: [ 3.73  2.32  2.87  1.21 10.14]\n",
      "Loss_Q: [2.07 2.2  1.17 0.   5.44] Loss_P: [ 3.68  2.35  2.84  1.21 10.08]\n",
      "Loss_Q: [2.02 2.21 1.18 0.   5.41] Loss_P: [ 3.74  2.36  2.87  1.24 10.21]\n",
      "Loss_Q: [2.02 2.18 1.17 0.   5.37] Loss_P: [ 3.74  2.33  2.83  1.23 10.13]\n",
      "Loss_Q: [2.02 2.27 1.13 0.   5.42] Loss_P: [ 3.73  2.31  2.84  1.24 10.13]\n",
      "Loss_Q: [2.   2.23 1.17 0.   5.41] Loss_P: [3.7  2.22 2.82 1.2  9.95]\n",
      "Loss_Q: [2.02 2.15 1.18 0.   5.35] Loss_P: [ 3.68  2.28  2.85  1.25 10.05]\n",
      "Loss_Q: [2.04 2.22 1.17 0.   5.43] Loss_P: [ 3.69  2.28  2.83  1.22 10.02]\n",
      "Loss_Q: [2.06 2.14 1.13 0.   5.33] Loss_P: [ 3.73  2.3   2.83  1.2  10.06]\n",
      "Loss_Q: [2.06 2.15 1.15 0.   5.36] Loss_P: [ 3.69  2.36  2.8   1.24 10.09]\n",
      "Loss_Q: [2.11 2.2  1.16 0.   5.47] Loss_P: [ 3.58  2.4   2.81  1.24 10.03]\n",
      "Loss_Q: [2.13 2.2  1.16 0.   5.49] Loss_P: [ 3.67  2.39  2.84  1.21 10.11]\n",
      "Loss_Q: [2.03 2.14 1.14 0.   5.31] Loss_P: [ 3.66  2.36  2.8   1.24 10.06]\n",
      "Loss_Q: [2.1  2.22 1.16 0.   5.48] Loss_P: [ 3.67  2.34  2.8   1.25 10.06]\n",
      "Loss_Q: [1.99 2.2  1.15 0.   5.34] Loss_P: [ 3.71  2.39  2.81  1.25 10.15]\n",
      "Loss_Q: [2.11 2.19 1.16 0.   5.47] Loss_P: [ 3.72  2.4   2.83  1.24 10.19]\n",
      "Loss_Q: [2.07 2.2  1.14 0.   5.41] Loss_P: [ 3.69  2.36  2.87  1.24 10.15]\n",
      "Loss_Q: [2.07 2.22 1.14 0.   5.44] Loss_P: [ 3.75  2.33  2.84  1.27 10.18]\n",
      "Loss_Q: [1.97 2.17 1.12 0.   5.26] Loss_P: [ 3.65  2.32  2.83  1.26 10.06]\n",
      "Loss_Q: [2.04 2.14 1.12 0.   5.3 ] Loss_P: [ 3.69  2.3   2.85  1.21 10.05]\n",
      "Loss_Q: [2.09 2.14 1.14 0.   5.37] Loss_P: [3.64 2.28 2.82 1.18 9.92]\n",
      "Loss_Q: [2.04 2.16 1.14 0.   5.34] Loss_P: [ 3.69  2.29  2.83  1.24 10.05]\n",
      "Loss_Q: [2.09 2.18 1.14 0.   5.41] Loss_P: [ 3.68  2.31  2.86  1.22 10.08]\n",
      "Loss_Q: [2.   2.18 1.14 0.   5.32] Loss_P: [ 3.68  2.36  2.83  1.2  10.07]\n",
      "Loss_Q: [1.98 2.26 1.15 0.   5.4 ] Loss_P: [3.72 2.24 2.81 1.21 9.98]\n",
      "Loss_Q: [2.04 2.27 1.18 0.   5.49] Loss_P: [ 3.74  2.33  2.85  1.24 10.16]\n",
      "Loss_Q: [2.07 2.17 1.13 0.   5.37] Loss_P: [ 3.71  2.26  2.84  1.22 10.04]\n",
      "Loss_Q: [2.03 2.21 1.12 0.   5.36] Loss_P: [ 3.71  2.27  2.84  1.23 10.05]\n",
      "Loss_Q: [1.98 2.24 1.14 0.   5.37] Loss_P: [ 3.69  2.27  2.84  1.22 10.02]\n",
      "Loss_Q: [2.01 2.21 1.13 0.   5.35] Loss_P: [ 3.76  2.31  2.83  1.25 10.15]\n",
      "Loss_Q: [2.07 2.24 1.15 0.   5.46] Loss_P: [ 3.67  2.31  2.86  1.24 10.09]\n",
      "Loss_Q: [2.02 2.18 1.18 0.   5.38] Loss_P: [ 3.75  2.36  2.89  1.22 10.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.05 2.21 1.13 0.   5.39] Loss_P: [ 3.7   2.33  2.87  1.22 10.12]\n",
      "Loss_Q: [2.05 2.15 1.16 0.   5.35] Loss_P: [ 3.77  2.29  2.86  1.24 10.16]\n",
      "Loss_Q: [1.96 2.23 1.15 0.   5.33] Loss_P: [ 3.73  2.3   2.9   1.22 10.15]\n",
      "Loss_Q: [2.05 2.2  1.17 0.   5.42] Loss_P: [ 3.7   2.26  2.87  1.22 10.05]\n",
      "Loss_Q: [1.83 2.31 1.16 0.   5.3 ] Loss_P: [ 3.77  2.28  2.89  1.24 10.17]\n",
      "Loss_Q: [2.03 2.22 1.18 0.   5.43] Loss_P: [ 3.73  2.33  2.88  1.23 10.18]\n",
      "Loss_Q: [1.89 2.22 1.18 0.   5.3 ] Loss_P: [ 3.72  2.28  2.87  1.24 10.11]\n",
      "Loss_Q: [2.03 2.21 1.2  0.   5.43] Loss_P: [ 3.7   2.29  2.86  1.27 10.11]\n",
      "Loss_Q: [1.96 2.2  1.21 0.   5.37] Loss_P: [ 3.74  2.27  2.87  1.27 10.15]\n",
      "Loss_Q: [1.92 2.24 1.2  0.   5.36] Loss_P: [ 3.65  2.39  2.87  1.26 10.16]\n",
      "Loss_Q: [2.03 2.26 1.16 0.   5.45] Loss_P: [ 3.75  2.28  2.88  1.28 10.2 ]\n",
      "Loss_Q: [2.03 2.19 1.17 0.   5.38] Loss_P: [ 3.77  2.14  2.87  1.24 10.02]\n",
      "Loss_Q: [1.97 2.22 1.16 0.   5.35] Loss_P: [ 3.78  2.24  2.85  1.26 10.13]\n",
      "Loss_Q: [2.02 2.18 1.14 0.   5.34] Loss_P: [ 3.76  2.19  2.87  1.24 10.05]\n",
      "Loss_Q: [2.05 2.12 1.14 0.   5.31] Loss_P: [ 3.7   2.24  2.86  1.23 10.02]\n",
      "Loss_Q: [2.   2.21 1.15 0.   5.37] Loss_P: [ 3.75  2.23  2.83  1.22 10.03]\n",
      "Loss_Q: [1.95 2.24 1.13 0.   5.32] Loss_P: [ 3.65  2.33  2.84  1.24 10.06]\n",
      "Loss_Q: [2.01 2.21 1.19 0.   5.41] Loss_P: [ 3.66  2.33  2.83  1.24 10.06]\n",
      "Loss_Q: [1.99 2.14 1.17 0.   5.31] Loss_P: [ 3.69  2.32  2.85  1.24 10.1 ]\n",
      "Loss_Q: [1.98 2.21 1.17 0.   5.35] Loss_P: [ 3.7   2.32  2.83  1.21 10.06]\n",
      "Loss_Q: [2.01 2.2  1.11 0.   5.33] Loss_P: [ 3.77  2.26  2.87  1.22 10.12]\n",
      "Loss_Q: [1.98 2.27 1.11 0.   5.37] Loss_P: [3.72 2.2  2.85 1.21 9.98]\n",
      "Loss_Q: [1.95 2.2  1.12 0.   5.27] Loss_P: [ 3.66  2.27  2.86  1.22 10.01]\n",
      "Loss_Q: [2.02 2.22 1.12 0.   5.36] Loss_P: [3.69 2.2  2.89 1.2  9.99]\n",
      "Loss_Q: [2.02 2.2  1.17 0.   5.38] Loss_P: [ 3.76  2.28  2.89  1.2  10.14]\n",
      "Loss_Q: [1.98 2.19 1.16 0.   5.33] Loss_P: [ 3.68  2.3   2.89  1.2  10.07]\n",
      "Loss_Q: [2.1  2.17 1.14 0.   5.42] Loss_P: [ 3.71  2.34  2.84  1.23 10.13]\n",
      "Loss_Q: [1.96 2.23 1.13 0.   5.32] Loss_P: [ 3.72  2.3   2.9   1.23 10.14]\n",
      "Loss_Q: [1.99 2.18 1.16 0.   5.33] Loss_P: [ 3.69  2.26  2.88  1.2  10.03]\n",
      "Loss_Q: [2.02 2.25 1.18 0.   5.46] Loss_P: [ 3.81  2.27  2.84  1.24 10.15]\n",
      "Loss_Q: [2.   2.21 1.17 0.   5.37] Loss_P: [ 3.74  2.3   2.87  1.19 10.1 ]\n",
      "Loss_Q: [2.14 2.19 1.13 0.   5.46] Loss_P: [ 3.66  2.28  2.84  1.24 10.02]\n",
      "Loss_Q: [2.11 2.12 1.18 0.   5.42] Loss_P: [ 3.7   2.31  2.83  1.23 10.06]\n",
      "Loss_Q: [1.96 2.2  1.18 0.   5.34] Loss_P: [ 3.74  2.37  2.84  1.25 10.2 ]\n",
      "Loss_Q: [1.98 2.19 1.14 0.   5.32] Loss_P: [ 3.68  2.33  2.85  1.26 10.12]\n",
      "Loss_Q: [2.04 2.16 1.16 0.   5.37] Loss_P: [ 3.68  2.34  2.8   1.25 10.07]\n",
      "Loss_Q: [1.99 2.23 1.15 0.   5.37] Loss_P: [ 3.67  2.3   2.86  1.21 10.03]\n",
      "Loss_Q: [2.   2.2  1.14 0.   5.33] Loss_P: [ 3.72  2.26  2.85  1.25 10.08]\n",
      "Loss_Q: [1.98 2.22 1.12 0.   5.32] Loss_P: [ 3.78  2.25  2.85  1.24 10.12]\n",
      "Loss_Q: [2.11 2.2  1.17 0.   5.48] Loss_P: [ 3.76  2.28  2.84  1.25 10.14]\n",
      "Loss_Q: [2.   2.16 1.15 0.   5.3 ] Loss_P: [3.71 2.18 2.86 1.25 9.99]\n",
      "Loss_Q: [2.09 2.16 1.16 0.   5.41] Loss_P: [ 3.7   2.26  2.85  1.25 10.06]\n",
      "Loss_Q: [2.03 2.18 1.16 0.   5.36] Loss_P: [ 3.73  2.31  2.82  1.24 10.1 ]\n",
      "Loss_Q: [1.96 2.15 1.19 0.   5.29] Loss_P: [ 3.72  2.26  2.85  1.27 10.09]\n",
      "Loss_Q: [2.02 2.21 1.16 0.   5.38] Loss_P: [ 3.71  2.27  2.83  1.24 10.05]\n",
      "Loss_Q: [2.07 2.21 1.18 0.   5.46] Loss_P: [ 3.73  2.27  2.89  1.21 10.11]\n",
      "Loss_Q: [2.   2.21 1.17 0.   5.38] Loss_P: [ 3.7   2.32  2.84  1.25 10.12]\n",
      "Loss_Q: [1.94 2.21 1.17 0.   5.32] Loss_P: [ 3.75  2.35  2.86  1.28 10.24]\n",
      "Loss_Q: [1.99 2.19 1.15 0.   5.33] Loss_P: [ 3.62  2.34  2.86  1.26 10.07]\n",
      "Loss_Q: [1.99 2.27 1.2  0.   5.47] Loss_P: [ 3.65  2.29  2.85  1.25 10.03]\n",
      "Loss_Q: [2.04 2.21 1.15 0.   5.4 ] Loss_P: [3.68 2.24 2.85 1.22 9.99]\n",
      "Loss_Q: [1.99 2.19 1.14 0.   5.32] Loss_P: [ 3.74  2.25  2.85  1.24 10.08]\n",
      "Loss_Q: [2.01 2.18 1.1  0.   5.29] Loss_P: [ 3.73  2.26  2.83  1.25 10.07]\n",
      "Loss_Q: [2.11 2.14 1.15 0.   5.41] Loss_P: [3.63 2.24 2.84 1.24 9.96]\n",
      "Loss_Q: [1.96 2.21 1.12 0.   5.29] Loss_P: [ 3.68  2.27  2.81  1.26 10.02]\n",
      "Loss_Q: [2.04 2.21 1.16 0.   5.42] Loss_P: [ 3.65  2.3   2.84  1.26 10.05]\n",
      "Loss_Q: [1.94 2.25 1.19 0.   5.37] Loss_P: [ 3.71  2.29  2.84  1.24 10.08]\n",
      "Loss_Q: [1.99 2.15 1.17 0.   5.31] Loss_P: [ 3.69  2.31  2.84  1.25 10.09]\n",
      "Loss_Q: [2.   2.21 1.13 0.   5.33] Loss_P: [ 3.7   2.28  2.85  1.24 10.07]\n",
      "Loss_Q: [2.05 2.21 1.15 0.   5.42] Loss_P: [ 3.73  2.27  2.84  1.23 10.06]\n",
      "Loss_Q: [2.03 2.2  1.13 0.   5.36] Loss_P: [ 3.67  2.35  2.83  1.25 10.09]\n",
      "Loss_Q: [2.05 2.2  1.16 0.   5.41] Loss_P: [ 3.75  2.29  2.82  1.23 10.08]\n",
      "Loss_Q: [1.98 2.2  1.18 0.   5.36] Loss_P: [3.62 2.23 2.84 1.24 9.94]\n",
      "Loss_Q: [2.02 2.23 1.18 0.   5.43] Loss_P: [ 3.66  2.28  2.84  1.27 10.06]\n",
      "Loss_Q: [1.94 2.21 1.14 0.   5.28] Loss_P: [ 3.7   2.35  2.85  1.28 10.19]\n",
      "Loss_Q: [2.04 2.19 1.19 0.   5.42] Loss_P: [ 3.73  2.29  2.87  1.25 10.13]\n",
      "Loss_Q: [1.93 2.21 1.18 0.   5.31] Loss_P: [ 3.73  2.28  2.86  1.27 10.13]\n",
      "Loss_Q: [2.03 2.17 1.17 0.   5.37] Loss_P: [ 3.74  2.26  2.88  1.25 10.14]\n",
      "Loss_Q: [1.93 2.26 1.17 0.   5.36] Loss_P: [ 3.73  2.3   2.86  1.23 10.11]\n",
      "Loss_Q: [1.95 2.15 1.14 0.   5.25] Loss_P: [ 3.67  2.29  2.84  1.25 10.05]\n",
      "Loss_Q: [1.96 2.18 1.16 0.   5.3 ] Loss_P: [ 3.7   2.38  2.84  1.23 10.16]\n",
      "Loss_Q: [2.05 2.17 1.18 0.   5.4 ] Loss_P: [ 3.69  2.29  2.83  1.26 10.08]\n",
      "Loss_Q: [2.06 2.23 1.18 0.   5.47] Loss_P: [ 3.65  2.29  2.82  1.26 10.01]\n",
      "Loss_Q: [1.9  2.25 1.16 0.   5.31] Loss_P: [ 3.68  2.31  2.81  1.27 10.07]\n",
      "Loss_Q: [2.02 2.2  1.14 0.   5.36] Loss_P: [3.61 2.27 2.84 1.23 9.94]\n",
      "Loss_Q: [2.   2.25 1.12 0.   5.37] Loss_P: [ 3.68  2.35  2.81  1.26 10.11]\n",
      "Loss_Q: [2.05 2.12 1.17 0.   5.33] Loss_P: [ 3.68  2.32  2.83  1.26 10.09]\n",
      "Loss_Q: [1.91 2.24 1.12 0.   5.26] Loss_P: [ 3.68  2.28  2.82  1.23 10.01]\n",
      "Loss_Q: [1.95 2.18 1.16 0.   5.29] Loss_P: [ 3.67  2.38  2.83  1.25 10.12]\n",
      "Loss_Q: [2.   2.19 1.2  0.   5.39] Loss_P: [ 3.68  2.34  2.8   1.26 10.07]\n",
      "Loss_Q: [2.07 2.14 1.16 0.   5.37] Loss_P: [ 3.73  2.29  2.8   1.25 10.06]\n",
      "Loss_Q: [2.01 2.22 1.15 0.   5.38] Loss_P: [ 3.68  2.3   2.81  1.26 10.06]\n",
      "Loss_Q: [2.   2.2  1.16 0.   5.35] Loss_P: [3.64 2.24 2.83 1.25 9.96]\n",
      "Loss_Q: [2.01 2.2  1.18 0.   5.39] Loss_P: [ 3.71  2.31  2.8   1.27 10.09]\n",
      "Loss_Q: [2.01 2.22 1.18 0.   5.41] Loss_P: [ 3.66  2.29  2.84  1.25 10.04]\n",
      "Loss_Q: [1.95 2.22 1.14 0.   5.31] Loss_P: [ 3.67  2.3   2.8   1.25 10.02]\n",
      "Loss_Q: [1.92 2.28 1.12 0.   5.32] Loss_P: [ 3.76  2.24  2.8   1.26 10.07]\n",
      "Loss_Q: [1.98 2.15 1.18 0.   5.31] Loss_P: [ 3.68  2.32  2.79  1.27 10.07]\n",
      "Loss_Q: [2.   2.15 1.14 0.   5.29] Loss_P: [ 3.65  2.36  2.84  1.25 10.09]\n",
      "Loss_Q: [1.98 2.21 1.16 0.   5.35] Loss_P: [ 3.77  2.33  2.77  1.26 10.13]\n",
      "Loss_Q: [2.07 2.17 1.14 0.   5.39] Loss_P: [ 3.69  2.34  2.83  1.24 10.12]\n",
      "Loss_Q: [2.03 2.2  1.16 0.   5.4 ] Loss_P: [ 3.71  2.34  2.76  1.29 10.11]\n",
      "Loss_Q: [2.03 2.17 1.17 0.   5.37] Loss_P: [ 3.63  2.39  2.8   1.29 10.1 ]\n",
      "Loss_Q: [1.91 2.25 1.19 0.   5.35] Loss_P: [ 3.69  2.37  2.81  1.3  10.17]\n",
      "Loss_Q: [2.02 2.18 1.2  0.   5.39] Loss_P: [ 3.74  2.3   2.82  1.27 10.14]\n",
      "Loss_Q: [1.99 2.19 1.22 0.   5.4 ] Loss_P: [ 3.68  2.32  2.82  1.29 10.11]\n",
      "Loss_Q: [2.04 2.19 1.23 0.   5.47] Loss_P: [ 3.69  2.36  2.79  1.33 10.17]\n",
      "Loss_Q: [2.05 2.2  1.18 0.   5.43] Loss_P: [ 3.74  2.35  2.8   1.31 10.19]\n",
      "Loss_Q: [2.16 2.21 1.18 0.   5.54] Loss_P: [ 3.62  2.33  2.82  1.31 10.08]\n",
      "Loss_Q: [2.01 2.2  1.2  0.   5.42] Loss_P: [ 3.63  2.4   2.81  1.31 10.15]\n",
      "Loss_Q: [1.93 2.24 1.15 0.   5.32] Loss_P: [ 3.64  2.37  2.83  1.31 10.15]\n",
      "Loss_Q: [2.1  2.2  1.21 0.   5.5 ] Loss_P: [ 3.59  2.36  2.82  1.31 10.08]\n",
      "Loss_Q: [2.08 2.16 1.17 0.   5.41] Loss_P: [ 3.69  2.33  2.81  1.3  10.13]\n",
      "Loss_Q: [2.15 2.21 1.22 0.   5.58] Loss_P: [ 3.68  2.32  2.82  1.32 10.14]\n",
      "Loss_Q: [2.1  2.22 1.21 0.   5.53] Loss_P: [ 3.65  2.3   2.79  1.32 10.06]\n",
      "Loss_Q: [2.09 2.15 1.21 0.   5.45] Loss_P: [ 3.73  2.29  2.84  1.33 10.2 ]\n",
      "Loss_Q: [2.08 2.19 1.2  0.   5.47] Loss_P: [ 3.76  2.26  2.8   1.28 10.1 ]\n",
      "Loss_Q: [2.06 2.21 1.19 0.   5.46] Loss_P: [ 3.65  2.28  2.84  1.29 10.06]\n",
      "Loss_Q: [2.13 2.24 1.2  0.   5.57] Loss_P: [ 3.69  2.34  2.82  1.28 10.13]\n",
      "Loss_Q: [2.05 2.22 1.16 0.   5.43] Loss_P: [ 3.72  2.28  2.83  1.3  10.13]\n",
      "Loss_Q: [2.06 2.14 1.17 0.   5.37] Loss_P: [ 3.76  2.31  2.85  1.27 10.18]\n",
      "Loss_Q: [2.01 2.1  1.18 0.   5.28] Loss_P: [ 3.65  2.32  2.86  1.32 10.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.02 2.17 1.18 0.   5.37] Loss_P: [ 3.68  2.26  2.83  1.31 10.07]\n",
      "Loss_Q: [2.08 2.17 1.2  0.   5.44] Loss_P: [ 3.72  2.33  2.8   1.31 10.15]\n",
      "Loss_Q: [2.01 2.15 1.23 0.   5.39] Loss_P: [ 3.73  2.36  2.81  1.32 10.22]\n",
      "Loss_Q: [2.14 2.15 1.25 0.   5.53] Loss_P: [ 3.76  2.24  2.86  1.31 10.16]\n",
      "Loss_Q: [2.08 2.2  1.23 0.   5.52] Loss_P: [ 3.83  2.3   2.86  1.3  10.29]\n",
      "Loss_Q: [2.06 2.2  1.2  0.   5.46] Loss_P: [ 3.73  2.34  2.85  1.28 10.19]\n",
      "Loss_Q: [1.98 2.18 1.22 0.   5.38] Loss_P: [ 3.71  2.33  2.81  1.32 10.17]\n",
      "Loss_Q: [2.05 2.15 1.22 0.   5.42] Loss_P: [ 3.7   2.29  2.84  1.29 10.12]\n",
      "Loss_Q: [1.99 2.21 1.25 0.   5.44] Loss_P: [ 3.72  2.3   2.81  1.32 10.15]\n",
      "Loss_Q: [2.1  2.13 1.22 0.   5.45] Loss_P: [ 3.68  2.35  2.85  1.33 10.21]\n",
      "Loss_Q: [1.98 2.16 1.25 0.   5.39] Loss_P: [ 3.66  2.38  2.85  1.33 10.21]\n",
      "Loss_Q: [2.1  2.15 1.22 0.   5.46] Loss_P: [ 3.7   2.34  2.8   1.33 10.18]\n",
      "Loss_Q: [2.06 2.16 1.22 0.   5.44] Loss_P: [ 3.61  2.34  2.8   1.33 10.08]\n",
      "Loss_Q: [2.15 2.12 1.25 0.   5.52] Loss_P: [ 3.67  2.36  2.85  1.31 10.2 ]\n",
      "Loss_Q: [2.1  2.18 1.16 0.   5.44] Loss_P: [ 3.62  2.38  2.84  1.29 10.13]\n",
      "Loss_Q: [2.12 2.18 1.25 0.   5.55] Loss_P: [ 3.6   2.41  2.86  1.31 10.18]\n",
      "Loss_Q: [2.03 2.2  1.2  0.   5.43] Loss_P: [ 3.75  2.38  2.83  1.31 10.28]\n",
      "Loss_Q: [2.1  2.22 1.22 0.   5.54] Loss_P: [ 3.71  2.41  2.81  1.32 10.25]\n",
      "Loss_Q: [2.05 2.23 1.19 0.   5.47] Loss_P: [ 3.69  2.35  2.86  1.3  10.2 ]\n",
      "Loss_Q: [2.07 2.18 1.23 0.   5.48] Loss_P: [ 3.7   2.36  2.84  1.3  10.2 ]\n",
      "Loss_Q: [2.11 2.17 1.21 0.   5.48] Loss_P: [ 3.67  2.36  2.85  1.3  10.18]\n",
      "Loss_Q: [2.16 2.16 1.2  0.   5.53] Loss_P: [ 3.73  2.32  2.87  1.29 10.22]\n",
      "Loss_Q: [2.06 2.16 1.19 0.   5.41] Loss_P: [ 3.72  2.37  2.86  1.28 10.22]\n",
      "Loss_Q: [2.04 2.22 1.21 0.   5.47] Loss_P: [ 3.73  2.36  2.89  1.28 10.26]\n",
      "Loss_Q: [2.04 2.19 1.16 0.   5.4 ] Loss_P: [ 3.74  2.34  2.85  1.29 10.22]\n",
      "Loss_Q: [2.11 2.2  1.24 0.   5.54] Loss_P: [ 3.68  2.34  2.86  1.29 10.17]\n",
      "Loss_Q: [2.04 2.23 1.21 0.   5.49] Loss_P: [ 3.62  2.32  2.85  1.29 10.08]\n",
      "Loss_Q: [2.12 2.17 1.24 0.   5.52] Loss_P: [ 3.63  2.44  2.79  1.3  10.16]\n",
      "Loss_Q: [2.11 2.2  1.24 0.   5.55] Loss_P: [ 3.71  2.43  2.82  1.33 10.29]\n",
      "Loss_Q: [2.06 2.17 1.22 0.   5.45] Loss_P: [ 3.75  2.29  2.83  1.32 10.19]\n",
      "Loss_Q: [2.09 2.18 1.22 0.   5.5 ] Loss_P: [ 3.68  2.29  2.85  1.33 10.15]\n",
      "Loss_Q: [2.12 2.19 1.22 0.   5.53] Loss_P: [ 3.74  2.34  2.8   1.34 10.22]\n",
      "Loss_Q: [2.04 2.14 1.18 0.   5.36] Loss_P: [ 3.66  2.23  2.86  1.29 10.04]\n",
      "Loss_Q: [2.08 2.15 1.21 0.   5.45] Loss_P: [ 3.68  2.33  2.83  1.3  10.15]\n",
      "Loss_Q: [2.04 2.2  1.19 0.   5.43] Loss_P: [ 3.81  2.25  2.86  1.33 10.24]\n",
      "Loss_Q: [2.1  2.23 1.22 0.   5.56] Loss_P: [ 3.71  2.29  2.83  1.33 10.16]\n",
      "Loss_Q: [2.07 2.18 1.24 0.   5.49] Loss_P: [ 3.71  2.25  2.84  1.32 10.12]\n",
      "Loss_Q: [2.03 2.21 1.21 0.   5.44] Loss_P: [ 3.78  2.3   2.87  1.32 10.26]\n",
      "Loss_Q: [2.1  2.18 1.25 0.   5.53] Loss_P: [ 3.71  2.31  2.86  1.3  10.18]\n",
      "Loss_Q: [2.01 2.21 1.19 0.   5.41] Loss_P: [ 3.72  2.29  2.85  1.3  10.15]\n",
      "Loss_Q: [2.12 2.19 1.22 0.   5.53] Loss_P: [ 3.72  2.23  2.86  1.3  10.11]\n",
      "Loss_Q: [2.04 2.27 1.22 0.   5.53] Loss_P: [ 3.68  2.3   2.81  1.34 10.14]\n",
      "Loss_Q: [2.15 2.17 1.23 0.   5.55] Loss_P: [ 3.7   2.35  2.88  1.3  10.23]\n",
      "Loss_Q: [2.05 2.12 1.2  0.   5.38] Loss_P: [ 3.71  2.31  2.84  1.29 10.15]\n",
      "Loss_Q: [2.13 2.13 1.23 0.   5.5 ] Loss_P: [ 3.71  2.3   2.83  1.3  10.13]\n",
      "Loss_Q: [2.05 2.18 1.14 0.   5.36] Loss_P: [ 3.73  2.26  2.88  1.25 10.12]\n",
      "Loss_Q: [2.09 2.24 1.16 0.   5.49] Loss_P: [ 3.69  2.36  2.84  1.28 10.17]\n",
      "Loss_Q: [2.09 2.12 1.22 0.   5.43] Loss_P: [ 3.72  2.3   2.87  1.24 10.14]\n",
      "Loss_Q: [2.   2.22 1.19 0.   5.42] Loss_P: [ 3.67  2.38  2.85  1.26 10.16]\n",
      "Loss_Q: [2.   2.29 1.14 0.   5.44] Loss_P: [ 3.71  2.29  2.89  1.23 10.12]\n",
      "Loss_Q: [2.07 2.24 1.19 0.   5.5 ] Loss_P: [ 3.66  2.3   2.86  1.26 10.07]\n",
      "Loss_Q: [2.07 2.24 1.2  0.   5.5 ] Loss_P: [ 3.78  2.31  2.88  1.25 10.22]\n",
      "Loss_Q: [2.03 2.21 1.16 0.   5.4 ] Loss_P: [ 3.68  2.32  2.87  1.22 10.08]\n",
      "Loss_Q: [1.99 2.19 1.19 0.   5.37] Loss_P: [ 3.67  2.31  2.83  1.25 10.07]\n",
      "Loss_Q: [1.96 2.23 1.19 0.   5.38] Loss_P: [ 3.74  2.31  2.9   1.25 10.2 ]\n",
      "Loss_Q: [2.03 2.23 1.18 0.   5.43] Loss_P: [ 3.74  2.24  2.85  1.31 10.14]\n",
      "Loss_Q: [2.11 2.17 1.17 0.   5.45] Loss_P: [ 3.71  2.26  2.85  1.27 10.1 ]\n",
      "Loss_Q: [2.02 2.19 1.16 0.   5.36] Loss_P: [ 3.65  2.27  2.84  1.25 10.02]\n",
      "Loss_Q: [1.97 2.25 1.17 0.   5.39] Loss_P: [ 3.75  2.28  2.85  1.28 10.16]\n",
      "Loss_Q: [1.97 2.22 1.21 0.   5.39] Loss_P: [ 3.72  2.31  2.84  1.24 10.11]\n",
      "Loss_Q: [1.99 2.18 1.2  0.   5.37] Loss_P: [ 3.7   2.35  2.82  1.28 10.16]\n",
      "Loss_Q: [2.1  2.19 1.18 0.   5.47] Loss_P: [ 3.72  2.23  2.85  1.27 10.08]\n",
      "Loss_Q: [2.05 2.24 1.19 0.   5.48] Loss_P: [ 3.73  2.3   2.83  1.3  10.16]\n",
      "Loss_Q: [2.11 2.16 1.17 0.   5.44] Loss_P: [ 3.75  2.32  2.85  1.27 10.2 ]\n",
      "Loss_Q: [2.05 2.24 1.2  0.   5.48] Loss_P: [ 3.69  2.37  2.81  1.31 10.19]\n",
      "Loss_Q: [2.03 2.21 1.15 0.   5.38] Loss_P: [ 3.69  2.27  2.82  1.29 10.06]\n",
      "Loss_Q: [2.07 2.16 1.21 0.   5.45] Loss_P: [ 3.76  2.3   2.85  1.26 10.16]\n",
      "Loss_Q: [2.08 2.22 1.17 0.   5.47] Loss_P: [ 3.68  2.35  2.83  1.25 10.1 ]\n",
      "Loss_Q: [2.07 2.18 1.16 0.   5.41] Loss_P: [ 3.77  2.35  2.85  1.25 10.23]\n",
      "Loss_Q: [2.09 2.21 1.16 0.   5.46] Loss_P: [ 3.74  2.34  2.8   1.25 10.14]\n",
      "Loss_Q: [2.07 2.21 1.12 0.   5.41] Loss_P: [ 3.71  2.33  2.83  1.24 10.11]\n",
      "Loss_Q: [2.07 2.22 1.15 0.   5.44] Loss_P: [ 3.74  2.31  2.85  1.25 10.14]\n",
      "Loss_Q: [2.04 2.19 1.13 0.   5.36] Loss_P: [ 3.77  2.31  2.85  1.24 10.17]\n",
      "Loss_Q: [2.   2.23 1.13 0.   5.37] Loss_P: [ 3.7   2.28  2.83  1.23 10.04]\n",
      "Loss_Q: [2.08 2.2  1.11 0.   5.39] Loss_P: [ 3.73  2.35  2.86  1.2  10.14]\n",
      "Loss_Q: [2.03 2.26 1.09 0.   5.38] Loss_P: [ 3.68  2.32  2.83  1.2  10.04]\n",
      "Loss_Q: [2.02 2.2  1.11 0.   5.33] Loss_P: [ 3.72  2.31  2.84  1.19 10.06]\n",
      "Loss_Q: [2.06 2.19 1.11 0.   5.36] Loss_P: [3.67 2.27 2.88 1.17 9.99]\n",
      "Loss_Q: [2.01 2.19 1.11 0.   5.3 ] Loss_P: [ 3.68  2.32  2.86  1.19 10.06]\n",
      "Loss_Q: [2.02 2.17 1.15 0.   5.34] Loss_P: [ 3.82  2.31  2.87  1.27 10.27]\n",
      "Loss_Q: [2.   2.19 1.14 0.   5.33] Loss_P: [ 3.73  2.37  2.85  1.2  10.15]\n",
      "Loss_Q: [2.09 2.17 1.13 0.   5.4 ] Loss_P: [ 3.66  2.31  2.85  1.24 10.06]\n",
      "Loss_Q: [2.02 2.18 1.18 0.   5.38] Loss_P: [ 3.76  2.36  2.85  1.22 10.2 ]\n",
      "Loss_Q: [2.04 2.15 1.15 0.   5.34] Loss_P: [ 3.66  2.33  2.86  1.27 10.11]\n",
      "Loss_Q: [2.06 2.18 1.2  0.   5.44] Loss_P: [ 3.64  2.31  2.82  1.26 10.04]\n",
      "Loss_Q: [2.   2.19 1.19 0.   5.38] Loss_P: [ 3.74  2.32  2.87  1.3  10.23]\n",
      "Loss_Q: [2.06 2.26 1.18 0.   5.5 ] Loss_P: [ 3.79  2.33  2.85  1.3  10.27]\n",
      "Loss_Q: [2.04 2.15 1.2  0.   5.38] Loss_P: [ 3.7   2.26  2.87  1.3  10.13]\n",
      "Loss_Q: [2.03 2.2  1.2  0.   5.43] Loss_P: [ 3.69  2.33  2.86  1.29 10.17]\n",
      "Loss_Q: [2.   2.17 1.18 0.   5.36] Loss_P: [ 3.7   2.4   2.82  1.34 10.27]\n",
      "Loss_Q: [2.06 2.15 1.2  0.   5.4 ] Loss_P: [ 3.7   2.36  2.86  1.34 10.25]\n",
      "Loss_Q: [2.06 2.2  1.22 0.   5.48] Loss_P: [ 3.79  2.37  2.83  1.31 10.3 ]\n",
      "Loss_Q: [2.09 2.12 1.22 0.   5.44] Loss_P: [ 3.74  2.33  2.87  1.33 10.28]\n",
      "Loss_Q: [2.07 2.21 1.23 0.   5.51] Loss_P: [ 3.76  2.3   2.84  1.38 10.27]\n",
      "Loss_Q: [2.05 2.22 1.24 0.   5.51] Loss_P: [ 3.68  2.29  2.82  1.36 10.14]\n",
      "Loss_Q: [2.05 2.17 1.19 0.   5.4 ] Loss_P: [ 3.77  2.32  2.83  1.36 10.27]\n",
      "Loss_Q: [2.12 2.22 1.23 0.   5.57] Loss_P: [ 3.69  2.32  2.83  1.37 10.21]\n",
      "Loss_Q: [2.12 2.21 1.26 0.   5.59] Loss_P: [ 3.68  2.33  2.82  1.33 10.16]\n",
      "Loss_Q: [2.05 2.16 1.26 0.   5.47] Loss_P: [ 3.76  2.35  2.82  1.39 10.31]\n",
      "Loss_Q: [2.08 2.2  1.3  0.   5.57] Loss_P: [ 3.8   2.29  2.83  1.41 10.32]\n",
      "Loss_Q: [2.06 2.22 1.23 0.   5.51] Loss_P: [ 3.76  2.32  2.84  1.38 10.3 ]\n",
      "Loss_Q: [2.07 2.22 1.23 0.   5.52] Loss_P: [ 3.72  2.36  2.84  1.35 10.27]\n",
      "Loss_Q: [2.04 2.19 1.24 0.   5.47] Loss_P: [ 3.69  2.37  2.84  1.38 10.28]\n",
      "Loss_Q: [2.11 2.2  1.26 0.   5.56] Loss_P: [ 3.68  2.36  2.85  1.36 10.25]\n",
      "Loss_Q: [1.96 2.23 1.26 0.   5.45] Loss_P: [ 3.74  2.31  2.86  1.36 10.27]\n",
      "Loss_Q: [2.02 2.2  1.25 0.   5.47] Loss_P: [ 3.75  2.29  2.82  1.37 10.23]\n",
      "Loss_Q: [1.97 2.25 1.25 0.   5.47] Loss_P: [ 3.76  2.31  2.8   1.39 10.26]\n",
      "Loss_Q: [1.97 2.18 1.3  0.   5.46] Loss_P: [ 3.76  2.2   2.81  1.39 10.17]\n",
      "Loss_Q: [2.02 2.2  1.28 0.   5.5 ] Loss_P: [ 3.8   2.19  2.84  1.41 10.24]\n",
      "Loss_Q: [2.05 2.24 1.27 0.   5.56] Loss_P: [ 3.76  2.33  2.84  1.39 10.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.05 2.25 1.25 0.   5.54] Loss_P: [ 3.73  2.22  2.82  1.37 10.14]\n",
      "Loss_Q: [1.99 2.26 1.26 0.   5.51] Loss_P: [ 3.74  2.27  2.87  1.36 10.24]\n",
      "Loss_Q: [1.98 2.24 1.26 0.   5.48] Loss_P: [ 3.76  2.29  2.83  1.32 10.2 ]\n",
      "Loss_Q: [2.01 2.19 1.26 0.   5.46] Loss_P: [ 3.75  2.28  2.79  1.37 10.19]\n",
      "Loss_Q: [2.01 2.18 1.23 0.   5.42] Loss_P: [ 3.69  2.36  2.85  1.36 10.26]\n",
      "Loss_Q: [1.99 2.25 1.24 0.   5.48] Loss_P: [ 3.77  2.22  2.86  1.36 10.21]\n",
      "Loss_Q: [2.02 2.25 1.25 0.   5.52] Loss_P: [ 3.75  2.3   2.84  1.35 10.24]\n",
      "Loss_Q: [2.05 2.2  1.25 0.   5.5 ] Loss_P: [ 3.85  2.25  2.86  1.36 10.32]\n",
      "Loss_Q: [2.04 2.2  1.21 0.   5.45] Loss_P: [ 3.8   2.24  2.85  1.33 10.21]\n",
      "Loss_Q: [2.01 2.21 1.24 0.   5.46] Loss_P: [ 3.79  2.28  2.84  1.33 10.24]\n",
      "Loss_Q: [2.07 2.21 1.27 0.   5.54] Loss_P: [ 3.77  2.32  2.83  1.34 10.25]\n",
      "Loss_Q: [1.98 2.26 1.27 0.   5.51] Loss_P: [ 3.79  2.21  2.82  1.37 10.19]\n",
      "Loss_Q: [2.01 2.24 1.2  0.   5.45] Loss_P: [ 3.74  2.21  2.83  1.34 10.11]\n",
      "Loss_Q: [1.94 2.17 1.26 0.   5.38] Loss_P: [ 3.75  2.22  2.85  1.37 10.18]\n",
      "Loss_Q: [1.99 2.21 1.25 0.   5.46] Loss_P: [ 3.78  2.18  2.81  1.35 10.12]\n",
      "Loss_Q: [2.05 2.18 1.22 0.   5.45] Loss_P: [ 3.84  2.21  2.84  1.32 10.21]\n",
      "Loss_Q: [2.03 2.23 1.24 0.   5.5 ] Loss_P: [ 3.78  2.21  2.81  1.36 10.15]\n",
      "Loss_Q: [1.96 2.23 1.21 0.   5.4 ] Loss_P: [ 3.8   2.27  2.86  1.34 10.27]\n",
      "Loss_Q: [1.9  2.24 1.22 0.   5.36] Loss_P: [ 3.8   2.28  2.79  1.33 10.2 ]\n",
      "Loss_Q: [1.96 2.21 1.22 0.   5.39] Loss_P: [ 3.72  2.2   2.83  1.32 10.07]\n",
      "Loss_Q: [1.87 2.24 1.2  0.   5.31] Loss_P: [ 3.76  2.14  2.84  1.31 10.06]\n",
      "Loss_Q: [1.96 2.23 1.21 0.   5.41] Loss_P: [ 3.77  2.17  2.84  1.34 10.12]\n",
      "Loss_Q: [2.   2.23 1.26 0.   5.48] Loss_P: [ 3.74  2.12  2.8   1.35 10.01]\n",
      "Loss_Q: [1.93 2.23 1.24 0.   5.4 ] Loss_P: [ 3.79  2.16  2.83  1.33 10.11]\n",
      "Loss_Q: [1.95 2.22 1.31 0.   5.47] Loss_P: [ 3.77  2.18  2.84  1.38 10.18]\n",
      "Loss_Q: [2.02 2.21 1.25 0.   5.48] Loss_P: [ 3.8   2.22  2.87  1.38 10.26]\n",
      "Loss_Q: [1.98 2.19 1.24 0.   5.41] Loss_P: [ 3.86  2.17  2.84  1.32 10.19]\n",
      "Loss_Q: [2.01 2.26 1.24 0.   5.5 ] Loss_P: [ 3.78  2.18  2.85  1.35 10.16]\n",
      "Loss_Q: [1.95 2.18 1.25 0.   5.39] Loss_P: [ 3.75  2.25  2.86  1.36 10.21]\n",
      "Loss_Q: [2.01 2.19 1.26 0.   5.46] Loss_P: [ 3.81  2.21  2.87  1.31 10.2 ]\n",
      "Loss_Q: [2.08 2.2  1.24 0.   5.53] Loss_P: [ 3.74  2.17  2.88  1.34 10.14]\n",
      "Loss_Q: [2.05 2.14 1.22 0.   5.41] Loss_P: [ 3.76  2.28  2.9   1.35 10.29]\n",
      "Loss_Q: [2.03 2.19 1.25 0.   5.48] Loss_P: [ 3.79  2.21  2.86  1.36 10.22]\n",
      "Loss_Q: [2.03 2.18 1.27 0.   5.49] Loss_P: [ 3.79  2.21  2.86  1.34 10.21]\n",
      "Loss_Q: [2.04 2.18 1.26 0.   5.47] Loss_P: [ 3.81  2.21  2.83  1.35 10.21]\n",
      "Loss_Q: [2.09 2.15 1.27 0.   5.52] Loss_P: [ 3.78  2.24  2.84  1.36 10.22]\n",
      "Loss_Q: [2.   2.21 1.29 0.   5.5 ] Loss_P: [ 3.8   2.18  2.82  1.37 10.17]\n",
      "Loss_Q: [2.06 2.21 1.29 0.   5.55] Loss_P: [ 3.79  2.22  2.86  1.36 10.24]\n",
      "Loss_Q: [2.   2.2  1.28 0.   5.48] Loss_P: [ 3.71  2.2   2.84  1.37 10.13]\n",
      "Loss_Q: [1.99 2.23 1.22 0.   5.44] Loss_P: [ 3.8   2.21  2.86  1.35 10.22]\n",
      "Loss_Q: [1.97 2.25 1.25 0.   5.46] Loss_P: [ 3.74  2.27  2.84  1.32 10.17]\n",
      "Loss_Q: [2.   2.21 1.21 0.   5.42] Loss_P: [ 3.8   2.12  2.82  1.34 10.08]\n",
      "Loss_Q: [1.99 2.23 1.22 0.   5.43] Loss_P: [ 3.81  2.19  2.83  1.34 10.17]\n",
      "Loss_Q: [2.02 2.23 1.23 0.   5.48] Loss_P: [ 3.79  2.21  2.81  1.32 10.13]\n",
      "Loss_Q: [1.89 2.17 1.17 0.   5.23] Loss_P: [ 3.84  2.18  2.77  1.33 10.13]\n",
      "Loss_Q: [1.97 2.16 1.22 0.   5.34] Loss_P: [ 3.83  2.17  2.8   1.3  10.1 ]\n",
      "Loss_Q: [2.05 2.2  1.25 0.   5.49] Loss_P: [ 3.79  2.15  2.76  1.31 10.01]\n",
      "Loss_Q: [1.96 2.11 1.23 0.   5.29] Loss_P: [ 3.77  2.16  2.79  1.32 10.05]\n",
      "Loss_Q: [1.84 2.17 1.24 0.   5.24] Loss_P: [3.72 2.21 2.73 1.33 9.99]\n",
      "Loss_Q: [2.   2.14 1.22 0.   5.37] Loss_P: [ 3.8   2.23  2.76  1.33 10.12]\n",
      "Loss_Q: [1.93 2.19 1.19 0.   5.31] Loss_P: [ 3.8   2.23  2.78  1.32 10.12]\n",
      "Loss_Q: [1.96 2.14 1.23 0.   5.33] Loss_P: [ 3.8   2.16  2.77  1.33 10.06]\n",
      "Loss_Q: [1.88 2.14 1.18 0.   5.2 ] Loss_P: [ 3.84  2.14  2.76  1.32 10.05]\n",
      "Loss_Q: [1.95 2.17 1.23 0.   5.35] Loss_P: [ 3.76  2.21  2.76  1.32 10.05]\n",
      "Loss_Q: [1.88 2.2  1.23 0.   5.31] Loss_P: [ 3.8   2.15  2.77  1.3  10.02]\n",
      "Loss_Q: [1.95 2.15 1.2  0.   5.31] Loss_P: [ 3.77  2.18  2.8   1.35 10.11]\n",
      "Loss_Q: [1.91 2.17 1.24 0.   5.32] Loss_P: [ 3.78  2.17  2.77  1.32 10.05]\n",
      "Loss_Q: [1.85 2.17 1.19 0.   5.21] Loss_P: [ 3.79  2.19  2.76  1.32 10.06]\n",
      "Loss_Q: [1.92 2.2  1.21 0.   5.33] Loss_P: [ 3.7   2.22  2.81  1.33 10.07]\n",
      "Loss_Q: [1.94 2.2  1.19 0.   5.34] Loss_P: [ 3.83  2.18  2.81  1.33 10.15]\n",
      "Loss_Q: [1.9  2.15 1.26 0.   5.31] Loss_P: [ 3.68  2.25  2.76  1.38 10.07]\n",
      "Loss_Q: [2.02 2.2  1.21 0.   5.43] Loss_P: [ 3.78  2.27  2.84  1.3  10.18]\n",
      "Loss_Q: [1.94 2.15 1.19 0.   5.29] Loss_P: [ 3.72  2.25  2.77  1.32 10.07]\n",
      "Loss_Q: [1.97 2.15 1.23 0.   5.36] Loss_P: [ 3.78  2.21  2.74  1.32 10.06]\n",
      "Loss_Q: [1.97 2.19 1.23 0.   5.39] Loss_P: [ 3.75  2.29  2.74  1.32 10.1 ]\n",
      "Loss_Q: [2.02 2.15 1.22 0.   5.38] Loss_P: [ 3.72  2.26  2.74  1.33 10.06]\n",
      "Loss_Q: [2.04 2.12 1.25 0.   5.41] Loss_P: [ 3.72  2.26  2.73  1.32 10.03]\n",
      "Loss_Q: [1.98 2.15 1.17 0.   5.3 ] Loss_P: [ 3.64  2.33  2.8   1.3  10.07]\n",
      "Loss_Q: [1.94 2.14 1.2  0.   5.27] Loss_P: [ 3.75  2.2   2.77  1.31 10.03]\n",
      "Loss_Q: [1.95 2.11 1.2  0.   5.26] Loss_P: [ 3.75  2.22  2.75  1.33 10.05]\n",
      "Loss_Q: [2.08 2.07 1.19 0.   5.34] Loss_P: [ 3.79  2.34  2.79  1.29 10.22]\n",
      "Loss_Q: [1.96 2.15 1.23 0.   5.34] Loss_P: [ 3.77  2.2   2.75  1.3  10.02]\n",
      "Loss_Q: [1.95 2.14 1.19 0.   5.28] Loss_P: [ 3.75  2.2   2.83  1.29 10.06]\n",
      "Loss_Q: [2.   2.14 1.13 0.   5.27] Loss_P: [3.66 2.22 2.78 1.29 9.95]\n",
      "Loss_Q: [1.95 2.2  1.18 0.   5.34] Loss_P: [ 3.7   2.29  2.8   1.3  10.08]\n",
      "Loss_Q: [1.95 2.12 1.2  0.   5.27] Loss_P: [ 3.73  2.22  2.79  1.26 10.  ]\n",
      "Loss_Q: [2.   2.13 1.15 0.   5.28] Loss_P: [ 3.7   2.32  2.77  1.26 10.06]\n",
      "Loss_Q: [2.01 2.07 1.14 0.   5.23] Loss_P: [3.77 2.2  2.77 1.24 9.98]\n",
      "Loss_Q: [1.99 2.14 1.14 0.   5.27] Loss_P: [3.67 2.3  2.75 1.24 9.96]\n",
      "Loss_Q: [2.   2.15 1.09 0.   5.24] Loss_P: [3.75 2.23 2.77 1.19 9.95]\n",
      "Loss_Q: [1.98 2.11 1.11 0.   5.2 ] Loss_P: [3.7  2.31 2.73 1.21 9.96]\n",
      "Loss_Q: [2.02 2.15 1.09 0.   5.26] Loss_P: [ 3.76  2.26  2.77  1.21 10.  ]\n",
      "Loss_Q: [2.1  2.13 1.13 0.   5.35] Loss_P: [ 3.77  2.28  2.77  1.19 10.02]\n",
      "Loss_Q: [1.96 2.15 1.1  0.   5.21] Loss_P: [3.73 2.22 2.79 1.19 9.93]\n",
      "Loss_Q: [1.97 2.14 1.11 0.   5.22] Loss_P: [3.75 2.28 2.79 1.15 9.97]\n",
      "Loss_Q: [2.03 2.22 1.11 0.   5.36] Loss_P: [ 3.82  2.21  2.79  1.21 10.03]\n"
     ]
    }
   ],
   "source": [
    "### Pre-training without constraints, accuracy 0.33, not ideal\n",
    "\n",
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ea16fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d25ba1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M_01': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 'M_12': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 'M_23': array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = {}\n",
    "for i in range(n_layer-2):\n",
    "    M[\"M_\" + str(i) + str(i+1)] = np.zeros((2**n_dz[0,i],2**n_dz[0,i+1]),dtype = int)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d93a75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 1, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c6d23830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "index = np.random.permutation(n_data)\n",
    "d0 = dataset[:,index[i]:index[i]+1]\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c6a7c593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0'],\n",
       "       ['0'],\n",
       "       ['0'],\n",
       "       ['0'],\n",
       "       ['0'],\n",
       "       ['1'],\n",
       "       ['1'],\n",
       "       ['1'],\n",
       "       ['1'],\n",
       "       ['0']], dtype='<U11')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb550636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1010001110'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary = ''.join(d0.reshape(10,).astype(str))\n",
    "binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cf5468c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(binary,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "518443da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 1021, 1022, 1023])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_set = np.unique(entire_set.astype(int),axis=1)\n",
    "decimal = np.zeros(entire_set.shape[1],dtype=int)\n",
    "for i in range(entire_set.shape[1]):\n",
    "    binary = ''.join(entire_set[:,i].reshape(10,).astype(str))\n",
    "    decimal[i] = int(binary,2)\n",
    "decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81295eea",
   "metadata": {},
   "source": [
    "After Pre-training, we continue with the uncentralized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb835d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "432bace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = Phi[\"Phi_01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b4f2655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17745421],\n",
       "       [0.55428628],\n",
       "       [0.01276379],\n",
       "       [0.35247297],\n",
       "       [0.676383  ],\n",
       "       [0.37199548],\n",
       "       [0.7145599 ],\n",
       "       [0.72375744]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = sigmoid(np.matmul(phi[:,:-1],d0))\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13fa7b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = (q+0.5).astype(int)\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cf2ab954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0_decimal = int(''.join(d0.reshape(10,).astype(str)),2)\n",
    "d0_decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d6129649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimalize(input_D):\n",
    "    \"\"\"\n",
    "    Broadcast\n",
    "    \n",
    "    Arguments:\n",
    "    D -- numpy array of binary values {0,1}, of shape (n_d, 1)\n",
    "    \n",
    "    Returns:\n",
    "    out_d -- a decimalized intger in range [0,2**n_d)\n",
    "    \"\"\"\n",
    "    n_d = input_D.size\n",
    "    binary = ''.join(input_D.reshape(n_d,).astype(str))\n",
    "    out_d = int(binary,2)\n",
    "    return out_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d81d4910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decimalize(d0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3cf9c7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(np.binary_repr(3, width=4))).astype(int).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1cbf9876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 256)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M['M_01'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2117505b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32254579],\n",
       "       [0.05428628],\n",
       "       [0.48723621],\n",
       "       [0.14752703],\n",
       "       [0.176383  ],\n",
       "       [0.12800452],\n",
       "       [0.2145599 ],\n",
       "       [0.22375744]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(q - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3ce104c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82254579],\n",
       "       [0.55428628],\n",
       "       [0.98723621],\n",
       "       [0.64752703],\n",
       "       [0.676383  ],\n",
       "       [0.62800452],\n",
       "       [0.7145599 ],\n",
       "       [0.72375744]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(q - 0.5) + 0.5  # probability of deterministic sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8dd730ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.53369154],\n",
       "       [0.21800445],\n",
       "       [4.34829705],\n",
       "       [0.60818661],\n",
       "       [0.73719878],\n",
       "       [0.52366568],\n",
       "       [0.91763462],\n",
       "       [0.963177  ]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vary = np.log((0.5 + np.abs(q - 0.5))/(0.5 - np.abs(q - 0.5))) # change value\n",
    "vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "550ec9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21800445],\n",
       "       [0.52366568],\n",
       "       [0.60818661],\n",
       "       [0.73719878],\n",
       "       [0.91763462],\n",
       "       [0.963177  ],\n",
       "       [1.53369154],\n",
       "       [4.34829705]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vary = np.sort(vary,axis=0)\n",
    "vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7c24e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_d = vary.shape[0]\n",
    "n_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b3deb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarilize(d,width):\n",
    "    \"\"\"\n",
    "    Broadcast\n",
    "    \n",
    "    Arguments:\n",
    "    d -- numpy array of decimal integers of shape (n_s, ), with each entry as a decimalized intger in range [0,2**width)\n",
    "    \n",
    "    Returns:\n",
    "    out_D -- numpy array of shape (width, n_s), with each column as a single output of binary representation\n",
    "    \"\"\"\n",
    "    n_s = d.size\n",
    "    out_D = np.zeros((width,n_s),dtype=int)\n",
    "    for i in range(n_s):\n",
    "        out_D[:,i] = np.array(list(np.binary_repr(d[i], width=width))).astype(int)\n",
    "    return out_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e7e2f18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarilize(np.arange(1,2**4),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7ffb6613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  3,  7,  2,  4,  8,  5,  9, 11,  6, 10, 12, 13, 14],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(np.sum(vary[:4][::-1]*binarilize(np.arange(1,2**4),4),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78b49c",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8f5fda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = {}\n",
    "for i in range(n_layer-2):\n",
    "    M[\"M_\" + str(i) + str(i+1)] = np.zeros((2**n_dz[0,i],2**n_dz[0,i+1]),dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "464427d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 1, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fbdb5679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 8\n",
    "d0 = dataset[:,index[i]:index[i]+1]\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "1080c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(Input):\n",
    "    \"\"\"\n",
    "    Find y for given x that increases the layer-wise accumulative mutual information\n",
    "    Iterative formula for entropy:\n",
    "    H_+1 = (s(H-log(s)) + (s+1)log(s+1) + [alog(a) - (a+1)log(a+1)])/(s+1)\n",
    "    \n",
    "    Arguments:\n",
    "    Input -- numpy array of shape (2,m+2), with each row comprises \n",
    "    [counts for m categories separately, sum of counts, entropy of this row]\n",
    "    first row: H(Y|x)    second row: H(Y) [summation over x]\n",
    "    \n",
    "    Returns:\n",
    "    MI_index -- list of indices of Y where the mutual information increases when this category counts +1, numpy array of shape (k, )\n",
    "    I_index -- mutual information of each choice, numpy array of shape (k, )\n",
    "    H_new -- numpy array of shape (2,m), first row: H(Y|x), second row: H(Y)\n",
    "    \"\"\"\n",
    "    s = Input[:,-2:-1]   #(2,1)\n",
    "    H = Input[:,-1:]     #(2,1)\n",
    "    C = s*(H - np.log(s)) + (s+1)*np.log(s+1)   #(2,1)\n",
    "    \n",
    "    M = Input[:,:-2]     #(2,m)\n",
    "    D = M*np.log(M) - (M+1)*np.log(M+1)     #(2,m)\n",
    "    H_new = (C + D)/(s+1)      #(2,m)\n",
    "    \n",
    "    I = H_new[1,:] - H_new[0,:]\n",
    "    I_diff = I - (H[1,:] - H[0,:])  #(m, )\n",
    "    MI_index = np.where(I_diff > 0)[0]\n",
    "    MI = I[MI_index]\n",
    "    \n",
    "    return MI_index, MI, H_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "fe942b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(H_prev,s,a):\n",
    "    \"\"\"\n",
    "    Iterative formula for entropy:\n",
    "    H_+1 = (s(H-log(s)) + (s+1)log(s+1) + [alog(a) - (a+1)log(a+1)])/(s+1)\n",
    "    \n",
    "    Arguments:\n",
    "    H_prev -- previous entropy\n",
    "    s -- total counts\n",
    "    a -- counts for the category being modified\n",
    "     \n",
    "    Returns:\n",
    "    H -- updated entropy\n",
    "    \"\"\"\n",
    "    H = (s*(H_prev-np.log(s)) + (s+1)*np.log(s+1) + [a*np.log(a) - (a+1)*np.log(a+1)])/(s+1)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "0d8d78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_samples(q,precision):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    q -- sigmoid output of a given layer\n",
    "    precision -- a decimal number in [0,0.5)\n",
    "    \n",
    "    Returns:\n",
    "    p_index -- numpy array of shape (2**num, ), categorical number of possible y's\n",
    "    probability -- numpy array of shape (2**num, ), probability of each category\n",
    "    num -- max number of neurons being changed\n",
    "    \"\"\"\n",
    "    \n",
    "    var_index = np.where(np.abs(q.reshape(-1,) - 0.5) < precision)[0]\n",
    "    num = len(var_index)\n",
    "    comb = binarilize(np.arange(2**num),num)   #(num, 2**num)\n",
    "    prob = q[var_index]\n",
    "    probability = np.prod(prob**comb * (1-prob)**(1-comb),axis=0)  #(2**num, )\n",
    "    \n",
    "    z = (q+0.5).astype(int)\n",
    "    z_all = np.repeat(z, 2**num, axis=1)\n",
    "    z_all[var_index,:] = comb\n",
    "    \n",
    "    p_index = np.zeros(2**num,dtype=int)\n",
    "    for i in range(2**num):\n",
    "        p_index[i] = decimalize(z_all[:,i])\n",
    "        \n",
    "    return p_index,probability,num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_y(MI_index, MI, p_index,probability,M,cat_x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    MI_index -- list of indices of Y where the mutual information increases when this category counts +1, numpy array of shape (k, )\n",
    "    MI -- mutual information of each choice, numpy array of shape (k, )\n",
    "    p_index -- numpy array of shape (2**num, ), categorical number of possible y's\n",
    "    probability -- numpy array of shape (2**num, ), probability of each category\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+2), last row: summation; -2 column: summation; last column: entropy\n",
    "    cat_x -- categorical number of given x\n",
    "    \n",
    "    Returns:\n",
    "    M -- updated count matrix\n",
    "    \"\"\"\n",
    "    intersect, p_i, MI_i = np.intersect1d(p_index, MI_index,return_indices=True)\n",
    "    if intersect.size > 0:\n",
    "        ind = np.argsort(probability[p_i] * I_index[MI_i])[::-1][0]  # the best y's index\n",
    "        cat_y = p_index[p_i[ind]]    # the best y's category\n",
    "    else:\n",
    "        if MI_index.size == 0:\n",
    "            cat_y = p_index[np.argsort(probability)[::-1][0]]\n",
    "        else:\n",
    "            cat_y = MI_index[np.argsort(MI)[::-1][0]]\n",
    "        \n",
    "    # update count matrix\n",
    "    M[cat_x,-1] = entropy(M[cat_x,-1],M[cat_x,-2],M[cat_x,cat_y])\n",
    "    M[-1,-1] = entropy(M[-1,-1],M[-1,-2],M[-1,cat_y])\n",
    "    M[cat_x,cat_y] += 1\n",
    "    M[-1,cat_y] += 1\n",
    "    M[cat_x,-2] += 1\n",
    "    M[-1,-2] += 1    \n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "id": "7aa9988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_M(M):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+2), last row: summation; -2 column: summation; last column: entropy\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    flag = True\n",
    "    epsilon = 1e-8\n",
    "    if np.all(np.abs(M[-1,:-1] - np.sum(M[:-1,:-1],axis=0))) > epsilon:\n",
    "        flag = False\n",
    "        raise Exception(\"row addition wrong\")\n",
    "    elif np.all(np.abs(M[:,-2] - np.sum(M[:,:-2],axis=1))) > epsilon:\n",
    "        flag = False\n",
    "        raise Exception(\"column addition wrong\")\n",
    "    elif np.all(np.abs(M[:,-1] + np.sum(M[:,:-2]*np.log(M[:,:-2]),axis=1)/M[:,-2]-np.log(M[:,-2]))) > epsilon:\n",
    "        flag = False\n",
    "        raise Exception(\"entropy computation wrong\")\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "52a23999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_M(n_x,n_y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_x -- number of neurons in input layer\n",
    "    n_y -- number of neurons in output layer\n",
    "    \n",
    "    Returns:\n",
    "    M -- count matrix, numpy array of shape (2**n_x+1, 2**n_y+2), last row: summation; -2 column: summation; last column: entropy\n",
    "    \"\"\"\n",
    "    M = np.zeros((2**n_x+1,2**n_y+2))\n",
    "    epsilon = 1e-10\n",
    "    M[0:2**n_x,0:2**n_y] += epsilon\n",
    "    M[-1,:-2] = np.sum(M[:-1,:-2],axis=0)\n",
    "    M[:,-2] = np.sum(M[:,:-2],axis=1)\n",
    "    M[:,-1] += np.log(2**n_y)\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "66e2497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = {}\n",
    "for i in range(n_layer-2):\n",
    "    M[\"M_\" + str(i) + str(i+1)] = init_M(n_dz[0,i],n_dz[0,i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "fae618b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_M(M['M_01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115db301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI_sample(n_dz,d0,value_set,Phi,activation_type,bias,M):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "    n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "    d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "    value_set -- list or array [a,b], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "    shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    bias -- list or array [instantiation bias, MLP bias], taking binary value in {True, False}. For example, [False,True] means \n",
    "    no instantiation bias but has MLP bias\n",
    "    M -- count matrix of each instantiation layer pair. Python dictionary of length m-2 with each value as a count matrix of \n",
    "    shape (2**n_zi, 2**n_z{i+1})\n",
    "    \n",
    "    Returns:\n",
    "    Alpha_MI -- assignment of each neuron (binary value), Python dictionary of length m-1 with each key-value pair being \n",
    "    a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "    \"\"\"\n",
    "    \n",
    "    n_layer = n_dz.shape[1]\n",
    "    S = d0  # assignment of each layer\n",
    "    Alpha_Q = {\"z0\":d0}\n",
    "    inst_bias = bias[0]\n",
    "    mlp_bias = bias[1]\n",
    "    a = value_set[0]\n",
    "    b = value_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "e8ee6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_Helmholtz:\n",
    "\n",
    "    def __init__(self, n_dz, bias, activation_type):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "        n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "        d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "        bias -- list or array [instantiation bias, MLP bias,data bias], taking binary value in {True, False}. For example, \n",
    "        [False,True,True] means no instantiation bias but has MLP bias and data bias\n",
    "        activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "        \"\"\"\n",
    "        self.n_dz = n_dz\n",
    "        self.n_fill = self.n_dz.shape[0]\n",
    "        self.n_layer = self.n_dz.shape[1]\n",
    "        \n",
    "        self.inst_bias = bias[0]\n",
    "        self.mlp_bias = bias[1]\n",
    "        self.data_bias = bias[2]\n",
    "        \n",
    "        self.ac = activation_type\n",
    "        \n",
    "    def parameter_init(self,init_type,value_set):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "        value_set -- numpy array [[a_inst,b_inst],[a_mlp,b_mlp],[a_data,b_data]], For the hidden layer, its activation is scaled as \n",
    "        $a_mlp*g+b_mlp$; for the inst layer, {a_inst,b_inst} is its binary outcomes as {positive, negative}\n",
    "\n",
    "        Returns:\n",
    "        Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "        shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "        Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "        shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "        Eg. {Phi_01:{Phi_01,Phi_12}, Phi_12:{Phi_01,Phi_12,Phi_23}}, dictionary of dictionary\n",
    "        \n",
    "        Scalar -- scaling factors for each layer, numpy array of shape (nm,2). nm is total number of inst and hidden layers, 2 \n",
    "        represents a and b. For the hidden layer, its activation is scaled as $a*g+b$; for the inst layer, {a,b} is its \n",
    "        binary outcomes as {positive, negative}\n",
    "        \"\"\"\n",
    "        Phi = {}\n",
    "        Theta = {}\n",
    "        Scalar_wake = {}\n",
    "        Scalar_sleep = {\"Sc_10\":value_set[2:,:]}\n",
    "        \n",
    "        for i in range(self.n_layer-1):\n",
    "            l = np.where(self.n_dz[1:,i] != 0)[0].size  # number of inserted layers between i and i+1\n",
    "            Scalar_wake[\"Sc_\"+str(i)+str(i+1)] = np.repeat(value_set[:2,:], [1, l], axis=0)\n",
    "            Scalar_sleep[\"Sc_\"+str(i+2)+str(i+1)] = np.repeat(value_set[:2,:], [1, l], axis=0)\n",
    "            Phi[\"Phi_\"+str(i)+str(i+1)],Theta[\"Theta_\"+str(i+1)+str(i)] = one_step_para_init(self.n_dz[:,i:i+2],self.mlp_bias,self.inst_bias,init_type)\n",
    "        if self.data_bias != self.inst_bias:\n",
    "            theta = Theta[\"Theta_10\"][[*Theta[\"Theta_10\"]][-1]]\n",
    "            if self.data_bias == True:\n",
    "                if init_type == \"zero\":\n",
    "                    Theta[\"Theta_10\"][[*Theta[\"Theta_10\"]][-1]] = np.append(theta,np.zeros((theta.shape[0],1)),axis=1)\n",
    "                elif init_type == \"random\":\n",
    "                    Theta[\"Theta_10\"][[*Theta[\"Theta_10\"]][-1]] = np.append(theta,np.random.randn(theta.shape[0],1),axis=1)\n",
    "            else:\n",
    "                Theta[\"Theta_10\"][[*Theta[\"Theta_10\"]][-1]] = np.delete(theta,-1,1)\n",
    "        self.Phi = Phi\n",
    "        self.Theta = Theta\n",
    "        self.Scalar_wake = Scalar_wake\n",
    "        self.Scalar_sleep = Scalar_sleep\n",
    "    \n",
    "    def wake_sample(self,d0):\n",
    "        \"\"\"\n",
    "        Stochastic sample in wake phase\n",
    "        \n",
    "        Arguments:\n",
    "        d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "\n",
    "        Returns:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "        \"\"\"\n",
    "        S = d0  # assignment of each layer\n",
    "        Alpha_Q = {\"z0\":d0}\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(S,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Scalar_wake[\"Sc_\"+str(i)+str(i+1)],self.mlp_bias,self.inst_bias,self.ac)\n",
    "            a = self.Scalar_wake[\"Sc_\"+str(i+1)+str(i+2)][0,0]\n",
    "            b = self.Scalar_wake[\"Sc_\"+str(i+1)+str(i+2)][0,1]\n",
    "            S = ((q > np.random.rand(len(q),1)).astype(int))*(a-b)+b\n",
    "            Alpha_Q[\"z\"+str(i+1)] = S\n",
    "        Alpha_Q[\"z\"+str(self.n_layer-1)] = [[1]]\n",
    "        return Alpha_Q\n",
    "    \n",
    "    def sleep_sample(self):\n",
    "        \"\"\"\n",
    "        Stochastic sample in sleep phase\n",
    "\n",
    "        Returns:\n",
    "        Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "        \"\"\"\n",
    "        S = [[1]]\n",
    "        Alpha_P = {\"z\"+str(self.n_layer-1):S}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            if i > 1:\n",
    "                out_bias = self.inst_bias\n",
    "            else:\n",
    "                out_bias = self.data_bias\n",
    "            G,p = one_step_forward(S,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Scalar_sleep[\"Sc_\"+str(i+1)+str(i)],self.mlp_bias,out_bias,self.ac)\n",
    "            a = self.Scalar_sleep[\"Sc_\"+str(i)+str(i-1)][0,0]\n",
    "            b = self.Scalar_sleep[\"Sc_\"+str(i)+str(i-1)][0,1]\n",
    "            S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "            Alpha_P[\"z\"+str(i-1)] = S\n",
    "        return Alpha_P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8b037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "83aa092e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "37e2bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_para_init(n_dz_slice,in_bias,out_bias,init_type):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz_slice -- 2 columns of n_dz\n",
    "    in_bias -- bias for hidden layer, True or False\n",
    "    out_bias -- bias for output layer, True or False\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "\n",
    "    Returns:\n",
    "    Phi, Theta\n",
    "    \"\"\"\n",
    "    l = np.where(n_dz_slice[:,0] != 0)[0].size  # number of layers\n",
    "    layer_vt = np.append(n_dz_slice[:l,0],n_dz_slice[0,1])\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    if init_type == \"zero\":\n",
    "        for i in range(l-1):\n",
    "            if in_bias == True:\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((layer_vt[i+1],layer_vt[i]+1))\n",
    "                Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.zeros((layer_vt[l-i-1],layer_vt[l-i]+1))\n",
    "            else:\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((layer_vt[i+1],layer_vt[i]))\n",
    "                Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.zeros((layer_vt[l-i-1],layer_vt[l-i]))\n",
    "        i = l-1\n",
    "        if out_bias == True:\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((layer_vt[i+1],layer_vt[i]+1))\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.zeros((layer_vt[l-i-1],layer_vt[l-i]+1))\n",
    "        else:\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((layer_vt[i+1],layer_vt[i]))\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.zeros((layer_vt[l-i-1],layer_vt[l-i]))\n",
    "    elif init_type == \"random\":\n",
    "        for i in range(l-1):\n",
    "            if in_bias == True:\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(layer_vt[i+1],layer_vt[i]+1)\n",
    "                Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.random.randn(layer_vt[l-i-1],layer_vt[l-i]+1)\n",
    "            else:\n",
    "                Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(layer_vt[i+1],layer_vt[i])\n",
    "                Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.random.randn(layer_vt[l-i-1],layer_vt[l-i])\n",
    "        i = l-1\n",
    "        if out_bias == True:\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(layer_vt[i+1],layer_vt[i]+1)\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.random.randn(layer_vt[l-i-1],layer_vt[l-i]+1)\n",
    "        else:\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(layer_vt[i+1],layer_vt[i])\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.random.randn(layer_vt[l-i-1],layer_vt[l-i])\n",
    "    else:\n",
    "        raise Exception(\"Wrong Init Type\")\n",
    "    return Phi,Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "e65f6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_forward(x,parameter_set,scalar_set,in_bias,out_bias,activation_type):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input instantiation layer, numpy array of shape (n,1)\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l. The keys are ordered sequentially from layer x to y.\n",
    "    scalar_set -- numpy array of shape (l+1,2)\n",
    "    in_bias -- bias for hidden layer, True or False\n",
    "    out_bias -- bias for output layer, True or False\n",
    "\n",
    "    Returns:\n",
    "    G -- activation of each layer including x\n",
    "    q -- probability of layer y\n",
    "    \"\"\"\n",
    "    l = len(parameter_set)\n",
    "    keys = [*parameter_set]\n",
    "    G = {'z0': x}\n",
    "    g = x\n",
    "\n",
    "    for i in range(l-1):\n",
    "        phi = parameter_set[keys[i]]\n",
    "        a = scalar_set[i+1,0]\n",
    "        b = scalar_set[i+1,1]\n",
    "        if activation_type == \"sigmoid\":\n",
    "            if in_bias == True:\n",
    "                g = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))*a+b  # scale by {a,b}\n",
    "            else:\n",
    "                g = sigmoid(np.matmul(phi,g))*a+b\n",
    "        elif activation_type == \"tanh\":\n",
    "            if in_bias == True:\n",
    "                g = np.tanh(np.matmul(phi,np.append(g,[[1]], axis=0)))*a+b  # scale by {a,b}\n",
    "            else:\n",
    "                g = np.tanh(np.matmul(phi,g))*a+b\n",
    "        G['z'+str(i+1)] = g\n",
    "\n",
    "    phi = parameter_set[keys[l-1]]\n",
    "    if out_bias == True:\n",
    "        q = sigmoid(np.matmul(phi,np.append(g,[[1]], axis=0)))\n",
    "    else:\n",
    "        q = sigmoid(np.matmul(phi,g))\n",
    "    G['z'+str(l)] = q\n",
    "    return G,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a579b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22659a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8788e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d0269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e344eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2243,
   "id": "354f99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_para_init(n_dz_slice,init_type):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_dz_slice -- 2 columns of n_dz\n",
    "    init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "\n",
    "    Returns:\n",
    "    Phi, Theta -- -2 column bias, -1 column scale\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row is counts, column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    \"\"\"\n",
    "    l = np.where(n_dz_slice[:,0] != 0)[0].size  # number of layers\n",
    "    layer_vt = np.append(n_dz_slice[:l,0],n_dz_slice[0,1])\n",
    "    Phi = {}\n",
    "    Theta = {}\n",
    "    norm_wake = {}\n",
    "    norm_sleep = {}\n",
    "    for i in range(l):\n",
    "        if init_type == \"zero\":\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.zeros((layer_vt[i+1],layer_vt[i]+2))\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.zeros((layer_vt[l-i-1],layer_vt[l-i]+2))\n",
    "        elif init_type == \"random\":\n",
    "            Phi[\"Phi_\" + str(i) + str(i+1)] = np.random.randn(layer_vt[i+1],layer_vt[i]+2)\n",
    "            Theta[\"Theta_\" + str(l-i) + str(l-i-1)] = np.random.randn(layer_vt[l-i-1],layer_vt[l-i]+2)\n",
    "        else:\n",
    "            raise Exception(\"Wrong Init Type\")\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)][:,-1] = 1          #scale\n",
    "        Theta[\"Theta_\" + str(l-i) + str(l-i-1)][:,-1] = 1\n",
    "        Phi[\"Phi_\" + str(i) + str(i+1)][:,-2] = 0              #bias\n",
    "        Theta[\"Theta_\" + str(l-i) + str(l-i-1)][:,-2] = 0\n",
    "        \n",
    "        norm_wake[\"l_\" + str(i+1)] = np.zeros((layer_vt[i+1]+1,3))\n",
    "        norm_sleep[\"l_\" + str(l-i-1)] = np.zeros((layer_vt[l-i-1]+1,3))\n",
    "        \n",
    "    return Phi,Theta,norm_wake,norm_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2244,
   "id": "5c3db55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2245,
   "id": "fdd53f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_forward(x,parameter_set,activation_type,norm_set,add_norm):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input instantiation layer, numpy array of shape (n,1)\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l. The keys are ordered sequentially from layer x to y.\n",
    "    scalar_set -- numpy array of shape (l+1,2)\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    add_norm -- True or False. Update norm matrix or not\n",
    "    \n",
    "    Returns:\n",
    "    G -- activation of each layer including x\n",
    "    q -- probability of layer y\n",
    "    \"\"\"\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    G = {'z0': x}\n",
    "    g = x\n",
    "    \n",
    "    for i in range(l):\n",
    "        phi = parameter_set[p_keys[i]]\n",
    "        norm = norm_set[n_keys[i]]\n",
    "        \n",
    "        pre_ac = np.matmul(phi[:,:-2],g)+norm[:-1,2:]  # pre-activation; bias placeholder\n",
    "        K = phi[:,-1:] * pre_ac + phi[:,-2:-1]  # rescale linear term\n",
    "        if activation_type == \"sigmoid\":\n",
    "            g = sigmoid(K)\n",
    "        elif activation_type == \"tanh\":\n",
    "            g = np.tanh(K)\n",
    "        if i == l-1:\n",
    "            g = sigmoid(K)\n",
    "        G['z'+str(i+1)] = g\n",
    "        \n",
    "        if add_norm == True:\n",
    "            norm[:-1,0:1] += pre_ac\n",
    "            norm[-1,0] += 1\n",
    "            norm[:-1,1:2] += pre_ac**2\n",
    "            norm[-1,1] += 1\n",
    "            norm_set[n_keys[i]] = norm\n",
    "    return G,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2424,
   "id": "cc8bf28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_update(x,y,parameter_set,G,value_set,lr,activation_type,norm_set,fixed_value,fixed_scale,layer_norm,check_lr):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input instantiation layer, numpy array of shape (n,1)\n",
    "    y -- target instantiation layer, numpy array of shape (m,1)\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l+1, l is the number of inserted layers. \n",
    "    The keys are ordered sequentially from layer x to y.Each array phi has: -2 column bias, -1 column scale\n",
    "    lr -- learning rate, decimals\n",
    "    value_set -- list or array [[a_y,b_y],[a_x,b_x]], where a is the positive outcome and b is the negative outcome of a Bernoulli experiment\n",
    "    activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    fixed_value -- True or Flase. If True, [a_x,b_x] are fixed values; if False, update [a_x,b_x]\n",
    "    fixed_scale -- True or Flase. If True, last 2 columns of phi are fixed; if False, update adaptive scale and bias\n",
    "    layer_norm -- True or Flase, whether to adjust row distribution of W (more evenly distributed) by adjusting the updating rate of dW\n",
    "    G -- output of one_step_forward_norm, activation of each layer including x\n",
    "    \n",
    "    Returns:\n",
    "    parameter_set -- updated parameters\n",
    "    loss -- value of loss function before updating, a number\n",
    "    grad_set -- gradients of parameters\n",
    "    lr -- updated learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    a_x = value_set[0,0]\n",
    "    b_x = value_set[0,1]\n",
    "    a_y = value_set[1,0]\n",
    "    b_y = value_set[1,1]\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    grad_set = {}\n",
    "    \n",
    "    # derivatives\n",
    "    q = G['z'+str(l)]\n",
    "    u = q - (y-b_y)/(a_y-b_y)   #[n_y,1]\n",
    "    loss = -np.sum(((y-b_y)*np.log(q) + (a_y-y)*np.log(1-q))/(a_y-b_y))  # layer entropy loss\n",
    "    # a = g(scale * [(Wz+b-mean)/deviation] + bias)\n",
    "    for i in range(l-1,-1,-1):\n",
    "        phi = parameter_set[p_keys[i]]\n",
    "        grad_set['d_'+p_keys[i]] = np.zeros(phi.shape)\n",
    "        W = phi[:,:-2]        #[n_y,n_z]\n",
    "        bias = phi[:,-2:-1]   #[n_y,1]\n",
    "        scale = phi[:,-1:]    #[n_y,1]\n",
    "        \n",
    "        z = G['z'+str(i)]      #[n_z,1]\n",
    "        dW = np.outer(u*scale,z)     #[n_y,n_z]\n",
    "        grad_set['d_'+p_keys[i]][:,:-2] = dW\n",
    "        \n",
    "        # update weights\n",
    "        if layer_norm == True:\n",
    "            layer_lr = layer_lr(W, dW, rate=2)\n",
    "            dW = dW * layer_lr\n",
    "        parameter_set[p_keys[i]][:,:-2] -= lr * dW\n",
    "        \n",
    "        if fixed_scale == False:\n",
    "            norm = norm_set[n_keys[i]]      #[n_y,3]\n",
    "            b = norm[:-1,2:]                #[n_y,1]\n",
    "            N = np.matmul(W,z)+b   #[n_y,1]\n",
    "            d_scale = u * N        #[n_y,1]\n",
    "            d_bias = u             #[n_y,1]\n",
    "            parameter_set[p_keys[i]][:,-1:] -= lr * d_scale\n",
    "            parameter_set[p_keys[i]][:,-2:-1] -= lr * d_bias\n",
    "            grad_set['d_'+p_keys[i]][:,-1:] = d_scale\n",
    "            grad_set['d_'+p_keys[i]][:,-2:-1] = d_bias\n",
    "        \n",
    "        dz = np.matmul(W.T,u*scale)  #[n_z,1]\n",
    "        if i > 0:\n",
    "            if activation_type == \"sigmoid\":\n",
    "                u = dz * z * (1-z)\n",
    "            elif activation_type == \"tanh\":\n",
    "                u = dz * (1-z**2)\n",
    "        else:\n",
    "            # input layer, parameters a_x, b_x\n",
    "            if fixed_value == False:\n",
    "                d_ax = np.mean(dz[np.where(np.abs(x - a_x) < 1e-5)[0]]) # use mean instead of sum\n",
    "                d_bx = np.mean(dz[np.where(np.abs(x - b_x) < 1e-5)[0]])\n",
    "                if np.where(np.abs(x - a_x) < 1e-8)[0].size + np.where(np.abs(x - b_x) < 1e-8)[0].size != dz.size:\n",
    "                    raise Exception(\"Incorrect input layer\" + str(a_x)+ \" \"+str(b_x)+ \" \"+str(x))\n",
    "                a_x -= lr * d_ax\n",
    "                b_x -= lr * d_bx\n",
    "    x_values = np.array([a_x,b_x])\n",
    "    \n",
    "    if check_lr == True:\n",
    "        G,q = one_step_forward(x,parameter_set,activation_type,norm_set,add_norm=False)\n",
    "        loss_new = -np.sum(((y-b_y)*np.log(q) + (a_y-y)*np.log(1-q))/(a_y-b_y))  # layer entropy loss\n",
    "        delta_loss = loss - loss_new\n",
    "        delta_f = 0\n",
    "        for keys in grad_set:\n",
    "            delta_f += np.sum(grad_set[keys]**2)*lr\n",
    "        print(\"delta_loss: \"+ str(delta_loss), \"delta_f: \"+ str(delta_f))\n",
    "        if delta_loss < delta_f/2:\n",
    "            lr = lr/2\n",
    "            print(\"learning rate change: \" + str(lr))\n",
    "    \n",
    "    return x_values,loss,grad_set,lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2425,
   "id": "2bf3f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_norm_update(parameter_set,norm_set):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameter_set -- parameters from x to y. Python dictionary of length l. The keys are ordered sequentially from layer x to y.\n",
    "    Each array phi has: -2 column bias, -1 column scale\n",
    "    norm_set -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "    a numpy array of shape (n_neuron+1,2), last row: counts; column 1: sum(z_i); column 2: sum(z_i**2); column 3: bias placeholder\n",
    "    \n",
    "    Returns:\n",
    "    parameter_set -- updated parameters\n",
    "    norm_set -- reset mean, variance; update bias placeholder\n",
    "    \"\"\"\n",
    "    l = len(parameter_set)\n",
    "    p_keys = [*parameter_set]\n",
    "    n_keys = [*norm_set]\n",
    "    \n",
    "    for i in range(l):\n",
    "        phi = parameter_set[p_keys[i]]  #[n_z,n_x+2]\n",
    "        norm = norm_set[n_keys[i]]      #[n_z,1]\n",
    "        mean = norm[:-1,0:1]/norm[-1,0]\n",
    "        variance = norm[:-1,1:2]/norm[-1,1] - mean**2\n",
    "        \n",
    "        parameter_set[p_keys[i]][:,:-2] = phi[:,:-2]/np.sqrt(variance)  # scale every row of W by deviations\n",
    "        b = norm[:-1,2:]\n",
    "        norm_set[n_keys[i]][:-1,2:] = (b-mean)/np.sqrt(variance) # update bias placeholder\n",
    "        norm_set[n_keys[i]][:,:2] = 0\n",
    "    return parameter_set,norm_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2426,
   "id": "bc392fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_lr(W, dW, rate):\n",
    "    \"\"\"\n",
    "    adjust row distribution of W (more evenly distributed) by adjusting the updating rate of dW\n",
    "    Arguments:\n",
    "    W -- phi[:,:-2], numpy array of sahpe (n_z,n_x)\n",
    "    dW -- derivatives of W\n",
    "    rate -- 10 folds, x rate; 100 folds, x 2 rate\n",
    "    \n",
    "    Returns:\n",
    "    layer_lr\n",
    "    \"\"\"\n",
    "    multiple = np.log10(np.abs(W)/np.min(np.abs(W),axis=1,keepdims=True)).astype(int)\n",
    "    index = np.where(multiple > 0)\n",
    "    layer_lr = np.ones(W.shape)\n",
    "    if index.size > 0:\n",
    "        sign_index = np.where(W[index]*dW[index] > 0)  # same sign\n",
    "        v = (index[0][sign_index[0]],index[1][sign_index[0]])\n",
    "        layer_lr[v] = multiple[v]*rate\n",
    "    return layer_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2427,
   "id": "fd091b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helmholtz_machine:\n",
    "\n",
    "    def __init__(self, n_dz, activation_type, init_lr = 0.1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        n_dz -- number of neurons for each layer, numpy array of shape (n+1,m), where m is the number of instantiation layers, \n",
    "        n is the maximum number of inserted layers between adjacent instantiation layers\n",
    "        d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "        bias -- list or array [instantiation bias, MLP bias,data bias], taking binary value in {True, False}. For example, \n",
    "        [False,True,True] means no instantiation bias but has MLP bias and data bias\n",
    "        activation_type -- we provide 2 choices of activation functions: tanh(x) and sigmoid(x)\n",
    "        lr -- different learning rate for instantiation layer pairs, numpy array of shape (2,m-1)\n",
    "        \"\"\"\n",
    "        self.n_dz = n_dz\n",
    "        self.n_layer = self.n_dz.shape[1]\n",
    "        self.ac = activation_type\n",
    "        self.lr = np.ones((2,self.n_layer-1))*init_lr\n",
    "        \n",
    "    def parameter_init(self,init_type,value_set):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        init_type -- \"zero\" or \"random\", \"zero\" assigns 0 to all parameters, \"random\" samples from standard Gaussian\n",
    "        value_set -- numpy array [a,b], binary outcomes as {positive, negative}\n",
    "\n",
    "        Returns:\n",
    "        Phi -- Recognition parameter set, Python dictionary of length m-1 with each key-value pair being a parameter matrix of \n",
    "        shape (n_z{i+1}, n_zi+1), where the last column represents bias b's\n",
    "        Theta -- Generative parameter set, Python dictionary of length m with each key-value pair being a parameter matrix of \n",
    "        shape (n_z{i}, n_z{i+1}+1), where the last column represents bias b's\n",
    "        Eg. {Phi_01:{Phi_01,Phi_12}, Phi_12:{Phi_01,Phi_12,Phi_23}}, dictionary of dictionary\n",
    "        Norm -- statistical mean and variance of each pre-activation layer. Python dictionary of length l, with each value \n",
    "        a numpy array of shape (n_neuron+1,2), last row is counts\n",
    "        Scalar -- numpy array of shape (n_layer-1,2), binary outcomes of every sample layer\n",
    "        \"\"\"\n",
    "        Phi = {}\n",
    "        Theta = {}\n",
    "        Norm_wake = {}\n",
    "        Norm_sleep = {}\n",
    "        for i in range(self.n_layer-1):\n",
    "            Phi[\"Phi_\"+str(i)+str(i+1)],Theta[\"Theta_\"+str(i+1)+str(i)],Norm_wake['sl_'+str(i+1)], Norm_sleep['sl_'+str(i)] \\\n",
    "            = one_step_para_init(self.n_dz[:,i:i+2],init_type)\n",
    "            \n",
    "        self.Phi = Phi\n",
    "        self.Theta = Theta\n",
    "        self.Norm_wake = Norm_wake\n",
    "        self.Norm_sleep = Norm_sleep\n",
    "        self.Scalar = np.repeat(value_set.reshape(1,-1),self.n_layer,axis=0).astype(float)\n",
    "    \n",
    "    def wake_sample(self,d0):\n",
    "        \"\"\"\n",
    "        Stochastic sample in wake phase\n",
    "        \n",
    "        Arguments:\n",
    "        d0 -- input pattern, numpy array of shape (n_d, 1)\n",
    "\n",
    "        Returns:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = 0,...m-1\n",
    "        Norm -- updated mean and variance on every sampling\n",
    "        \"\"\"\n",
    "        S = d0  # assignment of each layer\n",
    "        Alpha_Q = {\"z0\":d0}\n",
    "        for i in range(self.n_layer-2):\n",
    "            G,q = one_step_forward(S,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.ac,self.Norm_wake['sl_'+str(i+1)],add_norm=True)\n",
    "            a = self.Scalar[i+1,0]\n",
    "            b = self.Scalar[i+1,1]\n",
    "            S = ((q > np.random.rand(len(q),1)).astype(int))*(a-b)+b\n",
    "            Alpha_Q[\"z\"+str(i+1)] = S\n",
    "        Alpha_Q[\"z\"+str(self.n_layer-1)] = [[1]]\n",
    "        return Alpha_Q\n",
    "    \n",
    "    def sleep_sample(self):\n",
    "        \"\"\"\n",
    "        Stochastic sample in sleep phase\n",
    "\n",
    "        Returns:\n",
    "        Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "        \"\"\"\n",
    "        S = [[1]]\n",
    "        Alpha_P = {\"z\"+str(self.n_layer-1):S}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            G,p = one_step_forward(S,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.ac,self.Norm_sleep['sl_'+str(i-1)],add_norm=True)\n",
    "            a = self.Scalar[i-1,0]\n",
    "            b = self.Scalar[i-1,1]\n",
    "            S = ((p > np.random.rand(len(p),1)).astype(int))*(a-b)+b   # rejection sampling as a or b\n",
    "            Alpha_P[\"z\"+str(i-1)] = S\n",
    "        return Alpha_P\n",
    "    \n",
    "    def wake_update(self,Alpha_P,fixed_value=True,fixed_scale=True,add_norm=True,layer_norm=False,check_lr=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Alpha_P -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "        lr -- learning rate, decimals\n",
    "        fixed_value -- True or Flase. If True, [a_x,b_x] are fixed values; if False, update [a_x,b_x]\n",
    "        fixed_scale -- True or Flase. If True, last 2 columns of phi are fixed; if False, update adaptive scale and bias\n",
    "\n",
    "        Returns:\n",
    "        Phi, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        \"\"\"\n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "        for i in range(self.n_layer-2):\n",
    "            x = Alpha_P['z'+str(i)]\n",
    "            y = Alpha_P['z'+str(i+1)]\n",
    "            value_set = self.Scalar[[i,i+1],:]\n",
    "            G,q = one_step_forward(x,self.Phi[\"Phi_\"+str(i)+str(i+1)],self.ac,self.Norm_wake['sl_'+str(i+1)],add_norm)\n",
    "            self.Scalar[i,:],loss,Grad_set[\"grad_Phi_\"+str(i)+str(i+1)],self.lr[0,i] = one_step_update( \\\n",
    "                x,y,self.Phi[\"Phi_\"+str(i)+str(i+1)],G,value_set,self.lr[0,i],self.ac,self.Norm_wake['sl_'+str(i+1)],\\\n",
    "                fixed_value,fixed_scale,layer_norm,check_lr)\n",
    "            Loss[i] = loss\n",
    "            Loss[-1] += loss\n",
    "        return Loss,Grad_set\n",
    "    \n",
    "    def sleep_update(self,Alpha_Q,fixed_value=True,fixed_scale=True,add_norm=True,layer_norm=False,check_lr=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        Alpha_Q -- assignment of each neuron (binary value), Python dictionary of length m with each key-value pair being \n",
    "        a numpy array of shape (n_dz[0,i], 1),i = m-1,...,0\n",
    "        lr -- learning rate, decimals\n",
    "        fixed_value -- True or Flase. If True, [a_x,b_x] are fixed values; if False, update [a_x,b_x]\n",
    "        fixed_scale -- True or Flase. If True, last 2 columns of phi are fixed; if False, update adaptive scale and bias\n",
    "\n",
    "        Returns:\n",
    "        Theta, [a_x,b_x]\n",
    "        Loss -- numpy array of length m-1; the first m-2 values are layer loss, the last term is the total loss\n",
    "        Grad_set -- gradients of parameters\n",
    "        \"\"\"\n",
    "        Loss = np.zeros(self.n_layer)\n",
    "        Grad_set = {}\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            x = Alpha_Q['z'+str(i)]\n",
    "            y = Alpha_Q['z'+str(i-1)]\n",
    "            value_set = self.Scalar[[i,i-1],:]\n",
    "            G,p = one_step_forward(x,self.Theta[\"Theta_\"+str(i)+str(i-1)],self.ac,self.Norm_sleep['sl_'+str(i-1)],add_norm)\n",
    "            if i == self.n_layer-1:\n",
    "                fv = True\n",
    "            else:\n",
    "                fv = fixed_value\n",
    "            self.Scalar[i,:],loss,Grad_set[\"grad_Theta_\"+str(i)+str(i-1)],self.lr[1,i-1] = one_step_update( \\\n",
    "                x,y,self.Theta[\"Theta_\"+str(i)+str(i-1)],G,value_set,self.lr[1,i-1],self.ac,self.Norm_sleep['sl_'+str(i-1)],\\\n",
    "                fv,fixed_scale,layer_norm,check_lr)\n",
    "            Loss[i] = loss\n",
    "            Loss[-1] += loss\n",
    "        return Loss,Grad_set\n",
    "    \n",
    "    def norm_update(self):\n",
    "        \"\"\"\n",
    "        After eg.1000 steps,\n",
    "        Normalize layer pre-activation linear term to mean 0, variance 1 and offset norm matrices to count 0\n",
    "        Check current learning rate, and update it if needed\n",
    "        Check parameters, see if each matrix rows are evenly distributed, consider turning layer_norm on\n",
    "        \n",
    "        After enough training,\n",
    "        For last 2 columns of parameter matrix, adaptive scale and bias, may turn it on\n",
    "        For Scalar, binary values for each layer, could modify it by training\n",
    "        \"\"\"\n",
    "        for i in range(self.n_layer-2):\n",
    "            one_step_norm_update(self.Phi[\"Phi_\"+str(i)+str(i+1)],self.Norm_wake['sl_'+str(i+1)])\n",
    "        for i in range(self.n_layer-1,0,-1):\n",
    "            one_step_norm_update(self.Theta[\"Theta_\"+str(i)+str(i-1)],self.Norm_sleep['sl_'+str(i-1)])\n",
    "            \n",
    "    def check_learning_rate(self,Alpha_P,Alpha_Q):\n",
    "        self.wake_update(Alpha_P,fixed_value=True,fixed_scale=True,add_norm=False,check_lr=True) # see self.lr\n",
    "        self.sleep_update(Alpha_Q,fixed_value=True,fixed_scale=True,add_norm=False,check_lr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2450,
   "id": "c9ec7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  6,  3,  1],\n",
       "       [ 9,  0,  5,  2,  0],\n",
       "       [ 0,  0,  4,  0,  0]])"
      ]
     },
     "execution_count": 2450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[10,8,6,3,1],\n",
    "             [9, 0,5,2,0],\n",
    "             [0, 0,4,0,0]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2451,
   "id": "60f701f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vH = Helmholtz_machine(n_dz,'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2452,
   "id": "94b6a433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 2452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_set = np.array([1,0])\n",
    "value_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2453,
   "id": "a3874741",
   "metadata": {},
   "outputs": [],
   "source": [
    "vH.parameter_init('random',value_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2454,
   "id": "72675c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 2454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2478,
   "id": "7f14bb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'z0': array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]]),\n",
       " 'z1': array([[-0.11242753],\n",
       "        [ 0.932288  ],\n",
       "        [-0.11242753],\n",
       "        [ 0.932288  ],\n",
       "        [ 0.932288  ],\n",
       "        [ 0.932288  ],\n",
       "        [ 0.932288  ],\n",
       "        [-0.11242753]]),\n",
       " 'z2': array([[-0.12340016],\n",
       "        [-0.12340016],\n",
       "        [ 0.85788068],\n",
       "        [-0.12340016],\n",
       "        [ 0.85788068],\n",
       "        [-0.12340016]]),\n",
       " 'z3': array([[-0.02516303],\n",
       "        [ 1.03339744],\n",
       "        [ 1.03339744]]),\n",
       " 'z4': [[1]]}"
      ]
     },
     "execution_count": 2478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alpha_Q = vH.wake_sample(d0)\n",
    "Alpha_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2466,
   "id": "2b1af880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  7.55328602,  6.65701243,  4.30094831, 23.53449235]),\n",
       " {'grad_Theta_43': {'d_Theta_10': array([[ 0.11760781,  0.29281092,  0.        ,  0.        ],\n",
       "          [ 0.13864568,  0.34518939,  0.        ,  0.        ],\n",
       "          [-0.18459885, -0.45960008,  0.        ,  0.        ]]),\n",
       "   'd_Theta_21': array([[0.2268604 , 0.        , 0.        ],\n",
       "          [0.05448544, 0.        , 0.        ]])},\n",
       "  'grad_Theta_32': {'d_Theta_10': array([[ 0.08608364,  0.05481147,  0.03447295,  0.10013683,  0.01128531,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.2605185 ,  0.16587823,  0.10432692,  0.30304826,  0.0341532 ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.2365764 ,  0.15063374,  0.0947391 ,  0.27519761,  0.03101446,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.22409989,  0.14268965,  0.08974277,  0.2606843 ,  0.02937883,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.26927842, -0.17145588, -0.10783491, -0.31323824, -0.0353016 ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.26797066,  0.1706232 ,  0.10731121,  0.31171699,  0.03513016,\n",
       "            0.        ,  0.        ]]),\n",
       "   'd_Theta_21': array([[ 0.1396563 ,  0.14906419,  0.07631197,  0.13964749,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.1905185 ,  0.2033527 ,  0.10410445,  0.19050648,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.15864898,  0.16933631,  0.08669009,  0.15863897,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.20384827, -0.21758043, -0.1113882 , -0.20383542,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.08236889, -0.08791764, -0.04500859, -0.0823637 ,  0.        ,\n",
       "            0.        ]]),\n",
       "   'd_Theta_32': array([[ 0.0002468 ,  0.0002468 , -0.01405136,  0.        ,  0.        ],\n",
       "          [-0.00042008, -0.00042008,  0.02391697,  0.        ,  0.        ],\n",
       "          [-0.00114413, -0.00114413,  0.0651403 ,  0.        ,  0.        ],\n",
       "          [ 0.00326697,  0.00326697, -0.18600197,  0.        ,  0.        ]])},\n",
       "  'grad_Theta_21': {'d_Theta_10': array([[ 0.10293452,  0.10293452,  0.10293452,  0.10293452, -0.59502589,\n",
       "            0.10293452,  0.        ,  0.        ],\n",
       "          [ 0.08865011,  0.08865011,  0.08865011,  0.08865011, -0.5124531 ,\n",
       "            0.08865011,  0.        ,  0.        ],\n",
       "          [-0.05118271, -0.05118271, -0.05118271, -0.05118271,  0.2958681 ,\n",
       "           -0.05118271,  0.        ,  0.        ],\n",
       "          [ 0.12905082,  0.12905082,  0.12905082,  0.12905082, -0.74599446,\n",
       "            0.12905082,  0.        ,  0.        ],\n",
       "          [ 0.05242347,  0.05242347,  0.05242347,  0.05242347, -0.30304045,\n",
       "            0.05242347,  0.        ,  0.        ],\n",
       "          [ 0.0552247 ,  0.0552247 ,  0.0552247 ,  0.0552247 , -0.31923329,\n",
       "            0.0552247 ,  0.        ,  0.        ],\n",
       "          [ 0.09359028,  0.09359028,  0.09359028,  0.09359028, -0.54101032,\n",
       "            0.09359028,  0.        ,  0.        ],\n",
       "          [ 0.13009771,  0.13009771,  0.13009771,  0.13009771, -0.75204614,\n",
       "            0.13009771,  0.        ,  0.        ]])},\n",
       "  'grad_Theta_10': {'d_Theta_10': array([[-0.06433617, -0.00470334, -0.03565781, -0.07066315, -0.01922802,\n",
       "           -0.00174952, -0.07099522, -0.06730271, -0.05002659,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.38652789, -0.02825737, -0.21423   , -0.42454005, -0.1155208 ,\n",
       "           -0.01051104, -0.42653507, -0.40435067, -0.30055678,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.49365798, -0.03608919, -0.273606  , -0.54220559, -0.14753855,\n",
       "           -0.01342428, -0.54475356, -0.51642052, -0.38385911,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.68373339, -0.04998477, -0.37895377, -0.75097351, -0.20434599,\n",
       "           -0.01859309, -0.75450253, -0.71526029, -0.53165815,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.5002622 , -0.03657199, -0.27726633, -0.54945928, -0.14951234,\n",
       "           -0.01360387, -0.55204134, -0.52332926, -0.38899442,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.09776929,  0.00714749,  0.05418785,  0.10738418,  0.02922011,\n",
       "            0.00265869,  0.10788881,  0.10227743,  0.07602355,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.4245626 , -0.03103792, -0.23531043, -0.46631519, -0.12688815,\n",
       "           -0.01154534, -0.46850653, -0.44413915, -0.33013185,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.39812522,  0.0291052 ,  0.22065773,  0.43727789,  0.11898687,\n",
       "            0.01082641,  0.43933278,  0.41648275,  0.30957464,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.09698087,  0.00708985,  0.05375087,  0.10651822,  0.02898447,\n",
       "            0.00263725,  0.10701877,  0.10145265,  0.07541049,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.15734934, -0.01150312, -0.08720961, -0.17282348, -0.04702667,\n",
       "           -0.00427888, -0.17363562, -0.1646047 , -0.12235187,  0.        ,\n",
       "            0.        ]]),\n",
       "   'd_Theta_21': array([[-0.09024551, -0.09024551,  0.02341171, -0.09024551, -0.09024551,\n",
       "           -0.09024551, -0.09024551, -0.09024551,  0.        ,  0.        ],\n",
       "          [ 0.04126909,  0.04126909, -0.01070613,  0.04126909,  0.04126909,\n",
       "            0.04126909,  0.04126909,  0.04126909,  0.        ,  0.        ],\n",
       "          [-0.33514109, -0.33514109,  0.08694312, -0.33514109, -0.33514109,\n",
       "           -0.33514109, -0.33514109, -0.33514109,  0.        ,  0.        ],\n",
       "          [ 0.11314623,  0.11314623, -0.02935267,  0.11314623,  0.11314623,\n",
       "            0.11314623,  0.11314623,  0.11314623,  0.        ,  0.        ],\n",
       "          [ 0.36389382,  0.36389382, -0.09440223,  0.36389382,  0.36389382,\n",
       "            0.36389382,  0.36389382,  0.36389382,  0.        ,  0.        ],\n",
       "          [-0.00696507, -0.00696507,  0.0018069 , -0.00696507, -0.00696507,\n",
       "           -0.00696507, -0.00696507, -0.00696507,  0.        ,  0.        ],\n",
       "          [ 0.14849445,  0.14849445, -0.0385228 ,  0.14849445,  0.14849445,\n",
       "            0.14849445,  0.14849445,  0.14849445,  0.        ,  0.        ],\n",
       "          [ 0.14458286,  0.14458286, -0.03750804,  0.14458286,  0.14458286,\n",
       "            0.14458286,  0.14458286,  0.14458286,  0.        ,  0.        ],\n",
       "          [-0.09489975, -0.09489975,  0.02461913, -0.09489975, -0.09489975,\n",
       "           -0.09489975, -0.09489975, -0.09489975,  0.        ,  0.        ]])}})"
      ]
     },
     "execution_count": 2466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.sleep_update(Alpha_Q,fixed_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2477,
   "id": "817c4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.05278453],\n",
       "       [ 0.932288  , -0.11242753],\n",
       "       [ 0.85788068, -0.12340016],\n",
       "       [ 1.03339744, -0.02516303],\n",
       "       [ 1.        ,  0.        ]])"
      ]
     },
     "execution_count": 2477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2479,
   "id": "3f5967a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'z4': [[1]],\n",
       " 'z3': array([[-0.02516303],\n",
       "        [ 1.03339744],\n",
       "        [-0.02516303]]),\n",
       " 'z2': array([[-0.12340016],\n",
       "        [ 0.85788068],\n",
       "        [-0.12340016],\n",
       "        [ 0.85788068],\n",
       "        [-0.12340016],\n",
       "        [ 0.85788068]]),\n",
       " 'z1': array([[ 0.932288  ],\n",
       "        [-0.11242753],\n",
       "        [ 0.932288  ],\n",
       "        [ 0.932288  ],\n",
       "        [-0.11242753],\n",
       "        [ 0.932288  ],\n",
       "        [ 0.932288  ],\n",
       "        [-0.11242753]]),\n",
       " 'z0': array([[1.        ],\n",
       "        [1.        ],\n",
       "        [0.05278453],\n",
       "        [0.05278453],\n",
       "        [1.        ],\n",
       "        [0.05278453],\n",
       "        [1.        ],\n",
       "        [0.05278453],\n",
       "        [0.05278453],\n",
       "        [1.        ]])}"
      ]
     },
     "execution_count": 2479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Alpha_P = vH.sleep_sample()\n",
    "Alpha_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2469,
   "id": "97eac106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12.03544949,  2.61165997,  2.59787067,  0.        , 17.24498013]),\n",
       " {'grad_Phi_01': {'d_Phi_12': array([[-0.58569004, -0.36133082, -0.51113579, -0.26678629, -0.46581377,\n",
       "           -0.05176591, -0.13889029, -0.22491882, -0.4559614 ,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.06060787, -0.03739092, -0.05289291, -0.02760735, -0.04820294,\n",
       "           -0.0053568 , -0.01437253, -0.02327485, -0.0471834 ,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.36558204,  0.22553919,  0.319046  ,  0.16652541,  0.29075643,\n",
       "            0.03231178,  0.08669397,  0.14039214,  0.28460668,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.82930073, -0.511622  , -0.72373653, -0.37775282, -0.65956337,\n",
       "           -0.07329732, -0.19666003, -0.31847108, -0.64561303,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.91962762,  0.56734754,  0.80256544,  0.41889742,  0.73140258,\n",
       "            0.08128082,  0.21808011,  0.35315874,  0.71593279,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.93066316,  0.57415571,  0.81219624,  0.42392419,  0.74017942,\n",
       "            0.08225619,  0.22069707,  0.35739665,  0.72452398,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.82770192,  0.51063565,  0.72234125,  0.37702456,  0.6582918 ,\n",
       "            0.07315601,  0.19628089,  0.3178571 ,  0.64436836,  0.        ,\n",
       "            0.        ],\n",
       "          [ 0.3308169 ,  0.20409147,  0.28870622,  0.15068963,  0.26310686,\n",
       "            0.02923908,  0.07844978,  0.12704151,  0.25754192,  0.        ,\n",
       "            0.        ]]),\n",
       "   'd_Phi_01': array([[ 0.00420542,  0.09221596,  0.00420542,  0.00420542,  0.00420542,\n",
       "            0.00420542,  0.00420542,  0.00420542,  0.00420542,  0.09221596,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.02935595,  0.64371466,  0.02935595,  0.02935595,  0.02935595,\n",
       "            0.02935595,  0.02935595,  0.02935595,  0.02935595,  0.64371466,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.02227666,  0.48848068,  0.02227666,  0.02227666,  0.02227666,\n",
       "            0.02227666,  0.02227666,  0.02227666,  0.02227666,  0.48848068,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.02065493, -0.45291951, -0.02065493, -0.02065493, -0.02065493,\n",
       "           -0.02065493, -0.02065493, -0.02065493, -0.02065493, -0.45291951,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.00871192,  0.19103426,  0.00871192,  0.00871192,  0.00871192,\n",
       "            0.00871192,  0.00871192,  0.00871192,  0.00871192,  0.19103426,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.00988689,  0.21679878,  0.00988689,  0.00988689,  0.00988689,\n",
       "            0.00988689,  0.00988689,  0.00988689,  0.00988689,  0.21679878,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.01820553, -0.39920926, -0.01820553, -0.01820553, -0.01820553,\n",
       "           -0.01820553, -0.01820553, -0.01820553, -0.01820553, -0.39920926,\n",
       "            0.        ,  0.        ],\n",
       "          [-0.00672676, -0.14750387, -0.00672676, -0.00672676, -0.00672676,\n",
       "           -0.00672676, -0.00672676, -0.00672676, -0.00672676, -0.14750387,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.00332207,  0.07284601,  0.00332207,  0.00332207,  0.00332207,\n",
       "            0.00332207,  0.00332207,  0.00332207,  0.00332207,  0.07284601,\n",
       "            0.        ,  0.        ]])},\n",
       "  'grad_Phi_12': {'d_Phi_01': array([[ 0.39311042,  0.39311042, -0.06314415,  0.39311042, -0.06314415,\n",
       "           -0.06314415, -0.06314415, -0.06314415,  0.        ,  0.        ],\n",
       "          [ 0.19526403,  0.19526403, -0.03136468,  0.19526403, -0.03136468,\n",
       "           -0.03136468, -0.03136468, -0.03136468,  0.        ,  0.        ],\n",
       "          [-0.4990509 , -0.4990509 ,  0.08016105, -0.4990509 ,  0.08016105,\n",
       "            0.08016105,  0.08016105,  0.08016105,  0.        ,  0.        ],\n",
       "          [ 0.04712002,  0.04712002, -0.00756875,  0.04712002, -0.00756875,\n",
       "           -0.00756875, -0.00756875, -0.00756875,  0.        ,  0.        ],\n",
       "          [ 0.18908136,  0.18908136, -0.03037157,  0.18908136, -0.03037157,\n",
       "           -0.03037157, -0.03037157, -0.03037157,  0.        ,  0.        ],\n",
       "          [-0.4323159 , -0.4323159 ,  0.06944161, -0.4323159 ,  0.06944161,\n",
       "            0.06944161,  0.06944161,  0.06944161,  0.        ,  0.        ]])},\n",
       "  'grad_Phi_23': {'d_Phi_23': array([[ 0.10257926,  0.18346181,  0.06947018,  0.1311849 ,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.15136109, -0.27070754, -0.1025069 , -0.19357022,  0.        ,\n",
       "            0.        ],\n",
       "          [-0.14439874, -0.25825546, -0.09779176, -0.18466632,  0.        ,\n",
       "            0.        ]]),\n",
       "   'd_Phi_12': array([[-0.0395384 , -0.02621518, -0.04630296, -0.03164267, -0.01614483,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.18743952,  0.1242782 ,  0.21950823,  0.15000829,  0.07653775,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.07413131,  0.04915135,  0.08681431,  0.05932746,  0.03027026,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.04105559,  0.02722112,  0.04807972,  0.03285688,  0.01676435,\n",
       "            0.        ,  0.        ]]),\n",
       "   'd_Phi_01': array([[-0.00379641, -0.00379641,  0.02603493, -0.00379641, -0.00379641,\n",
       "            0.02603493,  0.        ,  0.        ],\n",
       "          [ 0.00082957,  0.00082957, -0.005689  ,  0.00082957,  0.00082957,\n",
       "           -0.005689  ,  0.        ,  0.        ],\n",
       "          [ 0.00515583,  0.00515583, -0.03535756,  0.00515583,  0.00515583,\n",
       "           -0.03535756,  0.        ,  0.        ],\n",
       "          [-0.00053263, -0.00053263,  0.00365263, -0.00053263, -0.00053263,\n",
       "            0.00365263,  0.        ,  0.        ],\n",
       "          [ 0.00612653,  0.00612653, -0.04201441,  0.00612653,  0.00612653,\n",
       "           -0.04201441,  0.        ,  0.        ]])}})"
      ]
     },
     "execution_count": 2469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.wake_update(Alpha_P,fixed_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2470,
   "id": "af1a80ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sl_1': {'l_1': array([[ 25.1924782 , 181.04338128,   0.        ],\n",
       "         [  1.59782844,   1.60585935,   0.        ],\n",
       "         [ 10.31533231,  29.13404101,   0.        ],\n",
       "         [  4.69323451,   8.58595293,   0.        ],\n",
       "         [  4.92321163,   6.45104797,   0.        ],\n",
       "         [-12.22373234,  46.48226861,   0.        ],\n",
       "         [  2.48415802,   6.66349392,   0.        ],\n",
       "         [  1.63474834,   2.20644081,   0.        ],\n",
       "         [ 15.15389797,  80.83013325,   0.        ],\n",
       "         [  4.        ,   4.        ,   0.        ]]),\n",
       "  'l_2': array([[-1.6828595 ,  0.9100801 ,  0.        ],\n",
       "         [13.91231363, 49.79029724,  0.        ],\n",
       "         [-9.01648722, 24.96056737,  0.        ],\n",
       "         [-5.30946843, 10.04606049,  0.        ],\n",
       "         [12.9868323 , 42.35282483,  0.        ],\n",
       "         [12.95845141, 42.56686356,  0.        ],\n",
       "         [ 8.58058517, 19.5860454 ,  0.        ],\n",
       "         [-2.10701727,  1.47427489,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]])},\n",
       " 'sl_2': {'l_1': array([[ -3.65798651,   5.12803594,   0.        ],\n",
       "         [ -5.2753207 ,  10.90915404,   0.        ],\n",
       "         [  7.73596399,  21.49125168,   0.        ],\n",
       "         [-19.47084741, 102.35856431,   0.        ],\n",
       "         [ -0.24462193,   3.09028269,   0.        ],\n",
       "         [ -2.96501909,   3.61568172,   0.        ],\n",
       "         [  4.        ,   4.        ,   0.        ]])},\n",
       " 'sl_3': {'l_1': array([[ 9.77710847, 39.58224101,  0.        ],\n",
       "         [ 2.7487972 ,  3.64307111,  0.        ],\n",
       "         [ 7.14167583, 15.80615866,  0.        ],\n",
       "         [ 0.33737638,  7.21244503,  0.        ],\n",
       "         [ 2.56338832,  5.18179766,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]]),\n",
       "  'l_2': array([[-3.6338599 ,  4.28622201,  0.        ],\n",
       "         [-0.18268521,  0.53528052,  0.        ],\n",
       "         [-9.40193034, 23.81265633,  0.        ],\n",
       "         [-5.0597667 ,  6.88046267,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]]),\n",
       "  'l_3': array([[-0.88973887,  0.20859377,  0.        ],\n",
       "         [-2.27299808,  1.30477921,  0.        ],\n",
       "         [-2.35186753,  1.46408443,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]])},\n",
       " 'sl_4': {'l_1': array([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       "  'l_2': array([[0., 0., 0.],\n",
       "         [0., 0., 0.]])}}"
      ]
     },
     "execution_count": 2470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.Norm_wake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2475,
   "id": "cc4f7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "vH.Scalar[0,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2476,
   "id": "16413b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.05278453],\n",
       "       [ 0.932288  , -0.11242753],\n",
       "       [ 0.85788068, -0.12340016],\n",
       "       [ 1.03339744, -0.02516303],\n",
       "       [ 1.        ,  0.        ]])"
      ]
     },
     "execution_count": 2476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2472,
   "id": "e1c1ac8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sl_0': {'l_1': array([[ 2.26948716,  9.51069734,  0.        ],\n",
       "         [-7.38377679, 17.48627508,  0.        ],\n",
       "         [ 0.39958913,  2.42860287,  0.        ],\n",
       "         [ 3.18719393, 13.07115388,  0.        ],\n",
       "         [-3.53616967, 12.4939888 ,  0.        ],\n",
       "         [-8.34032947, 27.20821623,  0.        ],\n",
       "         [ 4.93378294,  6.64566201,  0.        ],\n",
       "         [ 5.24082708,  8.09129437,  0.        ],\n",
       "         [ 1.84550862,  1.81948685,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]]),\n",
       "  'l_0': array([[  5.94974592,   9.58898763,   0.        ],\n",
       "         [  1.53090361,   1.60523672,   0.        ],\n",
       "         [ -4.35587282,   8.8646285 ,   0.        ],\n",
       "         [-10.34772701,  27.59072773,   0.        ],\n",
       "         [ -2.79492205,   2.4798262 ,   0.        ],\n",
       "         [ -8.37761051,  18.88235783,   0.        ],\n",
       "         [ -2.56978695,   2.48595452,   0.        ],\n",
       "         [  0.9446418 ,   1.89970223,   0.        ],\n",
       "         [ -9.85452618,  26.32951109,   0.        ],\n",
       "         [  3.68089105,   7.75484031,   0.        ],\n",
       "         [  4.        ,   4.        ,   0.        ]])},\n",
       " 'sl_1': {'l_0': array([[ 2.72147643, 14.3707816 ,  0.        ],\n",
       "         [-2.53763433,  5.2803916 ,  0.        ],\n",
       "         [ 5.53127858, 16.34688963,  0.        ],\n",
       "         [-0.10986867,  3.52265786,  0.        ],\n",
       "         [ 1.56825759,  1.7984123 ,  0.        ],\n",
       "         [ 1.9306744 ,  2.49087687,  0.        ],\n",
       "         [ 0.64842373,  2.29062565,  0.        ],\n",
       "         [-2.63404047,  2.42588031,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]])},\n",
       " 'sl_2': {'l_2': array([[ 3.72272546,  3.56491442,  0.        ],\n",
       "         [-1.64401026,  2.8469378 ,  0.        ],\n",
       "         [-0.06415963,  0.41876346,  0.        ],\n",
       "         [ 3.8126088 ,  3.76121963,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]]),\n",
       "  'l_1': array([[-3.04885396e+00,  2.78574750e+00,  0.00000000e+00],\n",
       "         [-4.62154370e+00,  5.42975187e+00,  0.00000000e+00],\n",
       "         [-4.48954085e+00,  5.24064635e+00,  0.00000000e+00],\n",
       "         [ 1.51182942e-01,  1.21479776e-02,  0.00000000e+00],\n",
       "         [-1.17387303e+01,  3.44885337e+01,  0.00000000e+00],\n",
       "         [ 4.00000000e+00,  4.00000000e+00,  0.00000000e+00]]),\n",
       "  'l_0': array([[-5.12335473,  6.60787196,  0.        ],\n",
       "         [ 1.0472181 ,  0.27716642,  0.        ],\n",
       "         [ 0.23953576,  0.01623014,  0.        ],\n",
       "         [-0.45443647,  0.07207732,  0.        ],\n",
       "         [-1.39751567,  0.50021746,  0.        ],\n",
       "         [ 1.57485959,  0.62071894,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]])},\n",
       " 'sl_3': {'l_1': array([[-4.03845287,  4.07917715,  0.        ],\n",
       "         [ 2.68795022,  1.80648581,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]]),\n",
       "  'l_0': array([[-0.87426618,  0.19338775,  0.        ],\n",
       "         [ 0.25710128,  0.01764583,  0.        ],\n",
       "         [-3.38225222,  2.86901287,  0.        ],\n",
       "         [ 4.        ,  4.        ,  0.        ]])}}"
      ]
     },
     "execution_count": 2472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.Norm_sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2473,
   "id": "6390a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_loss: 1.4633005228626619 delta_f: 1.4986837711246217\n",
      "delta_loss: 0.17974028891469107 delta_f: 0.18515649445737603\n",
      "delta_loss: 0.04673368750587059 delta_f: 0.04709281268195044\n",
      "delta_loss: 0.04946402576253828 delta_f: 0.04980711007405653\n",
      "delta_loss: 0.16415737811444897 delta_f: 0.16659604435356903\n",
      "delta_loss: 0.2236541238427341 delta_f: 0.22621988536862214\n",
      "delta_loss: 1.091776331770716 delta_f: 1.1443256009421225\n"
     ]
    }
   ],
   "source": [
    "vH.check_learning_rate(Alpha_P,Alpha_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2464,
   "id": "5540a1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, 0.1]])"
      ]
     },
     "execution_count": 2464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2443,
   "id": "d2f20414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': {'Theta_21': array([[-0.47469796, -0.93997046,  0.47074128,  1.73130789, -0.8627557 ,\n",
       "           0.71623255, -0.73054613, -1.28736495,  0.        ,  1.        ],\n",
       "         [ 0.06038769, -0.83277052, -0.5294552 ,  0.86853481,  0.17543465,\n",
       "          -1.04622675,  0.61410137,  0.40685964,  0.        ,  1.        ],\n",
       "         [-0.94397092, -0.03941665, -0.55415802, -0.9197692 ,  0.68484107,\n",
       "           0.8992795 , -1.39138546, -0.11977782,  0.        ,  1.        ],\n",
       "         [-0.4089057 , -1.79558654,  0.84949919, -0.32591018, -0.72167534,\n",
       "           1.33052984,  0.24990426, -1.25819805,  0.        ,  1.        ],\n",
       "         [ 0.5976124 ,  0.91536068,  0.80214499, -0.10564955, -0.28142783,\n",
       "           0.72854054,  0.64364878,  0.35826898,  0.        ,  1.        ],\n",
       "         [ 0.1851904 ,  0.37992632,  0.7535409 , -1.91061251,  0.79062973,\n",
       "          -0.70260716,  1.88239949,  0.32820783,  0.        ,  1.        ],\n",
       "         [-0.3454627 ,  0.83985779, -0.2158912 , -0.18977188,  2.01509757,\n",
       "           0.1034846 ,  0.83391002,  0.24001418,  0.        ,  1.        ],\n",
       "         [ 0.08885516, -1.03643227, -1.73556426,  0.26003479,  0.47691886,\n",
       "           0.45602545, -0.2832061 ,  0.06333875,  0.        ,  1.        ],\n",
       "         [-0.24463304,  0.43143364,  2.05663598, -0.85222741,  1.50922848,\n",
       "          -1.42328674, -0.49718712, -1.33793981,  0.        ,  1.        ]]),\n",
       "  'Theta_10': array([[-1.24923068e-01, -1.25426117e+00,  6.98041882e-01,\n",
       "           1.70584795e-01,  8.99664828e-01, -5.15605252e-01,\n",
       "           1.28066329e+00,  6.77888460e-01,  9.11536260e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-1.85005946e+00, -2.02838691e-01,  6.45331147e-01,\n",
       "          -3.14604970e-01, -9.21248819e-02,  2.45122800e+00,\n",
       "           9.15714787e-01, -6.38109852e-01,  2.35708507e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-1.08126967e+00, -2.33731632e-01,  1.05375208e+00,\n",
       "           6.25990670e-01,  9.37944985e-02,  1.78218982e+00,\n",
       "           6.59408699e-01,  4.93768288e-02, -1.47731958e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.17251943e-01,  5.53142912e-01, -1.45650801e+00,\n",
       "           3.39628404e-01,  1.48822901e-01, -4.18225253e-01,\n",
       "           1.30722867e-01, -5.00791339e-01,  1.88250072e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-8.65021654e-01, -3.07164509e-01,  2.20450781e+00,\n",
       "           1.73676398e-01, -3.89081046e-01,  6.69512720e-01,\n",
       "           2.78888825e-01,  8.45512998e-01,  3.08066952e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.40010487e+00, -1.60365362e+00, -7.37981937e-01,\n",
       "          -1.29573121e+00,  6.30578506e-02, -8.17251412e-01,\n",
       "          -7.01664712e-01, -1.05233066e-03, -4.71524713e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.53421778e+00, -6.48448198e-01,  4.22874517e-01,\n",
       "           8.07888503e-01,  1.01045098e+00, -3.88621502e-01,\n",
       "          -9.32747087e-01, -4.34936932e-02,  3.20320170e-02,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-1.34559183e+00, -8.18608853e-01, -6.97558448e-01,\n",
       "          -1.11781656e-01, -1.12688143e+00,  3.05442051e-01,\n",
       "          -1.00989365e-01, -9.48087652e-01,  9.31910015e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-5.15058935e-01,  2.29619965e+00,  7.20799983e-01,\n",
       "           1.41684377e+00,  5.61089586e-02, -1.39020410e+00,\n",
       "           1.62787425e-01,  7.37339282e-01, -6.74683367e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 3.53869972e-01, -8.60129815e-02, -1.25675487e+00,\n",
       "          -5.37737841e-01,  1.46364992e+00,  1.89568752e+00,\n",
       "          -8.59142190e-01, -2.33422870e-01, -1.21913491e+00,\n",
       "           0.00000000e+00,  1.00000000e+00]])},\n",
       " 'Theta_21': {'Theta_10': array([[ 0.68866378, -1.49415163, -0.71952189, -0.5168858 , -0.23988186,\n",
       "           1.02968231,  0.        ,  1.        ],\n",
       "         [-0.68840459,  0.32622834,  0.6794676 ,  1.44349923, -0.98553146,\n",
       "           0.55482502,  0.        ,  1.        ],\n",
       "         [ 0.7684106 ,  0.15754999,  2.22346931,  1.29261147,  1.90406225,\n",
       "           0.94039873,  0.        ,  1.        ],\n",
       "         [ 0.93366239, -0.12743554, -1.66130661,  0.93207175,  0.327731  ,\n",
       "          -0.713424  ,  0.        ,  1.        ],\n",
       "         [-1.50341219,  0.33513027, -0.12393044,  1.0878419 , -0.57629941,\n",
       "           0.57772181,  0.        ,  1.        ],\n",
       "         [ 0.47260404,  1.59312731,  0.29035736,  0.53186404, -0.69977965,\n",
       "          -1.01662924,  0.        ,  1.        ],\n",
       "         [ 1.7776995 , -0.73455363, -0.27694209, -1.42365664,  0.83659607,\n",
       "           0.54663981,  0.        ,  1.        ],\n",
       "         [ 0.02511654,  0.30592523, -0.09672757,  0.11907235, -0.82576419,\n",
       "           0.63849645,  0.        ,  1.        ]])},\n",
       " 'Theta_32': {'Theta_32': array([[-0.81457284,  0.33345809,  0.11403262,  0.        ,  1.        ],\n",
       "         [-0.1838458 ,  0.59363592, -0.63507485,  0.        ,  1.        ],\n",
       "         [-1.08160753, -0.52793279, -0.11559027,  0.        ,  1.        ],\n",
       "         [ 0.32774898, -0.59314201, -0.13471983,  0.        ,  1.        ]]),\n",
       "  'Theta_21': array([[ 0.72698658, -0.47551501, -1.02942732,  2.05287049,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.48072614, -0.09712041,  3.27507138, -0.68364146,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.76170845,  1.18765929, -1.35224899,  0.92721837,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.18048282,  0.55940798,  1.10415606, -0.61641227,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.76812899,  0.43813245,  1.18830512, -0.83531933,  0.        ,\n",
       "           1.        ]]),\n",
       "  'Theta_10': array([[ 1.31735621, -0.59091421,  0.39956199,  0.18904109, -0.54949167,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.01005168, -0.54956579, -0.44395997, -0.2190952 , -1.22518984,\n",
       "           0.        ,  1.        ],\n",
       "         [-0.97622258, -0.74635025,  0.46934868, -1.49833158,  1.54864704,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.67963293, -0.23340555,  0.67285376,  0.3822931 , -0.48156844,\n",
       "           0.        ,  1.        ],\n",
       "         [-0.77158249, -0.13348455, -1.41223614, -1.59694685,  0.56069429,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.32905593,  0.24157673, -1.54022185, -0.51109847,  0.25652038,\n",
       "           0.        ,  1.        ]])},\n",
       " 'Theta_43': {'Theta_21': array([[0.06205188, 0.        , 1.        ],\n",
       "         [1.36454804, 0.        , 1.        ]]),\n",
       "  'Theta_10': array([[ 0.41809559,  0.15969163,  0.        ,  1.        ],\n",
       "         [-0.91886862, -1.05311453,  0.        ,  1.        ],\n",
       "         [-0.49477395,  0.37034245,  0.        ,  1.        ]])}}"
      ]
     },
     "execution_count": 2443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2444,
   "id": "02e5d4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Phi_01': {'Phi_01': array([[ 0.42697905,  0.6882917 ,  3.02328678,  0.499483  , -0.27310031,\n",
       "          -0.5314409 , -0.88107581, -0.13446022, -0.2166245 ,  0.04459686,\n",
       "           0.        ,  1.        ],\n",
       "         [-0.60197163, -0.4709854 ,  1.51997718, -0.90906745,  0.02920594,\n",
       "          -1.14787776, -1.02385673,  0.93116877, -0.54653774,  0.26446859,\n",
       "           0.        ,  1.        ],\n",
       "         [-0.26010091,  0.25265193,  0.59972686, -0.60956555, -0.89931209,\n",
       "           0.52266028, -1.03912396, -0.32454908, -0.19563826, -0.31970498,\n",
       "           0.        ,  1.        ],\n",
       "         [ 0.46811784, -0.41655171,  0.10064851, -0.19033028,  0.83227988,\n",
       "          -0.79579701,  0.16037282,  0.14007259, -0.92233837, -0.98915098,\n",
       "           0.        ,  1.        ],\n",
       "         [-1.0584843 , -0.78353113,  0.95411492,  0.4908841 , -0.37646919,\n",
       "           0.94066778,  0.71863654,  0.26638682, -0.07845872, -0.33358408,\n",
       "           0.        ,  1.        ],\n",
       "         [ 0.72810594,  0.55097446, -0.25497182,  1.23534708,  1.10325157,\n",
       "           0.3412573 ,  0.68033599,  0.08759906,  0.04730441, -1.46606408,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.83488522,  1.17205565,  0.76081872,  0.57144337, -0.23181737,\n",
       "          -2.54442993, -0.21067231,  0.56414335, -0.11700254,  0.86103909,\n",
       "           0.        ,  1.        ],\n",
       "         [-1.68876834, -0.62839953,  0.07236208, -0.19527442, -0.0528968 ,\n",
       "           1.05814483,  0.34645439, -1.09115331, -0.41657412,  0.55632336,\n",
       "           0.        ,  1.        ],\n",
       "         [-0.06971374, -0.07321007, -0.75378726, -0.82493236,  0.39989582,\n",
       "          -0.64955733, -0.53267216,  0.0080633 ,  0.09248943, -0.60093053,\n",
       "           0.        ,  1.        ]]),\n",
       "  'Phi_12': array([[ 0.74674558, -0.3765222 ,  2.99285413,  0.32993504, -0.34213386,\n",
       "           1.22820813, -0.58412629,  0.30610055,  0.79148817,  0.        ,\n",
       "           1.        ],\n",
       "         [ 1.05947263, -0.15621183,  0.72154556,  1.07191764,  0.30369536,\n",
       "           0.11863549, -1.87640948,  1.01243041,  0.11171277,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.32635527, -2.73460174, -1.52647655,  0.14873512,  0.15134213,\n",
       "          -0.24407208, -0.45040499,  0.01457346,  1.30585705,  0.        ,\n",
       "           1.        ],\n",
       "         [-0.56557542,  0.878683  , -1.47041264, -0.24105829,  0.77673626,\n",
       "          -1.76917174, -0.62114798,  0.88068235, -0.34654063,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.20190218, -2.35732071,  1.21622765,  0.14375498, -0.03421637,\n",
       "          -0.5928307 ,  0.16859613,  1.23615116,  0.33905134,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.58977764, -1.89497563,  0.35473096,  0.08602751, -0.51084521,\n",
       "           0.24705503, -0.71674384,  0.20107098,  0.27112352,  0.        ,\n",
       "           1.        ],\n",
       "         [-1.75988093,  0.8937221 , -0.13719329,  1.70377997, -0.53928991,\n",
       "          -0.38822397, -0.67485155, -0.0942639 ,  0.60887804,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.65009716, -1.52280994,  0.05247853, -1.09297571,  0.88053035,\n",
       "          -1.4772208 ,  1.44666159, -1.45583229,  0.45699516,  0.        ,\n",
       "           1.        ]])},\n",
       " 'Phi_12': {'Phi_01': array([[-0.53788298, -0.19989575,  0.80523335,  0.22074577, -0.53681494,\n",
       "           2.44923738, -0.64776376,  0.25412072,  0.        ,  1.        ],\n",
       "         [-1.27104237, -1.59489799, -1.25908782,  0.91423825,  0.14300847,\n",
       "          -0.22239056,  0.99010154, -0.77136493,  0.        ,  1.        ],\n",
       "         [-0.27203133,  1.45391234, -0.43465277,  0.12287531,  1.1046414 ,\n",
       "          -0.26788195, -0.59085519, -0.71409655,  0.        ,  1.        ],\n",
       "         [-1.0515809 , -0.02778933, -0.01110723,  0.73371955,  0.39579847,\n",
       "           1.48630993,  1.67969896, -1.18306974,  0.        ,  1.        ],\n",
       "         [-0.23629407, -1.02429455,  1.67810509,  0.67349959,  0.50612046,\n",
       "           0.75775842,  0.22540843,  0.96044976,  0.        ,  1.        ],\n",
       "         [ 0.31569657,  0.78556286, -0.35855919,  0.9871065 , -1.59361767,\n",
       "          -0.04829172, -0.0643525 ,  0.12892181,  0.        ,  1.        ]])},\n",
       " 'Phi_23': {'Phi_01': array([[ 1.25259609,  0.11383253,  0.28948755, -0.5165963 ,  0.26136894,\n",
       "          -1.87919798,  0.        ,  1.        ],\n",
       "         [-0.16324673,  1.14090539,  0.89742513, -0.71168569, -1.3108378 ,\n",
       "          -0.33596702,  0.        ,  1.        ],\n",
       "         [-0.04437647, -0.07535916, -0.20817709,  0.10868975, -0.84325729,\n",
       "          -0.09163647,  0.        ,  1.        ],\n",
       "         [ 1.0043524 ,  0.42819657, -0.243702  ,  0.65345156, -0.15874875,\n",
       "           1.57255658,  0.        ,  1.        ],\n",
       "         [ 1.6625672 ,  0.94928146, -0.76898667,  1.36172381, -0.42770865,\n",
       "          -0.32363545,  0.        ,  1.        ]]),\n",
       "  'Phi_12': array([[ 0.21387481,  1.39500706,  1.21654201, -1.18408144, -1.23327961,\n",
       "           0.        ,  1.        ],\n",
       "         [ 0.58870638, -0.59687182,  0.61896325, -0.31051338, -0.75080972,\n",
       "           0.        ,  1.        ],\n",
       "         [ 0.37779935,  0.20188774, -0.62282005,  0.79567955,  0.36911953,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.08165674,  0.76135764, -0.12435598, -0.33897513,  1.02301412,\n",
       "           0.        ,  1.        ]]),\n",
       "  'Phi_23': array([[-1.36697324, -1.23205411, -1.50627409, -1.03052084,  0.        ,\n",
       "           1.        ],\n",
       "         [-0.89804455,  0.0867733 , -0.006071  , -0.06104116,  0.        ,\n",
       "           1.        ],\n",
       "         [-0.54451022,  1.25516087,  0.40517778, -0.90485759,  0.        ,\n",
       "           1.        ]])},\n",
       " 'Phi_34': {'Phi_01': array([[-1.66827141, -1.41551364, -0.49486954,  0.        ,  1.        ],\n",
       "         [ 1.63829324,  2.66653813, -0.18724773,  0.        ,  1.        ]]),\n",
       "  'Phi_12': array([[-0.5894648 , -0.12318898,  0.        ,  1.        ]])}}"
      ]
     },
     "execution_count": 2444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2445,
   "id": "91287186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  0,  5,  2,  0],\n",
       "       [10,  8,  6,  3,  1]])"
      ]
     },
     "execution_count": 2445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dz[[1,0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2446,
   "id": "3dbe9a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Theta_10': {'Theta_21': array([[-0.47469796, -0.93997046,  0.47074128,  1.73130789, -0.8627557 ,\n",
       "           0.71623255, -0.73054613, -1.28736495,  0.        ,  1.        ],\n",
       "         [ 0.06038769, -0.83277052, -0.5294552 ,  0.86853481,  0.17543465,\n",
       "          -1.04622675,  0.61410137,  0.40685964,  0.        ,  1.        ],\n",
       "         [-0.94397092, -0.03941665, -0.55415802, -0.9197692 ,  0.68484107,\n",
       "           0.8992795 , -1.39138546, -0.11977782,  0.        ,  1.        ],\n",
       "         [-0.4089057 , -1.79558654,  0.84949919, -0.32591018, -0.72167534,\n",
       "           1.33052984,  0.24990426, -1.25819805,  0.        ,  1.        ],\n",
       "         [ 0.5976124 ,  0.91536068,  0.80214499, -0.10564955, -0.28142783,\n",
       "           0.72854054,  0.64364878,  0.35826898,  0.        ,  1.        ],\n",
       "         [ 0.1851904 ,  0.37992632,  0.7535409 , -1.91061251,  0.79062973,\n",
       "          -0.70260716,  1.88239949,  0.32820783,  0.        ,  1.        ],\n",
       "         [-0.3454627 ,  0.83985779, -0.2158912 , -0.18977188,  2.01509757,\n",
       "           0.1034846 ,  0.83391002,  0.24001418,  0.        ,  1.        ],\n",
       "         [ 0.08885516, -1.03643227, -1.73556426,  0.26003479,  0.47691886,\n",
       "           0.45602545, -0.2832061 ,  0.06333875,  0.        ,  1.        ],\n",
       "         [-0.24463304,  0.43143364,  2.05663598, -0.85222741,  1.50922848,\n",
       "          -1.42328674, -0.49718712, -1.33793981,  0.        ,  1.        ]]),\n",
       "  'Theta_10': array([[-1.24923068e-01, -1.25426117e+00,  6.98041882e-01,\n",
       "           1.70584795e-01,  8.99664828e-01, -5.15605252e-01,\n",
       "           1.28066329e+00,  6.77888460e-01,  9.11536260e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-1.85005946e+00, -2.02838691e-01,  6.45331147e-01,\n",
       "          -3.14604970e-01, -9.21248819e-02,  2.45122800e+00,\n",
       "           9.15714787e-01, -6.38109852e-01,  2.35708507e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-1.08126967e+00, -2.33731632e-01,  1.05375208e+00,\n",
       "           6.25990670e-01,  9.37944985e-02,  1.78218982e+00,\n",
       "           6.59408699e-01,  4.93768288e-02, -1.47731958e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.17251943e-01,  5.53142912e-01, -1.45650801e+00,\n",
       "           3.39628404e-01,  1.48822901e-01, -4.18225253e-01,\n",
       "           1.30722867e-01, -5.00791339e-01,  1.88250072e+00,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-8.65021654e-01, -3.07164509e-01,  2.20450781e+00,\n",
       "           1.73676398e-01, -3.89081046e-01,  6.69512720e-01,\n",
       "           2.78888825e-01,  8.45512998e-01,  3.08066952e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.40010487e+00, -1.60365362e+00, -7.37981937e-01,\n",
       "          -1.29573121e+00,  6.30578506e-02, -8.17251412e-01,\n",
       "          -7.01664712e-01, -1.05233066e-03, -4.71524713e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 1.53421778e+00, -6.48448198e-01,  4.22874517e-01,\n",
       "           8.07888503e-01,  1.01045098e+00, -3.88621502e-01,\n",
       "          -9.32747087e-01, -4.34936932e-02,  3.20320170e-02,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-1.34559183e+00, -8.18608853e-01, -6.97558448e-01,\n",
       "          -1.11781656e-01, -1.12688143e+00,  3.05442051e-01,\n",
       "          -1.00989365e-01, -9.48087652e-01,  9.31910015e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [-5.15058935e-01,  2.29619965e+00,  7.20799983e-01,\n",
       "           1.41684377e+00,  5.61089586e-02, -1.39020410e+00,\n",
       "           1.62787425e-01,  7.37339282e-01, -6.74683367e-01,\n",
       "           0.00000000e+00,  1.00000000e+00],\n",
       "         [ 3.53869972e-01, -8.60129815e-02, -1.25675487e+00,\n",
       "          -5.37737841e-01,  1.46364992e+00,  1.89568752e+00,\n",
       "          -8.59142190e-01, -2.33422870e-01, -1.21913491e+00,\n",
       "           0.00000000e+00,  1.00000000e+00]])},\n",
       " 'Theta_21': {'Theta_10': array([[ 0.68866378, -1.49415163, -0.71952189, -0.5168858 , -0.23988186,\n",
       "           1.02968231,  0.        ,  1.        ],\n",
       "         [-0.68840459,  0.32622834,  0.6794676 ,  1.44349923, -0.98553146,\n",
       "           0.55482502,  0.        ,  1.        ],\n",
       "         [ 0.7684106 ,  0.15754999,  2.22346931,  1.29261147,  1.90406225,\n",
       "           0.94039873,  0.        ,  1.        ],\n",
       "         [ 0.93366239, -0.12743554, -1.66130661,  0.93207175,  0.327731  ,\n",
       "          -0.713424  ,  0.        ,  1.        ],\n",
       "         [-1.50341219,  0.33513027, -0.12393044,  1.0878419 , -0.57629941,\n",
       "           0.57772181,  0.        ,  1.        ],\n",
       "         [ 0.47260404,  1.59312731,  0.29035736,  0.53186404, -0.69977965,\n",
       "          -1.01662924,  0.        ,  1.        ],\n",
       "         [ 1.7776995 , -0.73455363, -0.27694209, -1.42365664,  0.83659607,\n",
       "           0.54663981,  0.        ,  1.        ],\n",
       "         [ 0.02511654,  0.30592523, -0.09672757,  0.11907235, -0.82576419,\n",
       "           0.63849645,  0.        ,  1.        ]])},\n",
       " 'Theta_32': {'Theta_32': array([[-0.81457284,  0.33345809,  0.11403262,  0.        ,  1.        ],\n",
       "         [-0.1838458 ,  0.59363592, -0.63507485,  0.        ,  1.        ],\n",
       "         [-1.08160753, -0.52793279, -0.11559027,  0.        ,  1.        ],\n",
       "         [ 0.32774898, -0.59314201, -0.13471983,  0.        ,  1.        ]]),\n",
       "  'Theta_21': array([[ 0.72698658, -0.47551501, -1.02942732,  2.05287049,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.48072614, -0.09712041,  3.27507138, -0.68364146,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.76170845,  1.18765929, -1.35224899,  0.92721837,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.18048282,  0.55940798,  1.10415606, -0.61641227,  0.        ,\n",
       "           1.        ],\n",
       "         [ 0.76812899,  0.43813245,  1.18830512, -0.83531933,  0.        ,\n",
       "           1.        ]]),\n",
       "  'Theta_10': array([[ 1.31735621, -0.59091421,  0.39956199,  0.18904109, -0.54949167,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.01005168, -0.54956579, -0.44395997, -0.2190952 , -1.22518984,\n",
       "           0.        ,  1.        ],\n",
       "         [-0.97622258, -0.74635025,  0.46934868, -1.49833158,  1.54864704,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.67963293, -0.23340555,  0.67285376,  0.3822931 , -0.48156844,\n",
       "           0.        ,  1.        ],\n",
       "         [-0.77158249, -0.13348455, -1.41223614, -1.59694685,  0.56069429,\n",
       "           0.        ,  1.        ],\n",
       "         [ 1.32905593,  0.24157673, -1.54022185, -0.51109847,  0.25652038,\n",
       "           0.        ,  1.        ]])},\n",
       " 'Theta_43': {'Theta_21': array([[0.06205188, 0.        , 1.        ],\n",
       "         [1.36454804, 0.        , 1.        ]]),\n",
       "  'Theta_10': array([[ 0.41809559,  0.15969163,  0.        ,  1.        ],\n",
       "         [-0.91886862, -1.05311453,  0.        ,  1.        ],\n",
       "         [-0.49477395,  0.37034245,  0.        ,  1.        ]])}}"
      ]
     },
     "execution_count": 2446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vH.Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "6e931d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "e82c6934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 10, 14, 18, 22, 26, 30, 66, 70, 74, 78, 82, 86, 90, 94],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "0705bfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03474425, 0.12259404, 0.07261799, 0.03192831, 0.01891258,\n",
       "       0.06673248, 0.03952867, 0.07294342, 0.04320769, 0.15245704,\n",
       "       0.0903072 , 0.03970581, 0.02351955, 0.08298802, 0.04915756])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability[p_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "bc20c9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_index[np.argsort(I_index)[::-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "0033c5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(probability[p_i] * I_index[MI_i])[::-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "9cce970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(probability)[::-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "ba4b6de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
       "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
       "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
       "       248, 249, 250, 251, 252, 253, 254, 255], dtype=int64)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9ee2185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([254,  79,  92,  91,  90,  89,  88,  87,  86,  85,  84,  83,  82,\n",
       "        81,  80,  78,  94,  77,  76,  75,  74,  73,  72,  71,  70,  69,\n",
       "        68,  67,  66,  65,  93,  95,  63, 111, 124, 123, 122, 121, 120,\n",
       "       119, 118, 117, 116, 115, 114, 113, 112, 110,  96, 109, 108, 107,\n",
       "       106, 105, 104, 103, 102, 101, 100,  99,  98,  97,  64,  62, 126,\n",
       "        15,  28,  27,  26,  25,  24,  23,  22,  21,  20,  19,  18,  17,\n",
       "        16,  14,  30,  13,  12,  11,  10,   9,   8,   7,   6,   5,   4,\n",
       "         3,   2,   1,  29,  31,  61,  47,  60,  59,  58,  57,  56,  55,\n",
       "        54,  53,  52,  51,  50,  49,  48,  46,  32,  45,  44,  43,  42,\n",
       "        41,  40,  39,  38,  37,  36,  35,  34,  33, 125, 127, 253, 207,\n",
       "       220, 219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208,\n",
       "       206, 222, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195,\n",
       "       194, 193, 221, 223, 191, 239, 252, 251, 250, 249, 248, 247, 246,\n",
       "       245, 244, 243, 242, 241, 240, 238, 224, 237, 236, 235, 234, 233,\n",
       "       232, 231, 230, 229, 228, 227, 226, 225, 192, 190, 128, 143, 156,\n",
       "       155, 154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 142,\n",
       "       158, 141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130,\n",
       "       129, 157, 159, 189, 175, 188, 187, 186, 185, 184, 183, 182, 181,\n",
       "       180, 179, 178, 177, 176, 174, 160, 173, 172, 171, 170, 169, 168,\n",
       "       167, 166, 165, 164, 163, 162, 161,   0], dtype=int64)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(I_index)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "5b6b908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e619e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_index,probability,num = top_samples(q,precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "7c3c465e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7, 11, 15, 19, 23, 27, 31, 67, 71, 75, 79, 83, 87, 91, 95])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "becba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_index = np.where(np.abs(q.reshape(-1,) - 0.5) < precision)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "e99a4a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32254579, 0.05428628, 0.48723621, 0.14752703, 0.176383  ,\n",
       "       0.12800452, 0.2145599 , 0.22375744])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(q.reshape(-1,) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "039ed467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55428628],\n",
       "       [0.35247297],\n",
       "       [0.676383  ],\n",
       "       [0.37199548]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = q[np.where(np.abs(q.reshape(-1,) - 0.5) < precision)[0]]\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "42370ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb = binarilize(np.arange(2**4),4)\n",
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "222cb918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0586554 , 0.03474425, 0.12259404, 0.07261799, 0.03192831,\n",
       "       0.01891258, 0.06673248, 0.03952867, 0.07294342, 0.04320769,\n",
       "       0.15245704, 0.0903072 , 0.03970581, 0.02351955, 0.08298802,\n",
       "       0.04915756])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.prod(prob**comb * (1-prob)**(1-comb),axis=0)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8fec39b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb[:,np.argsort(p)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "2ec93168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01891258, 0.02351955, 0.03192831, 0.03474425, 0.03952867,\n",
       "       0.03970581, 0.04320769, 0.04915756, 0.0586554 , 0.06673248,\n",
       "       0.07261799, 0.07294342, 0.08298802, 0.0903072 , 0.12259404,\n",
       "       0.15245704])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "32560029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = (q+0.5).astype(int)\n",
    "z_all = np.repeat(z, 2**4, axis=1)\n",
    "z_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0b4aafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_all[var_index,:] = comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b0f34133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "19448542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17745421],\n",
       "       [0.55428628],\n",
       "       [0.01276379],\n",
       "       [0.35247297],\n",
       "       [0.676383  ],\n",
       "       [0.37199548],\n",
       "       [0.7145599 ],\n",
       "       [0.72375744]])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911d418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68f8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f56586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be40e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "5a9ad8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.e-08, 1.e-08, 1.e-08, ..., 1.e-08, 1.e-08, 0.e+00],\n",
       "       [1.e-08, 1.e-08, 1.e-08, ..., 1.e-08, 1.e-08, 0.e+00],\n",
       "       [1.e-08, 1.e-08, 1.e-08, ..., 1.e-08, 1.e-08, 0.e+00],\n",
       "       ...,\n",
       "       [1.e-08, 1.e-08, 1.e-08, ..., 1.e-08, 1.e-08, 0.e+00],\n",
       "       [1.e-08, 1.e-08, 1.e-08, ..., 1.e-08, 1.e-08, 0.e+00],\n",
       "       [1.e-08, 1.e-08, 1.e-08, ..., 1.e-08, 1.e-08, 0.e+00]])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=2**10\n",
    "m=2**8\n",
    "epsilon=1e-8\n",
    "MI = np.zeros((n+1,m+2))\n",
    "MI[:,:-1] += epsilon\n",
    "MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "cb0e11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI[-1,3] = 1\n",
    "MI[-1,-2] = 1\n",
    "Input = MI[[1,-1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fb9d682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Input[:,-2:-1]   #(2,1)\n",
    "H = Input[:,-1:]     #(2,1)\n",
    "C = s*(H - np.log(s)) + (s+1)*np.log(s+1)   #(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "013a128b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.94206807e-07],\n",
       "       [1.38629436e+00]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "5b7a09a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.94206807e-07, -1.94206807e-07, -1.94206807e-07,\n",
       "        -1.94206807e-07, -1.94206807e-07, -1.94206807e-07,\n",
       "        -1.94206807e-07, -1.94206807e-07],\n",
       "       [-1.94206807e-07, -1.94206807e-07, -1.38629436e+00,\n",
       "        -1.94206807e-07, -1.94206807e-07, -1.94206807e-07,\n",
       "        -1.94206807e-07, -1.94206807e-07]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = Input[:,:-2]     #(2,m)\n",
    "D = M*np.log(M) - (M+1)*np.log(M+1)     #(2,m)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "cfeba71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314708, 0.69314708, 0.        , 0.69314708, 0.69314708,\n",
       "       0.69314708, 0.69314708, 0.69314708])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_new = (C + D)/(s+1)\n",
    "I_diff = H_new[1,:] - H_new[0,:] - (H[1,:] - H[0,:])\n",
    "I_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "838b1d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.69314708, 0.69314708, 0.        , 0.69314708, 0.69314708,\n",
       "        0.69314708, 0.69314708, 0.69314708]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ce72c927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(I_diff > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "5fa07825",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_index, MI, H_new = mutual_info(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "8700208a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 256)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b009c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "205ec3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 100\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_data):\n",
    "    index = np.random.permutation(n_data)\n",
    "    d0 = dataset[:,index[i]:index[i]+1]\n",
    "    \n",
    "    phi = Phi[\"Phi_01\"]\n",
    "    q1 = sigmoid(np.matmul(phi[:,:-1],d0))\n",
    "    z1 = (q+0.5).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decimalize(d0)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2967d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a600f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f9ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5032148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34419041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912fd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbd57991",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd43e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 10000\n",
    "generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c12f6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,dataset)\n",
    "# distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,values_data) # for generated_dataset\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6be0ff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6733678714859438"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABS_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f0073bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19cf74b9a10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - values_d.size))] = values_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6afa7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.327,\n",
       " 'FN': array([20]),\n",
       " 'n_fn': 1,\n",
       " 'FP': array([[ 249,  250,  251, ..., 1021, 1022, 1023],\n",
       "        [   8,   14,    8, ...,    3,    8,    2]], dtype=int64),\n",
       " 'n_fp': 767}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5675dc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48498682"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90ed62",
   "metadata": {},
   "source": [
    "'percent': 0.5181, 0.5236; 0.5698,0.6098,0.6213\n",
    "\n",
    "n_fn': 4,0,0; 4,3,4\n",
    "\n",
    "'n_fp': 640,653,641; 615,619,599\n",
    "\n",
    "MSE: 0.3995,0.3457,0.9405; 0.8118,0.7598,0.7739"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
