{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6df78",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 4</span> \n",
    "\n",
    "# Helmholtz Machine Test on Random Set\n",
    "\n",
    "Instead of the toy model on well-formed set, we and more randomness to the dataset with Baysian mixture of Gaussians.\n",
    "\n",
    "*Created:* December 24, 2023\n",
    "\n",
    "*Updated:* December 24, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cb0071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  8,  7,  6,  5,  3,  1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[10,8,7,6,5,3,1]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30e8efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, Theta = ut.parameter_initialization(\"zero\",n_dz)  # \"zero\" or \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e0d756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_set = [1,0]\n",
    "activation_type = \"tanh\"\n",
    "bias = [False,False,True] # [instantiation bias, MLP bias,data bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8cda5cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 1, 0],\n",
       "       [0, 1, 1, ..., 1, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = n_dz[0,0]\n",
    "k = 2\n",
    "n_data = 500\n",
    "random_set = ut.random_generate(k,n,n_data,value_set)\n",
    "random_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9c0dd709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values,counts = np.unique(random_set, axis=1, return_counts = True)\n",
    "counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "010f8580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(np.arange(counts.size),counts)\n",
    "# ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d54d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 0., ..., 1., 1., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed_set = ut.well_formed_generate(n,value_set)\n",
    "well_formed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1929b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = random_set\n",
    "entire_set = ut.all_comb(n, value_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "17cdeb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_set = ut.reorder_all_comb(entire_set,dataset)\n",
    "reordered_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d2a5b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(generation,reordered_set,dataset):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    generation -- generated instances after training, numpy array of shape (n,n_sample), n is the length of input layer, \n",
    "    n_sample is the number of datapoints generated\n",
    "    reordered_set -- entire_set reordered as columns 0-k represents valid instances contained in the dataset, \n",
    "    columns k-2^n represents false instances not in the dataset. numpy array of shape (n,2^n)\n",
    "    dataset -- numpy array of shape (n,n_data), n_data is the number of datapoints.\n",
    "    \n",
    "    Returns:\n",
    "    distribution -- assigned category for generated samples based on reordered set, numpy array of shape (n_sample, )\n",
    "    data_dist -- assigned category for dataset  based on reordered set, numpy array of shape (n_data, )\n",
    "    statistics -- python dictionary with keys:\n",
    "        percent -- percentage of positive instances\n",
    "        n_fn -- number of false negative samples, missing evidence\n",
    "        FN -- position of false negative samples, numpy array of shape (k-n_fn, )\n",
    "        n_fp -- number of false positive samples, outliers\n",
    "        FP -- position and counts of false positive samples, numpy array of shape (2,n_fp)\n",
    "    MSE -- mean squared error between the generation Q and the data evidence P on the support of P (on positive instances only).\n",
    "    \"\"\"\n",
    "    n_sample = generation.shape[1]\n",
    "    n_data = dataset.shape[1]\n",
    "    distribution = np.zeros((n_sample, ),dtype = int)\n",
    "    for i in range(n_sample):\n",
    "        for j in range(reordered_set.shape[1]):\n",
    "            if np.array_equal(generation[:,i], reordered_set[:,j]):\n",
    "                distribution[i] = j\n",
    "                break\n",
    "    values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "    \n",
    "    data_dist = np.zeros((n_data, ),dtype = int)\n",
    "    for i in range(n_data):\n",
    "        for j in range(reordered_set.shape[1]):\n",
    "            if np.array_equal(dataset[:,i], reordered_set[:,j]):\n",
    "                data_dist[i] = j\n",
    "                break\n",
    "    values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "    k = counts_d.size\n",
    "    \n",
    "    \n",
    "    # statistics\n",
    "    percent = np.sum(counts_t[values_t < k])/n_sample\n",
    "    n_fn = k-values_t[values_t < k].size\n",
    "    FN = np.zeros((n_fn,),dtype = int)\n",
    "    dist_positive = np.array([values_t[values_t < k], counts_t[values_t < k]])\n",
    "    s = 0\n",
    "    values_t[values_t < k]\n",
    "    dist_values = np.append(np.append(-1,values_t[values_t < k]),k)   # append 0 and k in the range\n",
    "    \n",
    "    for i in range(dist_values.size-1):\n",
    "        diff = dist_values[i+1] - dist_values[i]\n",
    "        for j in range(1,diff):\n",
    "            FN[s] = dist_values[i]+j\n",
    "            dist_positive = np.append(dist_positive, np.array([[dist_values[i]+j],[0]]),axis = 1)\n",
    "            s += 1\n",
    "    dist_positive = np.unique(dist_positive,axis = 1)\n",
    "    n_fp = values_t[values_t >= k].size\n",
    "    FP = np.array([values_t[values_t >= k], counts_t[values_t >= k]])\n",
    "    statistics = {'percent': percent, 'FN': FN, 'n_fn':n_fn, 'FP': FP, 'n_fp':n_fp}\n",
    "    \n",
    "    # metric 2: distribution difference. Since our ditributions are discrete, we calculate a mean squared error (MSE) between \n",
    "    #           the generation Q and the data evidence P on the support of P (on positive instances only).\n",
    "    \n",
    "    counts_t = counts_t/n_sample*n_data  # distribution in the same scale as dataset\n",
    "    MSE = np.sum((dist_positive[1,:]/n_sample*n_data - counts_d)**2)/k\n",
    "    ABS_Error = np.abs(dist_positive[1,:]/n_sample*n_data - counts_d).sum()/k\n",
    "    \n",
    "    return distribution,data_dist,statistics, MSE,ABS_Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa978494",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b1e7a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 1000\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "64ba5f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.09 1.59 1.6  1.33 0.61 0.   7.22] Loss_P: [ 3.52  2.8   1.88  1.79  1.32  0.66 11.99]\n",
      "Loss_Q: [1.83 1.65 1.61 1.31 0.55 0.   6.95] Loss_P: [ 3.43  2.63  1.8   1.73  1.34  0.62 11.55]\n",
      "Loss_Q: [1.79 1.63 1.65 1.31 0.55 0.   6.94] Loss_P: [ 3.38  2.68  1.74  1.78  1.39  0.67 11.64]\n",
      "Loss_Q: [1.86 1.62 1.57 1.29 0.57 0.   6.91] Loss_P: [ 3.4   2.67  1.64  1.71  1.34  0.66 11.43]\n",
      "Loss_Q: [1.85 1.6  1.57 1.27 0.59 0.   6.87] Loss_P: [ 3.35  2.75  1.69  1.73  1.33  0.64 11.48]\n",
      "Loss_Q: [1.83 1.57 1.57 1.27 0.56 0.   6.8 ] Loss_P: [ 3.42  2.77  1.65  1.74  1.28  0.63 11.5 ]\n",
      "Loss_Q: [1.9  1.55 1.64 1.26 0.57 0.   6.93] Loss_P: [ 3.38  2.79  1.71  1.79  1.31  0.66 11.63]\n",
      "Loss_Q: [1.85 1.64 1.7  1.37 0.62 0.   7.17] Loss_P: [ 3.38  2.78  1.68  1.83  1.31  0.64 11.63]\n",
      "Loss_Q: [1.81 1.67 1.66 1.33 0.57 0.   7.05] Loss_P: [ 3.37  2.75  1.69  1.82  1.34  0.69 11.66]\n",
      "Loss_Q: [1.89 1.62 1.69 1.32 0.54 0.   7.06] Loss_P: [ 3.38  2.79  1.67  1.84  1.32  0.64 11.64]\n",
      "Loss_Q: [1.9  1.65 1.67 1.33 0.56 0.   7.11] Loss_P: [ 3.4   2.77  1.63  1.81  1.39  0.61 11.62]\n",
      "Loss_Q: [1.86 1.64 1.65 1.33 0.56 0.   7.04] Loss_P: [ 3.41  2.8   1.68  1.84  1.4   0.62 11.75]\n",
      "Loss_Q: [1.85 1.63 1.63 1.33 0.54 0.   6.99] Loss_P: [ 3.42  2.78  1.62  1.8   1.34  0.6  11.56]\n",
      "Loss_Q: [1.85 1.64 1.66 1.36 0.59 0.   7.1 ] Loss_P: [ 3.35  2.78  1.73  1.85  1.38  0.61 11.71]\n",
      "Loss_Q: [1.9  1.6  1.58 1.28 0.57 0.   6.92] Loss_P: [ 3.37  2.81  1.65  1.76  1.36  0.65 11.58]\n",
      "Loss_Q: [1.95 1.67 1.63 1.3  0.58 0.   7.14] Loss_P: [ 3.4   2.78  1.69  1.74  1.32  0.66 11.59]\n",
      "Loss_Q: [1.87 1.67 1.68 1.27 0.56 0.   7.06] Loss_P: [ 3.4   2.77  1.71  1.79  1.3   0.63 11.61]\n",
      "Loss_Q: [1.88 1.7  1.66 1.3  0.53 0.   7.08] Loss_P: [ 3.4   2.78  1.72  1.78  1.32  0.64 11.64]\n",
      "Loss_Q: [1.86 1.76 1.77 1.36 0.58 0.   7.33] Loss_P: [ 3.42  2.8   1.82  1.85  1.33  0.62 11.84]\n",
      "Loss_Q: [1.87 1.76 1.71 1.3  0.53 0.   7.18] Loss_P: [ 3.4   2.77  1.85  1.83  1.33  0.64 11.82]\n",
      "Loss_Q: [1.86 1.76 1.75 1.28 0.54 0.   7.19] Loss_P: [ 3.39  2.75  1.8   1.89  1.35  0.58 11.76]\n",
      "Loss_Q: [1.86 1.73 1.74 1.33 0.55 0.   7.22] Loss_P: [ 3.44  2.72  1.8   1.91  1.37  0.65 11.88]\n",
      "Loss_Q: [1.86 1.69 1.69 1.23 0.53 0.   7.  ] Loss_P: [ 3.4   2.72  1.84  1.85  1.32  0.61 11.75]\n",
      "Loss_Q: [1.85 1.73 1.69 1.23 0.54 0.   7.04] Loss_P: [ 3.39  2.8   1.82  1.87  1.28  0.6  11.76]\n",
      "Loss_Q: [1.96 1.7  1.72 1.25 0.54 0.   7.16] Loss_P: [ 3.39  2.73  1.82  1.83  1.29  0.58 11.64]\n",
      "Loss_Q: [1.9  1.68 1.74 1.23 0.5  0.   7.05] Loss_P: [ 3.34  2.76  1.8   1.89  1.31  0.6  11.7 ]\n",
      "Loss_Q: [1.89 1.7  1.74 1.23 0.52 0.   7.09] Loss_P: [ 3.44  2.73  1.76  1.86  1.24  0.59 11.63]\n",
      "Loss_Q: [1.91 1.72 1.77 1.23 0.56 0.   7.2 ] Loss_P: [ 3.45  2.78  1.76  1.85  1.28  0.64 11.75]\n",
      "Loss_Q: [1.88 1.68 1.64 1.27 0.55 0.   7.03] Loss_P: [ 3.44  2.68  1.74  1.79  1.28  0.6  11.53]\n",
      "Loss_Q: [1.9  1.72 1.69 1.23 0.55 0.   7.1 ] Loss_P: [ 3.44  2.72  1.84  1.81  1.31  0.66 11.78]\n",
      "Loss_Q: [1.8  1.74 1.77 1.27 0.59 0.   7.17] Loss_P: [ 3.47  2.67  1.75  1.86  1.22  0.64 11.61]\n",
      "Loss_Q: [1.85 1.79 1.8  1.24 0.59 0.   7.27] Loss_P: [ 3.43  2.69  1.84  1.89  1.25  0.68 11.78]\n",
      "Loss_Q: [1.85 1.73 1.75 1.23 0.6  0.   7.15] Loss_P: [ 3.41  2.75  1.86  2.01  1.3   0.7  12.04]\n",
      "Loss_Q: [1.86 1.72 1.72 1.21 0.6  0.   7.1 ] Loss_P: [ 3.46  2.71  1.86  1.87  1.25  0.7  11.85]\n",
      "Loss_Q: [1.89 1.75 1.75 1.23 0.6  0.   7.22] Loss_P: [ 3.45  2.76  1.8   1.88  1.25  0.66 11.8 ]\n",
      "Loss_Q: [1.93 1.76 1.73 1.27 0.58 0.   7.27] Loss_P: [ 3.41  2.72  1.8   1.85  1.33  0.67 11.77]\n",
      "Loss_Q: [1.88 1.71 1.73 1.25 0.61 0.   7.18] Loss_P: [ 3.47  2.72  1.75  1.83  1.28  0.66 11.71]\n",
      "Loss_Q: [1.9  1.75 1.69 1.23 0.61 0.   7.19] Loss_P: [ 3.45  2.67  1.84  1.87  1.32  0.68 11.84]\n",
      "Loss_Q: [1.88 1.83 1.73 1.24 0.59 0.   7.28] Loss_P: [ 3.44  2.65  1.86  1.91  1.27  0.7  11.83]\n",
      "Loss_Q: [1.95 1.79 1.77 1.26 0.61 0.   7.38] Loss_P: [ 3.45  2.69  1.83  1.89  1.3   0.68 11.84]\n",
      "Loss_Q: [1.9  1.76 1.73 1.22 0.63 0.   7.24] Loss_P: [ 3.48  2.69  1.79  1.85  1.24  0.69 11.75]\n",
      "Loss_Q: [1.96 1.71 1.7  1.28 0.64 0.   7.29] Loss_P: [ 3.47  2.72  1.81  1.8   1.22  0.69 11.71]\n",
      "Loss_Q: [1.94 1.76 1.71 1.27 0.64 0.   7.32] Loss_P: [ 3.51  2.72  1.85  1.85  1.28  0.74 11.96]\n",
      "Loss_Q: [1.89 1.76 1.63 1.29 0.64 0.   7.22] Loss_P: [ 3.5   2.69  1.82  1.76  1.27  0.75 11.79]\n",
      "Loss_Q: [1.86 1.7  1.66 1.26 0.67 0.   7.17] Loss_P: [ 3.5   2.63  1.82  1.8   1.31  0.75 11.81]\n",
      "Loss_Q: [1.83 1.71 1.64 1.24 0.69 0.   7.12] Loss_P: [ 3.48  2.55  1.8   1.79  1.27  0.77 11.66]\n",
      "Loss_Q: [1.76 1.68 1.62 1.28 0.71 0.   7.06] Loss_P: [ 3.5   2.57  1.75  1.77  1.25  0.77 11.62]\n",
      "Loss_Q: [1.86 1.63 1.62 1.25 0.69 0.   7.05] Loss_P: [ 3.52  2.63  1.74  1.7   1.2   0.77 11.56]\n",
      "Loss_Q: [1.84 1.6  1.61 1.18 0.72 0.   6.95] Loss_P: [ 3.43  2.67  1.74  1.74  1.16  0.78 11.52]\n",
      "Loss_Q: [1.8  1.69 1.59 1.23 0.71 0.   7.03] Loss_P: [ 3.56  2.57  1.68  1.68  1.2   0.76 11.44]\n",
      "Loss_Q: [1.83 1.58 1.55 1.23 0.69 0.   6.88] Loss_P: [ 3.52  2.56  1.7   1.68  1.24  0.75 11.45]\n",
      "Loss_Q: [1.79 1.68 1.6  1.19 0.66 0.   6.91] Loss_P: [ 3.51  2.6   1.75  1.72  1.18  0.75 11.51]\n",
      "Loss_Q: [1.77 1.67 1.55 1.1  0.66 0.   6.74] Loss_P: [ 3.46  2.65  1.71  1.68  1.13  0.72 11.36]\n",
      "Loss_Q: [1.87 1.6  1.55 1.11 0.69 0.   6.82] Loss_P: [ 3.47  2.66  1.68  1.64  1.11  0.74 11.29]\n",
      "Loss_Q: [1.82 1.58 1.51 1.09 0.67 0.   6.67] Loss_P: [ 3.47  2.68  1.72  1.6   1.14  0.74 11.35]\n",
      "Loss_Q: [1.82 1.67 1.61 1.09 0.67 0.   6.86] Loss_P: [ 3.52  2.64  1.69  1.63  1.12  0.72 11.33]\n",
      "Loss_Q: [1.77 1.64 1.59 1.08 0.68 0.   6.76] Loss_P: [ 3.54  2.57  1.81  1.68  1.11  0.74 11.45]\n",
      "Loss_Q: [1.77 1.66 1.61 1.08 0.66 0.   6.79] Loss_P: [ 3.5   2.59  1.76  1.7   1.11  0.75 11.41]\n",
      "Loss_Q: [1.79 1.66 1.61 1.07 0.7  0.   6.84] Loss_P: [ 3.53  2.54  1.73  1.7   1.07  0.71 11.28]\n",
      "Loss_Q: [1.74 1.67 1.6  1.09 0.67 0.   6.77] Loss_P: [ 3.52  2.54  1.78  1.74  1.14  0.75 11.46]\n",
      "Loss_Q: [1.73 1.69 1.56 1.07 0.68 0.   6.74] Loss_P: [ 3.5   2.53  1.77  1.64  1.11  0.72 11.26]\n",
      "Loss_Q: [1.79 1.7  1.57 1.14 0.66 0.   6.86] Loss_P: [ 3.46  2.53  1.78  1.69  1.11  0.73 11.3 ]\n",
      "Loss_Q: [1.77 1.7  1.57 1.13 0.66 0.   6.83] Loss_P: [ 3.45  2.62  1.79  1.73  1.15  0.75 11.48]\n",
      "Loss_Q: [1.81 1.78 1.6  1.13 0.63 0.   6.96] Loss_P: [ 3.48  2.58  1.87  1.72  1.14  0.72 11.51]\n",
      "Loss_Q: [1.82 1.87 1.57 1.11 0.64 0.   7.02] Loss_P: [ 3.44  2.63  1.94  1.76  1.12  0.73 11.61]\n",
      "Loss_Q: [1.82 1.86 1.62 1.13 0.67 0.   7.1 ] Loss_P: [ 3.43  2.58  1.92  1.65  1.08  0.71 11.38]\n",
      "Loss_Q: [1.8  1.76 1.54 1.11 0.67 0.   6.88] Loss_P: [ 3.44  2.54  1.92  1.72  1.17  0.73 11.51]\n",
      "Loss_Q: [1.82 1.8  1.62 1.17 0.71 0.   7.13] Loss_P: [ 3.36  2.67  1.89  1.73  1.14  0.71 11.49]\n",
      "Loss_Q: [1.85 1.87 1.61 1.13 0.66 0.   7.12] Loss_P: [ 3.37  2.68  1.96  1.8   1.21  0.73 11.75]\n",
      "Loss_Q: [1.85 1.85 1.69 1.12 0.66 0.   7.17] Loss_P: [ 3.34  2.74  1.95  1.85  1.15  0.73 11.77]\n",
      "Loss_Q: [1.82 1.85 1.7  1.1  0.68 0.   7.15] Loss_P: [ 3.37  2.7   1.91  1.84  1.07  0.74 11.62]\n",
      "Loss_Q: [1.88 1.82 1.64 1.07 0.67 0.   7.08] Loss_P: [ 3.4   2.71  1.91  1.85  1.06  0.75 11.67]\n",
      "Loss_Q: [1.9  1.78 1.61 1.09 0.65 0.   7.02] Loss_P: [ 3.38  2.73  1.95  1.79  1.12  0.75 11.71]\n",
      "Loss_Q: [1.91 1.9  1.72 1.1  0.67 0.   7.3 ] Loss_P: [ 3.42  2.68  1.95  1.84  1.06  0.73 11.69]\n",
      "Loss_Q: [1.89 1.87 1.69 1.04 0.64 0.   7.14] Loss_P: [ 3.38  2.71  1.92  1.83  1.03  0.71 11.58]\n",
      "Loss_Q: [1.86 1.84 1.7  1.06 0.64 0.   7.11] Loss_P: [ 3.44  2.73  1.93  1.83  1.05  0.7  11.68]\n",
      "Loss_Q: [1.82 1.8  1.62 1.04 0.67 0.   6.96] Loss_P: [ 3.41  2.78  1.93  1.86  1.09  0.73 11.8 ]\n",
      "Loss_Q: [1.85 1.86 1.65 1.06 0.67 0.   7.07] Loss_P: [ 3.41  2.77  1.88  1.77  1.11  0.7  11.64]\n",
      "Loss_Q: [1.83 1.82 1.64 1.2  0.67 0.   7.16] Loss_P: [ 3.44  2.65  2.    1.78  1.16  0.72 11.75]\n",
      "Loss_Q: [1.75 1.93 1.61 1.15 0.65 0.   7.09] Loss_P: [ 3.46  2.68  1.93  1.76  1.16  0.7  11.69]\n",
      "Loss_Q: [1.87 1.87 1.63 1.2  0.67 0.   7.25] Loss_P: [ 3.41  2.68  1.93  1.78  1.2   0.7  11.7 ]\n",
      "Loss_Q: [1.8  1.86 1.73 1.21 0.66 0.   7.26] Loss_P: [ 3.41  2.64  1.95  1.86  1.22  0.73 11.8 ]\n",
      "Loss_Q: [1.84 1.86 1.74 1.24 0.67 0.   7.34] Loss_P: [ 3.44  2.69  2.01  1.87  1.26  0.71 11.97]\n",
      "Loss_Q: [1.87 1.86 1.71 1.22 0.64 0.   7.3 ] Loss_P: [ 3.42  2.61  2.02  1.88  1.28  0.7  11.92]\n",
      "Loss_Q: [1.84 1.87 1.68 1.31 0.64 0.   7.34] Loss_P: [ 3.42  2.65  1.96  1.8   1.24  0.73 11.8 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.9  1.82 1.65 1.29 0.66 0.   7.32] Loss_P: [ 3.39  2.69  1.97  1.74  1.28  0.73 11.8 ]\n",
      "Loss_Q: [1.92 1.86 1.71 1.29 0.67 0.   7.44] Loss_P: [ 3.38  2.71  1.91  1.82  1.3   0.72 11.84]\n",
      "Loss_Q: [1.81 1.82 1.63 1.2  0.65 0.   7.11] Loss_P: [ 3.45  2.71  1.94  1.81  1.24  0.73 11.88]\n",
      "Loss_Q: [1.89 1.79 1.71 1.24 0.68 0.   7.31] Loss_P: [ 3.45  2.71  1.93  1.8   1.23  0.69 11.81]\n",
      "Loss_Q: [1.84 1.91 1.69 1.22 0.66 0.   7.32] Loss_P: [ 3.51  2.64  1.93  1.76  1.25  0.72 11.81]\n",
      "Loss_Q: [1.87 1.84 1.73 1.25 0.65 0.   7.35] Loss_P: [ 3.39  2.69  1.95  1.83  1.27  0.72 11.86]\n",
      "Loss_Q: [1.9  1.83 1.68 1.29 0.66 0.   7.37] Loss_P: [ 3.41  2.7   1.96  1.82  1.28  0.73 11.9 ]\n",
      "Loss_Q: [1.86 1.86 1.7  1.27 0.65 0.   7.35] Loss_P: [ 3.37  2.72  1.97  1.85  1.26  0.71 11.89]\n",
      "Loss_Q: [1.91 1.82 1.71 1.28 0.65 0.   7.37] Loss_P: [ 3.42  2.7   1.89  1.83  1.28  0.72 11.83]\n",
      "Loss_Q: [1.85 1.89 1.75 1.26 0.66 0.   7.41] Loss_P: [ 3.37  2.66  1.97  1.93  1.27  0.74 11.93]\n",
      "Loss_Q: [1.86 1.91 1.76 1.27 0.65 0.   7.46] Loss_P: [ 3.39  2.71  2.08  1.93  1.3   0.71 12.11]\n",
      "Loss_Q: [1.88 1.94 1.79 1.27 0.67 0.   7.55] Loss_P: [ 3.4   2.75  2.05  1.94  1.26  0.74 12.13]\n",
      "Loss_Q: [1.92 1.86 1.74 1.28 0.67 0.   7.47] Loss_P: [ 3.42  2.69  1.96  1.92  1.31  0.74 12.05]\n",
      "Loss_Q: [1.82 1.87 1.82 1.27 0.67 0.   7.45] Loss_P: [ 3.39  2.65  1.93  1.94  1.27  0.75 11.93]\n",
      "Loss_Q: [1.86 1.83 1.82 1.27 0.67 0.   7.45] Loss_P: [ 3.46  2.6   1.98  1.98  1.33  0.74 12.09]\n",
      "Loss_Q: [1.73 1.83 1.82 1.33 0.62 0.   7.34] Loss_P: [ 3.43  2.6   1.95  1.98  1.33  0.73 12.02]\n",
      "Loss_Q: [1.81 1.83 1.83 1.37 0.67 0.   7.51] Loss_P: [ 3.37  2.71  1.94  1.89  1.35  0.72 11.99]\n",
      "Loss_Q: [1.83 1.81 1.8  1.37 0.67 0.   7.48] Loss_P: [ 3.44  2.65  1.92  1.96  1.37  0.75 12.08]\n",
      "Loss_Q: [1.82 1.75 1.77 1.34 0.68 0.   7.36] Loss_P: [ 3.41  2.62  1.96  1.88  1.38  0.73 11.99]\n",
      "Loss_Q: [1.88 1.74 1.75 1.26 0.66 0.   7.29] Loss_P: [ 3.45  2.65  1.88  1.9   1.33  0.73 11.94]\n",
      "Loss_Q: [1.82 1.79 1.83 1.32 0.67 0.   7.44] Loss_P: [ 3.34  2.72  1.9   1.96  1.32  0.7  11.94]\n",
      "Loss_Q: [1.87 1.81 1.81 1.28 0.66 0.   7.44] Loss_P: [ 3.43  2.65  1.92  2.    1.38  0.72 12.1 ]\n",
      "Loss_Q: [1.89 1.77 1.87 1.34 0.65 0.   7.53] Loss_P: [ 3.34  2.69  2.    2.04  1.39  0.71 12.17]\n",
      "Loss_Q: [1.93 1.87 1.78 1.31 0.65 0.   7.53] Loss_P: [ 3.36  2.79  1.92  1.95  1.34  0.71 12.08]\n",
      "Loss_Q: [1.92 1.85 1.76 1.26 0.66 0.   7.44] Loss_P: [ 3.42  2.69  1.94  1.89  1.34  0.68 11.95]\n",
      "Loss_Q: [1.84 1.85 1.78 1.26 0.63 0.   7.35] Loss_P: [ 3.36  2.73  1.94  1.9   1.33  0.7  11.95]\n",
      "Loss_Q: [1.89 1.87 1.8  1.35 0.66 0.   7.57] Loss_P: [ 3.39  2.74  1.99  1.99  1.36  0.72 12.19]\n",
      "Loss_Q: [1.91 1.9  1.85 1.31 0.62 0.   7.59] Loss_P: [ 3.41  2.72  2.03  2.    1.37  0.69 12.22]\n",
      "Loss_Q: [1.84 1.91 1.85 1.31 0.64 0.   7.55] Loss_P: [ 3.41  2.66  2.01  2.03  1.38  0.65 12.15]\n",
      "Loss_Q: [1.85 1.81 1.8  1.31 0.62 0.   7.38] Loss_P: [ 3.39  2.68  1.88  2.    1.32  0.69 11.95]\n",
      "Loss_Q: [1.8  1.83 1.91 1.37 0.66 0.   7.56] Loss_P: [ 3.42  2.68  1.93  2.09  1.37  0.7  12.19]\n",
      "Loss_Q: [1.78 1.81 1.92 1.34 0.63 0.   7.49] Loss_P: [ 3.47  2.62  1.89  2.03  1.33  0.71 12.05]\n",
      "Loss_Q: [1.76 1.76 1.85 1.34 0.66 0.   7.36] Loss_P: [ 3.46  2.58  1.89  2.03  1.38  0.71 12.05]\n",
      "Loss_Q: [1.86 1.8  1.87 1.25 0.61 0.   7.39] Loss_P: [ 3.45  2.57  1.88  2.03  1.34  0.67 11.93]\n",
      "Loss_Q: [1.74 1.81 1.87 1.28 0.64 0.   7.33] Loss_P: [ 3.39  2.6   2.01  2.03  1.34  0.7  12.08]\n",
      "Loss_Q: [1.86 1.87 1.85 1.32 0.63 0.   7.54] Loss_P: [ 3.4   2.64  1.96  2.03  1.28  0.68 11.99]\n",
      "Loss_Q: [1.81 1.88 1.93 1.28 0.63 0.   7.53] Loss_P: [ 3.45  2.64  1.95  2.08  1.34  0.7  12.17]\n",
      "Loss_Q: [1.73 1.79 1.9  1.28 0.65 0.   7.35] Loss_P: [ 3.44  2.64  1.87  2.09  1.31  0.69 12.02]\n",
      "Loss_Q: [1.79 1.78 1.8  1.25 0.64 0.   7.25] Loss_P: [ 3.48  2.59  1.85  2.01  1.32  0.69 11.93]\n",
      "Loss_Q: [1.78 1.8  1.78 1.24 0.61 0.   7.21] Loss_P: [ 3.48  2.57  1.87  1.94  1.24  0.67 11.77]\n",
      "Loss_Q: [1.84 1.87 1.82 1.26 0.63 0.   7.42] Loss_P: [ 3.44  2.64  1.89  1.95  1.35  0.67 11.92]\n",
      "Loss_Q: [1.85 1.89 1.86 1.26 0.61 0.   7.48] Loss_P: [ 3.39  2.63  1.96  1.98  1.32  0.69 11.97]\n",
      "Loss_Q: [1.85 1.86 1.84 1.28 0.61 0.   7.44] Loss_P: [ 3.46  2.65  1.96  1.98  1.34  0.68 12.08]\n",
      "Loss_Q: [1.88 1.88 1.82 1.28 0.6  0.   7.46] Loss_P: [ 3.44  2.66  1.95  1.9   1.31  0.67 11.92]\n",
      "Loss_Q: [1.86 1.89 1.82 1.23 0.61 0.   7.4 ] Loss_P: [ 3.42  2.75  1.93  2.01  1.29  0.67 12.06]\n",
      "Loss_Q: [1.86 1.87 1.89 1.24 0.63 0.   7.49] Loss_P: [ 3.4   2.72  1.91  2.03  1.25  0.68 11.99]\n",
      "Loss_Q: [1.91 1.84 1.83 1.26 0.59 0.   7.43] Loss_P: [ 3.38  2.69  1.93  1.98  1.27  0.65 11.91]\n",
      "Loss_Q: [1.89 1.87 1.81 1.17 0.6  0.   7.34] Loss_P: [ 3.37  2.74  1.93  2.    1.24  0.64 11.92]\n",
      "Loss_Q: [1.81 1.79 1.84 1.15 0.61 0.   7.19] Loss_P: [ 3.4   2.73  1.84  1.93  1.2   0.66 11.75]\n",
      "Loss_Q: [1.88 1.81 1.81 1.19 0.6  0.   7.28] Loss_P: [ 3.4   2.73  1.9   1.93  1.21  0.64 11.8 ]\n",
      "Loss_Q: [1.91 1.81 1.82 1.2  0.6  0.   7.34] Loss_P: [ 3.37  2.73  1.89  1.9   1.24  0.65 11.79]\n",
      "Loss_Q: [1.87 1.76 1.79 1.22 0.6  0.   7.24] Loss_P: [ 3.38  2.74  1.9   1.91  1.26  0.68 11.86]\n",
      "Loss_Q: [1.83 1.81 1.85 1.25 0.6  0.   7.33] Loss_P: [ 3.44  2.65  1.92  1.98  1.26  0.64 11.89]\n",
      "Loss_Q: [1.9  1.79 1.83 1.21 0.57 0.   7.3 ] Loss_P: [ 3.39  2.71  1.91  1.96  1.25  0.63 11.86]\n",
      "Loss_Q: [1.79 1.83 1.81 1.21 0.56 0.   7.19] Loss_P: [ 3.41  2.69  1.89  2.04  1.26  0.64 11.94]\n",
      "Loss_Q: [1.78 1.83 1.83 1.19 0.55 0.   7.18] Loss_P: [ 3.47  2.65  1.93  1.98  1.26  0.6  11.9 ]\n",
      "Loss_Q: [1.82 1.75 1.87 1.18 0.58 0.   7.19] Loss_P: [ 3.42  2.63  1.86  2.02  1.27  0.63 11.83]\n",
      "Loss_Q: [1.88 1.73 1.93 1.26 0.58 0.   7.37] Loss_P: [ 3.44  2.68  1.86  2.07  1.27  0.66 11.97]\n",
      "Loss_Q: [1.82 1.68 1.95 1.28 0.6  0.   7.33] Loss_P: [ 3.4   2.69  1.81  2.07  1.31  0.66 11.93]\n",
      "Loss_Q: [1.85 1.72 1.94 1.3  0.61 0.   7.42] Loss_P: [ 3.39  2.71  1.81  2.07  1.32  0.64 11.93]\n",
      "Loss_Q: [1.92 1.77 1.97 1.34 0.6  0.   7.61] Loss_P: [ 3.41  2.75  1.78  2.08  1.32  0.65 11.99]\n",
      "Loss_Q: [1.88 1.7  1.93 1.22 0.6  0.   7.33] Loss_P: [ 3.41  2.72  1.77  2.06  1.28  0.65 11.89]\n",
      "Loss_Q: [1.86 1.69 1.9  1.22 0.61 0.   7.28] Loss_P: [ 3.41  2.72  1.79  1.98  1.27  0.65 11.82]\n",
      "Loss_Q: [1.87 1.72 1.91 1.21 0.6  0.   7.31] Loss_P: [ 3.41  2.77  1.81  2.05  1.27  0.63 11.94]\n",
      "Loss_Q: [1.9  1.71 1.94 1.23 0.59 0.   7.36] Loss_P: [ 3.36  2.8   1.84  2.05  1.25  0.65 11.94]\n",
      "Loss_Q: [1.85 1.72 1.97 1.2  0.61 0.   7.35] Loss_P: [ 3.37  2.75  1.79  2.1   1.19  0.69 11.89]\n",
      "Loss_Q: [1.89 1.77 1.96 1.18 0.58 0.   7.38] Loss_P: [ 3.39  2.73  1.83  2.04  1.21  0.63 11.82]\n",
      "Loss_Q: [1.85 1.78 1.93 1.18 0.55 0.   7.29] Loss_P: [ 3.39  2.74  1.9   2.1   1.17  0.62 11.92]\n",
      "Loss_Q: [1.91 1.83 1.92 1.13 0.59 0.   7.38] Loss_P: [ 3.44  2.77  1.97  2.09  1.18  0.65 12.1 ]\n",
      "Loss_Q: [1.83 1.87 1.91 1.16 0.58 0.   7.35] Loss_P: [ 3.37  2.8   1.96  2.12  1.2   0.64 12.09]\n",
      "Loss_Q: [1.86 1.8  1.93 1.18 0.59 0.   7.35] Loss_P: [ 3.37  2.78  1.84  2.04  1.16  0.64 11.83]\n",
      "Loss_Q: [1.82 1.71 1.94 1.15 0.58 0.   7.21] Loss_P: [ 3.44  2.69  1.85  2.12  1.15  0.65 11.9 ]\n",
      "Loss_Q: [1.85 1.74 1.94 1.15 0.56 0.   7.23] Loss_P: [ 3.41  2.67  1.86  2.08  1.21  0.62 11.85]\n",
      "Loss_Q: [1.8  1.75 1.95 1.23 0.56 0.   7.3 ] Loss_P: [ 3.38  2.74  1.83  2.07  1.22  0.61 11.85]\n",
      "Loss_Q: [1.84 1.73 1.92 1.17 0.58 0.   7.25] Loss_P: [ 3.38  2.72  1.75  2.01  1.17  0.61 11.65]\n",
      "Loss_Q: [1.85 1.72 1.98 1.13 0.57 0.   7.25] Loss_P: [ 3.42  2.71  1.82  2.03  1.19  0.64 11.81]\n",
      "Loss_Q: [1.85 1.8  1.95 1.19 0.57 0.   7.37] Loss_P: [ 3.41  2.7   1.83  2.06  1.21  0.59 11.8 ]\n",
      "Loss_Q: [1.83 1.77 1.93 1.21 0.57 0.   7.31] Loss_P: [ 3.39  2.73  1.84  2.07  1.23  0.59 11.86]\n",
      "Loss_Q: [1.83 1.72 1.88 1.18 0.53 0.   7.13] Loss_P: [ 3.39  2.72  1.77  2.04  1.21  0.6  11.73]\n",
      "Loss_Q: [1.9  1.73 1.88 1.21 0.53 0.   7.24] Loss_P: [ 3.37  2.77  1.85  1.98  1.25  0.57 11.79]\n",
      "Loss_Q: [1.87 1.8  1.92 1.18 0.48 0.   7.26] Loss_P: [ 3.4   2.69  1.94  2.04  1.27  0.56 11.9 ]\n",
      "Loss_Q: [1.84 1.76 1.93 1.17 0.54 0.   7.25] Loss_P: [ 3.36  2.73  1.84  2.04  1.24  0.59 11.8 ]\n",
      "Loss_Q: [1.79 1.67 1.91 1.09 0.5  0.   6.96] Loss_P: [ 3.35  2.75  1.85  2.03  1.18  0.56 11.73]\n",
      "Loss_Q: [1.86 1.76 1.93 1.12 0.55 0.   7.22] Loss_P: [ 3.32  2.76  1.83  2.05  1.14  0.6  11.7 ]\n",
      "Loss_Q: [1.85 1.8  1.98 1.21 0.56 0.   7.39] Loss_P: [ 3.39  2.78  1.87  2.1   1.24  0.61 11.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.81 1.71 1.93 1.16 0.6  0.   7.22] Loss_P: [ 3.38  2.77  1.79  2.14  1.22  0.64 11.94]\n",
      "Loss_Q: [1.85 1.72 1.99 1.2  0.59 0.   7.36] Loss_P: [ 3.33  2.79  1.75  2.09  1.19  0.64 11.8 ]\n",
      "Loss_Q: [1.9  1.74 1.99 1.12 0.59 0.   7.34] Loss_P: [ 3.39  2.79  1.84  2.15  1.2   0.64 12.01]\n",
      "Loss_Q: [1.91 1.68 1.98 1.24 0.61 0.   7.42] Loss_P: [ 3.37  2.78  1.77  2.14  1.25  0.66 11.96]\n",
      "Loss_Q: [1.87 1.74 2.   1.13 0.56 0.   7.3 ] Loss_P: [ 3.43  2.76  1.71  2.08  1.18  0.63 11.78]\n",
      "Loss_Q: [1.88 1.68 1.98 1.1  0.58 0.   7.23] Loss_P: [ 3.35  2.84  1.78  2.16  1.16  0.67 11.96]\n",
      "Loss_Q: [1.8  1.68 1.97 1.13 0.59 0.   7.17] Loss_P: [ 3.38  2.75  1.74  2.08  1.14  0.64 11.73]\n",
      "Loss_Q: [1.9  1.63 1.98 1.2  0.63 0.   7.33] Loss_P: [ 3.35  2.74  1.76  2.1   1.18  0.65 11.77]\n",
      "Loss_Q: [1.86 1.63 1.95 1.07 0.6  0.   7.12] Loss_P: [ 3.31  2.82  1.71  2.07  1.12  0.64 11.66]\n",
      "Loss_Q: [1.88 1.62 1.96 1.1  0.64 0.   7.19] Loss_P: [ 3.34  2.8   1.72  2.08  1.04  0.65 11.64]\n",
      "Loss_Q: [1.84 1.6  1.92 1.   0.62 0.   6.98] Loss_P: [ 3.33  2.81  1.72  2.07  1.08  0.66 11.68]\n",
      "Loss_Q: [1.86 1.61 1.93 1.05 0.6  0.   7.04] Loss_P: [ 3.34  2.78  1.7   2.03  1.05  0.65 11.55]\n",
      "Loss_Q: [1.82 1.63 1.88 1.01 0.61 0.   6.94] Loss_P: [ 3.38  2.77  1.69  2.    1.03  0.66 11.53]\n",
      "Loss_Q: [1.9  1.62 1.81 0.93 0.57 0.   6.83] Loss_P: [ 3.36  2.82  1.72  1.95  0.97  0.65 11.47]\n",
      "Loss_Q: [1.82 1.66 1.81 0.97 0.59 0.   6.85] Loss_P: [ 3.38  2.73  1.72  1.96  1.04  0.67 11.51]\n",
      "Loss_Q: [1.83 1.65 1.87 1.02 0.56 0.   6.93] Loss_P: [ 3.39  2.75  1.73  1.97  1.02  0.62 11.47]\n",
      "Loss_Q: [1.89 1.66 1.89 0.99 0.55 0.   6.98] Loss_P: [ 3.4   2.76  1.74  1.97  1.05  0.61 11.54]\n",
      "Loss_Q: [1.87 1.65 1.86 0.99 0.54 0.   6.91] Loss_P: [ 3.34  2.78  1.68  1.98  1.04  0.62 11.43]\n",
      "Loss_Q: [1.83 1.66 1.94 1.   0.53 0.   6.96] Loss_P: [ 3.39  2.78  1.69  2.05  1.01  0.58 11.49]\n",
      "Loss_Q: [1.84 1.61 2.   1.02 0.56 0.   7.03] Loss_P: [ 3.33  2.76  1.69  2.1   1.06  0.59 11.53]\n",
      "Loss_Q: [1.86 1.67 1.91 1.03 0.55 0.   7.01] Loss_P: [ 3.38  2.73  1.79  2.07  1.04  0.6  11.6 ]\n",
      "Loss_Q: [1.87 1.71 1.91 1.07 0.58 0.   7.13] Loss_P: [ 3.4   2.73  1.74  2.05  1.05  0.61 11.58]\n",
      "Loss_Q: [1.85 1.67 1.86 1.05 0.54 0.   6.96] Loss_P: [ 3.38  2.72  1.79  2.06  1.08  0.65 11.68]\n",
      "Loss_Q: [1.84 1.64 1.92 1.06 0.55 0.   7.01] Loss_P: [ 3.39  2.73  1.72  2.04  1.07  0.62 11.57]\n",
      "Loss_Q: [1.85 1.64 1.87 1.01 0.52 0.   6.89] Loss_P: [ 3.37  2.84  1.71  2.02  1.08  0.6  11.62]\n",
      "Loss_Q: [1.84 1.67 1.91 1.08 0.58 0.   7.09] Loss_P: [ 3.31  2.82  1.73  2.06  1.12  0.59 11.63]\n",
      "Loss_Q: [1.85 1.67 1.89 1.07 0.56 0.   7.03] Loss_P: [ 3.29  2.79  1.73  2.01  1.16  0.59 11.57]\n",
      "Loss_Q: [1.85 1.7  1.85 1.06 0.54 0.   6.99] Loss_P: [ 3.38  2.76  1.72  2.    1.11  0.58 11.56]\n",
      "Loss_Q: [1.83 1.64 1.94 1.08 0.5  0.   6.99] Loss_P: [ 3.33  2.77  1.74  2.08  1.16  0.59 11.66]\n",
      "Loss_Q: [1.83 1.7  1.91 1.13 0.51 0.   7.08] Loss_P: [ 3.35  2.67  1.77  2.06  1.16  0.58 11.6 ]\n",
      "Loss_Q: [1.85 1.62 1.88 1.12 0.52 0.   6.99] Loss_P: [ 3.3   2.73  1.71  2.    1.13  0.57 11.45]\n",
      "Loss_Q: [1.82 1.64 1.89 1.09 0.53 0.   6.97] Loss_P: [ 3.38  2.8   1.65  2.01  1.12  0.58 11.54]\n",
      "Loss_Q: [1.86 1.64 1.88 1.11 0.54 0.   7.03] Loss_P: [ 3.42  2.75  1.77  1.99  1.17  0.63 11.74]\n",
      "Loss_Q: [1.83 1.61 1.86 1.14 0.53 0.   6.97] Loss_P: [ 3.35  2.72  1.72  1.96  1.17  0.59 11.51]\n",
      "Loss_Q: [1.85 1.66 1.79 1.19 0.52 0.   7.01] Loss_P: [ 3.38  2.75  1.69  1.93  1.13  0.57 11.45]\n",
      "Loss_Q: [1.84 1.64 1.84 1.1  0.5  0.   6.91] Loss_P: [ 3.39  2.75  1.73  1.91  1.18  0.59 11.56]\n",
      "Loss_Q: [1.87 1.63 1.76 1.13 0.52 0.   6.92] Loss_P: [ 3.38  2.77  1.68  1.92  1.14  0.57 11.46]\n",
      "Loss_Q: [1.86 1.6  1.84 1.14 0.5  0.   6.94] Loss_P: [ 3.41  2.72  1.62  1.91  1.09  0.59 11.36]\n",
      "Loss_Q: [1.85 1.59 1.83 1.07 0.51 0.   6.85] Loss_P: [ 3.39  2.74  1.64  1.9   1.05  0.58 11.31]\n",
      "Loss_Q: [1.78 1.56 1.81 1.09 0.51 0.   6.74] Loss_P: [ 3.38  2.78  1.66  1.96  1.11  0.6  11.49]\n",
      "Loss_Q: [1.83 1.47 1.81 1.1  0.53 0.   6.73] Loss_P: [ 3.37  2.75  1.57  1.89  1.14  0.6  11.33]\n",
      "Loss_Q: [1.83 1.42 1.78 1.09 0.52 0.   6.63] Loss_P: [ 3.39  2.76  1.54  1.85  1.12  0.56 11.21]\n",
      "Loss_Q: [1.83 1.55 1.75 1.12 0.48 0.   6.72] Loss_P: [ 3.39  2.76  1.59  1.87  1.1   0.53 11.25]\n",
      "Loss_Q: [1.83 1.52 1.72 1.09 0.52 0.   6.68] Loss_P: [ 3.37  2.75  1.64  1.88  1.09  0.56 11.29]\n",
      "Loss_Q: [1.75 1.53 1.78 1.12 0.55 0.   6.73] Loss_P: [ 3.35  2.7   1.62  1.88  1.08  0.59 11.22]\n",
      "Loss_Q: [1.83 1.59 1.74 1.12 0.54 0.   6.82] Loss_P: [ 3.35  2.8   1.71  1.84  1.13  0.61 11.45]\n",
      "Loss_Q: [1.77 1.56 1.74 1.13 0.57 0.   6.76] Loss_P: [ 3.38  2.81  1.71  1.86  1.12  0.6  11.47]\n",
      "Loss_Q: [1.84 1.61 1.78 1.16 0.54 0.   6.93] Loss_P: [ 3.41  2.76  1.65  1.86  1.1   0.57 11.36]\n",
      "Loss_Q: [1.86 1.61 1.77 1.14 0.52 0.   6.91] Loss_P: [ 3.41  2.72  1.65  1.89  1.14  0.58 11.39]\n",
      "Loss_Q: [1.82 1.6  1.82 1.09 0.53 0.   6.86] Loss_P: [ 3.39  2.69  1.7   1.87  1.09  0.59 11.32]\n",
      "Loss_Q: [1.8  1.58 1.78 1.07 0.55 0.   6.79] Loss_P: [ 3.42  2.72  1.76  1.87  1.08  0.61 11.45]\n",
      "Loss_Q: [1.78 1.56 1.78 1.06 0.57 0.   6.75] Loss_P: [ 3.42  2.66  1.71  1.9   1.07  0.68 11.44]\n",
      "Loss_Q: [1.83 1.62 1.74 1.06 0.55 0.   6.8 ] Loss_P: [ 3.44  2.67  1.64  1.89  1.09  0.67 11.4 ]\n",
      "Loss_Q: [1.84 1.58 1.81 1.08 0.6  0.   6.9 ] Loss_P: [ 3.37  2.73  1.66  1.94  1.07  0.67 11.43]\n",
      "Loss_Q: [1.83 1.59 1.81 1.08 0.6  0.   6.9 ] Loss_P: [ 3.38  2.75  1.74  1.92  1.12  0.66 11.57]\n",
      "Loss_Q: [1.79 1.58 1.84 1.07 0.6  0.   6.88] Loss_P: [ 3.38  2.69  1.67  1.95  1.07  0.7  11.46]\n",
      "Loss_Q: [1.81 1.5  1.79 1.04 0.6  0.   6.75] Loss_P: [ 3.32  2.76  1.64  1.88  1.04  0.66 11.3 ]\n",
      "Loss_Q: [1.86 1.51 1.81 1.11 0.63 0.   6.91] Loss_P: [ 3.33  2.76  1.68  1.9   1.05  0.68 11.41]\n",
      "Loss_Q: [1.89 1.54 1.77 1.13 0.58 0.   6.91] Loss_P: [ 3.34  2.77  1.68  1.93  1.09  0.66 11.48]\n",
      "Loss_Q: [1.86 1.51 1.83 1.09 0.61 0.   6.89] Loss_P: [ 3.38  2.84  1.65  1.91  1.06  0.65 11.48]\n",
      "Loss_Q: [1.87 1.55 1.83 1.13 0.62 0.   7.  ] Loss_P: [ 3.39  2.74  1.65  1.95  1.1   0.69 11.51]\n",
      "Loss_Q: [1.87 1.51 1.78 1.06 0.56 0.   6.78] Loss_P: [ 3.42  2.72  1.64  1.9   1.08  0.65 11.41]\n",
      "Loss_Q: [1.81 1.53 1.81 1.07 0.58 0.   6.8 ] Loss_P: [ 3.42  2.75  1.69  1.96  1.09  0.7  11.61]\n",
      "Loss_Q: [1.83 1.53 1.79 1.07 0.55 0.   6.77] Loss_P: [ 3.4   2.74  1.59  2.    1.08  0.64 11.47]\n",
      "Loss_Q: [1.77 1.45 1.79 1.09 0.54 0.   6.64] Loss_P: [ 3.36  2.83  1.61  1.95  1.08  0.63 11.45]\n",
      "Loss_Q: [1.83 1.48 1.83 1.08 0.57 0.   6.79] Loss_P: [ 3.36  2.76  1.56  1.98  1.07  0.65 11.39]\n",
      "Loss_Q: [1.81 1.52 1.74 1.06 0.6  0.   6.74] Loss_P: [ 3.43  2.76  1.61  1.81  1.07  0.66 11.32]\n",
      "Loss_Q: [1.84 1.49 1.73 1.05 0.58 0.   6.69] Loss_P: [ 3.31  2.8   1.64  1.8   1.09  0.65 11.29]\n",
      "Loss_Q: [1.85 1.48 1.78 1.02 0.58 0.   6.71] Loss_P: [ 3.36  2.79  1.64  1.88  1.02  0.67 11.36]\n",
      "Loss_Q: [1.84 1.57 1.75 0.98 0.57 0.   6.7 ] Loss_P: [ 3.41  2.8   1.57  1.85  1.01  0.66 11.32]\n",
      "Loss_Q: [1.86 1.47 1.75 0.98 0.6  0.   6.66] Loss_P: [ 3.4   2.79  1.61  1.81  1.04  0.7  11.35]\n",
      "Loss_Q: [1.86 1.49 1.78 1.02 0.58 0.   6.74] Loss_P: [ 3.38  2.77  1.59  1.87  1.03  0.7  11.34]\n",
      "Loss_Q: [1.83 1.49 1.85 1.   0.61 0.   6.78] Loss_P: [ 3.47  2.67  1.62  1.9   1.04  0.68 11.37]\n",
      "Loss_Q: [1.8  1.52 1.92 1.02 0.63 0.   6.89] Loss_P: [ 3.4   2.74  1.69  2.06  1.07  0.69 11.64]\n",
      "Loss_Q: [1.77 1.57 1.91 1.02 0.63 0.   6.9 ] Loss_P: [ 3.42  2.71  1.62  2.05  1.05  0.72 11.57]\n",
      "Loss_Q: [1.78 1.59 1.96 0.98 0.67 0.   6.98] Loss_P: [ 3.43  2.7   1.74  2.05  1.02  0.74 11.67]\n",
      "Loss_Q: [1.8  1.58 1.95 1.03 0.66 0.   7.02] Loss_P: [ 3.35  2.73  1.71  2.05  1.01  0.73 11.58]\n",
      "Loss_Q: [1.8  1.59 1.88 1.06 0.66 0.   6.99] Loss_P: [ 3.4   2.65  1.65  2.02  1.03  0.73 11.48]\n",
      "Loss_Q: [1.8  1.61 1.91 1.   0.66 0.   6.98] Loss_P: [ 3.4   2.72  1.65  2.02  0.98  0.71 11.49]\n",
      "Loss_Q: [1.8  1.58 1.88 0.97 0.67 0.   6.9 ] Loss_P: [ 3.38  2.7   1.73  2.03  1.04  0.76 11.65]\n",
      "Loss_Q: [1.83 1.58 1.82 1.   0.65 0.   6.88] Loss_P: [ 3.35  2.75  1.72  2.02  1.    0.75 11.59]\n",
      "Loss_Q: [1.89 1.57 1.85 0.94 0.62 0.   6.87] Loss_P: [ 3.34  2.8   1.63  2.01  0.96  0.71 11.45]\n",
      "Loss_Q: [1.84 1.53 1.9  0.95 0.64 0.   6.86] Loss_P: [ 3.37  2.8   1.71  2.03  0.94  0.7  11.55]\n",
      "Loss_Q: [1.87 1.6  1.85 0.89 0.62 0.   6.83] Loss_P: [ 3.43  2.76  1.71  1.99  0.95  0.69 11.52]\n",
      "Loss_Q: [1.89 1.55 1.86 0.99 0.61 0.   6.91] Loss_P: [ 3.41  2.74  1.72  2.01  0.99  0.7  11.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.82 1.56 1.94 0.99 0.63 0.   6.93] Loss_P: [ 3.37  2.71  1.74  1.99  0.97  0.69 11.47]\n",
      "Loss_Q: [1.88 1.59 1.87 0.93 0.63 0.   6.9 ] Loss_P: [ 3.45  2.74  1.7   1.97  0.93  0.73 11.53]\n",
      "Loss_Q: [1.9  1.61 1.79 0.96 0.65 0.   6.91] Loss_P: [ 3.38  2.75  1.69  1.91  1.    0.72 11.46]\n",
      "Loss_Q: [1.85 1.52 1.74 0.94 0.62 0.   6.68] Loss_P: [ 3.39  2.73  1.66  1.92  0.97  0.72 11.39]\n",
      "Loss_Q: [1.85 1.6  1.83 0.98 0.64 0.   6.9 ] Loss_P: [ 3.45  2.69  1.79  1.97  0.95  0.7  11.55]\n",
      "Loss_Q: [1.81 1.6  1.86 1.   0.64 0.   6.91] Loss_P: [ 3.37  2.71  1.65  1.93  0.95  0.71 11.32]\n",
      "Loss_Q: [1.8  1.55 1.86 0.99 0.63 0.   6.84] Loss_P: [ 3.46  2.71  1.66  1.99  0.98  0.69 11.5 ]\n",
      "Loss_Q: [1.81 1.57 1.84 0.98 0.62 0.   6.83] Loss_P: [ 3.38  2.71  1.67  1.97  0.91  0.66 11.31]\n",
      "Loss_Q: [1.84 1.58 1.88 0.93 0.6  0.   6.84] Loss_P: [ 3.4   2.68  1.66  1.97  1.02  0.69 11.42]\n",
      "Loss_Q: [1.83 1.53 1.89 0.97 0.61 0.   6.83] Loss_P: [ 3.39  2.67  1.65  1.99  0.99  0.68 11.37]\n",
      "Loss_Q: [1.86 1.56 1.9  1.05 0.59 0.   6.96] Loss_P: [ 3.39  2.73  1.68  2.03  1.04  0.66 11.52]\n",
      "Loss_Q: [1.89 1.56 1.87 1.03 0.59 0.   6.94] Loss_P: [ 3.37  2.77  1.65  2.05  1.06  0.64 11.55]\n",
      "Loss_Q: [1.85 1.56 1.87 1.04 0.54 0.   6.87] Loss_P: [ 3.41  2.71  1.61  1.98  1.06  0.63 11.42]\n",
      "Loss_Q: [1.82 1.57 1.84 1.02 0.56 0.   6.82] Loss_P: [ 3.45  2.69  1.65  2.03  1.06  0.61 11.5 ]\n",
      "Loss_Q: [1.88 1.57 1.84 1.01 0.54 0.   6.85] Loss_P: [ 3.5   2.65  1.66  1.97  1.08  0.6  11.44]\n",
      "Loss_Q: [1.75 1.56 1.79 1.06 0.54 0.   6.69] Loss_P: [ 3.36  2.66  1.68  1.98  0.98  0.56 11.21]\n",
      "Loss_Q: [1.8  1.6  1.75 1.06 0.5  0.   6.7 ] Loss_P: [ 3.44  2.63  1.7   1.93  1.04  0.56 11.3 ]\n",
      "Loss_Q: [1.78 1.57 1.82 1.02 0.51 0.   6.69] Loss_P: [ 3.41  2.65  1.65  1.9   1.03  0.56 11.2 ]\n",
      "Loss_Q: [1.82 1.57 1.78 1.   0.49 0.   6.66] Loss_P: [ 3.42  2.67  1.64  1.92  1.04  0.58 11.27]\n",
      "Loss_Q: [1.8  1.56 1.75 1.07 0.53 0.   6.7 ] Loss_P: [ 3.4   2.72  1.59  1.9   1.04  0.58 11.24]\n",
      "Loss_Q: [1.8  1.59 1.8  1.05 0.56 0.   6.8 ] Loss_P: [ 3.42  2.66  1.64  1.92  1.02  0.61 11.26]\n",
      "Loss_Q: [1.78 1.56 1.78 1.04 0.61 0.   6.78] Loss_P: [ 3.36  2.68  1.61  1.92  1.05  0.61 11.24]\n",
      "Loss_Q: [1.79 1.52 1.73 1.02 0.55 0.   6.62] Loss_P: [ 3.39  2.73  1.58  1.87  1.06  0.63 11.25]\n",
      "Loss_Q: [1.83 1.46 1.68 1.   0.56 0.   6.54] Loss_P: [ 3.43  2.72  1.59  1.79  1.    0.6  11.15]\n",
      "Loss_Q: [1.86 1.48 1.73 1.03 0.55 0.   6.64] Loss_P: [ 3.42  2.65  1.58  1.88  1.03  0.62 11.18]\n",
      "Loss_Q: [1.77 1.46 1.72 0.96 0.55 0.   6.45] Loss_P: [ 3.38  2.65  1.57  1.88  0.94  0.64 11.06]\n",
      "Loss_Q: [1.86 1.45 1.69 0.99 0.54 0.   6.53] Loss_P: [ 3.39  2.61  1.62  1.86  1.04  0.6  11.13]\n",
      "Loss_Q: [1.78 1.5  1.73 0.98 0.54 0.   6.54] Loss_P: [ 3.46  2.62  1.58  1.85  1.    0.61 11.13]\n",
      "Loss_Q: [1.81 1.56 1.77 1.01 0.56 0.   6.71] Loss_P: [ 3.37  2.62  1.62  1.91  1.04  0.6  11.17]\n",
      "Loss_Q: [1.84 1.49 1.74 0.98 0.54 0.   6.59] Loss_P: [ 3.39  2.71  1.61  1.86  1.    0.6  11.16]\n",
      "Loss_Q: [1.83 1.48 1.8  0.99 0.54 0.   6.64] Loss_P: [ 3.35  2.7   1.62  1.97  1.    0.58 11.21]\n",
      "Loss_Q: [1.84 1.46 1.84 1.02 0.5  0.   6.67] Loss_P: [ 3.43  2.69  1.58  1.97  1.06  0.58 11.3 ]\n",
      "Loss_Q: [1.84 1.48 1.83 1.06 0.53 0.   6.75] Loss_P: [ 3.4   2.69  1.52  1.92  1.05  0.59 11.17]\n",
      "Loss_Q: [1.79 1.45 1.92 1.03 0.52 0.   6.7 ] Loss_P: [ 3.38  2.67  1.61  1.99  1.01  0.58 11.23]\n",
      "Loss_Q: [1.87 1.52 1.84 0.95 0.49 0.   6.67] Loss_P: [ 3.41  2.68  1.6   2.    0.98  0.59 11.25]\n",
      "Loss_Q: [1.87 1.5  1.81 0.96 0.53 0.   6.68] Loss_P: [ 3.36  2.76  1.68  1.98  0.97  0.57 11.32]\n",
      "Loss_Q: [1.86 1.56 1.78 1.01 0.49 0.   6.69] Loss_P: [ 3.31  2.73  1.69  1.88  1.    0.52 11.13]\n",
      "Loss_Q: [1.85 1.52 1.74 1.   0.51 0.   6.62] Loss_P: [ 3.36  2.73  1.69  1.85  1.    0.56 11.21]\n",
      "Loss_Q: [1.86 1.53 1.74 1.02 0.49 0.   6.62] Loss_P: [ 3.34  2.72  1.65  1.87  1.04  0.54 11.17]\n",
      "Loss_Q: [1.82 1.56 1.73 1.02 0.45 0.   6.58] Loss_P: [ 3.42  2.66  1.68  1.88  1.06  0.53 11.22]\n",
      "Loss_Q: [1.8  1.58 1.67 1.03 0.5  0.   6.58] Loss_P: [ 3.44  2.59  1.69  1.87  1.11  0.56 11.26]\n",
      "Loss_Q: [1.8  1.54 1.72 1.08 0.52 0.   6.66] Loss_P: [ 3.44  2.61  1.66  1.78  1.08  0.56 11.13]\n",
      "Loss_Q: [1.77 1.54 1.7  1.06 0.54 0.   6.61] Loss_P: [ 3.48  2.59  1.68  1.81  1.09  0.6  11.25]\n",
      "Loss_Q: [1.75 1.55 1.69 1.06 0.54 0.   6.59] Loss_P: [ 3.42  2.63  1.66  1.81  1.06  0.6  11.19]\n",
      "Loss_Q: [1.76 1.57 1.7  1.03 0.53 0.   6.58] Loss_P: [ 3.44  2.6   1.71  1.86  1.07  0.59 11.26]\n",
      "Loss_Q: [1.81 1.65 1.73 1.07 0.52 0.   6.78] Loss_P: [ 3.48  2.59  1.72  1.79  1.08  0.6  11.26]\n",
      "Loss_Q: [1.78 1.67 1.67 1.07 0.54 0.   6.71] Loss_P: [ 3.42  2.62  1.75  1.81  1.14  0.61 11.35]\n",
      "Loss_Q: [1.81 1.58 1.67 1.1  0.53 0.   6.7 ] Loss_P: [ 3.47  2.58  1.71  1.89  1.14  0.63 11.41]\n",
      "Loss_Q: [1.81 1.65 1.7  1.06 0.51 0.   6.72] Loss_P: [ 3.46  2.6   1.71  1.85  1.11  0.62 11.34]\n",
      "Loss_Q: [1.78 1.6  1.73 1.08 0.52 0.   6.7 ] Loss_P: [ 3.43  2.64  1.71  1.87  1.09  0.6  11.34]\n",
      "Loss_Q: [1.77 1.64 1.72 1.09 0.48 0.   6.7 ] Loss_P: [ 3.45  2.64  1.76  1.83  1.12  0.57 11.37]\n",
      "Loss_Q: [1.77 1.62 1.72 1.07 0.53 0.   6.71] Loss_P: [ 3.49  2.6   1.76  1.82  1.05  0.56 11.27]\n",
      "Loss_Q: [1.81 1.61 1.69 1.1  0.51 0.   6.72] Loss_P: [ 3.4   2.64  1.69  1.78  1.11  0.54 11.16]\n",
      "Loss_Q: [1.83 1.6  1.67 1.08 0.5  0.   6.67] Loss_P: [ 3.43  2.65  1.7   1.76  1.1   0.55 11.19]\n",
      "Loss_Q: [1.79 1.62 1.66 1.06 0.46 0.   6.6 ] Loss_P: [ 3.42  2.67  1.79  1.76  1.06  0.5  11.2 ]\n",
      "Loss_Q: [1.84 1.66 1.66 1.08 0.51 0.   6.75] Loss_P: [ 3.44  2.64  1.79  1.79  1.09  0.56 11.3 ]\n",
      "Loss_Q: [1.82 1.71 1.66 1.08 0.53 0.   6.81] Loss_P: [ 3.43  2.71  1.83  1.78  1.11  0.61 11.47]\n",
      "Loss_Q: [1.89 1.72 1.66 1.09 0.51 0.   6.86] Loss_P: [ 3.42  2.66  1.82  1.77  1.06  0.57 11.29]\n",
      "Loss_Q: [1.84 1.7  1.65 1.02 0.47 0.   6.68] Loss_P: [ 3.42  2.66  1.8   1.74  1.05  0.58 11.25]\n",
      "Loss_Q: [1.85 1.7  1.65 1.06 0.49 0.   6.75] Loss_P: [ 3.47  2.6   1.84  1.77  1.1   0.55 11.32]\n",
      "Loss_Q: [1.87 1.67 1.66 1.1  0.52 0.   6.82] Loss_P: [ 3.4   2.67  1.8   1.79  1.07  0.57 11.3 ]\n",
      "Loss_Q: [1.85 1.65 1.73 1.03 0.48 0.   6.75] Loss_P: [ 3.42  2.63  1.77  1.77  1.06  0.58 11.23]\n",
      "Loss_Q: [1.8  1.63 1.65 1.05 0.47 0.   6.61] Loss_P: [ 3.47  2.57  1.78  1.81  1.09  0.57 11.3 ]\n",
      "Loss_Q: [1.77 1.72 1.69 1.12 0.5  0.   6.81] Loss_P: [ 3.47  2.54  1.77  1.81  1.1   0.58 11.28]\n",
      "Loss_Q: [1.82 1.66 1.68 1.07 0.51 0.   6.73] Loss_P: [ 3.49  2.55  1.82  1.81  1.1   0.57 11.34]\n",
      "Loss_Q: [1.74 1.65 1.68 1.1  0.52 0.   6.69] Loss_P: [ 3.44  2.61  1.77  1.79  1.1   0.56 11.27]\n",
      "Loss_Q: [1.83 1.65 1.6  1.13 0.51 0.   6.72] Loss_P: [ 3.49  2.59  1.82  1.75  1.16  0.61 11.4 ]\n",
      "Loss_Q: [1.81 1.68 1.61 1.09 0.53 0.   6.73] Loss_P: [ 3.49  2.57  1.81  1.71  1.13  0.57 11.28]\n",
      "Loss_Q: [1.82 1.7  1.55 1.06 0.45 0.   6.58] Loss_P: [ 3.42  2.63  1.81  1.71  1.15  0.56 11.3 ]\n",
      "Loss_Q: [1.89 1.69 1.59 1.15 0.51 0.   6.83] Loss_P: [ 3.42  2.66  1.83  1.67  1.11  0.55 11.23]\n",
      "Loss_Q: [1.8  1.73 1.58 1.13 0.54 0.   6.78] Loss_P: [ 3.35  2.64  1.78  1.69  1.11  0.53 11.1 ]\n",
      "Loss_Q: [1.82 1.74 1.54 1.09 0.54 0.   6.72] Loss_P: [ 3.41  2.58  1.89  1.7   1.16  0.6  11.35]\n",
      "Loss_Q: [1.84 1.72 1.57 1.11 0.54 0.   6.79] Loss_P: [ 3.38  2.65  1.89  1.6   1.11  0.58 11.22]\n",
      "Loss_Q: [1.81 1.74 1.53 1.1  0.53 0.   6.71] Loss_P: [ 3.43  2.63  1.88  1.65  1.15  0.61 11.36]\n",
      "Loss_Q: [1.79 1.79 1.52 1.07 0.51 0.   6.67] Loss_P: [ 3.44  2.6   1.9   1.64  1.15  0.58 11.31]\n",
      "Loss_Q: [1.74 1.75 1.53 1.08 0.47 0.   6.58] Loss_P: [ 3.45  2.53  1.94  1.62  1.15  0.57 11.27]\n",
      "Loss_Q: [1.73 1.71 1.49 1.09 0.52 0.   6.54] Loss_P: [ 3.48  2.48  1.86  1.63  1.16  0.6  11.2 ]\n",
      "Loss_Q: [1.81 1.8  1.59 1.13 0.57 0.   6.88] Loss_P: [ 3.45  2.47  1.96  1.6   1.11  0.59 11.18]\n",
      "Loss_Q: [1.8  1.84 1.59 1.12 0.57 0.   6.92] Loss_P: [ 3.43  2.56  2.01  1.73  1.17  0.64 11.54]\n",
      "Loss_Q: [1.82 1.86 1.58 1.09 0.54 0.   6.88] Loss_P: [ 3.51  2.51  1.98  1.74  1.11  0.63 11.47]\n",
      "Loss_Q: [1.74 1.82 1.56 1.09 0.54 0.   6.75] Loss_P: [ 3.47  2.47  1.91  1.66  1.08  0.62 11.21]\n",
      "Loss_Q: [1.8  1.84 1.65 1.06 0.52 0.   6.86] Loss_P: [ 3.43  2.5   1.99  1.74  1.15  0.62 11.43]\n",
      "Loss_Q: [1.77 1.81 1.59 1.11 0.57 0.   6.85] Loss_P: [ 3.46  2.48  2.    1.66  1.1   0.63 11.34]\n",
      "Loss_Q: [1.74 1.92 1.56 1.11 0.58 0.   6.91] Loss_P: [ 3.46  2.49  2.03  1.63  1.1   0.66 11.37]\n",
      "Loss_Q: [1.74 1.87 1.55 1.07 0.56 0.   6.79] Loss_P: [ 3.47  2.49  2.    1.65  1.14  0.61 11.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.76 1.77 1.59 1.11 0.6  0.   6.83] Loss_P: [ 3.49  2.53  1.91  1.67  1.12  0.63 11.35]\n",
      "Loss_Q: [1.82 1.69 1.56 1.06 0.53 0.   6.67] Loss_P: [ 3.44  2.62  1.85  1.64  1.12  0.62 11.29]\n",
      "Loss_Q: [1.78 1.65 1.57 1.14 0.5  0.   6.65] Loss_P: [ 3.47  2.62  1.81  1.66  1.2   0.58 11.34]\n",
      "Loss_Q: [1.82 1.67 1.53 1.13 0.5  0.   6.65] Loss_P: [ 3.41  2.64  1.8   1.68  1.17  0.58 11.28]\n",
      "Loss_Q: [1.82 1.59 1.57 1.14 0.5  0.   6.63] Loss_P: [ 3.42  2.67  1.77  1.69  1.17  0.6  11.32]\n",
      "Loss_Q: [1.79 1.66 1.68 1.13 0.54 0.   6.8 ] Loss_P: [ 3.39  2.62  1.8   1.73  1.17  0.61 11.32]\n",
      "Loss_Q: [1.81 1.59 1.63 1.1  0.55 0.   6.67] Loss_P: [ 3.49  2.6   1.69  1.7   1.19  0.6  11.27]\n",
      "Loss_Q: [1.8  1.54 1.64 1.1  0.54 0.   6.61] Loss_P: [ 3.4   2.62  1.67  1.7   1.16  0.57 11.12]\n",
      "Loss_Q: [1.75 1.6  1.68 1.12 0.57 0.   6.73] Loss_P: [ 3.47  2.66  1.69  1.74  1.17  0.66 11.4 ]\n",
      "Loss_Q: [1.76 1.55 1.69 1.16 0.59 0.   6.75] Loss_P: [ 3.46  2.57  1.68  1.76  1.2   0.62 11.3 ]\n",
      "Loss_Q: [1.8  1.57 1.63 1.14 0.59 0.   6.73] Loss_P: [ 3.43  2.59  1.74  1.7   1.21  0.67 11.35]\n",
      "Loss_Q: [1.79 1.58 1.69 1.12 0.56 0.   6.75] Loss_P: [ 3.43  2.6   1.75  1.76  1.17  0.64 11.35]\n",
      "Loss_Q: [1.73 1.59 1.64 1.1  0.57 0.   6.63] Loss_P: [ 3.45  2.62  1.72  1.79  1.2   0.65 11.43]\n",
      "Loss_Q: [1.81 1.59 1.7  1.16 0.56 0.   6.82] Loss_P: [ 3.46  2.57  1.74  1.78  1.22  0.63 11.4 ]\n",
      "Loss_Q: [1.78 1.62 1.72 1.18 0.53 0.   6.83] Loss_P: [ 3.42  2.63  1.72  1.78  1.26  0.61 11.41]\n",
      "Loss_Q: [1.81 1.66 1.69 1.16 0.5  0.   6.82] Loss_P: [ 3.48  2.6   1.82  1.78  1.24  0.62 11.54]\n",
      "Loss_Q: [1.77 1.65 1.7  1.18 0.59 0.   6.88] Loss_P: [ 3.46  2.59  1.74  1.8   1.2   0.63 11.43]\n",
      "Loss_Q: [1.78 1.64 1.71 1.15 0.54 0.   6.82] Loss_P: [ 3.43  2.66  1.72  1.78  1.19  0.6  11.39]\n",
      "Loss_Q: [1.79 1.66 1.73 1.23 0.56 0.   6.98] Loss_P: [ 3.44  2.66  1.8   1.83  1.18  0.6  11.52]\n",
      "Loss_Q: [1.81 1.7  1.79 1.14 0.53 0.   6.98] Loss_P: [ 3.44  2.67  1.81  1.86  1.18  0.62 11.58]\n",
      "Loss_Q: [1.86 1.67 1.73 1.15 0.53 0.   6.93] Loss_P: [ 3.44  2.63  1.79  1.83  1.2   0.6  11.49]\n",
      "Loss_Q: [1.8  1.63 1.71 1.14 0.52 0.   6.81] Loss_P: [ 3.4   2.65  1.74  1.79  1.19  0.58 11.36]\n",
      "Loss_Q: [1.8  1.64 1.61 1.2  0.48 0.   6.73] Loss_P: [ 3.49  2.56  1.72  1.75  1.16  0.58 11.27]\n",
      "Loss_Q: [1.85 1.64 1.67 1.2  0.5  0.   6.85] Loss_P: [ 3.42  2.55  1.8   1.72  1.24  0.58 11.31]\n",
      "Loss_Q: [1.8  1.72 1.58 1.27 0.49 0.   6.86] Loss_P: [ 3.48  2.53  1.78  1.69  1.25  0.59 11.32]\n",
      "Loss_Q: [1.86 1.68 1.59 1.2  0.48 0.   6.8 ] Loss_P: [ 3.54  2.55  1.83  1.62  1.23  0.6  11.37]\n",
      "Loss_Q: [1.77 1.78 1.6  1.23 0.48 0.   6.85] Loss_P: [ 3.45  2.5   1.88  1.66  1.24  0.6  11.33]\n",
      "Loss_Q: [1.8  1.74 1.58 1.23 0.48 0.   6.83] Loss_P: [ 3.48  2.5   1.85  1.65  1.26  0.59 11.32]\n",
      "Loss_Q: [1.82 1.65 1.63 1.28 0.5  0.   6.88] Loss_P: [ 3.41  2.58  1.83  1.68  1.31  0.61 11.41]\n",
      "Loss_Q: [1.85 1.64 1.6  1.22 0.51 0.   6.82] Loss_P: [ 3.54  2.59  1.8   1.67  1.27  0.65 11.52]\n",
      "Loss_Q: [1.77 1.59 1.6  1.21 0.56 0.   6.74] Loss_P: [ 3.46  2.51  1.81  1.69  1.25  0.68 11.41]\n",
      "Loss_Q: [1.76 1.71 1.64 1.18 0.57 0.   6.86] Loss_P: [ 3.45  2.46  1.86  1.71  1.23  0.65 11.37]\n",
      "Loss_Q: [1.82 1.66 1.72 1.17 0.58 0.   6.95] Loss_P: [ 3.45  2.49  1.85  1.74  1.23  0.64 11.39]\n",
      "Loss_Q: [1.71 1.73 1.72 1.16 0.56 0.   6.88] Loss_P: [ 3.47  2.48  1.87  1.81  1.22  0.61 11.46]\n",
      "Loss_Q: [1.73 1.67 1.78 1.13 0.55 0.   6.86] Loss_P: [ 3.52  2.46  1.91  1.87  1.2   0.64 11.58]\n",
      "Loss_Q: [1.78 1.74 1.75 1.2  0.57 0.   7.04] Loss_P: [ 3.49  2.53  1.83  1.85  1.18  0.62 11.51]\n",
      "Loss_Q: [1.81 1.66 1.74 1.2  0.55 0.   6.95] Loss_P: [ 3.5   2.54  1.82  1.87  1.2   0.62 11.55]\n",
      "Loss_Q: [1.77 1.72 1.76 1.19 0.53 0.   6.97] Loss_P: [ 3.52  2.49  1.82  1.88  1.25  0.62 11.58]\n",
      "Loss_Q: [1.74 1.67 1.84 1.22 0.53 0.   7.  ] Loss_P: [ 3.5   2.52  1.8   1.91  1.23  0.61 11.58]\n",
      "Loss_Q: [1.77 1.63 1.85 1.24 0.51 0.   6.99] Loss_P: [ 3.53  2.48  1.78  1.96  1.25  0.63 11.63]\n",
      "Loss_Q: [1.78 1.66 1.82 1.25 0.57 0.   7.08] Loss_P: [ 3.54  2.46  1.83  1.95  1.24  0.63 11.65]\n",
      "Loss_Q: [1.82 1.63 1.79 1.22 0.56 0.   7.03] Loss_P: [ 3.44  2.51  1.82  1.91  1.27  0.67 11.62]\n",
      "Loss_Q: [1.76 1.65 1.8  1.22 0.54 0.   6.97] Loss_P: [ 3.45  2.55  1.78  1.87  1.24  0.63 11.51]\n",
      "Loss_Q: [1.81 1.68 1.81 1.22 0.54 0.   7.07] Loss_P: [ 3.44  2.55  1.76  1.92  1.24  0.61 11.52]\n",
      "Loss_Q: [1.84 1.68 1.79 1.23 0.5  0.   7.04] Loss_P: [ 3.47  2.51  1.83  1.91  1.26  0.6  11.59]\n",
      "Loss_Q: [1.76 1.68 1.8  1.21 0.51 0.   6.97] Loss_P: [ 3.44  2.53  1.8   1.97  1.24  0.59 11.57]\n",
      "Loss_Q: [1.76 1.68 1.84 1.2  0.47 0.   6.95] Loss_P: [ 3.41  2.55  1.79  1.95  1.22  0.58 11.51]\n",
      "Loss_Q: [1.82 1.66 1.83 1.22 0.51 0.   7.04] Loss_P: [ 3.43  2.59  1.82  1.97  1.22  0.6  11.63]\n",
      "Loss_Q: [1.81 1.68 1.81 1.16 0.53 0.   7.  ] Loss_P: [ 3.38  2.61  1.87  1.93  1.17  0.64 11.61]\n",
      "Loss_Q: [1.84 1.67 1.81 1.15 0.53 0.   7.  ] Loss_P: [ 3.41  2.58  1.82  1.97  1.14  0.61 11.52]\n",
      "Loss_Q: [1.84 1.71 1.77 1.16 0.55 0.   7.03] Loss_P: [ 3.39  2.62  1.86  1.94  1.2   0.67 11.67]\n",
      "Loss_Q: [1.8  1.69 1.77 1.18 0.56 0.   7.01] Loss_P: [ 3.43  2.59  1.79  1.92  1.16  0.67 11.56]\n",
      "Loss_Q: [1.84 1.63 1.79 1.14 0.58 0.   6.98] Loss_P: [ 3.46  2.59  1.78  1.93  1.1   0.64 11.5 ]\n",
      "Loss_Q: [1.78 1.71 1.8  1.13 0.54 0.   6.95] Loss_P: [ 3.47  2.53  1.83  1.93  1.14  0.64 11.54]\n",
      "Loss_Q: [1.81 1.69 1.83 1.17 0.55 0.   7.06] Loss_P: [ 3.42  2.54  1.85  1.95  1.21  0.66 11.63]\n",
      "Loss_Q: [1.82 1.73 1.81 1.21 0.54 0.   7.12] Loss_P: [ 3.46  2.51  1.94  1.91  1.16  0.61 11.59]\n",
      "Loss_Q: [1.84 1.7  1.83 1.18 0.54 0.   7.09] Loss_P: [ 3.45  2.57  1.88  1.9   1.2   0.66 11.66]\n",
      "Loss_Q: [1.86 1.75 1.8  1.16 0.52 0.   7.09] Loss_P: [ 3.42  2.56  1.92  1.94  1.17  0.63 11.64]\n",
      "Loss_Q: [1.83 1.7  1.86 1.2  0.54 0.   7.13] Loss_P: [ 3.41  2.64  1.81  2.    1.19  0.62 11.66]\n",
      "Loss_Q: [1.76 1.66 1.93 1.2  0.54 0.   7.08] Loss_P: [ 3.45  2.5   1.86  2.02  1.22  0.66 11.71]\n",
      "Loss_Q: [1.82 1.69 1.88 1.17 0.53 0.   7.09] Loss_P: [ 3.53  2.47  1.82  2.04  1.2   0.65 11.72]\n",
      "Loss_Q: [1.81 1.72 1.93 1.25 0.55 0.   7.26] Loss_P: [ 3.49  2.54  1.84  2.06  1.21  0.65 11.79]\n",
      "Loss_Q: [1.82 1.68 1.92 1.2  0.57 0.   7.2 ] Loss_P: [ 3.42  2.54  1.79  2.03  1.19  0.66 11.62]\n",
      "Loss_Q: [1.82 1.73 1.97 1.28 0.58 0.   7.38] Loss_P: [ 3.45  2.53  1.88  2.06  1.22  0.7  11.84]\n",
      "Loss_Q: [1.8  1.69 1.96 1.25 0.6  0.   7.31] Loss_P: [ 3.46  2.59  1.86  2.05  1.22  0.69 11.88]\n",
      "Loss_Q: [1.8  1.69 1.91 1.21 0.58 0.   7.2 ] Loss_P: [ 3.46  2.52  1.87  2.06  1.25  0.75 11.91]\n",
      "Loss_Q: [1.81 1.64 2.01 1.28 0.65 0.   7.39] Loss_P: [ 3.44  2.58  1.84  2.1   1.28  0.76 12.  ]\n",
      "Loss_Q: [1.81 1.67 2.01 1.28 0.66 0.   7.43] Loss_P: [ 3.48  2.51  1.77  2.09  1.28  0.76 11.9 ]\n",
      "Loss_Q: [1.79 1.65 1.97 1.27 0.66 0.   7.33] Loss_P: [ 3.48  2.52  1.74  2.06  1.23  0.77 11.8 ]\n",
      "Loss_Q: [1.77 1.61 2.03 1.23 0.69 0.   7.34] Loss_P: [ 3.4   2.57  1.77  2.11  1.26  0.81 11.9 ]\n",
      "Loss_Q: [1.8  1.6  2.04 1.2  0.64 0.   7.27] Loss_P: [ 3.46  2.55  1.67  2.12  1.22  0.75 11.77]\n",
      "Loss_Q: [1.84 1.6  2.02 1.23 0.66 0.   7.34] Loss_P: [ 3.44  2.6   1.74  2.1   1.2   0.76 11.84]\n",
      "Loss_Q: [1.74 1.55 1.95 1.21 0.62 0.   7.08] Loss_P: [ 3.44  2.57  1.74  2.02  1.22  0.75 11.74]\n",
      "Loss_Q: [1.79 1.58 1.91 1.17 0.64 0.   7.09] Loss_P: [ 3.51  2.55  1.7   2.02  1.27  0.73 11.78]\n",
      "Loss_Q: [1.76 1.6  1.91 1.21 0.63 0.   7.11] Loss_P: [ 3.45  2.52  1.71  2.05  1.26  0.75 11.75]\n",
      "Loss_Q: [1.69 1.51 1.92 1.2  0.66 0.   6.98] Loss_P: [ 3.42  2.55  1.71  1.98  1.15  0.75 11.56]\n",
      "Loss_Q: [1.77 1.55 1.91 1.18 0.62 0.   7.03] Loss_P: [ 3.41  2.52  1.76  1.99  1.24  0.77 11.7 ]\n",
      "Loss_Q: [1.76 1.64 1.89 1.2  0.68 0.   7.17] Loss_P: [ 3.4   2.57  1.75  2.    1.28  0.81 11.8 ]\n",
      "Loss_Q: [1.76 1.58 1.81 1.24 0.65 0.   7.03] Loss_P: [ 3.43  2.58  1.79  1.93  1.27  0.76 11.76]\n",
      "Loss_Q: [1.77 1.61 1.83 1.26 0.63 0.   7.09] Loss_P: [ 3.44  2.53  1.79  1.85  1.22  0.73 11.57]\n",
      "Loss_Q: [1.77 1.75 1.75 1.31 0.63 0.   7.22] Loss_P: [ 3.4   2.52  1.82  1.84  1.28  0.74 11.6 ]\n",
      "Loss_Q: [1.81 1.6  1.78 1.24 0.64 0.   7.07] Loss_P: [ 3.43  2.52  1.74  1.83  1.3   0.75 11.57]\n",
      "Loss_Q: [1.75 1.66 1.65 1.26 0.67 0.   6.97] Loss_P: [ 3.48  2.49  1.69  1.79  1.3   0.74 11.49]\n",
      "Loss_Q: [1.73 1.57 1.73 1.33 0.65 0.   7.02] Loss_P: [ 3.41  2.53  1.72  1.8   1.29  0.75 11.5 ]\n",
      "Loss_Q: [1.74 1.61 1.77 1.29 0.66 0.   7.07] Loss_P: [ 3.45  2.51  1.75  1.77  1.29  0.75 11.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.74 1.68 1.7  1.27 0.62 0.   7.01] Loss_P: [ 3.5   2.52  1.77  1.81  1.28  0.73 11.62]\n",
      "Loss_Q: [1.78 1.61 1.76 1.27 0.62 0.   7.05] Loss_P: [ 3.44  2.54  1.74  1.86  1.34  0.76 11.68]\n",
      "Loss_Q: [1.77 1.64 1.76 1.29 0.65 0.   7.1 ] Loss_P: [ 3.43  2.53  1.81  1.86  1.34  0.78 11.75]\n",
      "Loss_Q: [1.71 1.68 1.71 1.27 0.65 0.   7.02] Loss_P: [ 3.45  2.53  1.79  1.79  1.28  0.78 11.62]\n",
      "Loss_Q: [1.79 1.61 1.67 1.29 0.65 0.   7.01] Loss_P: [ 3.41  2.55  1.79  1.79  1.31  0.75 11.6 ]\n",
      "Loss_Q: [1.73 1.64 1.76 1.22 0.66 0.   7.01] Loss_P: [ 3.42  2.6   1.74  1.79  1.27  0.78 11.59]\n",
      "Loss_Q: [1.76 1.6  1.74 1.22 0.62 0.   6.94] Loss_P: [ 3.41  2.56  1.7   1.84  1.29  0.73 11.54]\n",
      "Loss_Q: [1.81 1.65 1.82 1.22 0.63 0.   7.13] Loss_P: [ 3.4   2.6   1.79  1.86  1.26  0.75 11.67]\n",
      "Loss_Q: [1.85 1.72 1.8  1.23 0.64 0.   7.24] Loss_P: [ 3.41  2.6   1.77  1.84  1.22  0.74 11.59]\n",
      "Loss_Q: [1.79 1.63 1.79 1.21 0.65 0.   7.06] Loss_P: [ 3.4   2.63  1.77  1.86  1.19  0.77 11.61]\n",
      "Loss_Q: [1.85 1.63 1.73 1.18 0.67 0.   7.06] Loss_P: [ 3.43  2.59  1.78  1.87  1.23  0.77 11.66]\n",
      "Loss_Q: [1.81 1.66 1.67 1.16 0.62 0.   6.92] Loss_P: [ 3.37  2.6   1.82  1.76  1.19  0.72 11.46]\n",
      "Loss_Q: [1.88 1.64 1.67 1.11 0.61 0.   6.91] Loss_P: [ 3.42  2.64  1.81  1.76  1.15  0.68 11.46]\n",
      "Loss_Q: [1.75 1.67 1.64 1.13 0.62 0.   6.82] Loss_P: [ 3.42  2.65  1.81  1.75  1.17  0.73 11.53]\n",
      "Loss_Q: [1.78 1.74 1.64 1.17 0.57 0.   6.91] Loss_P: [ 3.4   2.66  1.81  1.72  1.2   0.71 11.5 ]\n",
      "Loss_Q: [1.8  1.72 1.71 1.14 0.57 0.   6.94] Loss_P: [ 3.38  2.66  1.83  1.81  1.16  0.72 11.56]\n",
      "Loss_Q: [1.8  1.71 1.75 1.23 0.59 0.   7.08] Loss_P: [ 3.41  2.6   1.83  1.86  1.24  0.7  11.64]\n",
      "Loss_Q: [1.86 1.64 1.78 1.24 0.63 0.   7.15] Loss_P: [ 3.42  2.58  1.79  1.92  1.24  0.73 11.68]\n",
      "Loss_Q: [1.77 1.64 1.84 1.23 0.61 0.   7.09] Loss_P: [ 3.43  2.52  1.85  1.93  1.28  0.7  11.71]\n",
      "Loss_Q: [1.8  1.66 1.82 1.21 0.61 0.   7.1 ] Loss_P: [ 3.43  2.59  1.81  1.86  1.26  0.69 11.65]\n",
      "Loss_Q: [1.81 1.7  1.78 1.21 0.61 0.   7.11] Loss_P: [ 3.45  2.57  1.79  1.86  1.23  0.71 11.62]\n",
      "Loss_Q: [1.72 1.59 1.76 1.17 0.65 0.   6.9 ] Loss_P: [ 3.44  2.53  1.75  1.9   1.2   0.73 11.54]\n",
      "Loss_Q: [1.73 1.68 1.72 1.21 0.65 0.   6.99] Loss_P: [ 3.47  2.52  1.76  1.84  1.19  0.72 11.5 ]\n",
      "Loss_Q: [1.71 1.65 1.68 1.14 0.62 0.   6.8 ] Loss_P: [ 3.48  2.48  1.82  1.84  1.22  0.73 11.58]\n",
      "Loss_Q: [1.79 1.7  1.65 1.19 0.63 0.   6.97] Loss_P: [ 3.43  2.52  1.84  1.74  1.22  0.73 11.47]\n",
      "Loss_Q: [1.72 1.73 1.69 1.24 0.62 0.   7.  ] Loss_P: [ 3.45  2.49  1.87  1.82  1.21  0.74 11.58]\n",
      "Loss_Q: [1.77 1.65 1.7  1.18 0.6  0.   6.9 ] Loss_P: [ 3.47  2.55  1.87  1.78  1.16  0.71 11.55]\n",
      "Loss_Q: [1.81 1.72 1.81 1.16 0.62 0.   7.13] Loss_P: [ 3.35  2.54  1.88  1.91  1.17  0.66 11.51]\n",
      "Loss_Q: [1.74 1.72 1.79 1.18 0.61 0.   7.04] Loss_P: [ 3.44  2.53  1.92  1.87  1.21  0.72 11.69]\n",
      "Loss_Q: [1.77 1.71 1.69 1.21 0.58 0.   6.96] Loss_P: [ 3.47  2.52  1.86  1.86  1.19  0.69 11.58]\n",
      "Loss_Q: [1.77 1.67 1.66 1.19 0.58 0.   6.87] Loss_P: [ 3.39  2.53  1.92  1.73  1.18  0.67 11.41]\n",
      "Loss_Q: [1.76 1.66 1.7  1.2  0.59 0.   6.91] Loss_P: [ 3.37  2.58  1.87  1.73  1.22  0.69 11.47]\n",
      "Loss_Q: [1.75 1.67 1.68 1.19 0.6  0.   6.87] Loss_P: [ 3.37  2.66  1.86  1.78  1.19  0.66 11.51]\n",
      "Loss_Q: [1.78 1.59 1.67 1.2  0.6  0.   6.84] Loss_P: [ 3.37  2.57  1.85  1.75  1.2   0.68 11.42]\n",
      "Loss_Q: [1.78 1.65 1.72 1.21 0.6  0.   6.96] Loss_P: [ 3.36  2.55  1.89  1.83  1.25  0.73 11.6 ]\n",
      "Loss_Q: [1.8  1.74 1.68 1.19 0.65 0.   7.06] Loss_P: [ 3.4   2.53  1.91  1.81  1.23  0.73 11.61]\n",
      "Loss_Q: [1.76 1.7  1.61 1.17 0.59 0.   6.84] Loss_P: [ 3.41  2.57  1.82  1.73  1.18  0.75 11.47]\n",
      "Loss_Q: [1.83 1.59 1.56 1.18 0.6  0.   6.75] Loss_P: [ 3.41  2.52  1.83  1.69  1.23  0.73 11.41]\n",
      "Loss_Q: [1.68 1.58 1.61 1.22 0.64 0.   6.72] Loss_P: [ 3.43  2.53  1.84  1.67  1.19  0.74 11.4 ]\n",
      "Loss_Q: [1.78 1.65 1.66 1.22 0.63 0.   6.94] Loss_P: [ 3.4   2.49  1.85  1.71  1.17  0.72 11.34]\n",
      "Loss_Q: [1.8  1.71 1.66 1.17 0.62 0.   6.96] Loss_P: [ 3.45  2.51  1.87  1.76  1.23  0.76 11.59]\n",
      "Loss_Q: [1.8  1.65 1.58 1.18 0.64 0.   6.85] Loss_P: [ 3.42  2.5   1.85  1.64  1.16  0.82 11.38]\n",
      "Loss_Q: [1.76 1.72 1.7  1.23 0.65 0.   7.06] Loss_P: [ 3.42  2.47  1.93  1.75  1.25  0.77 11.59]\n",
      "Loss_Q: [1.77 1.66 1.63 1.21 0.63 0.   6.9 ] Loss_P: [ 3.42  2.49  1.97  1.71  1.24  0.79 11.63]\n",
      "Loss_Q: [1.84 1.71 1.63 1.22 0.65 0.   7.05] Loss_P: [ 3.45  2.46  1.93  1.7   1.22  0.75 11.52]\n",
      "Loss_Q: [1.78 1.66 1.69 1.16 0.65 0.   6.94] Loss_P: [ 3.41  2.49  1.9   1.7   1.2   0.78 11.49]\n",
      "Loss_Q: [1.8  1.69 1.64 1.22 0.64 0.   6.97] Loss_P: [ 3.4   2.52  1.87  1.7   1.19  0.74 11.43]\n",
      "Loss_Q: [1.73 1.58 1.68 1.19 0.6  0.   6.78] Loss_P: [ 3.43  2.43  1.91  1.76  1.18  0.74 11.46]\n",
      "Loss_Q: [1.77 1.67 1.74 1.2  0.59 0.   6.96] Loss_P: [ 3.41  2.47  1.98  1.75  1.25  0.71 11.57]\n",
      "Loss_Q: [1.69 1.62 1.68 1.15 0.62 0.   6.76] Loss_P: [ 3.41  2.46  1.96  1.76  1.22  0.73 11.54]\n",
      "Loss_Q: [1.82 1.74 1.65 1.15 0.61 0.   6.98] Loss_P: [ 3.39  2.49  1.96  1.71  1.16  0.74 11.46]\n",
      "Loss_Q: [1.77 1.69 1.62 1.16 0.6  0.   6.85] Loss_P: [ 3.39  2.52  1.97  1.66  1.2   0.74 11.49]\n",
      "Loss_Q: [1.84 1.69 1.63 1.13 0.63 0.   6.92] Loss_P: [ 3.37  2.59  1.97  1.69  1.22  0.74 11.57]\n",
      "Loss_Q: [1.82 1.7  1.59 1.12 0.64 0.   6.88] Loss_P: [ 3.37  2.57  1.91  1.68  1.17  0.76 11.47]\n",
      "Loss_Q: [1.85 1.69 1.58 1.15 0.59 0.   6.86] Loss_P: [ 3.38  2.61  1.9   1.63  1.15  0.76 11.44]\n",
      "Loss_Q: [1.82 1.67 1.55 1.1  0.6  0.   6.75] Loss_P: [ 3.42  2.57  1.93  1.63  1.11  0.72 11.38]\n",
      "Loss_Q: [1.83 1.65 1.63 1.1  0.62 0.   6.83] Loss_P: [ 3.35  2.62  1.96  1.67  1.15  0.73 11.49]\n",
      "Loss_Q: [1.77 1.66 1.66 1.1  0.62 0.   6.81] Loss_P: [ 3.42  2.54  1.95  1.64  1.06  0.72 11.33]\n",
      "Loss_Q: [1.83 1.7  1.65 1.07 0.57 0.   6.83] Loss_P: [ 3.35  2.62  1.92  1.62  1.12  0.7  11.34]\n",
      "Loss_Q: [1.8  1.65 1.62 1.11 0.61 0.   6.79] Loss_P: [ 3.38  2.61  1.91  1.67  1.12  0.72 11.42]\n",
      "Loss_Q: [1.8  1.7  1.67 1.13 0.67 0.   6.96] Loss_P: [ 3.41  2.54  2.    1.69  1.14  0.73 11.5 ]\n",
      "Loss_Q: [1.75 1.69 1.65 1.13 0.66 0.   6.87] Loss_P: [ 3.39  2.53  2.    1.71  1.19  0.71 11.52]\n",
      "Loss_Q: [1.78 1.65 1.6  1.09 0.61 0.   6.74] Loss_P: [ 3.43  2.53  1.93  1.7   1.13  0.7  11.42]\n",
      "Loss_Q: [1.75 1.69 1.65 1.14 0.62 0.   6.86] Loss_P: [ 3.43  2.52  1.97  1.7   1.11  0.76 11.49]\n",
      "Loss_Q: [1.8  1.63 1.55 1.08 0.6  0.   6.67] Loss_P: [ 3.42  2.55  1.88  1.67  1.08  0.71 11.31]\n",
      "Loss_Q: [1.76 1.75 1.63 1.06 0.62 0.   6.82] Loss_P: [ 3.39  2.5   1.99  1.69  1.11  0.69 11.37]\n",
      "Loss_Q: [1.76 1.79 1.64 1.08 0.57 0.   6.84] Loss_P: [ 3.44  2.58  2.    1.7   1.08  0.72 11.53]\n",
      "Loss_Q: [1.78 1.67 1.68 1.03 0.58 0.   6.74] Loss_P: [ 3.41  2.56  1.89  1.77  1.06  0.68 11.37]\n",
      "Loss_Q: [1.74 1.69 1.65 0.99 0.6  0.   6.69] Loss_P: [ 3.42  2.55  1.92  1.7   1.06  0.68 11.32]\n",
      "Loss_Q: [1.8  1.71 1.67 1.03 0.58 0.   6.79] Loss_P: [ 3.42  2.59  1.95  1.71  1.03  0.63 11.33]\n",
      "Loss_Q: [1.88 1.66 1.68 1.05 0.61 0.   6.88] Loss_P: [ 3.4   2.58  1.89  1.73  1.13  0.68 11.4 ]\n",
      "Loss_Q: [1.84 1.64 1.76 1.08 0.59 0.   6.91] Loss_P: [ 3.44  2.6   1.88  1.74  1.1   0.7  11.46]\n",
      "Loss_Q: [1.86 1.67 1.73 1.08 0.58 0.   6.92] Loss_P: [ 3.43  2.59  1.94  1.79  1.14  0.68 11.57]\n",
      "Loss_Q: [1.79 1.73 1.69 1.05 0.59 0.   6.85] Loss_P: [ 3.41  2.57  1.9   1.74  1.13  0.71 11.47]\n",
      "Loss_Q: [1.77 1.62 1.7  1.07 0.61 0.   6.77] Loss_P: [ 3.44  2.59  1.9   1.76  1.09  0.71 11.49]\n",
      "Loss_Q: [1.83 1.63 1.72 1.1  0.6  0.   6.88] Loss_P: [ 3.47  2.61  1.84  1.84  1.17  0.72 11.63]\n",
      "Loss_Q: [1.75 1.68 1.73 1.08 0.6  0.   6.84] Loss_P: [ 3.45  2.58  1.82  1.78  1.11  0.68 11.41]\n",
      "Loss_Q: [1.77 1.57 1.78 1.11 0.6  0.   6.84] Loss_P: [ 3.47  2.56  1.68  1.8   1.08  0.7  11.3 ]\n",
      "Loss_Q: [1.76 1.57 1.75 1.07 0.62 0.   6.78] Loss_P: [ 3.43  2.61  1.74  1.8   1.09  0.7  11.36]\n",
      "Loss_Q: [1.83 1.54 1.75 1.08 0.6  0.   6.82] Loss_P: [ 3.46  2.57  1.72  1.79  1.09  0.69 11.32]\n",
      "Loss_Q: [1.76 1.6  1.77 1.11 0.59 0.   6.82] Loss_P: [ 3.39  2.6   1.83  1.82  1.11  0.75 11.51]\n",
      "Loss_Q: [1.81 1.59 1.8  1.07 0.66 0.   6.93] Loss_P: [ 3.4   2.56  1.8   1.82  1.09  0.71 11.38]\n",
      "Loss_Q: [1.74 1.72 1.78 1.1  0.58 0.   6.92] Loss_P: [ 3.38  2.6   1.85  1.79  1.11  0.71 11.44]\n",
      "Loss_Q: [1.72 1.7  1.76 1.09 0.6  0.   6.87] Loss_P: [ 3.38  2.56  1.86  1.81  1.11  0.7  11.42]\n",
      "Loss_Q: [1.83 1.6  1.8  1.12 0.63 0.   6.98] Loss_P: [ 3.39  2.6   1.85  1.78  1.11  0.74 11.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.8  1.71 1.74 1.08 0.64 0.   6.96] Loss_P: [ 3.41  2.54  1.89  1.8   1.09  0.75 11.48]\n",
      "Loss_Q: [1.78 1.72 1.69 1.08 0.64 0.   6.92] Loss_P: [ 3.4   2.56  1.92  1.73  1.11  0.71 11.43]\n",
      "Loss_Q: [1.8  1.71 1.74 1.12 0.63 0.   7.01] Loss_P: [ 3.38  2.63  1.88  1.75  1.12  0.72 11.49]\n",
      "Loss_Q: [1.78 1.71 1.73 1.03 0.67 0.   6.92] Loss_P: [ 3.38  2.66  1.88  1.79  1.09  0.81 11.61]\n",
      "Loss_Q: [1.87 1.68 1.78 1.05 0.7  0.   7.09] Loss_P: [ 3.37  2.65  1.86  1.81  1.11  0.77 11.58]\n",
      "Loss_Q: [1.81 1.74 1.77 1.09 0.65 0.   7.06] Loss_P: [ 3.4   2.66  1.9   1.84  1.12  0.79 11.72]\n",
      "Loss_Q: [1.79 1.78 1.77 1.09 0.63 0.   7.06] Loss_P: [ 3.38  2.63  1.98  1.81  1.11  0.75 11.67]\n",
      "Loss_Q: [1.78 1.77 1.74 1.   0.66 0.   6.95] Loss_P: [ 3.4   2.55  1.94  1.87  1.14  0.78 11.68]\n",
      "Loss_Q: [1.86 1.75 1.77 1.09 0.67 0.   7.13] Loss_P: [ 3.4   2.56  1.98  1.83  1.08  0.79 11.63]\n",
      "Loss_Q: [1.84 1.73 1.81 1.02 0.68 0.   7.07] Loss_P: [ 3.45  2.53  1.96  1.84  1.06  0.76 11.61]\n",
      "Loss_Q: [1.75 1.8  1.79 1.12 0.66 0.   7.12] Loss_P: [ 3.42  2.51  1.97  1.89  1.11  0.75 11.65]\n",
      "Loss_Q: [1.79 1.69 1.84 1.11 0.65 0.   7.08] Loss_P: [ 3.48  2.54  1.98  1.85  1.14  0.79 11.79]\n",
      "Loss_Q: [1.78 1.74 1.83 1.05 0.72 0.   7.12] Loss_P: [ 3.43  2.56  1.89  1.85  1.13  0.78 11.64]\n",
      "Loss_Q: [1.72 1.7  1.83 1.03 0.69 0.   6.98] Loss_P: [ 3.45  2.56  1.88  1.83  1.11  0.82 11.64]\n",
      "Loss_Q: [1.82 1.72 1.85 1.07 0.68 0.   7.14] Loss_P: [ 3.42  2.61  1.84  1.91  1.07  0.8  11.66]\n",
      "Loss_Q: [1.76 1.72 1.78 1.08 0.7  0.   7.04] Loss_P: [ 3.38  2.6   1.93  1.86  1.12  0.79 11.68]\n",
      "Loss_Q: [1.79 1.78 1.78 1.16 0.68 0.   7.19] Loss_P: [ 3.42  2.56  1.97  1.79  1.14  0.78 11.67]\n",
      "Loss_Q: [1.77 1.76 1.74 1.1  0.68 0.   7.05] Loss_P: [ 3.39  2.54  1.94  1.82  1.2   0.77 11.66]\n",
      "Loss_Q: [1.79 1.75 1.75 1.05 0.69 0.   7.04] Loss_P: [ 3.41  2.56  1.88  1.85  1.13  0.79 11.62]\n",
      "Loss_Q: [1.8  1.79 1.77 1.06 0.7  0.   7.12] Loss_P: [ 3.42  2.58  1.92  1.77  1.05  0.76 11.49]\n",
      "Loss_Q: [1.79 1.79 1.77 1.05 0.71 0.   7.11] Loss_P: [ 3.41  2.56  1.95  1.86  1.06  0.81 11.65]\n",
      "Loss_Q: [1.76 1.72 1.81 1.05 0.68 0.   7.02] Loss_P: [ 3.39  2.59  1.94  1.83  1.01  0.8  11.56]\n",
      "Loss_Q: [1.81 1.79 1.72 1.04 0.65 0.   7.01] Loss_P: [ 3.35  2.61  1.98  1.83  1.11  0.76 11.65]\n",
      "Loss_Q: [1.81 1.73 1.75 1.03 0.67 0.   6.99] Loss_P: [ 3.38  2.67  1.9   1.79  1.02  0.77 11.53]\n",
      "Loss_Q: [1.87 1.66 1.73 1.02 0.64 0.   6.93] Loss_P: [ 3.36  2.64  1.86  1.79  1.07  0.72 11.43]\n",
      "Loss_Q: [1.84 1.65 1.73 0.99 0.65 0.   6.86] Loss_P: [ 3.42  2.6   1.87  1.79  1.02  0.7  11.4 ]\n",
      "Loss_Q: [1.77 1.7  1.79 0.98 0.66 0.   6.9 ] Loss_P: [ 3.41  2.62  1.91  1.89  1.03  0.72 11.58]\n",
      "Loss_Q: [1.75 1.73 1.76 1.02 0.61 0.   6.88] Loss_P: [ 3.39  2.56  1.91  1.86  1.07  0.71 11.5 ]\n",
      "Loss_Q: [1.78 1.67 1.79 1.04 0.63 0.   6.9 ] Loss_P: [ 3.41  2.5   1.92  1.82  1.06  0.7  11.42]\n",
      "Loss_Q: [1.76 1.65 1.8  1.   0.62 0.   6.83] Loss_P: [ 3.41  2.6   1.87  1.88  1.1   0.72 11.57]\n",
      "Loss_Q: [1.79 1.67 1.92 1.02 0.64 0.   7.02] Loss_P: [ 3.44  2.58  1.85  1.92  1.04  0.69 11.52]\n",
      "Loss_Q: [1.81 1.68 1.93 1.04 0.62 0.   7.08] Loss_P: [ 3.38  2.62  1.81  1.96  1.09  0.73 11.59]\n",
      "Loss_Q: [1.71 1.65 1.94 1.02 0.64 0.   6.97] Loss_P: [ 3.43  2.57  1.82  1.96  1.08  0.72 11.59]\n",
      "Loss_Q: [1.74 1.72 1.88 1.05 0.65 0.   7.05] Loss_P: [ 3.41  2.58  1.79  1.92  1.09  0.76 11.55]\n",
      "Loss_Q: [1.67 1.64 1.89 1.02 0.66 0.   6.9 ] Loss_P: [ 3.46  2.51  1.85  1.93  1.12  0.76 11.62]\n",
      "Loss_Q: [1.72 1.69 1.94 1.01 0.63 0.   6.99] Loss_P: [ 3.41  2.62  1.86  2.    1.11  0.75 11.76]\n",
      "Loss_Q: [1.73 1.72 1.9  1.03 0.71 0.   7.09] Loss_P: [ 3.42  2.56  1.89  2.    1.05  0.77 11.69]\n",
      "Loss_Q: [1.82 1.77 1.93 1.06 0.67 0.   7.24] Loss_P: [ 3.39  2.51  1.84  1.96  1.08  0.73 11.51]\n",
      "Loss_Q: [1.79 1.77 1.93 1.09 0.67 0.   7.25] Loss_P: [ 3.44  2.53  1.89  1.97  1.09  0.76 11.68]\n",
      "Loss_Q: [1.72 1.75 1.93 1.06 0.66 0.   7.13] Loss_P: [ 3.42  2.55  1.91  1.93  1.05  0.74 11.6 ]\n",
      "Loss_Q: [1.73 1.72 1.86 1.04 0.74 0.   7.1 ] Loss_P: [ 3.44  2.52  1.96  1.95  1.14  0.82 11.82]\n",
      "Loss_Q: [1.72 1.7  1.87 1.07 0.7  0.   7.06] Loss_P: [ 3.42  2.52  1.95  1.88  1.14  0.78 11.69]\n",
      "Loss_Q: [1.74 1.7  1.78 1.11 0.71 0.   7.04] Loss_P: [ 3.48  2.48  1.95  1.86  1.12  0.8  11.68]\n",
      "Loss_Q: [1.66 1.82 1.71 1.09 0.72 0.   7.  ] Loss_P: [ 3.47  2.47  1.96  1.83  1.16  0.81 11.69]\n",
      "Loss_Q: [1.68 1.77 1.79 1.06 0.73 0.   7.04] Loss_P: [ 3.41  2.53  1.98  1.85  1.12  0.82 11.7 ]\n",
      "Loss_Q: [1.74 1.76 1.84 1.04 0.73 0.   7.11] Loss_P: [ 3.44  2.45  1.99  1.86  1.11  0.8  11.64]\n",
      "Loss_Q: [1.74 1.74 1.8  1.05 0.7  0.   7.03] Loss_P: [ 3.44  2.51  1.93  1.88  1.14  0.79 11.69]\n",
      "Loss_Q: [1.7  1.74 1.84 1.13 0.69 0.   7.1 ] Loss_P: [ 3.48  2.44  2.03  1.82  1.14  0.77 11.67]\n",
      "Loss_Q: [1.74 1.73 1.8  1.08 0.73 0.   7.08] Loss_P: [ 3.5   2.49  1.96  1.81  1.12  0.82 11.71]\n",
      "Loss_Q: [1.72 1.8  1.75 1.1  0.75 0.   7.11] Loss_P: [ 3.45  2.49  1.99  1.79  1.11  0.8  11.62]\n",
      "Loss_Q: [1.74 1.73 1.76 1.09 0.71 0.   7.04] Loss_P: [ 3.48  2.47  2.01  1.76  1.17  0.77 11.66]\n",
      "Loss_Q: [1.79 1.79 1.73 1.06 0.67 0.   7.05] Loss_P: [ 3.42  2.49  2.03  1.81  1.12  0.76 11.62]\n",
      "Loss_Q: [1.75 1.77 1.77 1.14 0.67 0.   7.1 ] Loss_P: [ 3.42  2.52  1.98  1.81  1.14  0.76 11.62]\n",
      "Loss_Q: [1.79 1.82 1.75 1.05 0.62 0.   7.03] Loss_P: [ 3.4   2.53  2.01  1.8   1.18  0.74 11.66]\n",
      "Loss_Q: [1.87 1.86 1.77 1.14 0.67 0.   7.31] Loss_P: [ 3.38  2.57  2.07  1.79  1.15  0.74 11.69]\n",
      "Loss_Q: [1.78 1.75 1.72 1.08 0.65 0.   6.97] Loss_P: [ 3.43  2.54  2.02  1.72  1.15  0.73 11.59]\n",
      "Loss_Q: [1.84 1.78 1.64 1.1  0.64 0.   7.  ] Loss_P: [ 3.38  2.56  1.99  1.67  1.12  0.73 11.44]\n",
      "Loss_Q: [1.78 1.78 1.62 1.07 0.62 0.   6.87] Loss_P: [ 3.44  2.59  2.02  1.7   1.14  0.74 11.63]\n",
      "Loss_Q: [1.75 1.82 1.66 1.12 0.64 0.   6.98] Loss_P: [ 3.4   2.49  2.02  1.71  1.17  0.71 11.5 ]\n",
      "Loss_Q: [1.73 1.85 1.67 1.06 0.65 0.   6.97] Loss_P: [ 3.47  2.52  2.02  1.73  1.13  0.72 11.59]\n",
      "Loss_Q: [1.77 1.81 1.69 1.1  0.67 0.   7.04] Loss_P: [ 3.42  2.5   2.02  1.69  1.12  0.75 11.5 ]\n",
      "Loss_Q: [1.82 1.81 1.66 1.02 0.69 0.   7.01] Loss_P: [ 3.39  2.6   2.06  1.72  1.11  0.78 11.65]\n",
      "Loss_Q: [1.81 1.81 1.7  1.02 0.7  0.   7.04] Loss_P: [ 3.36  2.61  2.02  1.72  1.08  0.78 11.58]\n",
      "Loss_Q: [1.81 1.81 1.69 0.97 0.77 0.   7.05] Loss_P: [ 3.43  2.52  2.03  1.74  1.    0.86 11.58]\n",
      "Loss_Q: [1.76 1.8  1.73 1.   0.76 0.   7.04] Loss_P: [ 3.41  2.52  2.04  1.78  1.02  0.84 11.61]\n",
      "Loss_Q: [1.76 1.73 1.7  0.97 0.72 0.   6.89] Loss_P: [ 3.45  2.49  2.02  1.74  1.06  0.8  11.57]\n",
      "Loss_Q: [1.69 1.77 1.65 0.96 0.72 0.   6.79] Loss_P: [ 3.47  2.49  1.93  1.76  1.05  0.83 11.53]\n",
      "Loss_Q: [1.74 1.74 1.71 1.01 0.74 0.   6.93] Loss_P: [ 3.43  2.48  1.98  1.74  1.06  0.82 11.51]\n",
      "Loss_Q: [1.75 1.76 1.69 1.01 0.7  0.   6.9 ] Loss_P: [ 3.44  2.47  1.98  1.76  1.07  0.83 11.55]\n",
      "Loss_Q: [1.77 1.69 1.63 0.92 0.75 0.   6.77] Loss_P: [ 3.42  2.53  1.88  1.79  1.07  0.83 11.52]\n",
      "Loss_Q: [1.72 1.7  1.67 0.96 0.71 0.   6.77] Loss_P: [ 3.46  2.47  1.96  1.75  1.04  0.79 11.47]\n",
      "Loss_Q: [1.72 1.7  1.64 1.02 0.71 0.   6.79] Loss_P: [ 3.46  2.42  1.88  1.75  1.05  0.8  11.36]\n",
      "Loss_Q: [1.71 1.72 1.63 1.03 0.71 0.   6.8 ] Loss_P: [ 3.55  2.42  1.95  1.67  1.05  0.79 11.42]\n",
      "Loss_Q: [1.67 1.72 1.62 0.97 0.67 0.   6.66] Loss_P: [ 3.5   2.42  1.89  1.67  1.05  0.76 11.29]\n",
      "Loss_Q: [1.68 1.72 1.61 1.05 0.63 0.   6.7 ] Loss_P: [ 3.49  2.45  1.93  1.71  1.05  0.75 11.39]\n",
      "Loss_Q: [1.65 1.77 1.6  1.   0.69 0.   6.71] Loss_P: [ 3.48  2.45  1.97  1.75  1.04  0.8  11.49]\n",
      "Loss_Q: [1.67 1.77 1.65 1.02 0.67 0.   6.78] Loss_P: [ 3.5   2.4   1.91  1.78  1.03  0.82 11.46]\n",
      "Loss_Q: [1.67 1.69 1.67 1.03 0.74 0.   6.8 ] Loss_P: [ 3.44  2.41  1.88  1.75  1.06  0.83 11.38]\n",
      "Loss_Q: [1.66 1.74 1.66 1.06 0.73 0.   6.86] Loss_P: [ 3.52  2.33  1.97  1.73  1.01  0.82 11.37]\n",
      "Loss_Q: [1.67 1.69 1.7  1.05 0.67 0.   6.78] Loss_P: [ 3.46  2.41  1.93  1.78  1.09  0.83 11.51]\n",
      "Loss_Q: [1.67 1.78 1.7  1.   0.71 0.   6.85] Loss_P: [ 3.52  2.35  1.95  1.77  1.01  0.79 11.37]\n",
      "Loss_Q: [1.66 1.85 1.73 1.05 0.75 0.   7.04] Loss_P: [ 3.45  2.38  1.98  1.82  1.06  0.84 11.52]\n",
      "Loss_Q: [1.58 1.81 1.69 1.03 0.73 0.   6.85] Loss_P: [ 3.5   2.35  2.02  1.85  1.06  0.83 11.61]\n",
      "Loss_Q: [1.67 1.88 1.77 1.   0.73 0.   7.05] Loss_P: [ 3.48  2.39  2.07  1.86  1.07  0.85 11.72]\n",
      "Loss_Q: [1.69 1.77 1.8  1.02 0.75 0.   7.03] Loss_P: [ 3.47  2.43  2.02  1.83  1.04  0.86 11.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.7  1.77 1.77 1.   0.76 0.   7.01] Loss_P: [ 3.44  2.51  1.97  1.86  1.04  0.85 11.67]\n",
      "Loss_Q: [1.72 1.74 1.79 0.99 0.77 0.   7.01] Loss_P: [ 3.46  2.47  1.97  1.91  1.05  0.84 11.7 ]\n",
      "Loss_Q: [1.72 1.74 1.85 1.03 0.77 0.   7.11] Loss_P: [ 3.46  2.45  1.94  1.89  1.    0.84 11.59]\n",
      "Loss_Q: [1.68 1.72 1.84 1.01 0.78 0.   7.03] Loss_P: [ 3.46  2.44  1.96  1.89  1.07  0.87 11.69]\n",
      "Loss_Q: [1.68 1.67 1.77 0.99 0.77 0.   6.89] Loss_P: [ 3.42  2.42  1.85  1.87  1.05  0.85 11.46]\n",
      "Loss_Q: [1.66 1.67 1.8  1.02 0.75 0.   6.89] Loss_P: [ 3.46  2.49  1.84  1.88  1.    0.84 11.51]\n",
      "Loss_Q: [1.69 1.65 1.81 1.05 0.72 0.   6.92] Loss_P: [ 3.46  2.54  1.79  1.91  1.02  0.85 11.57]\n",
      "Loss_Q: [1.66 1.66 1.75 1.03 0.72 0.   6.82] Loss_P: [ 3.44  2.41  1.82  1.84  1.05  0.8  11.37]\n",
      "Loss_Q: [1.67 1.69 1.77 1.1  0.78 0.   7.  ] Loss_P: [ 3.41  2.48  1.91  1.82  1.04  0.88 11.54]\n",
      "Loss_Q: [1.62 1.7  1.69 1.03 0.75 0.   6.79] Loss_P: [ 3.47  2.43  1.89  1.83  1.07  0.85 11.54]\n",
      "Loss_Q: [1.71 1.72 1.72 1.07 0.74 0.   6.95] Loss_P: [ 3.44  2.41  1.87  1.83  1.03  0.86 11.45]\n",
      "Loss_Q: [1.74 1.69 1.74 1.03 0.78 0.   6.98] Loss_P: [ 3.4   2.49  1.94  1.8   1.03  0.85 11.51]\n",
      "Loss_Q: [1.7  1.74 1.67 1.05 0.77 0.   6.93] Loss_P: [ 3.44  2.47  1.9   1.76  1.02  0.87 11.46]\n",
      "Loss_Q: [1.68 1.67 1.61 0.99 0.77 0.   6.72] Loss_P: [ 3.44  2.48  1.97  1.74  1.02  0.85 11.5 ]\n",
      "Loss_Q: [1.7  1.74 1.7  1.06 0.75 0.   6.95] Loss_P: [ 3.5   2.45  1.9   1.72  1.07  0.84 11.47]\n",
      "Loss_Q: [1.72 1.75 1.66 1.06 0.76 0.   6.96] Loss_P: [ 3.41  2.44  1.92  1.76  1.05  0.83 11.41]\n",
      "Loss_Q: [1.71 1.78 1.63 1.02 0.75 0.   6.88] Loss_P: [ 3.47  2.47  1.96  1.72  1.04  0.86 11.52]\n",
      "Loss_Q: [1.71 1.78 1.7  0.98 0.79 0.   6.96] Loss_P: [ 3.45  2.53  1.9   1.78  0.99  0.87 11.52]\n",
      "Loss_Q: [1.73 1.73 1.78 1.   0.78 0.   7.02] Loss_P: [ 3.41  2.5   1.87  1.91  1.02  0.87 11.58]\n",
      "Loss_Q: [1.7  1.78 1.85 1.01 0.78 0.   7.13] Loss_P: [ 3.42  2.54  1.92  1.94  1.04  0.85 11.71]\n",
      "Loss_Q: [1.7  1.7  1.81 1.   0.78 0.   6.99] Loss_P: [ 3.42  2.52  1.92  1.95  1.03  0.85 11.69]\n",
      "Loss_Q: [1.76 1.72 1.79 1.03 0.75 0.   7.05] Loss_P: [ 3.4   2.55  1.83  1.87  1.01  0.82 11.49]\n",
      "Loss_Q: [1.67 1.75 1.81 0.99 0.74 0.   6.96] Loss_P: [ 3.47  2.51  1.87  1.93  1.05  0.84 11.66]\n",
      "Loss_Q: [1.71 1.66 1.85 0.95 0.77 0.   6.94] Loss_P: [ 3.42  2.48  1.85  1.89  1.    0.81 11.45]\n",
      "Loss_Q: [1.66 1.64 1.85 0.99 0.77 0.   6.92] Loss_P: [ 3.45  2.49  1.8   1.92  0.96  0.81 11.43]\n",
      "Loss_Q: [1.74 1.76 1.81 0.96 0.79 0.   7.07] Loss_P: [ 3.41  2.48  1.87  1.9   0.97  0.84 11.48]\n",
      "Loss_Q: [1.67 1.73 1.85 0.97 0.77 0.   7.  ] Loss_P: [ 3.42  2.43  1.95  1.96  1.02  0.86 11.63]\n",
      "Loss_Q: [1.69 1.71 1.8  0.99 0.72 0.   6.92] Loss_P: [ 3.41  2.44  1.91  1.9   1.02  0.83 11.5 ]\n",
      "Loss_Q: [1.72 1.69 1.8  0.98 0.77 0.   6.95] Loss_P: [ 3.43  2.49  1.89  1.91  1.02  0.82 11.55]\n",
      "Loss_Q: [1.77 1.64 1.88 1.01 0.76 0.   7.05] Loss_P: [ 3.43  2.54  1.79  1.96  1.    0.81 11.53]\n",
      "Loss_Q: [1.67 1.68 1.77 0.98 0.75 0.   6.86] Loss_P: [ 3.42  2.54  1.87  1.91  1.04  0.81 11.59]\n",
      "Loss_Q: [1.73 1.69 1.75 0.97 0.74 0.   6.88] Loss_P: [ 3.44  2.54  1.8   1.85  0.99  0.82 11.43]\n",
      "Loss_Q: [1.66 1.63 1.7  0.95 0.73 0.   6.68] Loss_P: [ 3.42  2.48  1.79  1.82  1.    0.78 11.29]\n",
      "Loss_Q: [1.7  1.69 1.67 0.94 0.74 0.   6.73] Loss_P: [ 3.4   2.41  1.84  1.8   0.95  0.8  11.2 ]\n",
      "Loss_Q: [1.72 1.64 1.77 0.96 0.75 0.   6.83] Loss_P: [ 3.43  2.46  1.81  1.81  0.94  0.82 11.27]\n",
      "Loss_Q: [1.74 1.71 1.75 0.95 0.74 0.   6.89] Loss_P: [ 3.4   2.51  1.92  1.89  1.    0.82 11.54]\n",
      "Loss_Q: [1.63 1.66 1.78 0.97 0.76 0.   6.79] Loss_P: [ 3.4   2.48  1.87  1.89  0.99  0.84 11.48]\n",
      "Loss_Q: [1.73 1.85 1.77 1.03 0.77 0.   7.15] Loss_P: [ 3.4   2.48  2.01  1.91  0.99  0.81 11.59]\n",
      "Loss_Q: [1.7  1.82 1.78 0.99 0.79 0.   7.08] Loss_P: [ 3.44  2.52  1.99  1.91  1.    0.82 11.69]\n",
      "Loss_Q: [1.71 1.78 1.82 1.   0.78 0.   7.09] Loss_P: [ 3.37  2.53  1.99  1.96  1.01  0.85 11.69]\n",
      "Loss_Q: [1.74 1.8  1.86 1.01 0.79 0.   7.21] Loss_P: [ 3.45  2.51  1.97  1.98  1.06  0.89 11.85]\n",
      "Loss_Q: [1.7  1.76 1.86 0.98 0.75 0.   7.05] Loss_P: [ 3.44  2.41  1.96  1.97  1.01  0.82 11.61]\n",
      "Loss_Q: [1.71 1.77 1.86 1.01 0.78 0.   7.13] Loss_P: [ 3.45  2.41  1.97  1.97  1.03  0.86 11.7 ]\n",
      "Loss_Q: [1.71 1.74 1.8  0.99 0.78 0.   7.02] Loss_P: [ 3.39  2.43  1.96  1.9   1.01  0.85 11.53]\n",
      "Loss_Q: [1.65 1.78 1.84 1.01 0.77 0.   7.04] Loss_P: [ 3.42  2.48  1.97  1.99  1.02  0.84 11.72]\n",
      "Loss_Q: [1.7  1.73 1.82 1.   0.77 0.   7.01] Loss_P: [ 3.37  2.52  1.99  1.92  1.03  0.83 11.66]\n",
      "Loss_Q: [1.69 1.77 1.81 1.04 0.76 0.   7.06] Loss_P: [ 3.41  2.46  1.98  1.92  1.07  0.85 11.68]\n",
      "Loss_Q: [1.71 1.83 1.73 1.01 0.74 0.   7.01] Loss_P: [ 3.44  2.45  2.08  1.9   1.05  0.85 11.77]\n",
      "Loss_Q: [1.72 1.78 1.71 0.98 0.76 0.   6.95] Loss_P: [ 3.4   2.53  1.95  1.82  1.01  0.84 11.56]\n",
      "Loss_Q: [1.74 1.73 1.7  0.95 0.77 0.   6.89] Loss_P: [ 3.45  2.49  1.9   1.8   1.04  0.83 11.51]\n",
      "Loss_Q: [1.71 1.66 1.7  1.   0.78 0.   6.86] Loss_P: [ 3.43  2.52  1.93  1.74  1.02  0.87 11.52]\n",
      "Loss_Q: [1.7  1.64 1.73 1.02 0.79 0.   6.88] Loss_P: [ 3.41  2.58  1.92  1.82  1.07  0.87 11.67]\n",
      "Loss_Q: [1.78 1.64 1.68 0.99 0.76 0.   6.86] Loss_P: [ 3.36  2.63  1.81  1.77  1.02  0.83 11.42]\n",
      "Loss_Q: [1.77 1.57 1.65 0.97 0.77 0.   6.73] Loss_P: [ 3.39  2.6   1.82  1.75  1.01  0.85 11.41]\n",
      "Loss_Q: [1.73 1.62 1.72 1.   0.79 0.   6.86] Loss_P: [ 3.4   2.58  1.84  1.78  1.09  0.86 11.54]\n",
      "Loss_Q: [1.75 1.58 1.72 1.   0.8  0.   6.85] Loss_P: [ 3.37  2.61  1.79  1.83  1.08  0.9  11.58]\n",
      "Loss_Q: [1.71 1.55 1.78 1.04 0.78 0.   6.86] Loss_P: [ 3.35  2.6   1.79  1.82  1.09  0.86 11.51]\n",
      "Loss_Q: [1.74 1.62 1.76 1.05 0.78 0.   6.95] Loss_P: [ 3.37  2.62  1.81  1.84  1.08  0.84 11.55]\n",
      "Loss_Q: [1.7  1.63 1.73 1.04 0.76 0.   6.85] Loss_P: [ 3.38  2.55  1.82  1.82  1.05  0.86 11.49]\n",
      "Loss_Q: [1.76 1.7  1.79 1.07 0.78 0.   7.1 ] Loss_P: [ 3.35  2.56  1.81  1.82  1.04  0.83 11.42]\n",
      "Loss_Q: [1.69 1.7  1.82 1.09 0.78 0.   7.08] Loss_P: [ 3.42  2.56  1.77  1.87  1.09  0.86 11.58]\n",
      "Loss_Q: [1.7  1.63 1.67 1.06 0.8  0.   6.87] Loss_P: [ 3.38  2.53  1.86  1.8   1.1   0.87 11.54]\n",
      "Loss_Q: [1.73 1.65 1.67 1.08 0.77 0.   6.91] Loss_P: [ 3.39  2.53  1.83  1.73  1.14  0.86 11.48]\n",
      "Loss_Q: [1.75 1.62 1.64 1.11 0.78 0.   6.91] Loss_P: [ 3.37  2.54  1.81  1.75  1.11  0.81 11.38]\n",
      "Loss_Q: [1.75 1.68 1.72 1.08 0.77 0.   7.  ] Loss_P: [ 3.38  2.6   1.82  1.8   1.12  0.86 11.57]\n",
      "Loss_Q: [1.76 1.64 1.71 1.13 0.78 0.   7.02] Loss_P: [ 3.37  2.55  1.85  1.78  1.14  0.84 11.54]\n",
      "Loss_Q: [1.79 1.65 1.75 1.15 0.78 0.   7.11] Loss_P: [ 3.41  2.55  1.75  1.82  1.13  0.87 11.53]\n",
      "Loss_Q: [1.78 1.68 1.77 1.1  0.77 0.   7.1 ] Loss_P: [ 3.36  2.57  1.85  1.82  1.18  0.87 11.64]\n",
      "Loss_Q: [1.75 1.62 1.79 1.19 0.79 0.   7.13] Loss_P: [ 3.36  2.54  1.85  1.89  1.21  0.88 11.73]\n",
      "Loss_Q: [1.76 1.74 1.79 1.16 0.76 0.   7.22] Loss_P: [ 3.36  2.56  1.87  1.86  1.19  0.88 11.73]\n",
      "Loss_Q: [1.74 1.75 1.79 1.12 0.77 0.   7.18] Loss_P: [ 3.4   2.55  1.9   1.85  1.17  0.85 11.73]\n",
      "Loss_Q: [1.8  1.75 1.78 1.1  0.78 0.   7.21] Loss_P: [ 3.33  2.59  1.91  1.88  1.16  0.87 11.74]\n",
      "Loss_Q: [1.69 1.7  1.76 1.15 0.78 0.   7.07] Loss_P: [ 3.39  2.58  1.93  1.89  1.21  0.84 11.83]\n",
      "Loss_Q: [1.76 1.74 1.82 1.17 0.76 0.   7.25] Loss_P: [ 3.32  2.61  1.89  1.88  1.18  0.84 11.72]\n",
      "Loss_Q: [1.72 1.72 1.72 1.13 0.77 0.   7.06] Loss_P: [ 3.41  2.55  1.85  1.83  1.18  0.85 11.67]\n",
      "Loss_Q: [1.75 1.6  1.73 1.13 0.74 0.   6.96] Loss_P: [ 3.39  2.58  1.86  1.81  1.18  0.87 11.69]\n",
      "Loss_Q: [1.76 1.67 1.72 1.14 0.74 0.   7.03] Loss_P: [ 3.38  2.6   1.81  1.84  1.14  0.88 11.65]\n",
      "Loss_Q: [1.74 1.6  1.77 1.17 0.73 0.   7.01] Loss_P: [ 3.42  2.54  1.84  1.84  1.15  0.8  11.59]\n",
      "Loss_Q: [1.78 1.57 1.82 1.07 0.73 0.   6.98] Loss_P: [ 3.4   2.62  1.73  1.87  1.09  0.82 11.52]\n",
      "Loss_Q: [1.8  1.69 1.78 1.11 0.73 0.   7.11] Loss_P: [ 3.38  2.56  1.79  1.83  1.08  0.82 11.47]\n",
      "Loss_Q: [1.73 1.67 1.77 1.14 0.73 0.   7.04] Loss_P: [ 3.41  2.57  1.84  1.84  1.13  0.82 11.61]\n",
      "Loss_Q: [1.79 1.72 1.75 1.12 0.71 0.   7.1 ] Loss_P: [ 3.4   2.57  1.89  1.84  1.12  0.79 11.61]\n",
      "Loss_Q: [1.74 1.78 1.7  1.09 0.72 0.   7.03] Loss_P: [ 3.36  2.58  1.89  1.87  1.11  0.77 11.58]\n",
      "Loss_Q: [1.82 1.73 1.74 1.14 0.7  0.   7.12] Loss_P: [ 3.4   2.61  1.83  1.82  1.16  0.8  11.63]\n",
      "Loss_Q: [1.77 1.8  1.72 1.09 0.74 0.   7.12] Loss_P: [ 3.35  2.61  1.99  1.83  1.1   0.8  11.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.82 1.78 1.73 1.03 0.72 0.   7.09] Loss_P: [ 3.41  2.52  2.    1.85  1.06  0.82 11.66]\n",
      "Loss_Q: [1.79 1.84 1.67 1.02 0.7  0.   7.02] Loss_P: [ 3.42  2.58  2.03  1.83  1.06  0.75 11.66]\n",
      "Loss_Q: [1.79 1.83 1.7  1.01 0.72 0.   7.06] Loss_P: [ 3.4   2.58  2.01  1.78  1.01  0.77 11.54]\n",
      "Loss_Q: [1.8  1.79 1.63 1.03 0.7  0.   6.96] Loss_P: [ 3.44  2.63  2.02  1.78  1.03  0.82 11.71]\n",
      "Loss_Q: [1.8  1.88 1.67 1.04 0.78 0.   7.18] Loss_P: [ 3.45  2.54  2.1   1.75  1.06  0.83 11.73]\n",
      "Loss_Q: [1.85 1.91 1.66 1.1  0.78 0.   7.29] Loss_P: [ 3.34  2.59  2.1   1.77  1.09  0.92 11.82]\n",
      "Loss_Q: [1.83 1.86 1.64 1.08 0.81 0.   7.22] Loss_P: [ 3.41  2.54  2.04  1.75  1.16  0.91 11.81]\n",
      "Loss_Q: [1.81 1.89 1.72 1.12 0.84 0.   7.39] Loss_P: [ 3.41  2.61  2.09  1.8   1.15  0.92 11.98]\n",
      "Loss_Q: [1.77 1.89 1.72 1.11 0.78 0.   7.28] Loss_P: [ 3.45  2.56  2.04  1.78  1.17  0.91 11.91]\n",
      "Loss_Q: [1.81 1.84 1.7  1.13 0.83 0.   7.31] Loss_P: [ 3.5   2.54  2.05  1.79  1.13  0.91 11.93]\n",
      "Loss_Q: [1.8  1.79 1.69 1.13 0.8  0.   7.21] Loss_P: [ 3.44  2.55  1.97  1.79  1.15  0.89 11.8 ]\n",
      "Loss_Q: [1.79 1.8  1.73 1.1  0.84 0.   7.26] Loss_P: [ 3.44  2.55  1.95  1.74  1.09  0.94 11.71]\n",
      "Loss_Q: [1.77 1.78 1.7  1.07 0.83 0.   7.15] Loss_P: [ 3.51  2.51  1.92  1.8   1.1   0.92 11.77]\n",
      "Loss_Q: [1.79 1.81 1.67 1.08 0.83 0.   7.19] Loss_P: [ 3.41  2.58  2.03  1.74  1.09  0.94 11.78]\n",
      "Loss_Q: [1.78 1.77 1.66 1.07 0.85 0.   7.13] Loss_P: [ 3.45  2.54  2.01  1.76  1.09  0.94 11.79]\n",
      "Loss_Q: [1.77 1.75 1.65 1.09 0.87 0.   7.13] Loss_P: [ 3.47  2.5   1.96  1.72  1.07  0.95 11.65]\n",
      "Loss_Q: [1.77 1.82 1.67 1.06 0.89 0.   7.21] Loss_P: [ 3.47  2.5   1.99  1.74  1.09  0.97 11.77]\n",
      "Loss_Q: [1.75 1.91 1.63 1.02 0.86 0.   7.17] Loss_P: [ 3.49  2.47  2.04  1.71  1.06  0.98 11.74]\n",
      "Loss_Q: [1.77 1.78 1.64 1.03 0.89 0.   7.1 ] Loss_P: [ 3.44  2.53  2.01  1.63  1.1   0.95 11.66]\n",
      "Loss_Q: [1.79 1.78 1.58 1.04 0.87 0.   7.06] Loss_P: [ 3.47  2.53  1.96  1.68  1.09  0.96 11.69]\n",
      "Loss_Q: [1.81 1.72 1.6  1.04 0.87 0.   7.05] Loss_P: [ 3.4   2.51  1.97  1.66  1.05  0.95 11.55]\n",
      "Loss_Q: [1.75 1.76 1.61 1.04 0.84 0.   6.99] Loss_P: [ 3.42  2.55  1.98  1.68  1.07  0.94 11.64]\n",
      "Loss_Q: [1.71 1.86 1.58 1.04 0.86 0.   7.05] Loss_P: [ 3.45  2.5   2.01  1.63  1.03  0.95 11.57]\n",
      "Loss_Q: [1.78 1.84 1.56 1.04 0.89 0.   7.11] Loss_P: [ 3.38  2.49  2.05  1.61  1.08  0.96 11.57]\n",
      "Loss_Q: [1.77 1.82 1.57 1.05 0.86 0.   7.06] Loss_P: [ 3.46  2.51  1.98  1.59  1.04  0.97 11.55]\n",
      "Loss_Q: [1.72 1.76 1.48 0.98 0.87 0.   6.81] Loss_P: [ 3.46  2.46  2.    1.63  1.11  0.98 11.64]\n",
      "Loss_Q: [1.75 1.8  1.54 1.06 0.87 0.   7.02] Loss_P: [ 3.44  2.5   1.9   1.59  1.06  0.99 11.48]\n",
      "Loss_Q: [1.78 1.74 1.54 1.05 0.89 0.   7.  ] Loss_P: [ 3.44  2.5   1.96  1.61  1.08  1.01 11.6 ]\n",
      "Loss_Q: [1.7  1.75 1.51 1.03 0.88 0.   6.87] Loss_P: [ 3.44  2.49  1.96  1.63  1.07  0.96 11.55]\n",
      "Loss_Q: [1.74 1.76 1.56 1.07 0.84 0.   6.97] Loss_P: [ 3.45  2.44  1.92  1.65  1.13  0.98 11.57]\n",
      "Loss_Q: [1.71 1.68 1.56 1.   0.91 0.   6.85] Loss_P: [ 3.51  2.43  1.9   1.67  1.05  1.   11.56]\n",
      "Loss_Q: [1.76 1.75 1.61 0.98 0.87 0.   6.96] Loss_P: [ 3.45  2.48  1.9   1.68  1.02  0.97 11.5 ]\n",
      "Loss_Q: [1.75 1.79 1.58 1.01 0.86 0.   7.  ] Loss_P: [ 3.48  2.46  1.97  1.62  0.96  0.99 11.47]\n",
      "Loss_Q: [1.77 1.72 1.54 0.95 0.86 0.   6.83] Loss_P: [ 3.42  2.5   1.99  1.66  1.01  0.94 11.52]\n",
      "Loss_Q: [1.77 1.82 1.59 0.95 0.85 0.   6.98] Loss_P: [ 3.42  2.49  1.91  1.7   1.02  0.95 11.5 ]\n",
      "Loss_Q: [1.68 1.76 1.63 0.99 0.86 0.   6.93] Loss_P: [ 3.5   2.52  1.93  1.73  1.02  0.95 11.65]\n",
      "Loss_Q: [1.68 1.77 1.65 0.97 0.85 0.   6.92] Loss_P: [ 3.51  2.48  1.92  1.72  1.04  0.99 11.66]\n",
      "Loss_Q: [1.7  1.81 1.63 0.98 0.85 0.   6.96] Loss_P: [ 3.43  2.53  1.88  1.7   0.97  0.98 11.5 ]\n",
      "Loss_Q: [1.75 1.8  1.57 0.94 0.87 0.   6.93] Loss_P: [ 3.45  2.5   1.91  1.68  0.96  0.92 11.43]\n",
      "Loss_Q: [1.71 1.83 1.58 0.86 0.85 0.   6.83] Loss_P: [ 3.49  2.48  2.01  1.69  0.96  0.94 11.56]\n",
      "Loss_Q: [1.74 1.89 1.56 0.91 0.83 0.   6.94] Loss_P: [ 3.49  2.51  1.94  1.68  0.89  0.91 11.44]\n",
      "Loss_Q: [1.73 1.85 1.61 0.92 0.83 0.   6.95] Loss_P: [ 3.4   2.53  2.04  1.74  0.98  0.92 11.61]\n",
      "Loss_Q: [1.8  1.88 1.61 0.93 0.84 0.   7.06] Loss_P: [ 3.45  2.48  2.03  1.72  1.    0.93 11.6 ]\n",
      "Loss_Q: [1.76 1.93 1.54 0.9  0.81 0.   6.95] Loss_P: [ 3.44  2.52  2.08  1.76  0.98  0.92 11.7 ]\n",
      "Loss_Q: [1.74 1.98 1.57 0.94 0.82 0.   7.06] Loss_P: [ 3.46  2.49  2.14  1.68  0.95  0.92 11.65]\n",
      "Loss_Q: [1.73 2.07 1.6  0.91 0.84 0.   7.15] Loss_P: [ 3.44  2.47  2.19  1.73  0.96  0.92 11.72]\n",
      "Loss_Q: [1.83 1.96 1.69 0.95 0.85 0.   7.27] Loss_P: [ 3.42  2.52  2.2   1.77  0.98  0.94 11.83]\n",
      "Loss_Q: [1.71 2.03 1.62 0.87 0.87 0.   7.1 ] Loss_P: [ 3.43  2.44  2.22  1.73  0.93  1.   11.74]\n",
      "Loss_Q: [1.72 1.96 1.6  0.93 0.89 0.   7.09] Loss_P: [ 3.4   2.48  2.18  1.77  0.94  0.99 11.76]\n",
      "Loss_Q: [1.75 1.93 1.61 0.92 0.89 0.   7.1 ] Loss_P: [ 3.43  2.44  2.2   1.71  0.94  1.   11.72]\n",
      "Loss_Q: [1.83 1.84 1.67 0.94 0.9  0.   7.17] Loss_P: [ 3.4   2.52  2.08  1.75  0.97  0.98 11.7 ]\n",
      "Loss_Q: [1.76 1.83 1.71 0.97 0.9  0.   7.17] Loss_P: [ 3.43  2.53  2.    1.77  0.93  0.98 11.65]\n",
      "Loss_Q: [1.75 1.82 1.76 1.01 0.9  0.   7.24] Loss_P: [ 3.36  2.59  2.    1.76  1.01  0.99 11.72]\n",
      "Loss_Q: [1.75 1.79 1.59 0.97 0.9  0.   6.99] Loss_P: [ 3.38  2.57  2.01  1.73  1.    1.01 11.7 ]\n",
      "Loss_Q: [1.84 1.81 1.66 0.93 0.87 0.   7.11] Loss_P: [ 3.36  2.59  2.02  1.72  1.    0.97 11.64]\n",
      "Loss_Q: [1.74 1.83 1.68 0.93 0.86 0.   7.05] Loss_P: [ 3.35  2.58  2.03  1.76  0.93  0.98 11.64]\n",
      "Loss_Q: [1.78 1.85 1.69 0.89 0.86 0.   7.06] Loss_P: [ 3.39  2.52  2.02  1.76  0.93  0.98 11.61]\n",
      "Loss_Q: [1.83 1.87 1.74 0.91 0.9  0.   7.25] Loss_P: [ 3.39  2.58  2.08  1.85  0.93  0.95 11.79]\n",
      "Loss_Q: [1.83 1.87 1.72 0.84 0.89 0.   7.15] Loss_P: [ 3.38  2.58  2.13  1.84  0.88  0.96 11.76]\n",
      "Loss_Q: [1.8  1.84 1.73 0.81 0.9  0.   7.07] Loss_P: [ 3.42  2.58  2.09  1.85  0.83  0.98 11.75]\n",
      "Loss_Q: [1.81 1.87 1.68 0.82 0.86 0.   7.06] Loss_P: [ 3.38  2.59  2.14  1.83  0.89  0.96 11.8 ]\n",
      "Loss_Q: [1.79 1.83 1.74 0.83 0.89 0.   7.07] Loss_P: [ 3.42  2.55  2.11  1.85  0.87  0.95 11.74]\n",
      "Loss_Q: [1.77 1.86 1.77 0.85 0.89 0.   7.14] Loss_P: [ 3.36  2.58  2.08  1.89  0.87  0.98 11.76]\n",
      "Loss_Q: [1.84 1.85 1.77 0.82 0.87 0.   7.15] Loss_P: [ 3.41  2.55  2.1   1.89  0.86  0.98 11.79]\n",
      "Loss_Q: [1.83 1.86 1.75 0.82 0.87 0.   7.13] Loss_P: [ 3.33  2.64  2.05  1.88  0.86  0.98 11.73]\n",
      "Loss_Q: [1.82 1.85 1.72 0.83 0.91 0.   7.12] Loss_P: [ 3.42  2.54  2.04  1.85  0.83  0.99 11.67]\n",
      "Loss_Q: [1.78 1.8  1.68 0.78 0.9  0.   6.94] Loss_P: [ 3.42  2.59  1.98  1.79  0.79  0.96 11.54]\n",
      "Loss_Q: [1.78 1.87 1.69 0.79 0.88 0.   7.02] Loss_P: [ 3.39  2.55  2.04  1.82  0.81  0.97 11.57]\n",
      "Loss_Q: [1.74 1.88 1.67 0.76 0.88 0.   6.94] Loss_P: [ 3.37  2.58  2.11  1.82  0.85  0.96 11.7 ]\n",
      "Loss_Q: [1.81 1.94 1.71 0.83 0.87 0.   7.16] Loss_P: [ 3.41  2.54  2.14  1.85  0.92  0.98 11.84]\n",
      "Loss_Q: [1.78 1.98 1.77 0.91 0.88 0.   7.31] Loss_P: [ 3.35  2.63  2.11  1.87  0.88  0.94 11.79]\n",
      "Loss_Q: [1.8  1.88 1.77 0.96 0.87 0.   7.29] Loss_P: [ 3.38  2.57  2.14  1.84  0.99  0.93 11.84]\n",
      "Loss_Q: [1.75 1.89 1.69 0.93 0.85 0.   7.1 ] Loss_P: [ 3.39  2.61  2.09  1.84  0.97  0.95 11.85]\n",
      "Loss_Q: [1.89 1.85 1.72 0.86 0.86 0.   7.18] Loss_P: [ 3.41  2.64  2.1   1.81  0.94  0.96 11.86]\n",
      "Loss_Q: [1.81 1.87 1.72 0.91 0.86 0.   7.17] Loss_P: [ 3.38  2.62  2.07  1.84  0.97  0.96 11.83]\n",
      "Loss_Q: [1.84 1.87 1.75 0.83 0.9  0.   7.19] Loss_P: [ 3.32  2.61  1.98  1.83  0.92  0.92 11.59]\n",
      "Loss_Q: [1.82 1.93 1.72 0.9  0.89 0.   7.25] Loss_P: [ 3.35  2.63  2.09  1.86  0.92  0.94 11.8 ]\n",
      "Loss_Q: [1.83 1.94 1.69 0.85 0.84 0.   7.14] Loss_P: [ 3.39  2.61  2.07  1.82  0.95  0.94 11.79]\n",
      "Loss_Q: [1.81 2.04 1.7  0.91 0.83 0.   7.28] Loss_P: [ 3.32  2.61  2.25  1.75  1.    0.92 11.84]\n",
      "Loss_Q: [1.78 1.93 1.66 0.92 0.82 0.   7.11] Loss_P: [ 3.38  2.59  2.14  1.72  0.96  0.87 11.66]\n",
      "Loss_Q: [1.8  1.96 1.64 0.89 0.84 0.   7.14] Loss_P: [ 3.39  2.61  2.12  1.8   0.96  0.93 11.82]\n",
      "Loss_Q: [1.82 1.92 1.62 0.94 0.81 0.   7.11] Loss_P: [ 3.4   2.62  2.08  1.71  1.    0.89 11.71]\n",
      "Loss_Q: [1.87 1.95 1.66 0.98 0.82 0.   7.28] Loss_P: [ 3.35  2.66  2.1   1.69  1.07  0.89 11.76]\n",
      "Loss_Q: [1.83 1.88 1.59 1.   0.81 0.   7.12] Loss_P: [ 3.44  2.63  2.03  1.65  0.98  0.9  11.63]\n",
      "Loss_Q: [1.77 1.9  1.62 0.98 0.8  0.   7.07] Loss_P: [ 3.41  2.58  2.05  1.73  1.05  0.89 11.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.75 1.79 1.61 0.99 0.82 0.   6.96] Loss_P: [ 3.41  2.59  1.94  1.74  1.01  0.88 11.58]\n",
      "Loss_Q: [1.8  1.84 1.63 0.98 0.77 0.   7.01] Loss_P: [ 3.47  2.65  1.9   1.71  1.03  0.86 11.62]\n",
      "Loss_Q: [1.78 1.76 1.59 0.95 0.78 0.   6.86] Loss_P: [ 3.38  2.66  1.9   1.7   1.01  0.87 11.51]\n",
      "Loss_Q: [1.76 1.84 1.66 0.94 0.77 0.   6.97] Loss_P: [ 3.42  2.55  2.02  1.74  1.01  0.85 11.58]\n",
      "Loss_Q: [1.76 1.88 1.6  1.01 0.76 0.   7.03] Loss_P: [ 3.43  2.55  1.96  1.7   1.    0.83 11.46]\n",
      "Loss_Q: [1.83 1.75 1.68 1.04 0.79 0.   7.09] Loss_P: [ 3.48  2.54  1.99  1.68  1.09  0.87 11.64]\n",
      "Loss_Q: [1.7  1.73 1.48 1.06 0.81 0.   6.78] Loss_P: [ 3.45  2.49  1.98  1.64  1.07  0.88 11.51]\n",
      "Loss_Q: [1.81 1.77 1.54 1.03 0.86 0.   7.01] Loss_P: [ 3.42  2.55  1.97  1.64  1.01  0.9  11.49]\n",
      "Loss_Q: [1.78 1.81 1.53 1.   0.85 0.   6.96] Loss_P: [ 3.41  2.52  1.9   1.6   1.02  0.92 11.37]\n",
      "Loss_Q: [1.73 1.77 1.55 0.99 0.84 0.   6.88] Loss_P: [ 3.46  2.46  2.01  1.61  1.06  0.94 11.54]\n",
      "Loss_Q: [1.77 1.81 1.54 0.97 0.82 0.   6.91] Loss_P: [ 3.44  2.53  2.05  1.62  1.05  0.89 11.58]\n",
      "Loss_Q: [1.73 1.74 1.54 0.99 0.82 0.   6.83] Loss_P: [ 3.48  2.51  1.94  1.68  0.99  0.91 11.51]\n",
      "Loss_Q: [1.65 1.81 1.56 1.01 0.81 0.   6.83] Loss_P: [ 3.51  2.44  1.97  1.69  1.    0.88 11.48]\n",
      "Loss_Q: [1.69 1.83 1.57 0.98 0.84 0.   6.9 ] Loss_P: [ 3.43  2.52  1.96  1.71  0.99  0.91 11.52]\n",
      "Loss_Q: [1.78 1.8  1.58 0.95 0.85 0.   6.95] Loss_P: [ 3.45  2.54  2.01  1.74  1.    0.89 11.62]\n",
      "Loss_Q: [1.73 1.83 1.59 0.97 0.81 0.   6.92] Loss_P: [ 3.45  2.54  1.98  1.64  0.92  0.87 11.41]\n",
      "Loss_Q: [1.75 1.83 1.61 0.96 0.79 0.   6.94] Loss_P: [ 3.47  2.51  2.05  1.7   0.98  0.85 11.56]\n",
      "Loss_Q: [1.7  1.81 1.56 0.98 0.74 0.   6.78] Loss_P: [ 3.41  2.53  1.99  1.67  0.95  0.83 11.38]\n",
      "Loss_Q: [1.75 1.8  1.58 0.92 0.75 0.   6.8 ] Loss_P: [ 3.44  2.53  1.96  1.67  0.96  0.83 11.39]\n",
      "Loss_Q: [1.7  1.79 1.57 0.95 0.79 0.   6.79] Loss_P: [ 3.43  2.48  2.02  1.64  0.94  0.86 11.38]\n",
      "Loss_Q: [1.74 1.86 1.59 0.9  0.8  0.   6.88] Loss_P: [ 3.46  2.45  2.    1.72  0.95  0.88 11.46]\n",
      "Loss_Q: [1.71 1.82 1.63 0.93 0.79 0.   6.88] Loss_P: [ 3.49  2.52  1.95  1.68  0.94  0.87 11.45]\n",
      "Loss_Q: [1.75 1.8  1.65 0.92 0.81 0.   6.93] Loss_P: [ 3.48  2.44  2.    1.72  0.9   0.87 11.41]\n",
      "Loss_Q: [1.74 1.83 1.63 0.92 0.8  0.   6.92] Loss_P: [ 3.46  2.44  1.94  1.68  0.98  0.87 11.38]\n",
      "Loss_Q: [1.67 1.78 1.58 0.92 0.81 0.   6.76] Loss_P: [ 3.46  2.45  2.01  1.64  0.97  0.9  11.44]\n",
      "Loss_Q: [1.72 1.87 1.56 0.9  0.82 0.   6.87] Loss_P: [ 3.46  2.51  2.04  1.65  0.98  0.94 11.58]\n",
      "Loss_Q: [1.78 1.89 1.6  1.04 0.82 0.   7.13] Loss_P: [ 3.44  2.52  2.04  1.71  0.98  0.92 11.61]\n",
      "Loss_Q: [1.84 1.91 1.62 0.9  0.88 0.   7.14] Loss_P: [ 3.42  2.55  2.05  1.65  0.95  0.95 11.57]\n",
      "Loss_Q: [1.81 1.85 1.61 0.93 0.89 0.   7.08] Loss_P: [ 3.43  2.59  2.05  1.66  0.97  0.96 11.66]\n",
      "Loss_Q: [1.77 1.88 1.63 0.95 0.88 0.   7.11] Loss_P: [ 3.42  2.59  2.05  1.72  0.94  0.98 11.7 ]\n",
      "Loss_Q: [1.78 1.81 1.69 0.93 0.88 0.   7.09] Loss_P: [ 3.41  2.6   2.03  1.74  0.98  0.99 11.75]\n",
      "Loss_Q: [1.73 1.83 1.6  0.96 0.86 0.   6.98] Loss_P: [ 3.4   2.59  2.    1.69  0.91  0.95 11.53]\n",
      "Loss_Q: [1.78 1.85 1.6  0.94 0.82 0.   7.  ] Loss_P: [ 3.42  2.57  1.96  1.73  0.94  0.93 11.56]\n",
      "Loss_Q: [1.75 1.8  1.61 0.94 0.84 0.   6.94] Loss_P: [ 3.4   2.55  2.    1.74  0.96  0.92 11.57]\n",
      "Loss_Q: [1.78 1.81 1.6  0.93 0.85 0.   6.97] Loss_P: [ 3.44  2.59  1.97  1.74  0.93  0.95 11.61]\n",
      "Loss_Q: [1.83 1.73 1.59 0.95 0.84 0.   6.95] Loss_P: [ 3.37  2.66  1.88  1.71  0.94  0.95 11.52]\n",
      "Loss_Q: [1.72 1.8  1.6  0.92 0.87 0.   6.91] Loss_P: [ 3.45  2.58  1.96  1.66  0.96  0.95 11.55]\n",
      "Loss_Q: [1.7  1.81 1.58 0.87 0.88 0.   6.84] Loss_P: [ 3.43  2.55  2.01  1.69  0.91  0.98 11.56]\n",
      "Loss_Q: [1.81 1.78 1.55 0.93 0.87 0.   6.94] Loss_P: [ 3.38  2.67  2.01  1.71  0.94  0.95 11.65]\n",
      "Loss_Q: [1.75 1.81 1.58 0.94 0.85 0.   6.92] Loss_P: [ 3.44  2.58  1.96  1.65  0.96  0.96 11.55]\n",
      "Loss_Q: [1.75 1.88 1.54 0.92 0.86 0.   6.94] Loss_P: [ 3.39  2.54  2.06  1.63  0.89  0.98 11.48]\n",
      "Loss_Q: [1.74 1.84 1.53 0.9  0.9  0.   6.92] Loss_P: [ 3.46  2.58  1.99  1.63  0.93  1.01 11.59]\n",
      "Loss_Q: [1.71 1.87 1.49 0.84 0.88 0.   6.79] Loss_P: [ 3.41  2.52  2.05  1.57  0.84  0.96 11.35]\n",
      "Loss_Q: [1.72 1.89 1.49 0.86 0.9  0.   6.85] Loss_P: [ 3.47  2.52  2.06  1.6   0.88  0.94 11.47]\n",
      "Loss_Q: [1.73 1.85 1.53 0.81 0.87 0.   6.79] Loss_P: [ 3.45  2.56  2.06  1.61  0.84  0.97 11.48]\n",
      "Loss_Q: [1.69 1.83 1.52 0.8  0.89 0.   6.73] Loss_P: [ 3.44  2.53  2.06  1.62  0.8   0.95 11.41]\n",
      "Loss_Q: [1.72 1.81 1.58 0.77 0.84 0.   6.73] Loss_P: [ 3.45  2.59  1.99  1.6   0.73  0.96 11.31]\n",
      "Loss_Q: [1.75 1.85 1.54 0.74 0.85 0.   6.73] Loss_P: [ 3.38  2.68  2.02  1.63  0.81  0.92 11.44]\n",
      "Loss_Q: [1.79 1.84 1.57 0.79 0.84 0.   6.85] Loss_P: [ 3.39  2.66  1.94  1.67  0.76  0.93 11.36]\n",
      "Loss_Q: [1.78 1.9  1.53 0.8  0.86 0.   6.87] Loss_P: [ 3.43  2.59  2.04  1.66  0.78  0.94 11.44]\n",
      "Loss_Q: [1.76 1.89 1.56 0.74 0.86 0.   6.81] Loss_P: [ 3.4   2.6   2.05  1.7   0.8   0.92 11.48]\n",
      "Loss_Q: [1.74 1.94 1.57 0.81 0.84 0.   6.9 ] Loss_P: [ 3.44  2.6   2.06  1.66  0.82  0.93 11.51]\n",
      "Loss_Q: [1.77 1.9  1.53 0.74 0.82 0.   6.76] Loss_P: [ 3.4   2.6   2.07  1.7   0.81  0.89 11.47]\n",
      "Loss_Q: [1.68 1.84 1.55 0.8  0.81 0.   6.69] Loss_P: [ 3.39  2.58  2.07  1.64  0.75  0.9  11.34]\n",
      "Loss_Q: [1.74 1.88 1.53 0.77 0.84 0.   6.76] Loss_P: [ 3.42  2.58  2.1   1.6   0.78  0.93 11.41]\n",
      "Loss_Q: [1.76 1.98 1.47 0.68 0.81 0.   6.71] Loss_P: [ 3.44  2.54  2.19  1.62  0.73  0.9  11.42]\n",
      "Loss_Q: [1.73 1.98 1.47 0.74 0.8  0.   6.72] Loss_P: [ 3.47  2.52  2.21  1.61  0.78  0.88 11.47]\n",
      "Loss_Q: [1.74 2.03 1.42 0.74 0.78 0.   6.71] Loss_P: [ 3.38  2.51  2.24  1.55  0.81  0.87 11.36]\n",
      "Loss_Q: [1.8  2.02 1.43 0.72 0.8  0.   6.79] Loss_P: [ 3.38  2.59  2.25  1.57  0.73  0.86 11.38]\n",
      "Loss_Q: [1.76 2.01 1.41 0.75 0.79 0.   6.72] Loss_P: [ 3.4   2.54  2.25  1.55  0.77  0.92 11.43]\n",
      "Loss_Q: [1.76 2.05 1.46 0.8  0.82 0.   6.9 ] Loss_P: [ 3.41  2.56  2.18  1.53  0.76  0.9  11.34]\n",
      "Loss_Q: [1.77 2.03 1.52 0.78 0.85 0.   6.95] Loss_P: [ 3.45  2.58  2.22  1.56  0.76  0.91 11.47]\n",
      "Loss_Q: [1.74 2.01 1.44 0.7  0.84 0.   6.74] Loss_P: [ 3.41  2.56  2.32  1.6   0.77  0.93 11.59]\n",
      "Loss_Q: [1.78 2.05 1.42 0.73 0.84 0.   6.81] Loss_P: [ 3.44  2.53  2.32  1.55  0.76  0.93 11.54]\n",
      "Loss_Q: [1.72 2.17 1.4  0.74 0.84 0.   6.86] Loss_P: [ 3.42  2.55  2.33  1.56  0.8   0.9  11.55]\n",
      "Loss_Q: [1.74 2.18 1.4  0.83 0.8  0.   6.96] Loss_P: [ 3.41  2.51  2.37  1.47  0.8   0.92 11.47]\n",
      "Loss_Q: [1.71 2.2  1.35 0.85 0.87 0.   6.99] Loss_P: [ 3.43  2.48  2.39  1.51  0.83  0.93 11.58]\n",
      "Loss_Q: [1.75 2.2  1.38 0.81 0.83 0.   6.98] Loss_P: [ 3.46  2.49  2.4   1.51  0.85  0.94 11.66]\n",
      "Loss_Q: [1.71 2.3  1.42 0.88 0.84 0.   7.15] Loss_P: [ 3.4   2.49  2.4   1.52  0.88  0.97 11.66]\n",
      "Loss_Q: [1.73 2.21 1.31 0.78 0.85 0.   6.88] Loss_P: [ 3.43  2.46  2.43  1.53  0.9   0.94 11.69]\n",
      "Loss_Q: [1.7  2.19 1.42 0.88 0.84 0.   7.03] Loss_P: [ 3.47  2.49  2.41  1.53  0.88  0.92 11.71]\n",
      "Loss_Q: [1.75 2.17 1.38 0.84 0.81 0.   6.96] Loss_P: [ 3.4   2.49  2.35  1.51  0.91  0.9  11.56]\n",
      "Loss_Q: [1.76 2.18 1.36 0.88 0.8  0.   6.97] Loss_P: [ 3.45  2.45  2.41  1.5   0.92  0.91 11.64]\n",
      "Loss_Q: [1.74 2.15 1.34 0.84 0.8  0.   6.88] Loss_P: [ 3.42  2.47  2.37  1.5   0.84  0.91 11.5 ]\n",
      "Loss_Q: [1.76 2.21 1.37 0.88 0.85 0.   7.07] Loss_P: [ 3.42  2.52  2.44  1.48  0.85  0.95 11.66]\n",
      "Loss_Q: [1.8  2.16 1.34 0.81 0.89 0.   7.  ] Loss_P: [ 3.5   2.47  2.42  1.49  0.81  0.96 11.63]\n",
      "Loss_Q: [1.68 2.16 1.38 0.8  0.87 0.   6.88] Loss_P: [ 3.45  2.46  2.41  1.47  0.86  0.99 11.64]\n",
      "Loss_Q: [1.73 2.16 1.36 0.77 0.87 0.   6.89] Loss_P: [ 3.46  2.46  2.34  1.43  0.8   0.97 11.45]\n",
      "Loss_Q: [1.71 2.16 1.33 0.83 0.89 0.   6.92] Loss_P: [ 3.43  2.47  2.39  1.38  0.88  0.98 11.52]\n",
      "Loss_Q: [1.68 2.11 1.35 0.88 0.87 0.   6.9 ] Loss_P: [ 3.44  2.44  2.37  1.42  0.83  0.97 11.47]\n",
      "Loss_Q: [1.71 2.2  1.35 0.95 0.84 0.   7.05] Loss_P: [ 3.45  2.47  2.35  1.44  0.88  0.93 11.52]\n",
      "Loss_Q: [1.7  2.15 1.31 0.84 0.86 0.   6.86] Loss_P: [ 3.42  2.49  2.37  1.46  0.9   0.95 11.59]\n",
      "Loss_Q: [1.71 2.17 1.3  0.83 0.89 0.   6.9 ] Loss_P: [ 3.38  2.53  2.36  1.42  0.81  0.95 11.45]\n",
      "Loss_Q: [1.71 2.22 1.33 0.81 0.86 0.   6.94] Loss_P: [ 3.43  2.51  2.42  1.45  0.86  0.95 11.62]\n",
      "Loss_Q: [1.71 2.18 1.35 0.83 0.86 0.   6.94] Loss_P: [ 3.47  2.5   2.37  1.47  0.87  0.96 11.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.73 2.17 1.35 0.81 0.9  0.   6.96] Loss_P: [ 3.47  2.49  2.42  1.45  0.85  0.98 11.66]\n",
      "Loss_Q: [1.77 2.17 1.36 0.84 0.9  0.   7.04] Loss_P: [ 3.45  2.47  2.35  1.48  0.84  0.98 11.56]\n",
      "Loss_Q: [1.7  2.19 1.3  0.83 0.89 0.   6.92] Loss_P: [ 3.4   2.49  2.4   1.4   0.8   0.99 11.48]\n",
      "Loss_Q: [1.7  2.23 1.38 0.87 0.91 0.   7.09] Loss_P: [ 3.44  2.46  2.4   1.48  0.87  1.02 11.66]\n",
      "Loss_Q: [1.7  2.21 1.44 0.86 0.91 0.   7.11] Loss_P: [ 3.48  2.45  2.38  1.49  0.92  1.   11.7 ]\n",
      "Loss_Q: [1.69 2.09 1.26 0.81 0.91 0.   6.77] Loss_P: [ 3.44  2.44  2.37  1.45  0.82  1.01 11.53]\n",
      "Loss_Q: [1.71 2.05 1.35 0.86 0.91 0.   6.88] Loss_P: [ 3.44  2.5   2.19  1.47  0.82  0.98 11.42]\n",
      "Loss_Q: [1.75 2.05 1.35 0.84 0.91 0.   6.91] Loss_P: [ 3.48  2.5   2.17  1.43  0.87  0.99 11.43]\n",
      "Loss_Q: [1.7  2.06 1.36 0.86 0.9  0.   6.89] Loss_P: [ 3.44  2.46  2.23  1.45  0.91  0.96 11.44]\n",
      "Loss_Q: [1.69 2.05 1.35 0.81 0.89 0.   6.79] Loss_P: [ 3.44  2.48  2.22  1.49  0.86  0.98 11.48]\n",
      "Loss_Q: [1.7  2.05 1.32 0.76 0.89 0.   6.72] Loss_P: [ 3.42  2.5   2.24  1.5   0.83  0.96 11.45]\n",
      "Loss_Q: [1.79 2.08 1.36 0.86 0.91 0.   7.  ] Loss_P: [ 3.44  2.5   2.23  1.5   0.85  0.99 11.51]\n",
      "Loss_Q: [1.65 2.04 1.42 0.84 0.93 0.   6.88] Loss_P: [ 3.39  2.51  2.23  1.49  0.82  1.01 11.44]\n",
      "Loss_Q: [1.73 2.06 1.35 0.81 0.92 0.   6.86] Loss_P: [ 3.44  2.48  2.2   1.51  0.82  1.02 11.48]\n",
      "Loss_Q: [1.77 2.07 1.32 0.85 0.91 0.   6.9 ] Loss_P: [ 3.43  2.57  2.2   1.44  0.82  1.   11.45]\n",
      "Loss_Q: [1.73 2.1  1.36 0.82 0.92 0.   6.94] Loss_P: [ 3.47  2.46  2.23  1.43  0.84  1.01 11.44]\n",
      "Loss_Q: [1.73 2.08 1.37 0.89 0.92 0.   6.99] Loss_P: [ 3.38  2.51  2.29  1.48  0.87  1.01 11.55]\n",
      "Loss_Q: [1.71 2.07 1.36 0.83 0.95 0.   6.92] Loss_P: [ 3.41  2.51  2.23  1.54  0.87  1.02 11.58]\n",
      "Loss_Q: [1.74 2.12 1.44 0.84 0.9  0.   7.04] Loss_P: [ 3.43  2.52  2.23  1.54  0.88  1.01 11.61]\n",
      "Loss_Q: [1.7  2.16 1.38 0.8  0.92 0.   6.96] Loss_P: [ 3.41  2.47  2.34  1.56  0.91  0.99 11.67]\n",
      "Loss_Q: [1.66 2.16 1.31 0.85 0.89 0.   6.87] Loss_P: [ 3.42  2.47  2.31  1.44  0.85  0.95 11.45]\n",
      "Loss_Q: [1.73 2.17 1.3  0.85 0.86 0.   6.91] Loss_P: [ 3.45  2.47  2.33  1.46  0.89  0.95 11.55]\n",
      "Loss_Q: [1.69 2.21 1.33 0.85 0.85 0.   6.95] Loss_P: [ 3.45  2.47  2.37  1.49  0.9   0.96 11.64]\n",
      "Loss_Q: [1.72 2.23 1.38 0.87 0.88 0.   7.07] Loss_P: [ 3.42  2.52  2.42  1.57  0.87  0.99 11.8 ]\n",
      "Loss_Q: [1.79 2.28 1.44 0.92 0.86 0.   7.29] Loss_P: [ 3.44  2.48  2.47  1.52  0.92  0.97 11.8 ]\n",
      "Loss_Q: [1.65 2.23 1.38 0.88 0.88 0.   7.03] Loss_P: [ 3.43  2.43  2.42  1.57  0.91  0.97 11.72]\n",
      "Loss_Q: [1.73 2.23 1.44 0.91 0.89 0.   7.2 ] Loss_P: [ 3.44  2.43  2.43  1.53  0.94  0.97 11.74]\n",
      "Loss_Q: [1.62 2.22 1.41 0.9  0.85 0.   7.01] Loss_P: [ 3.5   2.48  2.37  1.59  0.92  0.95 11.81]\n",
      "Loss_Q: [1.7  2.14 1.42 0.94 0.88 0.   7.07] Loss_P: [ 3.39  2.5   2.31  1.53  0.89  0.97 11.6 ]\n",
      "Loss_Q: [1.69 2.14 1.39 0.91 0.92 0.   7.05] Loss_P: [ 3.47  2.54  2.3   1.54  0.95  0.99 11.79]\n",
      "Loss_Q: [1.71 2.15 1.42 0.9  0.9  0.   7.08] Loss_P: [ 3.46  2.51  2.31  1.58  0.88  0.99 11.74]\n",
      "Loss_Q: [1.73 2.16 1.49 0.92 0.92 0.   7.22] Loss_P: [ 3.43  2.51  2.3   1.63  0.96  1.   11.82]\n",
      "Loss_Q: [1.7  2.1  1.51 0.92 0.91 0.   7.14] Loss_P: [ 3.41  2.54  2.26  1.64  0.93  0.98 11.75]\n",
      "Loss_Q: [1.74 2.14 1.46 0.92 0.91 0.   7.17] Loss_P: [ 3.37  2.59  2.36  1.6   0.95  0.99 11.86]\n",
      "Loss_Q: [1.71 2.17 1.51 0.9  0.9  0.   7.19] Loss_P: [ 3.4   2.58  2.25  1.64  0.96  1.   11.82]\n",
      "Loss_Q: [1.79 2.09 1.48 0.92 0.9  0.   7.19] Loss_P: [ 3.43  2.57  2.25  1.62  0.91  1.   11.78]\n",
      "Loss_Q: [1.77 2.1  1.5  0.97 0.9  0.   7.23] Loss_P: [ 3.4   2.56  2.27  1.6   0.94  1.02 11.78]\n",
      "Loss_Q: [1.79 2.11 1.52 0.97 0.89 0.   7.29] Loss_P: [ 3.38  2.58  2.3   1.64  0.96  1.   11.85]\n",
      "Loss_Q: [1.83 2.08 1.52 0.93 0.92 0.   7.28] Loss_P: [ 3.41  2.64  2.23  1.65  0.95  1.   11.88]\n",
      "Loss_Q: [1.88 2.06 1.57 0.95 0.89 0.   7.34] Loss_P: [ 3.38  2.65  2.29  1.71  0.98  0.99 11.99]\n",
      "Loss_Q: [1.76 2.09 1.52 0.91 0.9  0.   7.19] Loss_P: [ 3.38  2.63  2.25  1.64  0.97  1.01 11.87]\n",
      "Loss_Q: [1.84 2.08 1.48 0.88 0.89 0.   7.17] Loss_P: [ 3.39  2.62  2.22  1.63  0.88  1.01 11.75]\n",
      "Loss_Q: [1.74 2.05 1.48 0.89 0.9  0.   7.07] Loss_P: [ 3.39  2.57  2.22  1.65  0.9   1.   11.74]\n",
      "Loss_Q: [1.74 2.07 1.5  0.84 0.91 0.   7.07] Loss_P: [ 3.39  2.57  2.26  1.64  0.89  0.96 11.72]\n",
      "Loss_Q: [1.81 2.13 1.53 0.87 0.87 0.   7.22] Loss_P: [ 3.38  2.6   2.29  1.66  0.92  0.99 11.83]\n",
      "Loss_Q: [1.87 2.1  1.41 0.85 0.87 0.   7.1 ] Loss_P: [ 3.34  2.6   2.32  1.59  0.87  0.96 11.68]\n",
      "Loss_Q: [1.82 2.17 1.46 0.86 0.9  0.   7.21] Loss_P: [ 3.37  2.65  2.33  1.6   0.86  0.94 11.75]\n",
      "Loss_Q: [1.78 2.14 1.42 0.87 0.88 0.   7.08] Loss_P: [ 3.38  2.67  2.32  1.57  0.85  0.94 11.72]\n",
      "Loss_Q: [1.75 2.23 1.43 0.88 0.87 0.   7.17] Loss_P: [ 3.39  2.63  2.35  1.62  0.9   0.95 11.84]\n",
      "Loss_Q: [1.78 2.21 1.44 0.87 0.84 0.   7.15] Loss_P: [ 3.37  2.65  2.33  1.55  0.91  0.92 11.72]\n",
      "Loss_Q: [1.88 2.1  1.43 0.83 0.88 0.   7.13] Loss_P: [ 3.38  2.64  2.26  1.55  0.84  0.97 11.65]\n",
      "Loss_Q: [1.76 2.14 1.42 0.89 0.92 0.   7.11] Loss_P: [ 3.39  2.59  2.28  1.55  0.82  0.99 11.63]\n",
      "Loss_Q: [1.79 2.11 1.36 0.83 0.9  0.   6.99] Loss_P: [ 3.41  2.55  2.25  1.47  0.84  0.98 11.5 ]\n",
      "Loss_Q: [1.77 2.12 1.31 0.85 0.91 0.   6.95] Loss_P: [ 3.43  2.57  2.26  1.4   0.84  0.99 11.48]\n",
      "Loss_Q: [1.78 2.08 1.28 0.83 0.88 0.   6.86] Loss_P: [ 3.41  2.61  2.24  1.4   0.82  0.97 11.45]\n",
      "Loss_Q: [1.76 2.13 1.29 0.81 0.88 0.   6.86] Loss_P: [ 3.4   2.64  2.29  1.43  0.84  0.99 11.59]\n",
      "Loss_Q: [1.88 2.1  1.28 0.84 0.87 0.   6.97] Loss_P: [ 3.37  2.62  2.32  1.46  0.85  0.98 11.6 ]\n",
      "Loss_Q: [1.83 2.11 1.35 0.91 0.86 0.   7.07] Loss_P: [ 3.4   2.63  2.29  1.51  0.94  0.97 11.75]\n",
      "Loss_Q: [1.8  2.02 1.3  0.85 0.88 0.   6.85] Loss_P: [ 3.41  2.63  2.21  1.5   0.9   0.96 11.61]\n",
      "Loss_Q: [1.79 2.08 1.35 0.83 0.89 0.   6.94] Loss_P: [ 3.4   2.62  2.24  1.46  0.81  0.95 11.48]\n",
      "Loss_Q: [1.84 1.98 1.41 0.83 0.85 0.   6.91] Loss_P: [ 3.45  2.62  2.2   1.5   0.85  0.92 11.54]\n",
      "Loss_Q: [1.81 2.02 1.3  0.84 0.86 0.   6.84] Loss_P: [ 3.39  2.63  2.22  1.36  0.81  0.94 11.34]\n",
      "Loss_Q: [1.75 2.06 1.24 0.86 0.87 0.   6.78] Loss_P: [ 3.44  2.63  2.19  1.39  0.85  0.97 11.46]\n",
      "Loss_Q: [1.82 1.94 1.32 0.82 0.87 0.   6.77] Loss_P: [ 3.47  2.62  2.16  1.38  0.85  0.93 11.42]\n",
      "Loss_Q: [1.82 2.03 1.28 0.8  0.89 0.   6.82] Loss_P: [ 3.38  2.63  2.19  1.41  0.79  0.93 11.33]\n",
      "Loss_Q: [1.77 2.   1.36 0.88 0.86 0.   6.87] Loss_P: [ 3.42  2.61  2.18  1.42  0.83  0.95 11.42]\n",
      "Loss_Q: [1.79 1.99 1.35 0.87 0.84 0.   6.85] Loss_P: [ 3.45  2.62  2.22  1.48  0.91  0.93 11.6 ]\n",
      "Loss_Q: [1.79 1.94 1.33 0.86 0.87 0.   6.79] Loss_P: [ 3.52  2.55  2.08  1.43  0.86  0.95 11.4 ]\n",
      "Loss_Q: [1.78 1.97 1.34 0.84 0.84 0.   6.76] Loss_P: [ 3.44  2.65  2.12  1.42  0.91  0.93 11.48]\n",
      "Loss_Q: [1.8  1.99 1.32 0.84 0.85 0.   6.8 ] Loss_P: [ 3.43  2.53  2.16  1.38  0.83  0.93 11.26]\n",
      "Loss_Q: [1.78 1.96 1.26 0.83 0.85 0.   6.68] Loss_P: [ 3.41  2.61  2.16  1.34  0.84  0.92 11.28]\n",
      "Loss_Q: [1.83 2.08 1.25 0.82 0.87 0.   6.85] Loss_P: [ 3.42  2.68  2.21  1.38  0.86  0.93 11.47]\n",
      "Loss_Q: [1.79 2.03 1.17 0.79 0.87 0.   6.65] Loss_P: [ 3.37  2.67  2.2   1.31  0.85  0.98 11.37]\n",
      "Loss_Q: [1.81 2.13 1.22 0.82 0.92 0.   6.9 ] Loss_P: [ 3.43  2.67  2.24  1.33  0.88  0.95 11.5 ]\n",
      "Loss_Q: [1.8  2.15 1.21 0.91 0.87 0.   6.93] Loss_P: [ 3.4   2.62  2.32  1.35  0.88  0.94 11.52]\n",
      "Loss_Q: [1.79 2.17 1.26 0.88 0.85 0.   6.95] Loss_P: [ 3.4   2.64  2.31  1.39  0.96  0.93 11.62]\n",
      "Loss_Q: [1.84 2.19 1.32 0.87 0.87 0.   7.1 ] Loss_P: [ 3.42  2.62  2.35  1.43  0.91  0.97 11.69]\n",
      "Loss_Q: [1.79 2.19 1.31 0.9  0.86 0.   7.05] Loss_P: [ 3.41  2.59  2.41  1.44  0.93  0.92 11.7 ]\n",
      "Loss_Q: [1.75 2.24 1.26 0.92 0.83 0.   7.  ] Loss_P: [ 3.4   2.58  2.43  1.39  0.92  0.93 11.66]\n",
      "Loss_Q: [1.79 2.21 1.25 0.9  0.84 0.   6.99] Loss_P: [ 3.41  2.53  2.36  1.36  0.94  0.89 11.5 ]\n",
      "Loss_Q: [1.79 2.14 1.3  0.96 0.84 0.   7.03] Loss_P: [ 3.44  2.59  2.36  1.42  0.92  0.9  11.63]\n",
      "Loss_Q: [1.83 2.09 1.32 0.93 0.88 0.   7.04] Loss_P: [ 3.4   2.59  2.27  1.35  0.94  0.93 11.47]\n",
      "Loss_Q: [1.8  2.08 1.33 0.91 0.88 0.   6.99] Loss_P: [ 3.41  2.59  2.26  1.42  0.94  0.96 11.59]\n",
      "Loss_Q: [1.81 2.12 1.36 0.95 0.86 0.   7.09] Loss_P: [ 3.37  2.62  2.3   1.44  1.    0.95 11.68]\n",
      "Loss_Q: [1.79 2.11 1.34 0.97 0.83 0.   7.04] Loss_P: [ 3.4   2.59  2.3   1.39  0.98  0.94 11.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.73 2.07 1.29 0.9  0.89 0.   6.89] Loss_P: [ 3.41  2.57  2.28  1.4   0.93  0.96 11.56]\n",
      "Loss_Q: [1.83 2.13 1.35 0.94 0.87 0.   7.11] Loss_P: [ 3.41  2.6   2.3   1.44  0.93  0.94 11.63]\n",
      "Loss_Q: [1.76 2.18 1.34 0.96 0.85 0.   7.09] Loss_P: [ 3.42  2.57  2.27  1.39  0.95  0.96 11.57]\n",
      "Loss_Q: [1.72 2.17 1.3  0.95 0.85 0.   6.98] Loss_P: [ 3.42  2.55  2.31  1.42  1.    0.94 11.64]\n",
      "Loss_Q: [1.76 2.16 1.29 0.9  0.85 0.   6.95] Loss_P: [ 3.41  2.53  2.28  1.44  0.95  0.92 11.54]\n",
      "Loss_Q: [1.79 2.14 1.3  0.94 0.84 0.   7.01] Loss_P: [ 3.36  2.6   2.38  1.36  0.92  0.91 11.53]\n",
      "Loss_Q: [1.82 2.18 1.29 0.96 0.87 0.   7.12] Loss_P: [ 3.41  2.56  2.38  1.4   0.95  0.93 11.62]\n",
      "Loss_Q: [1.8  2.13 1.27 0.88 0.85 0.   6.94] Loss_P: [ 3.41  2.6   2.33  1.3   0.9   0.95 11.48]\n",
      "Loss_Q: [1.79 2.16 1.26 0.88 0.88 0.   6.98] Loss_P: [ 3.42  2.61  2.35  1.41  0.94  0.97 11.7 ]\n",
      "Loss_Q: [1.78 2.17 1.28 0.88 0.89 0.   7.  ] Loss_P: [ 3.37  2.6   2.34  1.44  0.94  0.96 11.65]\n",
      "Loss_Q: [1.86 2.13 1.3  0.9  0.89 0.   7.08] Loss_P: [ 3.4   2.6   2.24  1.38  0.94  0.97 11.53]\n",
      "Loss_Q: [1.82 2.18 1.29 0.9  0.89 0.   7.08] Loss_P: [ 3.42  2.58  2.37  1.41  0.93  0.96 11.67]\n",
      "Loss_Q: [1.76 2.2  1.33 0.96 0.85 0.   7.1 ] Loss_P: [ 3.39  2.57  2.37  1.39  0.92  0.96 11.59]\n",
      "Loss_Q: [1.79 2.2  1.34 0.94 0.84 0.   7.11] Loss_P: [ 3.46  2.61  2.36  1.44  0.95  0.94 11.75]\n",
      "Loss_Q: [1.77 2.24 1.32 0.92 0.85 0.   7.09] Loss_P: [ 3.43  2.56  2.38  1.41  0.94  0.94 11.67]\n",
      "Loss_Q: [1.81 2.25 1.34 0.95 0.87 0.   7.21] Loss_P: [ 3.4   2.58  2.36  1.44  0.97  0.96 11.72]\n",
      "Loss_Q: [1.75 2.2  1.29 0.86 0.88 0.   6.97] Loss_P: [ 3.43  2.53  2.32  1.37  0.92  0.97 11.53]\n",
      "Loss_Q: [1.79 2.2  1.31 0.9  0.84 0.   7.04] Loss_P: [ 3.42  2.58  2.36  1.38  0.95  0.96 11.65]\n",
      "Loss_Q: [1.73 2.15 1.24 0.94 0.84 0.   6.89] Loss_P: [ 3.42  2.55  2.24  1.38  0.98  0.91 11.49]\n",
      "Loss_Q: [1.74 2.13 1.31 0.92 0.81 0.   6.91] Loss_P: [ 3.41  2.56  2.27  1.41  0.99  0.89 11.53]\n",
      "Loss_Q: [1.82 2.08 1.24 0.92 0.83 0.   6.88] Loss_P: [ 3.35  2.62  2.33  1.42  1.01  0.92 11.64]\n",
      "Loss_Q: [1.77 2.13 1.27 0.91 0.85 0.   6.91] Loss_P: [ 3.41  2.54  2.3   1.38  0.9   0.91 11.44]\n",
      "Loss_Q: [1.81 2.12 1.3  0.91 0.82 0.   6.96] Loss_P: [ 3.38  2.59  2.27  1.45  0.97  0.88 11.56]\n",
      "Loss_Q: [1.81 2.1  1.34 0.95 0.82 0.   7.02] Loss_P: [ 3.41  2.53  2.23  1.48  0.98  0.95 11.58]\n",
      "Loss_Q: [1.79 2.11 1.35 0.94 0.79 0.   6.98] Loss_P: [ 3.44  2.55  2.27  1.47  0.93  0.92 11.58]\n",
      "Loss_Q: [1.74 2.05 1.35 0.9  0.86 0.   6.9 ] Loss_P: [ 3.44  2.51  2.25  1.46  0.96  0.91 11.53]\n",
      "Loss_Q: [1.74 1.97 1.3  0.89 0.82 0.   6.72] Loss_P: [ 3.45  2.5   2.22  1.4   0.91  0.93 11.41]\n",
      "Loss_Q: [1.74 2.04 1.31 0.93 0.84 0.   6.87] Loss_P: [ 3.43  2.52  2.21  1.47  0.99  0.91 11.52]\n",
      "Loss_Q: [1.73 2.03 1.32 0.9  0.83 0.   6.83] Loss_P: [ 3.45  2.49  2.21  1.42  0.95  0.92 11.45]\n",
      "Loss_Q: [1.73 2.1  1.31 0.9  0.82 0.   6.86] Loss_P: [ 3.44  2.53  2.21  1.45  0.94  0.92 11.49]\n",
      "Loss_Q: [1.73 2.13 1.29 0.85 0.82 0.   6.83] Loss_P: [ 3.39  2.53  2.25  1.42  0.94  0.89 11.42]\n",
      "Loss_Q: [1.73 2.14 1.27 0.9  0.81 0.   6.85] Loss_P: [ 3.43  2.58  2.28  1.37  0.95  0.87 11.48]\n",
      "Loss_Q: [1.79 2.18 1.29 0.93 0.81 0.   7.  ] Loss_P: [ 3.39  2.56  2.3   1.4   0.97  0.86 11.48]\n",
      "Loss_Q: [1.76 2.19 1.32 0.93 0.78 0.   6.98] Loss_P: [ 3.42  2.55  2.32  1.44  0.94  0.86 11.54]\n",
      "Loss_Q: [1.75 2.18 1.32 0.96 0.76 0.   6.96] Loss_P: [ 3.4   2.57  2.28  1.45  0.97  0.84 11.52]\n",
      "Loss_Q: [1.76 2.12 1.25 0.96 0.79 0.   6.88] Loss_P: [ 3.42  2.5   2.34  1.42  1.03  0.85 11.56]\n",
      "Loss_Q: [1.73 2.15 1.35 0.95 0.81 0.   6.99] Loss_P: [ 3.46  2.46  2.29  1.4   0.92  0.88 11.4 ]\n",
      "Loss_Q: [1.72 2.13 1.26 0.95 0.77 0.   6.83] Loss_P: [ 3.49  2.46  2.31  1.4   0.93  0.87 11.46]\n",
      "Loss_Q: [1.74 2.14 1.26 0.96 0.8  0.   6.89] Loss_P: [ 3.46  2.51  2.35  1.37  1.01  0.84 11.54]\n",
      "Loss_Q: [1.77 2.08 1.26 0.93 0.78 0.   6.83] Loss_P: [ 3.41  2.51  2.25  1.32  0.97  0.83 11.28]\n",
      "Loss_Q: [1.8  2.01 1.28 0.88 0.79 0.   6.76] Loss_P: [ 3.46  2.5   2.27  1.32  0.94  0.84 11.33]\n",
      "Loss_Q: [1.68 2.   1.22 0.95 0.8  0.   6.65] Loss_P: [ 3.56  2.43  2.21  1.28  0.98  0.84 11.3 ]\n",
      "Loss_Q: [1.72 1.98 1.22 0.93 0.77 0.   6.61] Loss_P: [ 3.44  2.49  2.15  1.36  0.93  0.84 11.2 ]\n",
      "Loss_Q: [1.7  1.98 1.26 0.92 0.78 0.   6.65] Loss_P: [ 3.49  2.42  2.15  1.37  0.97  0.85 11.24]\n",
      "Loss_Q: [1.69 1.98 1.23 0.97 0.8  0.   6.67] Loss_P: [ 3.45  2.46  2.15  1.34  0.96  0.86 11.22]\n",
      "Loss_Q: [1.69 1.92 1.2  0.95 0.79 0.   6.55] Loss_P: [ 3.46  2.45  2.11  1.24  0.95  0.86 11.08]\n",
      "Loss_Q: [1.71 1.98 1.22 0.98 0.77 0.   6.66] Loss_P: [ 3.53  2.48  2.12  1.3   0.94  0.84 11.2 ]\n",
      "Loss_Q: [1.73 1.98 1.22 0.98 0.79 0.   6.7 ] Loss_P: [ 3.45  2.44  2.14  1.35  0.96  0.86 11.2 ]\n",
      "Loss_Q: [1.74 2.03 1.28 0.99 0.78 0.   6.82] Loss_P: [ 3.48  2.46  2.1   1.37  1.01  0.84 11.27]\n",
      "Loss_Q: [1.7  2.   1.2  0.95 0.75 0.   6.6 ] Loss_P: [ 3.43  2.56  2.17  1.32  1.    0.8  11.28]\n",
      "Loss_Q: [1.76 2.   1.22 0.95 0.73 0.   6.68] Loss_P: [ 3.45  2.52  2.18  1.31  1.03  0.78 11.28]\n",
      "Loss_Q: [1.67 2.02 1.17 1.02 0.7  0.   6.58] Loss_P: [ 3.42  2.46  2.2   1.24  1.05  0.77 11.13]\n",
      "Loss_Q: [1.7  2.06 1.14 0.97 0.69 0.   6.55] Loss_P: [ 3.46  2.49  2.23  1.22  1.01  0.78 11.18]\n",
      "Loss_Q: [1.7  2.08 1.12 1.   0.69 0.   6.6 ] Loss_P: [ 3.4   2.52  2.29  1.23  0.99  0.79 11.22]\n",
      "Loss_Q: [1.69 2.04 1.16 0.98 0.72 0.   6.58] Loss_P: [ 3.51  2.51  2.24  1.25  1.01  0.76 11.28]\n",
      "Loss_Q: [1.69 2.02 1.1  0.99 0.65 0.   6.45] Loss_P: [ 3.49  2.47  2.21  1.21  1.01  0.74 11.13]\n",
      "Loss_Q: [1.77 2.02 1.19 1.   0.69 0.   6.66] Loss_P: [ 3.49  2.53  2.15  1.26  1.06  0.75 11.24]\n",
      "Loss_Q: [1.74 1.93 1.18 0.99 0.73 0.   6.57] Loss_P: [ 3.49  2.5   2.16  1.23  1.03  0.77 11.2 ]\n",
      "Loss_Q: [1.71 1.95 1.2  0.97 0.68 0.   6.51] Loss_P: [ 3.48  2.51  2.09  1.23  1.    0.75 11.07]\n",
      "Loss_Q: [1.77 1.96 1.22 0.98 0.67 0.   6.6 ] Loss_P: [ 3.45  2.56  2.09  1.24  1.01  0.72 11.05]\n",
      "Loss_Q: [1.74 1.91 1.2  0.96 0.66 0.   6.48] Loss_P: [ 3.44  2.53  2.05  1.25  0.95  0.72 10.95]\n",
      "Loss_Q: [1.76 1.99 1.28 1.03 0.62 0.   6.68] Loss_P: [ 3.42  2.52  2.13  1.29  1.03  0.72 11.11]\n",
      "Loss_Q: [1.76 2.01 1.22 1.02 0.64 0.   6.65] Loss_P: [ 3.43  2.54  2.1   1.28  1.05  0.69 11.1 ]\n",
      "Loss_Q: [1.77 1.98 1.28 1.03 0.63 0.   6.7 ] Loss_P: [ 3.44  2.54  2.1   1.37  1.07  0.69 11.21]\n",
      "Loss_Q: [1.76 1.97 1.27 1.03 0.6  0.   6.63] Loss_P: [ 3.46  2.58  2.1   1.41  1.08  0.66 11.29]\n"
     ]
    }
   ],
   "source": [
    "### Training without constraints, accuracy 0.55, not ideal\n",
    "\n",
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4f2209ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = random_set\n",
    "values_data,counts_data = np.unique(dataset, axis=1, return_counts = True)\n",
    "counts_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2eb7ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training with false sample ignored, unbalanced training where dream phase trained more than wake phase\n",
    "### Doesn't really help with performance, and slows down the training significantly\n",
    "\n",
    "# for e in range (epoch):\n",
    "#     index = np.random.permutation(n_data)\n",
    "#     Loss_Q_total = np.zeros(n_layer)\n",
    "#     Loss_P_total = np.zeros(n_layer)\n",
    "#     for i in range(n_data):\n",
    "#         d0 = dataset[:,index[i]:index[i]+1]\n",
    "#         Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "#         Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "#         Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "#         gen = Alpha_P['z0']\n",
    "#         for j in range(counts_data.size):\n",
    "#             if np.array_equal(gen, values_data[:,j:j+1]):\n",
    "#                 Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "#                 break\n",
    "        \n",
    "#         Loss_Q_total += Loss_Q\n",
    "#         Loss_P_total += Loss_P\n",
    "#     Loss_Q_total = Loss_Q_total/n_data\n",
    "#     Loss_P_total = Loss_P_total/n_data\n",
    "#     print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bf15caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training with valid samples only, balanced training between dream phase and wake phase\n",
    "### Doesn't really help with performance, and slows down the training significantly\n",
    "\n",
    "# for e in range (epoch):\n",
    "#     index = np.random.permutation(n_data)\n",
    "#     Loss_Q_total = np.zeros(n_layer)\n",
    "#     Loss_P_total = np.zeros(n_layer)\n",
    "#     for i in range(n_data):\n",
    "#         d0 = dataset[:,index[i]:index[i]+1]\n",
    "#         Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "#         Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "#         flag = 0\n",
    "#         while flag == 0:\n",
    "#             Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "#             gen = Alpha_P['z0']\n",
    "#             for j in range(counts_data.size):\n",
    "#                 if np.array_equal(gen, values_data[:,j:j+1]):\n",
    "#                     flag = 1\n",
    "#                     Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "#                     break\n",
    "                \n",
    "        \n",
    "#         Loss_Q_total += Loss_Q\n",
    "#         Loss_P_total += Loss_P\n",
    "#     Loss_Q_total = Loss_Q_total/n_data\n",
    "#     Loss_P_total = Loss_P_total/n_data\n",
    "#     print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82ecef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = Phi_cache\n",
    "Theta = Theta_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "537dcdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (0,2) (3,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m d0 \u001b[38;5;241m=\u001b[39m dataset[:,index[i]:index[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     13\u001b[0m Alpha_Q \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39mwake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n\u001b[1;32m---> 14\u001b[0m Theta,Loss_P \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39msleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n\u001b[0;32m     15\u001b[0m Alpha_P \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39msleep_sample(n_dz,value_set,Theta,activation_type,bias)\n\u001b[0;32m     16\u001b[0m Phi,Loss_Q \u001b[38;5;241m=\u001b[39m ut\u001b[38;5;241m.\u001b[39mwake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
      "File \u001b[1;32m~\\MUS 206 Free Energy\\utils.py:336\u001b[0m, in \u001b[0;36msleep_update_delta\u001b[1;34m(Theta, Alpha_Q, lr, n_dz, value_set, activation_type, bias)\u001b[0m\n\u001b[0;32m    334\u001b[0m x \u001b[38;5;241m=\u001b[39m Alpha_Q[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)]\n\u001b[0;32m    335\u001b[0m y \u001b[38;5;241m=\u001b[39m Alpha_Q[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m--> 336\u001b[0m parameter_set,loss \u001b[38;5;241m=\u001b[39m multi_Bernoulli_update(x,y,parameter_set,lr,value_set,activation_type,bias)\n\u001b[0;32m    337\u001b[0m Loss[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    338\u001b[0m Loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32m~\\MUS 206 Free Energy\\utils.py:254\u001b[0m, in \u001b[0;36mmulti_Bernoulli_update\u001b[1;34m(x, y, parameter_set, lr, value_set, activation_type, bias)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m activation_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    252\u001b[0m         u \u001b[38;5;241m=\u001b[39m dz \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mz\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (a\u001b[38;5;241m-\u001b[39mb)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 254\u001b[0m parameter_set[keys[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mouter(u,np\u001b[38;5;241m.\u001b[39mappend(x,[[\u001b[38;5;241m1\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parameter_set,loss\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (0,2) (3,2) "
     ]
    }
   ],
   "source": [
    "### Training with modified data distribution as sampled salience\n",
    "### All datapoints are reserved with positive counts (at least 1); the distribution is modified with a controled dataset scale\n",
    "### Improves the performance!\n",
    "\n",
    "lr = 0.01\n",
    "epoch = 500\n",
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        gen = Alpha_P['z0']\n",
    "        for j in range(counts_data.size):\n",
    "            if np.array_equal(gen, values_data[:,j:j+1]):\n",
    "                dataset = np.append(dataset,gen,axis=1)\n",
    "                break\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    \n",
    "    values,counts = np.unique(dataset,axis=1,return_counts = True)\n",
    "    dataset = np.repeat(values, (counts/2+0.5).astype(int), axis=1)\n",
    "    n_data = dataset.shape[1]\n",
    "    \n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('dataset size: ' + str(n_data),'Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b8410507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189863a",
   "metadata": {},
   "source": [
    "Now I want to exclude all false examples, with a sacrifice of losing the original evidence support (generating false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9355dd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_data,counts_data = np.unique(dataset, axis=1, return_counts = True)\n",
    "counts_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6c7bcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gen_set = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2353df86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 828 dataset support: 108 Loss_Q: [2.17 1.59 1.68 1.32 0.63 0.   7.38] Loss_P: [1.71 2.57 1.63 1.8  1.27 0.69 9.68]\n",
      "dataset size: 883 dataset support: 94 Loss_Q: [2.17 1.55 1.63 1.3  0.63 0.   7.28] Loss_P: [1.51 2.46 1.61 1.77 1.3  0.7  9.34]\n",
      "dataset size: 902 dataset support: 73 Loss_Q: [2.07 1.61 1.72 1.28 0.62 0.   7.3 ] Loss_P: [1.38 2.42 1.61 1.85 1.32 0.68 9.26]\n",
      "dataset size: 916 dataset support: 71 Loss_Q: [2.06 1.6  1.75 1.31 0.6  0.   7.32] Loss_P: [1.33 2.37 1.61 1.91 1.34 0.66 9.22]\n",
      "dataset size: 915 dataset support: 62 Loss_Q: [2.17 1.57 1.74 1.33 0.61 0.   7.41] Loss_P: [1.26 2.35 1.53 1.85 1.29 0.64 8.93]\n",
      "dataset size: 917 dataset support: 73 Loss_Q: [2.2  1.55 1.71 1.31 0.58 0.   7.34] Loss_P: [1.28 2.41 1.55 1.79 1.31 0.64 8.98]\n",
      "dataset size: 943 dataset support: 59 Loss_Q: [2.2  1.48 1.7  1.28 0.63 0.   7.29] Loss_P: [1.25 2.39 1.48 1.77 1.31 0.67 8.86]\n",
      "dataset size: 946 dataset support: 59 Loss_Q: [2.22 1.38 1.71 1.25 0.59 0.   7.16] Loss_P: [1.2  2.4  1.46 1.83 1.25 0.64 8.78]\n",
      "dataset size: 957 dataset support: 55 Loss_Q: [2.26 1.36 1.72 1.25 0.61 0.   7.19] Loss_P: [1.14 2.39 1.31 1.79 1.21 0.66 8.51]\n",
      "dataset size: 950 dataset support: 49 Loss_Q: [2.19 1.29 1.64 1.22 0.56 0.   6.91] Loss_P: [1.08 2.35 1.3  1.66 1.26 0.65 8.3 ]\n"
     ]
    }
   ],
   "source": [
    "### Training by sampled valid instances, discarding hardly sampled support in the original set\n",
    "### accuracy 0.998, with 282 valid support (out of 672), outlier 16/10000, basically excluded false generations\n",
    "\n",
    "lr = 0.01\n",
    "epoch = 10\n",
    "for e in range (epoch):\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    \n",
    "    generation = ut.generate(n_gen_set,n_dz,value_set,Theta,activation_type,bias)\n",
    "    valid_index = []\n",
    "    for i in range(n_gen_set):\n",
    "        for j in range(counts_data.size):\n",
    "            if np.array_equal(generation[:,i], values_data[:,j]):\n",
    "                valid_index.append(i)\n",
    "                break\n",
    "    generated_dataset = generation[:,valid_index]\n",
    "    n_data = generated_dataset.shape[1]\n",
    "    n_support = np.unique(generated_dataset,axis=1).shape[1]\n",
    "    \n",
    "    for i in range(n_data):\n",
    "        d0 = generated_dataset[:,i:i+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('dataset size: ' + str(n_data), 'dataset support: ' + str(n_support), 'Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd57991",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dd43e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 1., ..., 1., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 1.],\n",
       "       [1., 0., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 10000\n",
    "generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c12f6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution,data_dist,statistics, MSE, ABS_Error = metrics(generation,reordered_set,dataset)\n",
    "distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,values_data) # for generated_dataset\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6be0ff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6062728682170543"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABS_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4f0073bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a02897c150>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - values_d.size))] = values_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6afa7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.4478,\n",
       " 'FN': array([], dtype=int32),\n",
       " 'n_fn': 0,\n",
       " 'FP': array([[ 387,  388,  389, ..., 1020, 1021, 1023],\n",
       "        [  18,   14,   14, ...,    4,    8,   12]], dtype=int64),\n",
       " 'n_fp': 636}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5675dc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41329566"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90ed62",
   "metadata": {},
   "source": [
    "'percent': 0.5181, 0.5236; 0.5698,0.6098,0.6213\n",
    "\n",
    "n_fn': 4,0,0; 4,3,4\n",
    "\n",
    "'n_fp': 640,653,641; 615,619,599\n",
    "\n",
    "MSE: 0.3995,0.3457,0.9405; 0.8118,0.7598,0.7739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "654803e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('1101',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e6e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
