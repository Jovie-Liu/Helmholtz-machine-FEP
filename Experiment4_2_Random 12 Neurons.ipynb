{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca6df78",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 4_2</span> \n",
    "\n",
    "# Helmholtz Machine on Self-organizing System (Expansion on Exploration)\n",
    "\n",
    "In this notebook we simulate the evolution of a self-organizing system by the training of the Helmholtz machine. By increasing the length of input layer from 10 to 12, we expanded the space from 1024 categories to 4096, which is way more difficult to fit and manipulate. Instead of fitting a given dataset, which presents bad performance with an accuracy of $0.22$ only, we construct a self-organizing system to guide the model to explore the large space by its own Helmholtz device, which expands its evidence by its self-guided exploration gradually, with steady improvement on generation accuracy and distribution error. \n",
    "\n",
    "*Experimental setup*: we start from a small randomly sampled set (100 datapoints) as the initial data input, then by the free sampling of the generative side of the Helmholtz machine, we add its frequently sampled new instances to the dataset to expand its evidence support. This process takes place gradually, with a few \"new findings of exploration\" added after every 100 epochs. In the meantime, the system keeps fitting to its current evidence by training under the wake-sleep algorithm. After a long period of thus back and forth on exploration and exploitation, the system is fitted quite well to its own constructed subspace (niche), with an accuracy of $0.82$ and distribution error $0.29$ (see the visualization below).\n",
    "\n",
    "<img src=\"self organizing result.png\" style=\"width:800px\">\n",
    "<caption><center> Self-organizing System Training Result    </center></caption>\n",
    "\n",
    "*Created:* December 26, 2023\n",
    "\n",
    "*Updated:* December 26, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cb0071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 10,  8,  7,  5,  4,  3,  1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[12,10,8,7,5,4,3,1]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30e8efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, Theta = ut.parameter_initialization(\"zero\",n_dz)  # \"zero\" or \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0d756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_set = [1,0]\n",
    "activation_type = \"tanh\"\n",
    "bias = [False,False,True] # [instantiation bias, MLP bias,data bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5985f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 1., 1.],\n",
       "       [0., 1., 0., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 1., 1.],\n",
       "       [1., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 100\n",
    "init_generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "init_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3561db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 99)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.unique(init_generation,axis=1)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b2a4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_set = ut.all_comb(n, value_set)\n",
    "reordered_set = ut.reorder_all_comb(entire_set,dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa978494",
   "metadata": {},
   "source": [
    "Self-organizing System:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "b1e7a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.008\n",
    "epoch = 10\n",
    "n = n_dz[0,0]\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3ec80396",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_rate = 100 # after how many epochs we update evidence\n",
    "n_add_sample = 560 # how many new samples to add at each update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "64ba5f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new sampled: 2517 dataset size: 679 Loss_Q: [1.96 1.87 2.27 1.42 1.14 0.69 0.   9.35] Loss_P: [ 3.18  2.54  2.27  2.54  1.4   1.12  1.07 14.12]\n",
      "Total new sampled: 2541 dataset size: 680 Loss_Q: [1.93 1.88 2.22 1.4  1.12 0.66 0.   9.21] Loss_P: [ 3.16  2.55  2.26  2.45  1.43  1.08  1.02 13.95]\n",
      "Total new sampled: 2551 dataset size: 680 Loss_Q: [1.85 1.8  2.27 1.56 1.15 0.72 0.   9.36] Loss_P: [ 3.18  2.46  2.18  2.52  1.6   1.11  1.08 14.13]\n",
      "Total new sampled: 2518 dataset size: 680 Loss_Q: [1.9  1.83 2.25 1.64 1.22 0.73 0.   9.57] Loss_P: [ 3.17  2.51  2.2   2.53  1.65  1.2   1.09 14.35]\n",
      "Total new sampled: 2554 dataset size: 680 Loss_Q: [1.92 1.82 2.27 1.65 1.2  0.69 0.   9.55] Loss_P: [ 3.17  2.51  2.23  2.56  1.65  1.15  1.03 14.31]\n",
      "Total new sampled: 2545 dataset size: 680 Loss_Q: [1.98 1.88 2.37 1.68 1.23 0.73 0.   9.87] Loss_P: [ 3.13  2.57  2.34  2.67  1.68  1.17  1.08 14.64]\n",
      "Total new sampled: 2528 dataset size: 682 Loss_Q: [1.89 2.   2.36 1.65 1.26 0.77 0.   9.93] Loss_P: [ 3.15  2.48  2.42  2.61  1.68  1.23  1.13 14.71]\n",
      "Total new sampled: 2549 dataset size: 683 Loss_Q: [1.82 1.97 2.37 1.67 1.28 0.77 0.   9.88] Loss_P: [ 3.13  2.41  2.4   2.65  1.69  1.25  1.15 14.67]\n",
      "Total new sampled: 2562 dataset size: 685 Loss_Q: [1.92 2.08 2.24 1.67 1.28 0.79 0.   9.99] Loss_P: [ 3.14  2.53  2.49  2.5   1.73  1.23  1.16 14.78]\n",
      "Total new sampled: 2564 dataset size: 685 Loss_Q: [ 1.98  1.99  2.34  1.72  1.3   0.79  0.   10.12] Loss_P: [ 3.16  2.6   2.38  2.61  1.77  1.25  1.17 14.94]\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    new_sampled = np.zeros((n,n_data*update_rate))\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for r in range (update_rate):\n",
    "        index = np.random.permutation(n_data)\n",
    "        for i in range(n_data):\n",
    "            d0 = dataset[:,index[i]:index[i]+1]\n",
    "            Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "            Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "            Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "            Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "\n",
    "            new_sampled[:,i+n_data*r:i+n_data*r+1] = Alpha_P['z0']\n",
    "\n",
    "            Loss_Q_total += Loss_Q\n",
    "            Loss_P_total += Loss_P\n",
    "\n",
    "    values,counts = np.unique(new_sampled,axis=1,return_counts = True)\n",
    "    n_new_total = np.unique(np.append(dataset,values,axis=1),axis=1).shape[1] - n_data\n",
    "    \n",
    "    new_samples = values[:,np.argsort(counts)[-n_add_sample:]]\n",
    "    dataset = np.unique(np.append(dataset,new_samples,axis=1),axis=1)  # renew dataset\n",
    "    n_data = dataset.shape[1]\n",
    "\n",
    "    Loss_Q_total = Loss_Q_total/(n_data*update_rate)\n",
    "    Loss_P_total = Loss_P_total/(n_data*update_rate)\n",
    "    print('Total new sampled: ' + str(n_new_total),'dataset size: ' + str(n_data),'Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617ac70",
   "metadata": {},
   "source": [
    "246, 281,338,340,379,394,432,464,476,484,523,569,586,621,623,627,656,672,678,685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "dd43e597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 1., 1., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 10000\n",
    "generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c12f6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_set = ut.reorder_all_comb(entire_set,dataset)\n",
    "distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,dataset)\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "6be0ff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4246129186602871"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABS_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4f0073bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f89bb11fd0>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - values_d.size))] = values_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6afa7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.8111,\n",
       " 'FN': array([ 38,  93,  98, 123, 171, 233, 241, 297, 332, 471, 475, 481, 483,\n",
       "        493, 494, 513, 514, 515, 531, 549, 555, 560, 562, 567, 572, 574,\n",
       "        577, 594, 598, 600, 618, 620, 625]),\n",
       " 'n_fn': 33,\n",
       " 'FP': array([[ 628,  629,  634, ..., 4091, 4093, 4094],\n",
       "        [   1,    1,    1, ...,    2,    2,    3]], dtype=int64),\n",
       " 'n_fp': 1184}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "5675dc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28733247"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861431a",
   "metadata": {},
   "source": [
    "'percent': 0.5393,0.6233, 0.6112,0.6729,0.6763,0.6908, 0.7262,0.7372,0.7601,0.7958,0.795,0.8092,0.8307,0.8224,0.8111\n",
    "\n",
    "n_fn': 17,19,11,23,23,27,23,26,28,32,36,36,38,37,33\n",
    "\n",
    "'n_fp': 1791,1593,1634,1486,1470,1402,1352,1276,1292,1181,1221,1132,1060,1087,1184\n",
    "\n",
    "MSE: 0.4562,0.4149,0.3870,0.4085,0.3982,0.3602,0.3329,0.3132,0.3042,0.3093,0.3055,0.2850,0.3021,0.2917,0.2873"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7c35b",
   "metadata": {},
   "source": [
    "Save dataset, parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8a1ba3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 627)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(dataset,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9d18e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('self_org_dataset.npy',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5b24d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'Phi': Phi, 'Theta': Theta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5c8b6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('self_org_parameters.npy',parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5df71569",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = np.load('self_org_parameters.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d9111457",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta = para['Theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fd9cf510",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('self_org_dataset.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0085802",
   "metadata": {},
   "source": [
    "A Test we can do now is to fit a default Helmholtz machine (initialized to 0) of same structure to this self-constructed dataset and see how well it could perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b0fcbaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, Theta = ut.parameter_initialization(\"random\",n_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "0e913091",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_set = ut.reorder_all_comb(entire_set,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a9cb6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.015\n",
    "epoch = 1000\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "f6c13c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 4.18  3.34  2.96  2.25  1.79  1.32  0.   15.83] Loss_P: [ 5.04  4.27  3.37  3.09  2.25  1.8   1.43 21.25]\n",
      "Loss_Q: [ 3.74  3.1   2.81  2.17  1.72  1.23  0.   14.77] Loss_P: [ 4.56  3.86  3.24  3.06  2.17  1.79  1.34 20.02]\n",
      "Loss_Q: [ 3.57  3.02  2.75  2.2   1.69  1.24  0.   14.48] Loss_P: [ 4.44  3.74  3.15  2.96  2.26  1.76  1.35 19.65]\n",
      "Loss_Q: [ 3.52  2.99  2.7   2.21  1.64  1.27  0.   14.32] Loss_P: [ 4.38  3.82  3.05  2.95  2.22  1.77  1.38 19.58]\n",
      "Loss_Q: [ 3.47  2.96  2.75  2.16  1.7   1.27  0.   14.31] Loss_P: [ 4.35  3.74  3.08  3.    2.18  1.79  1.39 19.53]\n",
      "Loss_Q: [ 3.43  2.97  2.66  2.13  1.7   1.27  0.   14.17] Loss_P: [ 4.31  3.67  3.04  2.93  2.2   1.8   1.37 19.33]\n",
      "Loss_Q: [ 3.41  2.92  2.72  2.16  1.69  1.22  0.   14.12] Loss_P: [ 4.32  3.65  3.04  2.94  2.19  1.81  1.33 19.27]\n",
      "Loss_Q: [ 3.39  2.93  2.7   2.07  1.71  1.24  0.   14.05] Loss_P: [ 4.26  3.7   3.06  2.96  2.11  1.82  1.32 19.24]\n",
      "Loss_Q: [ 3.4   2.98  2.75  2.05  1.72  1.24  0.   14.13] Loss_P: [ 4.22  3.72  3.12  2.95  2.11  1.8   1.33 19.26]\n",
      "Loss_Q: [ 3.37  2.99  2.79  2.08  1.76  1.24  0.   14.24] Loss_P: [ 4.21  3.7   3.14  2.99  2.16  1.85  1.37 19.41]\n",
      "Loss_Q: [ 3.48  2.96  2.77  2.11  1.74  1.26  0.   14.32] Loss_P: [ 4.14  3.75  3.12  2.97  2.22  1.84  1.38 19.43]\n",
      "Loss_Q: [ 3.41  2.97  2.77  2.1   1.75  1.27  0.   14.28] Loss_P: [ 4.13  3.75  3.16  3.02  2.18  1.8   1.36 19.41]\n",
      "Loss_Q: [ 3.4   2.94  2.75  2.09  1.75  1.24  0.   14.18] Loss_P: [ 4.1   3.71  3.14  2.99  2.22  1.79  1.33 19.28]\n",
      "Loss_Q: [ 3.46  2.96  2.76  2.12  1.7   1.22  0.   14.23] Loss_P: [ 4.12  3.69  3.09  2.99  2.24  1.8   1.32 19.25]\n",
      "Loss_Q: [ 3.47  2.95  2.8   2.1   1.69  1.23  0.   14.22] Loss_P: [ 4.17  3.77  3.06  3.05  2.18  1.79  1.31 19.33]\n",
      "Loss_Q: [ 3.4   2.83  2.8   2.13  1.72  1.2   0.   14.07] Loss_P: [ 4.14  3.66  2.96  3.02  2.22  1.8   1.29 19.11]\n",
      "Loss_Q: [ 3.3   2.83  2.8   2.13  1.7   1.18  0.   13.95] Loss_P: [ 4.16  3.55  2.95  3.03  2.22  1.8   1.28 18.99]\n",
      "Loss_Q: [ 3.32  2.86  2.79  2.12  1.71  1.18  0.   13.97] Loss_P: [ 4.14  3.64  2.93  2.98  2.25  1.81  1.28 19.03]\n",
      "Loss_Q: [ 3.4   2.94  2.79  2.1   1.72  1.21  0.   14.16] Loss_P: [ 4.16  3.64  3.01  2.97  2.22  1.81  1.31 19.11]\n",
      "Loss_Q: [ 3.38  2.98  2.75  2.12  1.73  1.23  0.   14.19] Loss_P: [ 4.15  3.69  3.05  2.93  2.26  1.84  1.31 19.24]\n",
      "Loss_Q: [ 3.45  3.    2.74  2.14  1.76  1.25  0.   14.34] Loss_P: [ 4.1   3.68  3.06  2.96  2.28  1.85  1.33 19.25]\n",
      "Loss_Q: [ 3.39  2.97  2.76  2.11  1.74  1.25  0.   14.21] Loss_P: [ 4.1   3.67  3.11  2.95  2.29  1.85  1.33 19.29]\n",
      "Loss_Q: [ 3.34  2.9   2.76  2.09  1.78  1.24  0.   14.1 ] Loss_P: [ 4.08  3.65  3.01  2.91  2.31  1.85  1.33 19.14]\n",
      "Loss_Q: [ 3.23  2.87  2.59  2.15  1.7   1.25  0.   13.77] Loss_P: [ 4.05  3.61  3.02  2.8   2.3   1.78  1.31 18.86]\n",
      "Loss_Q: [ 3.35  2.85  2.63  2.12  1.71  1.19  0.   13.85] Loss_P: [ 4.01  3.73  3.03  2.77  2.28  1.78  1.27 18.88]\n",
      "Loss_Q: [ 3.35  2.87  2.71  2.12  1.73  1.22  0.   14.  ] Loss_P: [ 4.    3.74  3.02  2.82  2.32  1.8   1.26 18.97]\n",
      "Loss_Q: [ 3.31  2.89  2.72  2.1   1.74  1.18  0.   13.95] Loss_P: [ 3.97  3.68  3.07  2.85  2.33  1.8   1.28 18.97]\n",
      "Loss_Q: [ 3.27  2.91  2.71  2.12  1.76  1.25  0.   14.03] Loss_P: [ 4.01  3.65  3.1   2.84  2.33  1.78  1.33 19.05]\n",
      "Loss_Q: [ 3.34  2.98  2.72  2.09  1.72  1.22  0.   14.07] Loss_P: [ 4.01  3.69  3.19  2.8   2.35  1.76  1.34 19.13]\n",
      "Loss_Q: [ 3.3   2.92  2.68  2.12  1.74  1.24  0.   13.99] Loss_P: [ 3.98  3.68  3.14  2.85  2.34  1.77  1.33 19.11]\n",
      "Loss_Q: [ 3.23  2.87  2.66  2.15  1.75  1.23  0.   13.89] Loss_P: [ 4.03  3.6   3.06  2.85  2.35  1.79  1.33 19.  ]\n",
      "Loss_Q: [ 3.2   2.94  2.74  2.13  1.79  1.28  0.   14.07] Loss_P: [ 4.    3.58  3.06  2.86  2.33  1.8   1.34 18.97]\n",
      "Loss_Q: [ 3.22  2.87  2.65  2.12  1.75  1.26  0.   13.85] Loss_P: [ 3.95  3.56  3.01  2.9   2.29  1.8   1.36 18.87]\n",
      "Loss_Q: [ 3.15  2.89  2.6   2.14  1.74  1.3   0.   13.82] Loss_P: [ 4.02  3.59  3.02  2.76  2.28  1.82  1.4  18.88]\n",
      "Loss_Q: [ 3.19  2.87  2.66  2.1   1.76  1.31  0.   13.88] Loss_P: [ 4.01  3.59  3.08  2.83  2.28  1.84  1.37 19.  ]\n",
      "Loss_Q: [ 3.14  2.91  2.68  2.09  1.75  1.32  0.   13.88] Loss_P: [ 3.94  3.57  3.11  2.84  2.26  1.83  1.37 18.93]\n",
      "Loss_Q: [ 3.17  2.88  2.7   2.05  1.77  1.32  0.   13.89] Loss_P: [ 3.98  3.59  3.07  2.85  2.24  1.87  1.39 18.96]\n",
      "Loss_Q: [ 3.06  2.84  2.63  2.03  1.79  1.33  0.   13.68] Loss_P: [ 3.93  3.48  3.06  2.82  2.19  1.87  1.37 18.73]\n",
      "Loss_Q: [ 3.15  2.75  2.61  2.04  1.8   1.26  0.   13.61] Loss_P: [ 3.89  3.45  3.01  2.73  2.21  1.86  1.36 18.51]\n",
      "Loss_Q: [ 3.07  2.7   2.62  2.03  1.75  1.23  0.   13.4 ] Loss_P: [ 3.95  3.45  2.98  2.77  2.2   1.8   1.33 18.49]\n",
      "Loss_Q: [ 2.96  2.71  2.58  1.99  1.73  1.23  0.   13.21] Loss_P: [ 3.92  3.32  2.9   2.78  2.12  1.83  1.31 18.18]\n",
      "Loss_Q: [ 3.1   2.77  2.65  2.    1.73  1.21  0.   13.45] Loss_P: [ 3.92  3.44  2.93  2.83  2.14  1.82  1.28 18.35]\n",
      "Loss_Q: [ 3.2   2.67  2.63  1.96  1.74  1.21  0.   13.41] Loss_P: [ 3.87  3.57  2.93  2.85  2.1   1.83  1.3  18.43]\n",
      "Loss_Q: [ 3.03  2.73  2.57  1.93  1.73  1.18  0.   13.16] Loss_P: [ 3.88  3.41  2.9   2.76  2.09  1.83  1.27 18.14]\n",
      "Loss_Q: [ 3.05  2.68  2.47  1.97  1.71  1.18  0.   13.05] Loss_P: [ 3.95  3.36  2.84  2.62  2.09  1.81  1.27 17.93]\n",
      "Loss_Q: [ 2.95  2.63  2.48  1.92  1.72  1.18  0.   12.87] Loss_P: [ 3.92  3.44  2.78  2.66  2.09  1.81  1.25 17.95]\n",
      "Loss_Q: [ 2.97  2.6   2.52  2.01  1.74  1.2   0.   13.05] Loss_P: [ 3.91  3.48  2.73  2.71  2.12  1.8   1.27 18.02]\n",
      "Loss_Q: [ 3.07  2.54  2.48  2.01  1.72  1.16  0.   12.99] Loss_P: [ 3.85  3.57  2.7   2.66  2.16  1.78  1.21 17.94]\n",
      "Loss_Q: [ 3.1   2.53  2.48  1.97  1.74  1.13  0.   12.95] Loss_P: [ 3.88  3.48  2.68  2.62  2.06  1.79  1.2  17.72]\n",
      "Loss_Q: [ 3.07  2.59  2.44  1.96  1.72  1.16  0.   12.94] Loss_P: [ 3.86  3.45  2.81  2.59  2.06  1.81  1.22 17.8 ]\n",
      "Loss_Q: [ 3.06  2.54  2.43  1.91  1.69  1.2   0.   12.83] Loss_P: [ 3.86  3.49  2.71  2.65  2.03  1.74  1.27 17.74]\n",
      "Loss_Q: [ 3.09  2.62  2.45  1.9   1.65  1.18  0.   12.88] Loss_P: [ 3.86  3.49  2.8   2.6   2.07  1.73  1.25 17.79]\n",
      "Loss_Q: [ 3.06  2.74  2.4   1.95  1.63  1.15  0.   12.93] Loss_P: [ 3.89  3.46  2.82  2.63  2.08  1.7   1.24 17.81]\n",
      "Loss_Q: [ 3.12  2.73  2.43  1.97  1.62  1.12  0.   12.98] Loss_P: [ 3.82  3.54  2.83  2.64  2.09  1.71  1.2  17.84]\n",
      "Loss_Q: [ 3.1   2.78  2.39  1.94  1.67  1.14  0.   13.02] Loss_P: [ 3.85  3.56  2.91  2.61  2.11  1.71  1.23 17.99]\n",
      "Loss_Q: [ 2.97  2.75  2.39  1.93  1.63  1.12  0.   12.8 ] Loss_P: [ 3.81  3.51  2.87  2.58  2.08  1.73  1.2  17.78]\n",
      "Loss_Q: [ 2.98  2.69  2.37  1.91  1.59  1.14  0.   12.68] Loss_P: [ 3.79  3.54  2.85  2.57  2.    1.67  1.21 17.63]\n",
      "Loss_Q: [ 3.05  2.81  2.47  1.91  1.58  1.15  0.   12.95] Loss_P: [ 3.83  3.45  2.83  2.66  2.06  1.63  1.25 17.7 ]\n",
      "Loss_Q: [ 3.05  2.74  2.49  1.93  1.6   1.15  0.   12.96] Loss_P: [ 3.75  3.57  2.84  2.69  2.06  1.58  1.23 17.72]\n",
      "Loss_Q: [ 3.14  2.82  2.54  1.92  1.55  1.15  0.   13.13] Loss_P: [ 3.72  3.59  2.88  2.74  2.09  1.59  1.24 17.85]\n",
      "Loss_Q: [ 3.12  2.79  2.5   1.9   1.56  1.15  0.   13.03] Loss_P: [ 3.75  3.62  2.86  2.73  2.08  1.61  1.24 17.89]\n",
      "Loss_Q: [ 3.06  2.74  2.56  1.91  1.56  1.17  0.   12.99] Loss_P: [ 3.79  3.62  2.81  2.8   2.04  1.61  1.26 17.93]\n",
      "Loss_Q: [ 3.02  2.74  2.63  1.87  1.57  1.16  0.   12.99] Loss_P: [ 3.77  3.59  2.87  2.86  2.    1.64  1.26 17.99]\n",
      "Loss_Q: [ 3.    2.83  2.59  1.89  1.49  1.17  0.   12.97] Loss_P: [ 3.76  3.52  2.92  2.85  2.02  1.63  1.26 17.95]\n",
      "Loss_Q: [ 2.98  2.76  2.51  1.88  1.56  1.11  0.   12.8 ] Loss_P: [ 3.79  3.52  2.88  2.73  1.98  1.66  1.21 17.77]\n",
      "Loss_Q: [ 2.95  2.79  2.54  1.86  1.56  1.1   0.   12.8 ] Loss_P: [ 3.82  3.46  2.88  2.79  2.    1.63  1.2  17.77]\n",
      "Loss_Q: [ 2.87  2.86  2.52  1.89  1.48  1.12  0.   12.74] Loss_P: [ 3.76  3.42  3.    2.7   2.    1.59  1.19 17.66]\n",
      "Loss_Q: [ 2.95  2.87  2.51  1.83  1.54  1.11  0.   12.81] Loss_P: [ 3.75  3.42  3.    2.7   1.98  1.59  1.22 17.66]\n",
      "Loss_Q: [ 2.86  2.84  2.52  1.78  1.55  1.09  0.   12.63] Loss_P: [ 3.73  3.44  2.86  2.69  1.92  1.58  1.24 17.46]\n",
      "Loss_Q: [ 2.91  2.81  2.48  1.83  1.49  1.1   0.   12.62] Loss_P: [ 3.71  3.4   2.91  2.68  1.97  1.55  1.18 17.4 ]\n",
      "Loss_Q: [ 2.83  2.8   2.54  1.86  1.46  1.08  0.   12.57] Loss_P: [ 3.75  3.46  2.89  2.69  2.07  1.5   1.17 17.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 2.91  2.89  2.57  1.87  1.47  1.08  0.   12.8 ] Loss_P: [ 3.77  3.42  3.06  2.73  2.08  1.52  1.17 17.74]\n",
      "Loss_Q: [ 2.99  2.91  2.46  1.86  1.48  1.08  0.   12.79] Loss_P: [ 3.76  3.48  3.06  2.68  2.05  1.5   1.17 17.69]\n",
      "Loss_Q: [ 2.94  2.86  2.47  1.97  1.54  1.09  0.   12.87] Loss_P: [ 3.77  3.48  3.03  2.62  2.13  1.6   1.16 17.81]\n",
      "Loss_Q: [ 3.05  2.8   2.44  2.    1.61  1.08  0.   12.97] Loss_P: [ 3.77  3.5   2.97  2.65  2.19  1.65  1.18 17.91]\n",
      "Loss_Q: [ 2.99  2.83  2.47  2.01  1.65  1.05  0.   12.99] Loss_P: [ 3.74  3.4   2.93  2.61  2.2   1.71  1.16 17.74]\n",
      "Loss_Q: [ 2.91  2.79  2.53  1.93  1.67  1.07  0.   12.92] Loss_P: [ 3.76  3.48  2.93  2.69  2.15  1.7   1.16 17.87]\n",
      "Loss_Q: [ 2.87  2.85  2.52  1.92  1.68  1.12  0.   12.96] Loss_P: [ 3.79  3.35  3.    2.66  2.1   1.69  1.19 17.79]\n",
      "Loss_Q: [ 2.89  2.95  2.59  1.98  1.7   1.12  0.   13.23] Loss_P: [ 3.76  3.36  3.08  2.72  2.17  1.75  1.19 18.04]\n",
      "Loss_Q: [ 3.04  2.94  2.62  1.98  1.72  1.08  0.   13.37] Loss_P: [ 3.77  3.4   3.08  2.72  2.17  1.74  1.19 18.07]\n",
      "Loss_Q: [ 2.93  2.93  2.58  1.94  1.72  1.11  0.   13.21] Loss_P: [ 3.74  3.43  3.03  2.71  2.15  1.77  1.17 18.  ]\n",
      "Loss_Q: [ 2.98  2.89  2.47  1.95  1.73  1.1   0.   13.12] Loss_P: [ 3.77  3.51  3.12  2.66  2.16  1.76  1.19 18.17]\n",
      "Loss_Q: [ 2.94  2.95  2.5   1.97  1.7   1.09  0.   13.15] Loss_P: [ 3.77  3.51  3.11  2.57  2.14  1.72  1.16 17.97]\n",
      "Loss_Q: [ 2.92  2.96  2.48  1.93  1.7   1.08  0.   13.06] Loss_P: [ 3.75  3.45  3.13  2.56  2.13  1.74  1.17 17.93]\n",
      "Loss_Q: [ 2.81  2.94  2.48  1.99  1.72  1.14  0.   13.08] Loss_P: [ 3.78  3.37  3.11  2.59  2.19  1.79  1.23 18.05]\n",
      "Loss_Q: [ 2.85  3.    2.44  2.    1.74  1.16  0.   13.18] Loss_P: [ 3.74  3.46  3.08  2.6   2.24  1.78  1.22 18.12]\n",
      "Loss_Q: [ 2.83  3.    2.49  1.96  1.72  1.13  0.   13.13] Loss_P: [ 3.74  3.38  3.15  2.52  2.22  1.72  1.24 17.98]\n",
      "Loss_Q: [ 2.83  2.99  2.48  1.92  1.74  1.13  0.   13.09] Loss_P: [ 3.74  3.34  3.1   2.54  2.21  1.76  1.2  17.9 ]\n",
      "Loss_Q: [ 2.75  3.01  2.52  1.89  1.7   1.14  0.   13.01] Loss_P: [ 3.76  3.34  3.15  2.63  2.16  1.77  1.23 18.04]\n",
      "Loss_Q: [ 2.79  3.01  2.49  1.91  1.66  1.13  0.   12.98] Loss_P: [ 3.72  3.38  3.1   2.6   2.17  1.74  1.22 17.93]\n",
      "Loss_Q: [ 2.88  3.    2.4   1.9   1.69  1.17  0.   13.04] Loss_P: [ 3.75  3.36  3.16  2.43  2.15  1.74  1.25 17.84]\n",
      "Loss_Q: [ 2.87  3.02  2.27  1.88  1.71  1.19  0.   12.94] Loss_P: [ 3.72  3.43  3.23  2.38  2.12  1.77  1.27 17.92]\n",
      "Loss_Q: [ 2.89  3.02  2.3   1.89  1.7   1.2   0.   13.  ] Loss_P: [ 3.75  3.4   3.23  2.43  2.12  1.79  1.26 17.99]\n",
      "Loss_Q: [ 2.9   3.03  2.36  1.86  1.68  1.12  0.   12.95] Loss_P: [ 3.72  3.4   3.18  2.43  2.07  1.76  1.2  17.77]\n",
      "Loss_Q: [ 2.87  3.07  2.44  1.86  1.74  1.12  0.   13.1 ] Loss_P: [ 3.73  3.38  3.19  2.55  2.12  1.79  1.24 17.99]\n",
      "Loss_Q: [ 2.75  2.95  2.53  1.87  1.73  1.19  0.   13.02] Loss_P: [ 3.66  3.4   3.06  2.56  2.14  1.82  1.27 17.91]\n",
      "Loss_Q: [ 2.86  2.92  2.47  1.91  1.74  1.2   0.   13.1 ] Loss_P: [ 3.68  3.43  3.07  2.56  2.18  1.8   1.26 17.98]\n",
      "Loss_Q: [ 2.83  2.91  2.55  1.91  1.77  1.2   0.   13.17] Loss_P: [ 3.73  3.36  3.04  2.62  2.22  1.84  1.27 18.06]\n",
      "Loss_Q: [ 2.85  2.97  2.53  1.88  1.8   1.22  0.   13.24] Loss_P: [ 3.71  3.39  3.13  2.58  2.18  1.85  1.26 18.1 ]\n",
      "Loss_Q: [ 2.81  2.9   2.48  1.87  1.78  1.14  0.   12.98] Loss_P: [ 3.72  3.35  3.07  2.56  2.18  1.85  1.2  17.92]\n",
      "Loss_Q: [ 2.79  2.88  2.49  1.89  1.76  1.18  0.   12.99] Loss_P: [ 3.68  3.34  3.    2.53  2.15  1.85  1.24 17.8 ]\n",
      "Loss_Q: [ 2.78  2.92  2.51  1.88  1.74  1.19  0.   13.03] Loss_P: [ 3.72  3.35  3.09  2.52  2.18  1.83  1.24 17.93]\n",
      "Loss_Q: [ 2.85  2.89  2.52  1.83  1.78  1.14  0.   13.02] Loss_P: [ 3.72  3.46  3.08  2.61  2.13  1.86  1.21 18.06]\n",
      "Loss_Q: [ 2.85  2.89  2.6   1.86  1.77  1.14  0.   13.1 ] Loss_P: [ 3.69  3.43  3.05  2.62  2.13  1.82  1.21 17.95]\n",
      "Loss_Q: [ 2.79  2.92  2.55  1.84  1.78  1.11  0.   12.98] Loss_P: [ 3.72  3.36  3.1   2.63  2.08  1.84  1.17 17.9 ]\n",
      "Loss_Q: [ 2.78  2.97  2.5   1.82  1.77  1.16  0.   13.  ] Loss_P: [ 3.74  3.3   3.08  2.55  2.08  1.84  1.21 17.79]\n",
      "Loss_Q: [ 2.76  2.97  2.55  1.86  1.75  1.19  0.   13.08] Loss_P: [ 3.73  3.35  3.11  2.61  2.11  1.81  1.27 17.99]\n",
      "Loss_Q: [ 2.82  3.02  2.47  1.88  1.72  1.2   0.   13.11] Loss_P: [ 3.67  3.36  3.18  2.56  2.1   1.8   1.22 17.88]\n",
      "Loss_Q: [ 2.8   3.02  2.36  1.87  1.75  1.17  0.   12.98] Loss_P: [ 3.66  3.31  3.16  2.47  2.09  1.83  1.26 17.78]\n",
      "Loss_Q: [ 2.72  2.98  2.36  1.82  1.74  1.18  0.   12.79] Loss_P: [ 3.62  3.21  3.14  2.41  2.08  1.82  1.26 17.55]\n",
      "Loss_Q: [ 2.64  2.93  2.31  1.84  1.73  1.17  0.   12.61] Loss_P: [ 3.71  3.2   3.08  2.41  2.12  1.81  1.26 17.59]\n",
      "Loss_Q: [ 2.79  2.96  2.4   1.85  1.74  1.18  0.   12.91] Loss_P: [ 3.7   3.33  3.05  2.42  2.07  1.83  1.24 17.64]\n",
      "Loss_Q: [ 2.76  2.9   2.36  1.87  1.75  1.19  0.   12.83] Loss_P: [ 3.67  3.34  3.03  2.48  2.08  1.82  1.28 17.69]\n",
      "Loss_Q: [ 2.8   2.89  2.32  1.89  1.77  1.22  0.   12.89] Loss_P: [ 3.68  3.32  3.03  2.44  2.14  1.83  1.24 17.69]\n",
      "Loss_Q: [ 2.82  2.94  2.43  1.92  1.76  1.19  0.   13.06] Loss_P: [ 3.73  3.31  3.05  2.57  2.12  1.82  1.27 17.87]\n",
      "Loss_Q: [ 2.76  2.92  2.43  1.93  1.75  1.19  0.   12.98] Loss_P: [ 3.7   3.28  3.04  2.54  2.19  1.81  1.3  17.86]\n",
      "Loss_Q: [ 2.81  2.87  2.41  1.89  1.74  1.2   0.   12.91] Loss_P: [ 3.71  3.3   3.    2.57  2.15  1.8   1.29 17.8 ]\n",
      "Loss_Q: [ 2.82  2.88  2.46  1.88  1.71  1.15  0.   12.92] Loss_P: [ 3.69  3.3   3.02  2.56  2.16  1.74  1.26 17.72]\n",
      "Loss_Q: [ 2.79  2.86  2.39  1.94  1.69  1.13  0.   12.81] Loss_P: [ 3.68  3.35  2.95  2.55  2.19  1.72  1.24 17.68]\n",
      "Loss_Q: [ 2.85  2.84  2.44  1.99  1.68  1.18  0.   12.98] Loss_P: [ 3.66  3.35  3.01  2.57  2.24  1.73  1.29 17.86]\n",
      "Loss_Q: [ 2.77  2.89  2.38  2.01  1.68  1.16  0.   12.89] Loss_P: [ 3.61  3.4   3.03  2.5   2.27  1.73  1.28 17.81]\n",
      "Loss_Q: [ 2.81  2.95  2.37  1.92  1.69  1.15  0.   12.89] Loss_P: [ 3.64  3.34  3.02  2.59  2.25  1.71  1.24 17.79]\n",
      "Loss_Q: [ 2.69  2.95  2.3   1.96  1.73  1.15  0.   12.78] Loss_P: [ 3.64  3.25  3.07  2.46  2.21  1.78  1.24 17.65]\n",
      "Loss_Q: [ 2.7   2.91  2.3   1.95  1.73  1.18  0.   12.78] Loss_P: [ 3.65  3.21  2.97  2.45  2.21  1.79  1.25 17.53]\n",
      "Loss_Q: [ 2.66  2.9   2.38  1.95  1.72  1.15  0.   12.76] Loss_P: [ 3.7   3.22  2.97  2.54  2.24  1.77  1.22 17.67]\n",
      "Loss_Q: [ 2.73  2.83  2.27  1.9   1.69  1.16  0.   12.57] Loss_P: [ 3.67  3.25  2.89  2.42  2.19  1.77  1.2  17.38]\n",
      "Loss_Q: [ 2.67  2.83  2.26  1.88  1.72  1.13  0.   12.49] Loss_P: [ 3.62  3.29  3.    2.44  2.15  1.78  1.24 17.53]\n",
      "Loss_Q: [ 2.71  2.83  2.33  1.94  1.71  1.13  0.   12.65] Loss_P: [ 3.68  3.26  2.95  2.46  2.19  1.76  1.2  17.49]\n",
      "Loss_Q: [ 2.76  2.88  2.36  1.93  1.75  1.15  0.   12.83] Loss_P: [ 3.64  3.27  3.02  2.53  2.22  1.79  1.19 17.64]\n",
      "Loss_Q: [ 2.76  2.87  2.39  1.96  1.74  1.16  0.   12.88] Loss_P: [ 3.7   3.23  3.    2.56  2.23  1.8   1.27 17.79]\n",
      "Loss_Q: [ 2.75  2.84  2.36  1.95  1.73  1.19  0.   12.81] Loss_P: [ 3.65  3.21  2.97  2.52  2.25  1.8   1.3  17.69]\n",
      "Loss_Q: [ 2.67  2.89  2.3   1.98  1.72  1.21  0.   12.77] Loss_P: [ 3.68  3.18  2.98  2.5   2.25  1.79  1.31 17.69]\n",
      "Loss_Q: [ 2.65  2.85  2.29  1.98  1.73  1.16  0.   12.67] Loss_P: [ 3.68  3.17  2.92  2.44  2.3   1.78  1.26 17.56]\n",
      "Loss_Q: [ 2.6   2.8   2.21  1.96  1.72  1.15  0.   12.45] Loss_P: [ 3.66  3.12  3.01  2.39  2.24  1.78  1.24 17.44]\n",
      "Loss_Q: [ 2.65  2.82  2.21  1.93  1.73  1.14  0.   12.5 ] Loss_P: [ 3.66  3.12  2.95  2.36  2.23  1.75  1.23 17.3 ]\n",
      "Loss_Q: [ 2.69  2.77  2.21  1.96  1.72  1.17  0.   12.53] Loss_P: [ 3.68  3.21  2.92  2.36  2.21  1.77  1.27 17.41]\n",
      "Loss_Q: [ 2.67  2.75  2.31  1.95  1.73  1.15  0.   12.56] Loss_P: [ 3.65  3.2   2.9   2.39  2.21  1.76  1.27 17.38]\n",
      "Loss_Q: [ 2.68  2.76  2.28  1.97  1.71  1.13  0.   12.53] Loss_P: [ 3.63  3.24  2.92  2.4   2.21  1.75  1.25 17.4 ]\n",
      "Loss_Q: [ 2.71  2.79  2.36  1.88  1.73  1.13  0.   12.61] Loss_P: [ 3.63  3.23  2.87  2.51  2.2   1.75  1.23 17.41]\n",
      "Loss_Q: [ 2.66  2.71  2.29  1.92  1.67  1.1   0.   12.35] Loss_P: [ 3.66  3.13  2.87  2.5   2.2   1.72  1.16 17.23]\n",
      "Loss_Q: [ 2.64  2.79  2.29  1.92  1.65  1.06  0.   12.35] Loss_P: [ 3.64  3.12  2.89  2.49  2.2   1.69  1.15 17.17]\n",
      "Loss_Q: [ 2.62  2.71  2.31  1.89  1.7   1.11  0.   12.35] Loss_P: [ 3.67  3.17  2.85  2.5   2.14  1.71  1.19 17.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 2.66  2.76  2.32  1.89  1.66  1.13  0.   12.42] Loss_P: [ 3.7   3.12  2.86  2.5   2.18  1.72  1.25 17.32]\n",
      "Loss_Q: [ 2.65  2.7   2.3   1.9   1.69  1.13  0.   12.37] Loss_P: [ 3.66  3.16  2.76  2.45  2.19  1.71  1.21 17.14]\n",
      "Loss_Q: [ 2.69  2.71  2.31  1.91  1.7   1.08  0.   12.41] Loss_P: [ 3.67  3.22  2.78  2.48  2.17  1.72  1.2  17.24]\n",
      "Loss_Q: [ 2.74  2.68  2.29  1.87  1.66  1.05  0.   12.29] Loss_P: [ 3.67  3.2   2.84  2.46  2.12  1.73  1.12 17.13]\n",
      "Loss_Q: [ 2.67  2.75  2.25  1.85  1.67  1.08  0.   12.27] Loss_P: [ 3.61  3.17  2.9   2.43  2.14  1.68  1.19 17.12]\n",
      "Loss_Q: [ 2.72  2.82  2.28  1.81  1.69  1.08  0.   12.39] Loss_P: [ 3.65  3.2   2.95  2.46  2.12  1.69  1.19 17.26]\n",
      "Loss_Q: [ 2.69  2.82  2.24  1.86  1.67  1.07  0.   12.35] Loss_P: [ 3.65  3.18  2.93  2.39  2.14  1.69  1.16 17.14]\n",
      "Loss_Q: [ 2.68  2.85  2.3   1.86  1.64  1.05  0.   12.38] Loss_P: [ 3.61  3.21  2.95  2.43  2.14  1.7   1.12 17.16]\n",
      "Loss_Q: [ 2.74  2.84  2.3   1.83  1.64  0.95  0.   12.3 ] Loss_P: [ 3.59  3.26  3.    2.44  2.11  1.66  1.05 17.11]\n",
      "Loss_Q: [ 2.73  2.91  2.29  1.8   1.68  0.91  0.   12.31] Loss_P: [ 3.66  3.24  3.    2.39  2.09  1.69  1.04 17.1 ]\n",
      "Loss_Q: [ 2.6   2.84  2.27  1.85  1.68  0.91  0.   12.15] Loss_P: [ 3.62  3.18  2.99  2.44  2.09  1.72  1.04 17.08]\n",
      "Loss_Q: [ 2.7   2.85  2.28  1.86  1.62  0.87  0.   12.18] Loss_P: [ 3.65  3.28  3.07  2.4   2.1   1.67  0.98 17.14]\n",
      "Loss_Q: [ 2.68  2.79  2.26  1.8   1.64  0.88  0.   12.06] Loss_P: [ 3.65  3.25  2.99  2.37  2.1   1.67  0.97 17.  ]\n",
      "Loss_Q: [ 2.63  2.79  2.25  1.82  1.61  0.9   0.   12.  ] Loss_P: [ 3.61  3.2   3.03  2.42  2.07  1.71  0.99 17.03]\n",
      "Loss_Q: [ 2.68  2.83  2.25  1.78  1.6   0.94  0.   12.08] Loss_P: [ 3.62  3.19  3.    2.38  2.02  1.66  1.01 16.89]\n",
      "Loss_Q: [ 2.69  2.69  2.31  1.71  1.61  0.9   0.   11.91] Loss_P: [ 3.6   3.3   2.96  2.39  1.9   1.63  0.96 16.73]\n",
      "Loss_Q: [ 2.6   2.7   2.26  1.67  1.6   0.88  0.   11.72] Loss_P: [ 3.59  3.24  2.94  2.38  1.95  1.68  0.99 16.76]\n",
      "Loss_Q: [ 2.65  2.7   2.2   1.68  1.61  0.9   0.   11.74] Loss_P: [ 3.6   3.25  2.89  2.36  1.93  1.66  1.   16.7 ]\n",
      "Loss_Q: [ 2.65  2.69  2.2   1.71  1.58  0.87  0.   11.7 ] Loss_P: [ 3.61  3.13  2.93  2.3   1.94  1.6   0.95 16.46]\n",
      "Loss_Q: [ 2.54  2.71  2.17  1.72  1.52  0.89  0.   11.55] Loss_P: [ 3.6   3.09  2.96  2.27  1.92  1.52  0.96 16.33]\n",
      "Loss_Q: [ 2.48  2.7   2.22  1.71  1.52  0.9   0.   11.53] Loss_P: [ 3.61  2.99  2.97  2.29  1.96  1.56  0.98 16.36]\n",
      "Loss_Q: [ 2.57  2.67  2.18  1.71  1.53  0.89  0.   11.54] Loss_P: [ 3.61  3.11  2.97  2.26  1.95  1.54  0.93 16.37]\n",
      "Loss_Q: [ 2.48  2.67  2.09  1.75  1.53  0.89  0.   11.41] Loss_P: [ 3.57  3.    2.94  2.21  1.95  1.57  0.99 16.23]\n",
      "Loss_Q: [ 2.41  2.59  2.06  1.73  1.53  0.94  0.   11.27] Loss_P: [ 3.61  2.98  2.87  2.2   1.97  1.58  1.03 16.23]\n",
      "Loss_Q: [ 2.5   2.45  2.01  1.74  1.54  0.9   0.   11.14] Loss_P: [ 3.63  3.07  2.68  2.19  1.95  1.58  0.94 16.04]\n",
      "Loss_Q: [ 2.49  2.42  2.09  1.76  1.53  0.93  0.   11.22] Loss_P: [ 3.61  3.    2.69  2.16  2.    1.55  1.   16.01]\n",
      "Loss_Q: [ 2.6   2.42  2.1   1.78  1.52  0.92  0.   11.33] Loss_P: [ 3.59  3.14  2.68  2.2   2.    1.59  1.02 16.21]\n",
      "Loss_Q: [ 2.49  2.43  2.07  1.85  1.54  0.94  0.   11.32] Loss_P: [ 3.61  3.1   2.67  2.18  2.1   1.57  1.02 16.25]\n",
      "Loss_Q: [ 2.48  2.47  2.18  1.87  1.57  0.98  0.   11.53] Loss_P: [ 3.63  3.14  2.67  2.29  2.16  1.62  1.06 16.57]\n",
      "Loss_Q: [ 2.51  2.45  2.11  1.89  1.64  1.05  0.   11.64] Loss_P: [ 3.6   3.05  2.66  2.22  2.12  1.72  1.13 16.5 ]\n",
      "Loss_Q: [ 2.43  2.4   2.2   1.84  1.67  1.09  0.   11.65] Loss_P: [ 3.62  3.07  2.68  2.27  2.05  1.7   1.17 16.56]\n",
      "Loss_Q: [ 2.45  2.49  2.25  1.8   1.68  1.09  0.   11.76] Loss_P: [ 3.66  3.01  2.71  2.29  2.05  1.72  1.2  16.62]\n",
      "Loss_Q: [ 2.51  2.57  2.24  1.83  1.69  1.15  0.   11.99] Loss_P: [ 3.6   3.11  2.74  2.31  2.09  1.75  1.23 16.82]\n",
      "Loss_Q: [ 2.51  2.61  2.14  1.89  1.72  1.23  0.   12.09] Loss_P: [ 3.58  3.14  2.82  2.22  2.12  1.74  1.28 16.9 ]\n",
      "Loss_Q: [ 2.63  2.63  2.23  1.93  1.7   1.23  0.   12.35] Loss_P: [ 3.6   3.16  2.86  2.35  2.22  1.8   1.3  17.29]\n",
      "Loss_Q: [ 2.55  2.63  2.28  1.99  1.69  1.15  0.   12.29] Loss_P: [ 3.6   3.08  2.83  2.4   2.22  1.73  1.24 17.11]\n",
      "Loss_Q: [ 2.54  2.65  2.25  1.95  1.68  1.2   0.   12.27] Loss_P: [ 3.61  3.06  2.83  2.37  2.2   1.73  1.3  17.11]\n",
      "Loss_Q: [ 2.54  2.55  2.32  1.93  1.74  1.22  0.   12.29] Loss_P: [ 3.62  3.06  2.83  2.48  2.22  1.77  1.31 17.28]\n",
      "Loss_Q: [ 2.43  2.61  2.32  1.98  1.72  1.22  0.   12.27] Loss_P: [ 3.6   2.99  2.81  2.49  2.23  1.77  1.28 17.17]\n",
      "Loss_Q: [ 2.39  2.58  2.27  1.94  1.75  1.2   0.   12.13] Loss_P: [ 3.65  2.95  2.78  2.44  2.21  1.8   1.3  17.13]\n",
      "Loss_Q: [ 2.47  2.6   2.27  1.96  1.74  1.25  0.   12.29] Loss_P: [ 3.6   2.98  2.83  2.44  2.23  1.79  1.35 17.22]\n",
      "Loss_Q: [ 2.46  2.64  2.3   1.97  1.71  1.22  0.   12.29] Loss_P: [ 3.58  2.93  2.82  2.44  2.23  1.75  1.31 17.05]\n",
      "Loss_Q: [ 2.57  2.61  2.35  1.95  1.71  1.24  0.   12.42] Loss_P: [ 3.63  3.08  2.85  2.45  2.22  1.81  1.33 17.37]\n",
      "Loss_Q: [ 2.5   2.64  2.26  1.96  1.76  1.23  0.   12.35] Loss_P: [ 3.56  3.18  2.8   2.47  2.22  1.8   1.3  17.32]\n",
      "Loss_Q: [ 2.54  2.67  2.31  1.94  1.72  1.24  0.   12.41] Loss_P: [ 3.6   3.12  2.83  2.49  2.22  1.8   1.32 17.37]\n",
      "Loss_Q: [ 2.55  2.66  2.33  1.94  1.73  1.25  0.   12.46] Loss_P: [ 3.59  3.1   2.79  2.53  2.18  1.78  1.33 17.3 ]\n",
      "Loss_Q: [ 2.55  2.63  2.3   1.88  1.73  1.23  0.   12.31] Loss_P: [ 3.62  3.1   2.84  2.41  2.15  1.79  1.3  17.22]\n",
      "Loss_Q: [ 2.47  2.57  2.35  1.86  1.72  1.18  0.   12.15] Loss_P: [ 3.59  3.03  2.78  2.48  2.1   1.79  1.26 17.03]\n",
      "Loss_Q: [ 2.43  2.63  2.36  1.89  1.69  1.16  0.   12.16] Loss_P: [ 3.57  3.05  2.78  2.53  2.17  1.72  1.24 17.06]\n",
      "Loss_Q: [ 2.5   2.65  2.38  1.84  1.71  1.13  0.   12.22] Loss_P: [ 3.54  3.05  2.85  2.51  2.15  1.76  1.23 17.09]\n",
      "Loss_Q: [ 2.49  2.63  2.35  1.86  1.72  1.18  0.   12.24] Loss_P: [ 3.54  3.04  2.9   2.52  2.17  1.76  1.28 17.2 ]\n",
      "Loss_Q: [ 2.45  2.67  2.27  1.83  1.75  1.16  0.   12.14] Loss_P: [ 3.54  3.    2.89  2.45  2.11  1.79  1.23 17.01]\n",
      "Loss_Q: [ 2.43  2.67  2.28  1.79  1.74  1.18  0.   12.08] Loss_P: [ 3.56  3.03  2.86  2.45  2.05  1.79  1.29 17.02]\n",
      "Loss_Q: [ 2.49  2.61  2.32  1.75  1.71  1.2   0.   12.06] Loss_P: [ 3.56  3.04  2.8   2.51  2.01  1.77  1.3  16.99]\n",
      "Loss_Q: [ 2.46  2.67  2.29  1.76  1.67  1.19  0.   12.04] Loss_P: [ 3.6   3.01  2.86  2.49  2.02  1.73  1.29 17.  ]\n",
      "Loss_Q: [ 2.52  2.62  2.31  1.77  1.68  1.17  0.   12.08] Loss_P: [ 3.57  3.05  2.84  2.51  2.06  1.74  1.29 17.06]\n",
      "Loss_Q: [ 2.47  2.57  2.25  1.75  1.66  1.18  0.   11.88] Loss_P: [ 3.58  3.05  2.84  2.45  2.02  1.69  1.25 16.88]\n",
      "Loss_Q: [ 2.44  2.69  2.36  1.8   1.65  1.19  0.   12.12] Loss_P: [ 3.52  2.98  2.87  2.42  2.04  1.69  1.29 16.82]\n",
      "Loss_Q: [ 2.54  2.72  2.37  1.76  1.66  1.18  0.   12.21] Loss_P: [ 3.53  3.08  2.92  2.52  1.99  1.71  1.26 17.01]\n",
      "Loss_Q: [ 2.42  2.71  2.31  1.8   1.62  1.15  0.   12.03] Loss_P: [ 3.54  3.06  2.9   2.51  2.05  1.7   1.23 17.  ]\n",
      "Loss_Q: [ 2.48  2.74  2.33  1.75  1.66  1.16  0.   12.11] Loss_P: [ 3.52  3.01  2.88  2.49  1.98  1.72  1.21 16.81]\n",
      "Loss_Q: [ 2.51  2.68  2.3   1.69  1.66  1.15  0.   12.  ] Loss_P: [ 3.56  3.05  2.88  2.49  1.97  1.74  1.24 16.93]\n",
      "Loss_Q: [ 2.41  2.69  2.26  1.58  1.65  1.13  0.   11.73] Loss_P: [ 3.51  3.04  2.92  2.46  1.87  1.72  1.23 16.74]\n",
      "Loss_Q: [ 2.51  2.72  2.32  1.68  1.69  1.15  0.   12.06] Loss_P: [ 3.56  3.05  2.87  2.43  1.91  1.74  1.25 16.8 ]\n",
      "Loss_Q: [ 2.52  2.68  2.24  1.65  1.66  1.1   0.   11.85] Loss_P: [ 3.49  3.18  2.92  2.44  1.88  1.75  1.17 16.84]\n",
      "Loss_Q: [ 2.48  2.71  2.28  1.68  1.65  1.08  0.   11.89] Loss_P: [ 3.53  3.04  2.78  2.39  1.89  1.73  1.15 16.52]\n",
      "Loss_Q: [ 2.47  2.67  2.35  1.66  1.7   1.12  0.   11.97] Loss_P: [ 3.49  3.08  2.86  2.47  1.88  1.74  1.2  16.72]\n",
      "Loss_Q: [ 2.49  2.64  2.32  1.63  1.67  1.09  0.   11.83] Loss_P: [ 3.47  3.13  2.81  2.5   1.88  1.72  1.19 16.69]\n",
      "Loss_Q: [ 2.54  2.61  2.36  1.68  1.62  1.07  0.   11.88] Loss_P: [ 3.51  3.12  2.78  2.46  1.95  1.7   1.2  16.73]\n",
      "Loss_Q: [ 2.58  2.56  2.29  1.67  1.64  1.1   0.   11.82] Loss_P: [ 3.51  3.17  2.73  2.48  1.92  1.73  1.19 16.74]\n",
      "Loss_Q: [ 2.53  2.65  2.28  1.76  1.65  1.1   0.   11.98] Loss_P: [ 3.48  3.15  2.78  2.5   2.02  1.72  1.19 16.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 2.49  2.67  2.26  1.73  1.64  1.13  0.   11.92] Loss_P: [ 3.44  3.11  2.86  2.5   1.95  1.72  1.21 16.78]\n",
      "Loss_Q: [ 2.5   2.64  2.25  1.74  1.6   1.13  0.   11.86] Loss_P: [ 3.45  3.09  2.82  2.46  1.97  1.68  1.26 16.72]\n",
      "Loss_Q: [ 2.48  2.62  2.26  1.73  1.61  1.18  0.   11.89] Loss_P: [ 3.47  3.09  2.77  2.46  1.94  1.68  1.26 16.67]\n",
      "Loss_Q: [ 2.51  2.6   2.28  1.7   1.65  1.23  0.   11.97] Loss_P: [ 3.45  3.08  2.82  2.51  1.94  1.71  1.33 16.84]\n",
      "Loss_Q: [ 2.49  2.65  2.32  1.7   1.61  1.19  0.   11.96] Loss_P: [ 3.48  3.18  2.85  2.46  2.    1.7   1.28 16.95]\n",
      "Loss_Q: [ 2.44  2.61  2.35  1.71  1.6   1.22  0.   11.93] Loss_P: [ 3.43  3.15  2.85  2.51  1.97  1.66  1.28 16.84]\n",
      "Loss_Q: [ 2.45  2.58  2.22  1.69  1.59  1.2   0.   11.74] Loss_P: [ 3.43  3.12  2.8   2.49  2.01  1.68  1.29 16.82]\n",
      "Loss_Q: [ 2.47  2.63  2.19  1.73  1.57  1.17  0.   11.76] Loss_P: [ 3.4   3.09  2.74  2.33  1.95  1.66  1.27 16.45]\n",
      "Loss_Q: [ 2.39  2.58  2.16  1.71  1.57  1.15  0.   11.56] Loss_P: [ 3.49  3.01  2.81  2.35  1.94  1.63  1.22 16.46]\n",
      "Loss_Q: [ 2.36  2.65  2.18  1.72  1.52  1.15  0.   11.58] Loss_P: [ 3.45  2.95  2.84  2.33  2.02  1.6   1.24 16.44]\n",
      "Loss_Q: [ 2.4   2.54  2.2   1.72  1.53  1.18  0.   11.57] Loss_P: [ 3.47  2.98  2.72  2.32  1.98  1.61  1.25 16.32]\n",
      "Loss_Q: [ 2.43  2.49  2.1   1.72  1.51  1.16  0.   11.41] Loss_P: [ 3.52  3.02  2.72  2.22  1.97  1.6   1.24 16.3 ]\n",
      "Loss_Q: [ 2.39  2.44  2.13  1.69  1.49  1.15  0.   11.29] Loss_P: [ 3.51  2.96  2.71  2.21  1.91  1.57  1.27 16.14]\n",
      "Loss_Q: [ 2.46  2.48  2.24  1.65  1.56  1.18  0.   11.57] Loss_P: [ 3.53  2.93  2.71  2.31  1.91  1.62  1.24 16.24]\n",
      "Loss_Q: [ 2.36  2.57  2.36  1.68  1.52  1.16  0.   11.64] Loss_P: [ 3.52  2.96  2.8   2.38  1.95  1.58  1.24 16.44]\n",
      "Loss_Q: [ 2.35  2.58  2.37  1.74  1.57  1.17  0.   11.77] Loss_P: [ 3.54  2.97  2.76  2.44  1.98  1.59  1.27 16.54]\n",
      "Loss_Q: [ 2.4   2.62  2.31  1.74  1.52  1.2   0.   11.8 ] Loss_P: [ 3.5   3.03  2.82  2.44  1.99  1.62  1.34 16.74]\n",
      "Loss_Q: [ 2.31  2.58  2.32  1.66  1.5   1.23  0.   11.59] Loss_P: [ 3.48  3.    2.76  2.48  1.94  1.61  1.35 16.62]\n",
      "Loss_Q: [ 2.36  2.59  2.31  1.71  1.5   1.22  0.   11.67] Loss_P: [ 3.47  2.95  2.75  2.49  1.96  1.55  1.32 16.49]\n",
      "Loss_Q: [ 2.31  2.63  2.29  1.74  1.52  1.2   0.   11.69] Loss_P: [ 3.48  3.    2.84  2.4   2.04  1.6   1.31 16.66]\n",
      "Loss_Q: [ 2.39  2.65  2.22  1.75  1.49  1.23  0.   11.72] Loss_P: [ 3.46  3.    2.9   2.37  1.98  1.56  1.31 16.58]\n",
      "Loss_Q: [ 2.47  2.65  2.3   1.69  1.48  1.2   0.   11.8 ] Loss_P: [ 3.47  3.05  2.89  2.47  2.    1.55  1.27 16.69]\n",
      "Loss_Q: [ 2.39  2.61  2.35  1.68  1.48  1.14  0.   11.65] Loss_P: [ 3.42  3.05  2.82  2.51  1.92  1.52  1.26 16.5 ]\n",
      "Loss_Q: [ 2.45  2.59  2.44  1.72  1.49  1.21  0.   11.9 ] Loss_P: [ 3.48  3.04  2.84  2.54  1.98  1.58  1.29 16.73]\n",
      "Loss_Q: [ 2.55  2.63  2.32  1.75  1.47  1.2   0.   11.92] Loss_P: [ 3.44  3.2   2.89  2.52  1.96  1.55  1.25 16.82]\n",
      "Loss_Q: [ 2.56  2.64  2.36  1.72  1.43  1.13  0.   11.84] Loss_P: [ 3.45  3.14  2.87  2.56  1.92  1.51  1.21 16.66]\n",
      "Loss_Q: [ 2.52  2.49  2.34  1.73  1.43  1.13  0.   11.63] Loss_P: [ 3.49  3.09  2.79  2.47  1.97  1.54  1.21 16.56]\n",
      "Loss_Q: [ 2.51  2.42  2.37  1.79  1.4   1.12  0.   11.61] Loss_P: [ 3.46  3.1   2.73  2.45  2.02  1.5   1.19 16.45]\n",
      "Loss_Q: [ 2.48  2.51  2.36  1.78  1.44  1.08  0.   11.64] Loss_P: [ 3.48  3.08  2.71  2.43  2.02  1.5   1.16 16.37]\n",
      "Loss_Q: [ 2.45  2.48  2.34  1.78  1.41  1.05  0.   11.51] Loss_P: [ 3.42  3.1   2.79  2.37  2.08  1.5   1.14 16.4 ]\n",
      "Loss_Q: [ 2.45  2.49  2.3   1.82  1.42  1.04  0.   11.52] Loss_P: [ 3.45  3.08  2.77  2.36  2.07  1.54  1.11 16.38]\n",
      "Loss_Q: [ 2.47  2.44  2.32  1.79  1.47  1.05  0.   11.54] Loss_P: [ 3.49  3.03  2.68  2.32  2.06  1.55  1.11 16.23]\n",
      "Loss_Q: [ 2.35  2.37  2.31  1.78  1.46  1.02  0.   11.29] Loss_P: [ 3.49  2.88  2.63  2.34  2.04  1.55  1.1  16.04]\n",
      "Loss_Q: [ 2.34  2.38  2.26  1.81  1.43  1.03  0.   11.24] Loss_P: [ 3.51  2.98  2.62  2.29  2.02  1.53  1.13 16.07]\n",
      "Loss_Q: [ 2.33  2.3   2.21  1.73  1.45  1.07  0.   11.08] Loss_P: [ 3.53  3.02  2.59  2.26  1.98  1.52  1.14 16.04]\n",
      "Loss_Q: [ 2.4   2.29  2.24  1.74  1.41  1.09  0.   11.18] Loss_P: [ 3.5   2.98  2.6   2.28  2.01  1.51  1.19 16.06]\n",
      "Loss_Q: [ 2.28  2.33  2.27  1.75  1.38  1.06  0.   11.07] Loss_P: [ 3.49  2.94  2.57  2.31  2.    1.47  1.16 15.94]\n",
      "Loss_Q: [ 2.43  2.4   2.25  1.81  1.41  1.02  0.   11.33] Loss_P: [ 3.51  2.97  2.68  2.34  2.05  1.47  1.16 16.19]\n",
      "Loss_Q: [ 2.48  2.45  2.33  1.78  1.41  1.09  0.   11.53] Loss_P: [ 3.47  3.17  2.71  2.38  2.07  1.49  1.18 16.47]\n",
      "Loss_Q: [ 2.5   2.47  2.28  1.79  1.39  1.09  0.   11.52] Loss_P: [ 3.44  3.17  2.71  2.39  1.96  1.49  1.2  16.36]\n",
      "Loss_Q: [ 2.44  2.44  2.29  1.72  1.42  1.09  0.   11.39] Loss_P: [ 3.46  3.02  2.69  2.34  1.96  1.5   1.17 16.14]\n",
      "Loss_Q: [ 2.33  2.49  2.2   1.77  1.44  1.05  0.   11.29] Loss_P: [ 3.45  2.95  2.7   2.29  2.02  1.48  1.14 16.04]\n",
      "Loss_Q: [ 2.43  2.48  2.26  1.79  1.41  1.    0.   11.36] Loss_P: [ 3.48  3.01  2.73  2.27  2.04  1.49  1.09 16.11]\n",
      "Loss_Q: [ 2.39  2.53  2.21  1.85  1.42  1.03  0.   11.42] Loss_P: [ 3.49  2.94  2.78  2.26  2.06  1.5   1.12 16.14]\n",
      "Loss_Q: [ 2.37  2.54  2.04  1.86  1.4   1.    0.   11.21] Loss_P: [ 3.52  2.88  2.82  2.19  2.04  1.45  1.11 16.02]\n",
      "Loss_Q: [ 2.43  2.43  2.15  1.81  1.41  0.96  0.   11.19] Loss_P: [ 3.49  2.96  2.69  2.23  1.99  1.51  1.07 15.93]\n",
      "Loss_Q: [ 2.37  2.47  2.08  1.82  1.43  0.99  0.   11.15] Loss_P: [ 3.5   2.95  2.74  2.23  1.98  1.49  1.09 15.98]\n",
      "Loss_Q: [ 2.34  2.53  2.19  1.75  1.43  1.01  0.   11.24] Loss_P: [ 3.47  2.89  2.76  2.31  1.94  1.5   1.09 15.97]\n",
      "Loss_Q: [ 2.28  2.62  2.19  1.75  1.47  1.04  0.   11.34] Loss_P: [ 3.51  2.79  2.82  2.27  2.02  1.51  1.16 16.08]\n",
      "Loss_Q: [ 2.23  2.52  2.19  1.73  1.48  1.08  0.   11.24] Loss_P: [ 3.56  2.76  2.79  2.29  1.96  1.54  1.2  16.09]\n",
      "Loss_Q: [ 2.29  2.36  2.12  1.67  1.47  1.05  0.   10.96] Loss_P: [ 3.49  2.84  2.65  2.19  1.89  1.54  1.15 15.75]\n",
      "Loss_Q: [ 2.27  2.32  2.04  1.69  1.5   1.06  0.   10.88] Loss_P: [ 3.53  2.78  2.57  2.14  1.89  1.51  1.17 15.6 ]\n",
      "Loss_Q: [ 2.34  2.4   2.07  1.71  1.51  1.06  0.   11.09] Loss_P: [ 3.5   2.82  2.65  2.14  1.88  1.55  1.19 15.74]\n",
      "Loss_Q: [ 2.16  2.35  2.04  1.61  1.47  1.08  0.   10.72] Loss_P: [ 3.57  2.74  2.68  2.16  1.83  1.53  1.2  15.71]\n",
      "Loss_Q: [ 2.11  2.37  2.08  1.64  1.46  1.07  0.   10.72] Loss_P: [ 3.57  2.66  2.57  2.19  1.81  1.51  1.17 15.48]\n",
      "Loss_Q: [ 2.1   2.43  2.09  1.62  1.5   1.03  0.   10.78] Loss_P: [ 3.52  2.73  2.7   2.17  1.78  1.53  1.12 15.55]\n",
      "Loss_Q: [ 2.21  2.45  2.12  1.62  1.51  1.03  0.   10.95] Loss_P: [ 3.51  2.77  2.72  2.19  1.82  1.57  1.14 15.73]\n",
      "Loss_Q: [ 2.31  2.42  2.21  1.58  1.5   1.03  0.   11.05] Loss_P: [ 3.53  2.83  2.68  2.28  1.8   1.56  1.13 15.81]\n",
      "Loss_Q: [ 2.26  2.37  2.17  1.65  1.47  0.97  0.   10.89] Loss_P: [ 3.55  2.84  2.65  2.24  1.86  1.54  1.13 15.83]\n",
      "Loss_Q: [ 2.32  2.44  2.18  1.66  1.46  1.06  0.   11.12] Loss_P: [ 3.49  2.91  2.74  2.3   1.84  1.5   1.17 15.94]\n",
      "Loss_Q: [ 2.32  2.4   2.22  1.68  1.45  1.1   0.   11.17] Loss_P: [ 3.51  2.92  2.69  2.32  1.86  1.51  1.23 16.04]\n",
      "Loss_Q: [ 2.26  2.4   2.2   1.71  1.43  1.11  0.   11.12] Loss_P: [ 3.53  2.85  2.6   2.25  1.88  1.5   1.24 15.83]\n",
      "Loss_Q: [ 2.32  2.44  2.16  1.71  1.41  1.08  0.   11.13] Loss_P: [ 3.54  2.87  2.64  2.23  1.93  1.52  1.23 15.96]\n",
      "Loss_Q: [ 2.27  2.35  2.15  1.69  1.41  1.1   0.   10.98] Loss_P: [ 3.49  2.82  2.61  2.25  1.87  1.49  1.24 15.77]\n",
      "Loss_Q: [ 2.3   2.39  2.13  1.66  1.42  1.06  0.   10.96] Loss_P: [ 3.45  2.86  2.67  2.2   1.78  1.47  1.18 15.61]\n",
      "Loss_Q: [ 2.29  2.31  2.18  1.65  1.44  1.03  0.   10.89] Loss_P: [ 3.44  2.8   2.63  2.22  1.79  1.49  1.17 15.55]\n",
      "Loss_Q: [ 2.19  2.31  2.09  1.7   1.46  1.02  0.   10.78] Loss_P: [ 3.48  2.79  2.59  2.15  1.86  1.5   1.16 15.54]\n",
      "Loss_Q: [ 2.23  2.31  2.19  1.64  1.42  1.02  0.   10.82] Loss_P: [ 3.49  2.84  2.64  2.21  1.85  1.48  1.19 15.7 ]\n",
      "Loss_Q: [ 2.14  2.25  2.12  1.66  1.43  0.97  0.   10.57] Loss_P: [ 3.52  2.75  2.53  2.19  1.83  1.49  1.12 15.44]\n",
      "Loss_Q: [ 2.22  2.31  2.19  1.63  1.46  0.99  0.   10.8 ] Loss_P: [ 3.53  2.74  2.58  2.29  1.82  1.52  1.15 15.62]\n",
      "Loss_Q: [ 2.2   2.35  2.24  1.63  1.45  1.02  0.   10.89] Loss_P: [ 3.48  2.79  2.57  2.35  1.77  1.52  1.14 15.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 2.04  2.2   2.19  1.65  1.46  1.08  0.   10.62] Loss_P: [ 3.53  2.71  2.45  2.31  1.78  1.55  1.17 15.49]\n",
      "Loss_Q: [ 2.19  2.26  2.19  1.61  1.48  1.03  0.   10.75] Loss_P: [ 3.52  2.83  2.52  2.26  1.77  1.55  1.23 15.69]\n",
      "Loss_Q: [ 2.18  2.21  2.23  1.62  1.51  1.08  0.   10.83] Loss_P: [ 3.49  2.8   2.46  2.35  1.8   1.55  1.27 15.73]\n",
      "Loss_Q: [ 2.25  2.34  2.31  1.72  1.52  1.13  0.   11.27] Loss_P: [ 3.49  2.84  2.6   2.41  1.85  1.54  1.25 15.96]\n",
      "Loss_Q: [ 2.25  2.29  2.4   1.75  1.47  1.12  0.   11.28] Loss_P: [ 3.49  2.84  2.63  2.47  1.87  1.55  1.26 16.12]\n",
      "Loss_Q: [ 2.25  2.27  2.34  1.73  1.5   1.1   0.   11.18] Loss_P: [ 3.49  2.94  2.59  2.53  1.88  1.62  1.25 16.3 ]\n",
      "Loss_Q: [ 2.28  2.25  2.28  1.71  1.53  1.14  0.   11.18] Loss_P: [ 3.49  2.88  2.55  2.4   1.91  1.59  1.26 16.08]\n",
      "Loss_Q: [ 2.24  2.27  2.29  1.76  1.49  1.06  0.   11.1 ] Loss_P: [ 3.53  2.8   2.55  2.4   1.93  1.57  1.23 16.02]\n",
      "Loss_Q: [ 2.22  2.26  2.27  1.82  1.59  1.13  0.   11.29] Loss_P: [ 3.52  2.82  2.6   2.39  1.99  1.64  1.23 16.19]\n",
      "Loss_Q: [ 2.3   2.29  2.35  1.79  1.52  1.08  0.   11.33] Loss_P: [ 3.51  2.87  2.58  2.45  1.95  1.57  1.25 16.18]\n",
      "Loss_Q: [ 2.39  2.27  2.36  1.86  1.54  1.08  0.   11.51] Loss_P: [ 3.48  2.96  2.57  2.5   2.03  1.57  1.25 16.36]\n",
      "Loss_Q: [ 2.36  2.24  2.41  1.82  1.51  1.06  0.   11.4 ] Loss_P: [ 3.5   2.96  2.48  2.47  2.02  1.55  1.21 16.19]\n",
      "Loss_Q: [ 2.31  2.2   2.34  1.86  1.48  1.04  0.   11.23] Loss_P: [ 3.5   2.89  2.55  2.46  2.    1.53  1.16 16.08]\n",
      "Loss_Q: [ 2.4   2.28  2.39  1.87  1.46  1.03  0.   11.43] Loss_P: [ 3.47  2.94  2.63  2.52  2.04  1.49  1.17 16.26]\n",
      "Loss_Q: [ 2.28  2.35  2.34  1.84  1.4   1.04  0.   11.25] Loss_P: [ 3.47  2.88  2.63  2.53  2.01  1.44  1.19 16.14]\n",
      "Loss_Q: [ 2.35  2.32  2.28  1.82  1.41  1.08  0.   11.26] Loss_P: [ 3.46  2.96  2.63  2.43  2.01  1.44  1.2  16.14]\n",
      "Loss_Q: [ 2.31  2.38  2.3   1.87  1.38  1.11  0.   11.34] Loss_P: [ 3.5   2.88  2.63  2.46  2.02  1.47  1.26 16.23]\n",
      "Loss_Q: [ 2.35  2.37  2.27  1.93  1.4   1.14  0.   11.45] Loss_P: [ 3.46  2.93  2.59  2.38  2.04  1.45  1.29 16.13]\n",
      "Loss_Q: [ 2.34  2.4   2.26  1.88  1.44  1.1   0.   11.43] Loss_P: [ 3.49  2.95  2.63  2.38  2.09  1.48  1.27 16.3 ]\n",
      "Loss_Q: [ 2.33  2.42  2.23  1.89  1.47  1.08  0.   11.43] Loss_P: [ 3.54  2.91  2.66  2.36  2.07  1.56  1.24 16.33]\n",
      "Loss_Q: [ 2.3   2.42  2.25  1.92  1.48  1.11  0.   11.47] Loss_P: [ 3.52  2.91  2.71  2.38  2.12  1.53  1.28 16.44]\n",
      "Loss_Q: [ 2.26  2.46  2.34  1.87  1.47  1.07  0.   11.48] Loss_P: [ 3.47  2.9   2.71  2.44  2.13  1.47  1.24 16.35]\n",
      "Loss_Q: [ 2.29  2.4   2.34  1.86  1.43  1.08  0.   11.39] Loss_P: [ 3.46  2.91  2.67  2.46  2.09  1.46  1.19 16.25]\n",
      "Loss_Q: [ 2.3   2.48  2.28  1.86  1.45  1.06  0.   11.43] Loss_P: [ 3.48  2.93  2.74  2.42  2.08  1.46  1.18 16.3 ]\n",
      "Loss_Q: [ 2.26  2.39  2.19  1.89  1.45  1.02  0.   11.2 ] Loss_P: [ 3.48  2.85  2.67  2.32  2.1   1.48  1.14 16.02]\n",
      "Loss_Q: [ 2.24  2.39  2.26  1.85  1.48  0.94  0.   11.15] Loss_P: [ 3.45  2.9   2.62  2.42  2.09  1.5   1.05 16.04]\n",
      "Loss_Q: [ 2.26  2.37  2.34  1.87  1.48  0.95  0.   11.28] Loss_P: [ 3.47  2.88  2.66  2.42  2.08  1.51  1.08 16.09]\n",
      "Loss_Q: [ 2.29  2.42  2.27  1.82  1.44  0.96  0.   11.2 ] Loss_P: [ 3.44  2.9   2.66  2.34  2.04  1.48  1.09 15.94]\n",
      "Loss_Q: [ 2.23  2.39  2.3   1.83  1.43  0.97  0.   11.15] Loss_P: [ 3.44  2.9   2.66  2.43  2.03  1.47  1.07 16.  ]\n",
      "Loss_Q: [ 2.23  2.39  2.35  1.8   1.38  0.93  0.   11.07] Loss_P: [ 3.47  2.83  2.6   2.42  2.06  1.46  1.04 15.87]\n",
      "Loss_Q: [ 2.2   2.36  2.39  1.89  1.38  0.93  0.   11.14] Loss_P: [ 3.46  2.81  2.59  2.46  2.13  1.41  1.09 15.96]\n",
      "Loss_Q: [ 2.23  2.39  2.33  1.88  1.41  0.99  0.   11.23] Loss_P: [ 3.44  2.91  2.62  2.46  2.09  1.43  1.11 16.06]\n",
      "Loss_Q: [ 2.21  2.35  2.36  1.81  1.43  1.    0.   11.15] Loss_P: [ 3.49  2.89  2.63  2.43  2.1   1.45  1.13 16.13]\n",
      "Loss_Q: [ 2.24  2.28  2.31  1.83  1.41  1.    0.   11.07] Loss_P: [ 3.49  2.9   2.58  2.43  2.08  1.44  1.14 16.06]\n",
      "Loss_Q: [ 2.19  2.19  2.28  1.89  1.44  1.03  0.   11.03] Loss_P: [ 3.45  2.87  2.61  2.39  2.11  1.48  1.18 16.1 ]\n",
      "Loss_Q: [ 2.22  2.33  2.32  1.86  1.48  1.03  0.   11.23] Loss_P: [ 3.48  2.78  2.61  2.46  2.05  1.52  1.17 16.07]\n",
      "Loss_Q: [ 2.14  2.34  2.39  1.79  1.53  1.03  0.   11.24] Loss_P: [ 3.49  2.72  2.62  2.49  2.03  1.57  1.14 16.07]\n",
      "Loss_Q: [ 2.24  2.27  2.23  1.79  1.55  1.01  0.   11.09] Loss_P: [ 3.45  2.8   2.49  2.4   1.95  1.57  1.15 15.82]\n",
      "Loss_Q: [ 2.19  2.31  2.22  1.8   1.53  1.05  0.   11.1 ] Loss_P: [ 3.48  2.82  2.57  2.38  1.94  1.58  1.16 15.93]\n",
      "Loss_Q: [ 2.16  2.35  2.23  1.77  1.51  1.05  0.   11.07] Loss_P: [ 3.48  2.76  2.58  2.41  1.96  1.58  1.24 16.  ]\n",
      "Loss_Q: [ 2.12  2.32  2.28  1.83  1.57  1.08  0.   11.19] Loss_P: [ 3.51  2.67  2.61  2.43  2.02  1.62  1.22 16.08]\n",
      "Loss_Q: [ 2.09  2.34  2.33  1.86  1.58  1.12  0.   11.31] Loss_P: [ 3.48  2.68  2.63  2.41  2.05  1.62  1.23 16.1 ]\n",
      "Loss_Q: [ 2.23  2.38  2.29  1.79  1.55  1.07  0.   11.3 ] Loss_P: [ 3.46  2.78  2.64  2.37  2.03  1.58  1.2  16.06]\n",
      "Loss_Q: [ 2.27  2.32  2.26  1.74  1.53  1.04  0.   11.16] Loss_P: [ 3.42  2.87  2.6   2.33  1.98  1.59  1.19 15.98]\n",
      "Loss_Q: [ 2.29  2.35  2.27  1.71  1.53  1.03  0.   11.18] Loss_P: [ 3.42  2.97  2.67  2.42  2.    1.54  1.17 16.2 ]\n",
      "Loss_Q: [ 2.21  2.36  2.23  1.74  1.54  1.03  0.   11.1 ] Loss_P: [ 3.42  2.83  2.65  2.38  1.94  1.54  1.16 15.92]\n",
      "Loss_Q: [ 2.24  2.31  2.28  1.69  1.48  0.95  0.   10.96] Loss_P: [ 3.36  2.9   2.59  2.36  1.92  1.54  1.09 15.76]\n",
      "Loss_Q: [ 2.23  2.35  2.27  1.69  1.52  0.94  0.   11.01] Loss_P: [ 3.44  2.89  2.62  2.39  1.91  1.57  1.08 15.89]\n",
      "Loss_Q: [ 2.2   2.29  2.15  1.68  1.53  0.94  0.   10.79] Loss_P: [ 3.48  2.81  2.56  2.27  1.94  1.59  1.11 15.76]\n",
      "Loss_Q: [ 2.19  2.38  2.2   1.67  1.5   0.94  0.   10.89] Loss_P: [ 3.39  2.86  2.59  2.31  1.97  1.52  1.07 15.71]\n",
      "Loss_Q: [ 2.19  2.35  2.14  1.67  1.41  0.95  0.   10.71] Loss_P: [ 3.39  2.85  2.61  2.24  1.93  1.47  1.06 15.56]\n",
      "Loss_Q: [ 2.19  2.31  2.2   1.69  1.46  0.95  0.   10.8 ] Loss_P: [ 3.44  2.85  2.59  2.2   1.96  1.45  1.11 15.6 ]\n",
      "Loss_Q: [ 2.21  2.31  2.09  1.69  1.44  0.94  0.   10.68] Loss_P: [ 3.44  2.85  2.5   2.19  1.92  1.48  1.06 15.43]\n",
      "Loss_Q: [ 2.19  2.25  2.11  1.6   1.47  0.96  0.   10.58] Loss_P: [ 3.41  2.88  2.64  2.08  1.82  1.43  1.07 15.34]\n",
      "Loss_Q: [ 2.22  2.3   2.13  1.59  1.4   0.92  0.   10.56] Loss_P: [ 3.42  2.89  2.63  2.11  1.8   1.42  1.06 15.32]\n",
      "Loss_Q: [ 2.26  2.26  2.05  1.65  1.42  0.93  0.   10.56] Loss_P: [ 3.43  2.82  2.59  2.16  1.85  1.49  1.06 15.4 ]\n",
      "Loss_Q: [ 2.2   2.21  2.11  1.61  1.43  0.87  0.   10.43] Loss_P: [ 3.43  2.82  2.5   2.16  1.86  1.47  1.05 15.29]\n",
      "Loss_Q: [ 2.23  2.22  2.16  1.61  1.42  0.91  0.   10.55] Loss_P: [ 3.38  2.9   2.52  2.26  1.88  1.45  1.05 15.44]\n",
      "Loss_Q: [ 2.23  2.21  2.12  1.63  1.36  0.91  0.   10.47] Loss_P: [ 3.36  2.86  2.52  2.18  1.85  1.41  1.08 15.25]\n",
      "Loss_Q: [ 2.22  2.22  1.94  1.66  1.32  0.87  0.   10.24] Loss_P: [ 3.41  2.84  2.55  2.02  1.85  1.4   1.07 15.13]\n",
      "Loss_Q: [ 2.18  2.2   1.91  1.64  1.36  0.91  0.   10.2 ] Loss_P: [ 3.44  2.81  2.57  2.04  1.86  1.38  1.03 15.13]\n",
      "Loss_Q: [ 2.27  2.25  1.93  1.65  1.34  0.92  0.   10.36] Loss_P: [ 3.38  2.83  2.57  1.95  1.84  1.31  1.09 14.97]\n",
      "Loss_Q: [ 2.21  2.24  1.94  1.65  1.4   0.9   0.   10.34] Loss_P: [ 3.4   2.83  2.56  1.96  1.89  1.4   1.08 15.12]\n",
      "Loss_Q: [ 2.15  2.25  1.95  1.66  1.32  0.87  0.   10.19] Loss_P: [ 3.41  2.77  2.6   1.93  1.85  1.34  1.01 14.91]\n",
      "Loss_Q: [ 2.16  2.31  1.94  1.66  1.32  0.83  0.   10.22] Loss_P: [ 3.42  2.72  2.6   1.98  1.91  1.32  0.97 14.93]\n",
      "Loss_Q: [2.03 2.23 1.85 1.68 1.35 0.8  0.   9.95] Loss_P: [ 3.47  2.66  2.57  1.93  1.9   1.4   0.95 14.87]\n",
      "Loss_Q: [ 2.1   2.24  1.81  1.72  1.39  0.84  0.   10.1 ] Loss_P: [ 3.42  2.71  2.63  1.87  2.01  1.45  0.98 15.07]\n",
      "Loss_Q: [ 2.04  2.21  1.81  1.8   1.48  0.82  0.   10.16] Loss_P: [ 3.37  2.61  2.48  1.85  2.06  1.49  0.96 14.83]\n",
      "Loss_Q: [ 1.94  2.22  1.91  1.81  1.47  0.83  0.   10.19] Loss_P: [ 3.41  2.55  2.44  1.8   2.05  1.46  0.91 14.62]\n",
      "Loss_Q: [1.87 2.15 1.89 1.81 1.41 0.76 0.   9.89] Loss_P: [ 3.46  2.53  2.5   1.86  2.07  1.43  0.9  14.74]\n",
      "Loss_Q: [1.91 2.2  1.87 1.78 1.34 0.78 0.   9.88] Loss_P: [ 3.46  2.45  2.51  1.88  2.    1.38  0.89 14.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.98 2.21 1.9  1.72 1.34 0.76 0.   9.91] Loss_P: [ 3.46  2.5   2.58  1.92  1.96  1.39  0.9  14.71]\n",
      "Loss_Q: [1.94 2.24 1.87 1.69 1.37 0.78 0.   9.89] Loss_P: [ 3.44  2.52  2.51  1.96  1.97  1.4   0.9  14.71]\n",
      "Loss_Q: [1.98 2.23 1.87 1.74 1.36 0.73 0.   9.92] Loss_P: [ 3.43  2.54  2.46  1.87  1.94  1.38  0.85 14.47]\n",
      "Loss_Q: [1.94 2.18 1.91 1.72 1.34 0.71 0.   9.8 ] Loss_P: [ 3.42  2.58  2.5   2.01  1.94  1.38  0.85 14.68]\n",
      "Loss_Q: [1.93 2.18 1.91 1.74 1.34 0.77 0.   9.87] Loss_P: [ 3.41  2.5   2.44  2.01  1.98  1.42  0.86 14.63]\n",
      "Loss_Q: [1.92 2.19 2.01 1.75 1.34 0.75 0.   9.96] Loss_P: [ 3.4   2.49  2.45  2.05  2.02  1.36  0.84 14.62]\n",
      "Loss_Q: [1.99 2.13 1.99 1.7  1.3  0.75 0.   9.86] Loss_P: [ 3.39  2.55  2.32  2.01  1.98  1.31  0.86 14.42]\n",
      "Loss_Q: [1.91 2.08 1.92 1.72 1.31 0.77 0.   9.71] Loss_P: [ 3.4   2.54  2.37  1.98  2.    1.31  0.9  14.5 ]\n",
      "Loss_Q: [1.86 2.09 1.82 1.75 1.24 0.77 0.   9.53] Loss_P: [ 3.44  2.51  2.39  1.88  2.04  1.26  0.92 14.43]\n",
      "Loss_Q: [1.84 2.13 1.88 1.74 1.24 0.82 0.   9.64] Loss_P: [ 3.44  2.5   2.43  1.91  2.09  1.24  0.95 14.56]\n",
      "Loss_Q: [1.81 2.15 1.8  1.78 1.24 0.8  0.   9.58] Loss_P: [ 3.38  2.45  2.46  1.84  2.04  1.26  0.98 14.41]\n",
      "Loss_Q: [1.8  2.19 1.78 1.76 1.19 0.75 0.   9.48] Loss_P: [ 3.41  2.42  2.5   1.73  2.    1.18  0.91 14.14]\n",
      "Loss_Q: [1.86 2.29 1.87 1.75 1.25 0.77 0.   9.79] Loss_P: [ 3.39  2.51  2.6   2.    1.98  1.26  0.93 14.67]\n",
      "Loss_Q: [1.86 2.3  1.88 1.65 1.19 0.76 0.   9.64] Loss_P: [ 3.39  2.49  2.65  1.96  1.98  1.25  0.9  14.62]\n",
      "Loss_Q: [1.9  2.3  2.01 1.63 1.19 0.76 0.   9.8 ] Loss_P: [ 3.4   2.56  2.57  2.01  1.9   1.2   0.89 14.53]\n",
      "Loss_Q: [1.89 2.27 2.01 1.64 1.18 0.74 0.   9.73] Loss_P: [ 3.39  2.57  2.52  2.05  1.86  1.26  0.9  14.55]\n",
      "Loss_Q: [2.04 2.25 1.97 1.7  1.22 0.79 0.   9.96] Loss_P: [ 3.32  2.71  2.57  2.1   1.93  1.23  0.94 14.81]\n",
      "Loss_Q: [2.12 2.32 1.99 1.61 1.19 0.73 0.   9.96] Loss_P: [ 3.37  2.77  2.55  2.04  1.88  1.22  0.85 14.68]\n",
      "Loss_Q: [2.03 2.3  2.04 1.5  1.19 0.72 0.   9.79] Loss_P: [ 3.33  2.75  2.53  2.15  1.78  1.25  0.88 14.67]\n",
      "Loss_Q: [2.05 2.22 1.92 1.52 1.17 0.7  0.   9.58] Loss_P: [ 3.4   2.67  2.55  1.97  1.74  1.21  0.86 14.4 ]\n",
      "Loss_Q: [2.11 2.27 2.03 1.49 1.19 0.69 0.   9.77] Loss_P: [ 3.39  2.74  2.55  2.1   1.75  1.19  0.85 14.57]\n",
      "Loss_Q: [2.16 2.2  1.95 1.58 1.21 0.73 0.   9.83] Loss_P: [ 3.4   2.72  2.47  2.07  1.79  1.21  0.89 14.56]\n",
      "Loss_Q: [2.03 2.19 1.94 1.56 1.24 0.74 0.   9.7 ] Loss_P: [ 3.37  2.68  2.46  2.12  1.77  1.27  0.93 14.61]\n",
      "Loss_Q: [2.15 2.12 1.96 1.55 1.27 0.79 0.   9.83] Loss_P: [ 3.36  2.73  2.42  2.07  1.86  1.26  0.96 14.65]\n",
      "Loss_Q: [2.16 2.22 1.89 1.61 1.28 0.81 0.   9.97] Loss_P: [ 3.31  2.79  2.39  2.08  1.87  1.31  0.98 14.73]\n",
      "Loss_Q: [2.06 2.17 1.9  1.65 1.28 0.74 0.   9.8 ] Loss_P: [ 3.32  2.81  2.46  2.08  1.87  1.28  0.95 14.76]\n",
      "Loss_Q: [ 2.17  2.21  1.87  1.63  1.34  0.81  0.   10.03] Loss_P: [ 3.32  2.88  2.51  2.08  1.91  1.3   1.   15.01]\n",
      "Loss_Q: [2.15 2.27 1.9  1.62 1.27 0.77 0.   9.97] Loss_P: [ 3.34  2.9   2.49  2.02  1.9   1.31  1.   14.96]\n",
      "Loss_Q: [ 2.18  2.25  2.    1.61  1.28  0.77  0.   10.09] Loss_P: [ 3.36  2.8   2.48  2.06  1.88  1.29  0.96 14.83]\n",
      "Loss_Q: [2.11 2.16 1.89 1.57 1.3  0.75 0.   9.78] Loss_P: [ 3.36  2.78  2.36  2.    1.87  1.25  1.   14.62]\n",
      "Loss_Q: [2.17 2.11 1.84 1.58 1.29 0.8  0.   9.79] Loss_P: [ 3.34  2.77  2.35  1.93  1.82  1.22  0.99 14.41]\n",
      "Loss_Q: [2.16 2.13 1.86 1.63 1.31 0.79 0.   9.89] Loss_P: [ 3.31  2.77  2.42  1.97  1.86  1.28  1.   14.61]\n",
      "Loss_Q: [2.23 2.14 1.89 1.65 1.31 0.77 0.   9.99] Loss_P: [ 3.32  2.82  2.44  1.95  1.95  1.26  0.99 14.72]\n",
      "Loss_Q: [ 2.19  2.28  1.97  1.68  1.32  0.82  0.   10.26] Loss_P: [ 3.31  2.85  2.51  2.11  1.95  1.31  1.03 15.06]\n",
      "Loss_Q: [ 2.09  2.24  1.9   1.7   1.29  0.81  0.   10.04] Loss_P: [ 3.35  2.75  2.58  2.11  1.97  1.26  1.04 15.05]\n",
      "Loss_Q: [ 2.03  2.35  1.92  1.71  1.27  0.81  0.   10.09] Loss_P: [ 3.39  2.62  2.51  2.07  1.92  1.28  1.03 14.82]\n",
      "Loss_Q: [2.   2.21 1.94 1.66 1.3  0.8  0.   9.9 ] Loss_P: [ 3.3   2.68  2.49  1.99  1.93  1.28  1.   14.66]\n",
      "Loss_Q: [ 2.12  2.3   1.87  1.65  1.26  0.83  0.   10.02] Loss_P: [ 3.32  2.76  2.53  2.    1.92  1.29  1.06 14.89]\n",
      "Loss_Q: [2.09 2.31 1.82 1.6  1.29 0.83 0.   9.93] Loss_P: [ 3.29  2.79  2.59  1.93  1.88  1.27  1.05 14.8 ]\n",
      "Loss_Q: [2.11 2.3  1.8  1.62 1.31 0.85 0.   9.99] Loss_P: [ 3.34  2.69  2.59  1.93  1.82  1.27  1.08 14.72]\n",
      "Loss_Q: [ 2.02  2.36  1.83  1.71  1.3   0.87  0.   10.1 ] Loss_P: [ 3.26  2.67  2.6   2.05  1.99  1.28  1.06 14.92]\n",
      "Loss_Q: [ 1.95  2.41  1.81  1.73  1.32  0.78  0.   10.  ] Loss_P: [ 3.32  2.58  2.56  2.    1.99  1.29  1.01 14.75]\n",
      "Loss_Q: [1.95 2.4  1.76 1.73 1.3  0.76 0.   9.89] Loss_P: [ 3.33  2.63  2.58  1.95  1.97  1.26  0.98 14.69]\n",
      "Loss_Q: [ 1.97  2.39  1.85  1.74  1.31  0.8   0.   10.06] Loss_P: [ 3.34  2.57  2.63  1.89  1.95  1.24  0.99 14.61]\n",
      "Loss_Q: [1.99 2.39 1.68 1.74 1.31 0.79 0.   9.89] Loss_P: [ 3.33  2.58  2.59  1.81  1.96  1.29  0.99 14.54]\n",
      "Loss_Q: [ 2.04  2.41  1.8   1.66  1.32  0.77  0.   10.  ] Loss_P: [ 3.34  2.7   2.63  1.86  1.94  1.32  1.   14.78]\n",
      "Loss_Q: [ 2.09  2.48  1.72  1.74  1.36  0.75  0.   10.14] Loss_P: [ 3.33  2.71  2.76  1.92  2.03  1.27  0.98 15.  ]\n",
      "Loss_Q: [ 2.22  2.41  1.77  1.72  1.37  0.8   0.   10.28] Loss_P: [ 3.32  2.76  2.66  1.86  1.96  1.29  0.98 14.83]\n",
      "Loss_Q: [ 2.21  2.38  1.73  1.72  1.33  0.8   0.   10.16] Loss_P: [ 3.26  2.82  2.7   1.91  1.99  1.3   0.98 14.96]\n",
      "Loss_Q: [ 2.26  2.52  1.71  1.8   1.25  0.75  0.   10.31] Loss_P: [ 3.29  2.9   2.81  1.87  1.98  1.3   0.98 15.14]\n",
      "Loss_Q: [ 2.19  2.53  1.68  1.85  1.3   0.78  0.   10.33] Loss_P: [ 3.35  2.78  2.86  1.75  2.03  1.27  0.98 15.01]\n",
      "Loss_Q: [ 2.14  2.45  1.7   1.86  1.28  0.73  0.   10.15] Loss_P: [ 3.34  2.68  2.77  1.9   2.08  1.27  0.97 15.01]\n",
      "Loss_Q: [ 2.24  2.43  1.82  1.78  1.29  0.77  0.   10.32] Loss_P: [ 3.3   2.83  2.75  1.9   2.02  1.31  0.98 15.1 ]\n",
      "Loss_Q: [ 2.09  2.38  1.8   1.79  1.33  0.73  0.   10.13] Loss_P: [ 3.34  2.73  2.71  1.95  2.    1.28  0.96 14.97]\n",
      "Loss_Q: [ 2.04  2.44  1.91  1.8   1.34  0.78  0.   10.3 ] Loss_P: [ 3.37  2.65  2.68  2.07  2.04  1.33  0.98 15.13]\n",
      "Loss_Q: [ 2.04  2.46  1.94  1.88  1.34  0.75  0.   10.4 ] Loss_P: [ 3.41  2.69  2.69  2.07  2.13  1.3   0.96 15.24]\n",
      "Loss_Q: [ 2.14  2.52  1.94  1.84  1.34  0.75  0.   10.53] Loss_P: [ 3.38  2.76  2.71  2.05  2.08  1.29  0.95 15.22]\n",
      "Loss_Q: [ 2.14  2.55  2.07  1.86  1.33  0.75  0.   10.7 ] Loss_P: [ 3.32  2.74  2.79  2.19  2.14  1.31  0.99 15.48]\n",
      "Loss_Q: [ 2.1   2.46  2.03  1.84  1.33  0.75  0.   10.51] Loss_P: [ 3.31  2.7   2.71  2.21  2.12  1.32  1.02 15.4 ]\n",
      "Loss_Q: [ 2.1   2.49  1.98  1.91  1.35  0.75  0.   10.59] Loss_P: [ 3.35  2.65  2.76  2.12  2.13  1.28  0.96 15.27]\n",
      "Loss_Q: [ 2.    2.53  1.95  1.84  1.31  0.75  0.   10.38] Loss_P: [ 3.36  2.67  2.75  2.14  2.12  1.3   0.93 15.28]\n",
      "Loss_Q: [ 2.01  2.43  2.01  1.75  1.33  0.75  0.   10.29] Loss_P: [ 3.38  2.56  2.69  2.13  2.06  1.28  0.93 15.03]\n",
      "Loss_Q: [ 2.02  2.49  2.03  1.8   1.26  0.68  0.   10.28] Loss_P: [ 3.4   2.58  2.73  2.17  2.06  1.24  0.91 15.09]\n",
      "Loss_Q: [ 1.99  2.53  2.02  1.78  1.19  0.67  0.   10.18] Loss_P: [ 3.38  2.58  2.71  2.17  1.96  1.19  0.92 14.92]\n",
      "Loss_Q: [ 2.02  2.5   2.11  1.7   1.24  0.68  0.   10.25] Loss_P: [ 3.37  2.64  2.73  2.21  2.    1.15  0.88 14.97]\n",
      "Loss_Q: [ 2.04  2.48  2.08  1.71  1.18  0.68  0.   10.18] Loss_P: [ 3.36  2.68  2.73  2.29  1.95  1.19  0.89 15.09]\n",
      "Loss_Q: [ 2.09  2.36  2.05  1.72  1.14  0.67  0.   10.03] Loss_P: [ 3.35  2.78  2.66  2.28  1.86  1.15  0.87 14.95]\n",
      "Loss_Q: [ 2.17  2.46  2.05  1.74  1.18  0.67  0.   10.27] Loss_P: [ 3.33  2.78  2.68  2.21  1.88  1.13  0.86 14.87]\n",
      "Loss_Q: [ 2.09  2.4   2.1   1.64  1.15  0.67  0.   10.05] Loss_P: [ 3.33  2.74  2.65  2.22  1.84  1.17  0.85 14.79]\n",
      "Loss_Q: [ 2.13  2.49  2.09  1.62  1.16  0.68  0.   10.17] Loss_P: [ 3.29  2.81  2.68  2.27  1.82  1.17  0.87 14.92]\n",
      "Loss_Q: [ 2.19  2.48  2.06  1.71  1.15  0.64  0.   10.23] Loss_P: [ 3.38  2.72  2.75  2.21  1.92  1.17  0.85 14.99]\n",
      "Loss_Q: [ 2.13  2.6   2.02  1.68  1.13  0.66  0.   10.21] Loss_P: [ 3.38  2.73  2.76  2.21  1.9   1.14  0.84 14.96]\n",
      "Loss_Q: [ 2.18  2.54  2.04  1.63  1.12  0.63  0.   10.13] Loss_P: [ 3.38  2.72  2.77  2.18  1.78  1.18  0.86 14.88]\n",
      "Loss_Q: [ 2.22  2.51  2.    1.67  1.16  0.67  0.   10.22] Loss_P: [ 3.35  2.76  2.79  2.2   1.86  1.16  0.87 14.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.08 2.52 1.92 1.65 1.12 0.68 0.   9.96] Loss_P: [ 3.33  2.64  2.72  2.14  1.81  1.12  0.89 14.65]\n",
      "Loss_Q: [1.99 2.47 2.01 1.62 1.16 0.68 0.   9.94] Loss_P: [ 3.36  2.66  2.69  2.17  1.76  1.15  0.9  14.69]\n",
      "Loss_Q: [2.01 2.44 2.03 1.63 1.14 0.68 0.   9.93] Loss_P: [ 3.35  2.64  2.66  2.19  1.83  1.14  0.9  14.7 ]\n",
      "Loss_Q: [1.97 2.41 1.99 1.57 1.12 0.65 0.   9.7 ] Loss_P: [ 3.38  2.59  2.68  2.18  1.78  1.09  0.86 14.56]\n",
      "Loss_Q: [2.01 2.56 1.99 1.61 1.06 0.66 0.   9.89] Loss_P: [ 3.35  2.69  2.73  2.19  1.76  1.08  0.8  14.59]\n",
      "Loss_Q: [2.   2.53 1.96 1.6  1.09 0.64 0.   9.81] Loss_P: [ 3.36  2.65  2.69  2.16  1.77  1.08  0.83 14.53]\n",
      "Loss_Q: [2.02 2.49 1.97 1.62 1.14 0.66 0.   9.9 ] Loss_P: [ 3.34  2.64  2.66  2.24  1.8   1.14  0.89 14.71]\n",
      "Loss_Q: [2.03 2.43 2.03 1.63 1.14 0.64 0.   9.9 ] Loss_P: [ 3.34  2.69  2.67  2.17  1.82  1.13  0.88 14.71]\n",
      "Loss_Q: [ 2.07  2.53  2.03  1.71  1.15  0.69  0.   10.17] Loss_P: [ 3.35  2.69  2.72  2.23  1.84  1.14  0.89 14.87]\n",
      "Loss_Q: [ 1.94  2.47  2.06  1.7   1.17  0.71  0.   10.06] Loss_P: [ 3.33  2.61  2.74  2.29  1.89  1.15  0.95 14.96]\n",
      "Loss_Q: [ 1.96  2.52  2.14  1.68  1.22  0.71  0.   10.23] Loss_P: [ 3.36  2.65  2.71  2.3   1.88  1.2   0.94 15.04]\n",
      "Loss_Q: [ 1.97  2.48  2.04  1.64  1.23  0.72  0.   10.09] Loss_P: [ 3.35  2.59  2.69  2.28  1.83  1.22  0.95 14.91]\n",
      "Loss_Q: [1.91 2.51 1.97 1.61 1.24 0.72 0.   9.96] Loss_P: [ 3.37  2.59  2.66  2.23  1.78  1.2   0.96 14.79]\n",
      "Loss_Q: [ 2.01  2.48  2.04  1.68  1.25  0.73  0.   10.19] Loss_P: [ 3.31  2.63  2.71  2.25  1.82  1.28  0.96 14.96]\n",
      "Loss_Q: [ 2.04  2.53  1.98  1.63  1.29  0.71  0.   10.18] Loss_P: [ 3.35  2.65  2.73  2.24  1.83  1.25  0.97 15.02]\n",
      "Loss_Q: [ 2.04  2.47  2.04  1.67  1.26  0.72  0.   10.2 ] Loss_P: [ 3.3   2.7   2.63  2.27  1.82  1.3   0.97 14.99]\n",
      "Loss_Q: [ 2.02  2.41  1.98  1.67  1.29  0.72  0.   10.1 ] Loss_P: [ 3.35  2.61  2.57  2.17  1.83  1.29  0.96 14.78]\n",
      "Loss_Q: [ 2.02  2.43  2.03  1.73  1.32  0.76  0.   10.3 ] Loss_P: [ 3.36  2.59  2.68  2.26  1.9   1.33  1.01 15.14]\n",
      "Loss_Q: [ 2.02  2.39  2.06  1.74  1.33  0.79  0.   10.32] Loss_P: [ 3.34  2.62  2.65  2.25  1.9   1.29  1.02 15.07]\n",
      "Loss_Q: [ 2.09  2.45  2.08  1.68  1.33  0.78  0.   10.4 ] Loss_P: [ 3.31  2.66  2.63  2.25  1.89  1.27  1.03 15.06]\n",
      "Loss_Q: [ 2.07  2.5   2.04  1.68  1.34  0.79  0.   10.42] Loss_P: [ 3.36  2.68  2.72  2.23  1.88  1.31  1.05 15.23]\n",
      "Loss_Q: [ 2.16  2.46  2.02  1.7   1.35  0.8   0.   10.49] Loss_P: [ 3.36  2.68  2.71  2.21  1.87  1.35  1.06 15.25]\n",
      "Loss_Q: [ 2.11  2.43  2.04  1.73  1.34  0.81  0.   10.47] Loss_P: [ 3.32  2.73  2.65  2.19  1.9   1.34  1.06 15.19]\n",
      "Loss_Q: [ 2.13  2.35  2.02  1.69  1.31  0.78  0.   10.28] Loss_P: [ 3.37  2.75  2.61  2.15  1.89  1.32  1.02 15.13]\n",
      "Loss_Q: [ 2.17  2.4   2.04  1.75  1.3   0.78  0.   10.43] Loss_P: [ 3.35  2.76  2.57  2.16  1.88  1.34  1.   15.06]\n",
      "Loss_Q: [ 2.14  2.35  1.98  1.7   1.29  0.73  0.   10.19] Loss_P: [ 3.32  2.74  2.54  2.14  1.9   1.29  1.   14.93]\n",
      "Loss_Q: [ 2.16  2.41  1.96  1.7   1.26  0.75  0.   10.24] Loss_P: [ 3.29  2.8   2.65  2.14  1.88  1.28  1.01 15.04]\n",
      "Loss_Q: [ 2.14  2.37  2.06  1.7   1.22  0.75  0.   10.25] Loss_P: [ 3.24  2.7   2.57  2.18  1.84  1.22  0.99 14.75]\n",
      "Loss_Q: [ 2.06  2.35  2.09  1.73  1.2   0.71  0.   10.14] Loss_P: [ 3.34  2.71  2.65  2.22  1.88  1.2   0.96 14.95]\n",
      "Loss_Q: [ 2.1   2.41  2.02  1.65  1.23  0.75  0.   10.16] Loss_P: [ 3.35  2.67  2.5   2.2   1.87  1.23  1.   14.81]\n",
      "Loss_Q: [1.99 2.31 1.99 1.68 1.26 0.73 0.   9.97] Loss_P: [ 3.29  2.7   2.53  2.21  1.84  1.26  0.98 14.81]\n",
      "Loss_Q: [ 2.07  2.19  2.02  1.65  1.32  0.79  0.   10.03] Loss_P: [ 3.25  2.71  2.42  2.24  1.81  1.31  0.99 14.73]\n",
      "Loss_Q: [2.04 2.12 1.97 1.63 1.31 0.77 0.   9.84] Loss_P: [ 3.32  2.65  2.38  2.16  1.82  1.28  1.04 14.65]\n",
      "Loss_Q: [2.04 2.21 2.03 1.63 1.25 0.8  0.   9.97] Loss_P: [ 3.36  2.59  2.42  2.18  1.86  1.25  1.03 14.68]\n",
      "Loss_Q: [ 2.06  2.28  2.12  1.68  1.29  0.75  0.   10.19] Loss_P: [ 3.35  2.66  2.52  2.28  1.86  1.29  0.99 14.94]\n",
      "Loss_Q: [ 2.1   2.2   2.15  1.69  1.28  0.75  0.   10.17] Loss_P: [ 3.32  2.73  2.43  2.26  1.89  1.29  1.02 14.94]\n",
      "Loss_Q: [ 2.1   2.19  2.1   1.74  1.27  0.75  0.   10.15] Loss_P: [ 3.28  2.7   2.45  2.24  1.89  1.21  1.01 14.78]\n",
      "Loss_Q: [ 2.03  2.26  2.07  1.71  1.27  0.78  0.   10.12] Loss_P: [ 3.33  2.67  2.54  2.27  1.92  1.23  1.05 15.01]\n",
      "Loss_Q: [ 2.06  2.39  2.15  1.76  1.22  0.8   0.   10.39] Loss_P: [ 3.28  2.68  2.52  2.34  1.92  1.24  1.08 15.05]\n",
      "Loss_Q: [ 2.01  2.4   2.17  1.76  1.28  0.82  0.   10.45] Loss_P: [ 3.29  2.62  2.6   2.38  2.01  1.25  1.1  15.25]\n",
      "Loss_Q: [ 1.92  2.37  2.18  1.75  1.3   0.83  0.   10.35] Loss_P: [ 3.35  2.6   2.65  2.38  1.99  1.27  1.09 15.34]\n",
      "Loss_Q: [ 1.96  2.46  2.15  1.78  1.33  0.85  0.   10.52] Loss_P: [ 3.32  2.54  2.64  2.31  1.96  1.32  1.12 15.21]\n",
      "Loss_Q: [ 2.09  2.44  2.24  1.8   1.34  0.87  0.   10.79] Loss_P: [ 3.27  2.73  2.7   2.4   2.02  1.34  1.14 15.6 ]\n",
      "Loss_Q: [ 2.03  2.41  2.24  1.8   1.29  0.83  0.   10.6 ] Loss_P: [ 3.3   2.73  2.71  2.37  2.11  1.31  1.14 15.66]\n",
      "Loss_Q: [ 2.02  2.44  2.19  1.81  1.3   0.84  0.   10.6 ] Loss_P: [ 3.29  2.72  2.7   2.33  2.11  1.29  1.13 15.56]\n",
      "Loss_Q: [ 2.06  2.45  2.28  1.86  1.29  0.82  0.   10.76] Loss_P: [ 3.25  2.73  2.72  2.39  2.14  1.28  1.11 15.62]\n",
      "Loss_Q: [ 2.11  2.4   2.28  1.82  1.3   0.83  0.   10.73] Loss_P: [ 3.27  2.77  2.71  2.43  2.09  1.28  1.11 15.67]\n",
      "Loss_Q: [ 2.09  2.42  2.26  1.82  1.31  0.83  0.   10.73] Loss_P: [ 3.26  2.82  2.66  2.43  2.12  1.25  1.12 15.66]\n",
      "Loss_Q: [ 2.08  2.34  2.27  1.83  1.33  0.85  0.   10.69] Loss_P: [ 3.31  2.75  2.56  2.43  2.06  1.27  1.11 15.5 ]\n",
      "Loss_Q: [ 2.13  2.39  2.29  1.81  1.36  0.88  0.   10.85] Loss_P: [ 3.29  2.8   2.66  2.38  2.08  1.34  1.15 15.71]\n",
      "Loss_Q: [ 2.09  2.54  2.34  1.82  1.39  0.87  0.   11.04] Loss_P: [ 3.22  2.82  2.77  2.44  2.06  1.37  1.15 15.84]\n",
      "Loss_Q: [ 2.08  2.53  2.36  1.84  1.38  0.87  0.   11.06] Loss_P: [ 3.28  2.75  2.7   2.48  2.04  1.36  1.12 15.73]\n",
      "Loss_Q: [ 2.14  2.46  2.38  1.82  1.41  0.86  0.   11.07] Loss_P: [ 3.3   2.82  2.69  2.48  2.08  1.35  1.16 15.88]\n",
      "Loss_Q: [ 2.07  2.52  2.35  1.84  1.36  0.86  0.   10.99] Loss_P: [ 3.3   2.77  2.72  2.5   2.03  1.36  1.13 15.81]\n",
      "Loss_Q: [ 2.14  2.46  2.37  1.88  1.33  0.89  0.   11.07] Loss_P: [ 3.27  2.8   2.76  2.51  1.99  1.31  1.14 15.78]\n",
      "Loss_Q: [ 2.13  2.47  2.4   1.75  1.32  0.87  0.   10.95] Loss_P: [ 3.32  2.76  2.7   2.5   1.94  1.27  1.15 15.65]\n",
      "Loss_Q: [ 2.13  2.38  2.38  1.75  1.24  0.87  0.   10.74] Loss_P: [ 3.32  2.79  2.68  2.49  1.92  1.25  1.1  15.54]\n",
      "Loss_Q: [ 2.17  2.29  2.4   1.71  1.26  0.83  0.   10.66] Loss_P: [ 3.27  2.87  2.61  2.46  1.88  1.25  1.05 15.39]\n",
      "Loss_Q: [ 2.13  2.26  2.36  1.71  1.28  0.81  0.   10.55] Loss_P: [ 3.31  2.79  2.53  2.54  1.9   1.31  1.04 15.44]\n",
      "Loss_Q: [ 2.13  2.19  2.36  1.74  1.25  0.82  0.   10.49] Loss_P: [ 3.26  2.88  2.48  2.46  1.9   1.28  1.08 15.34]\n",
      "Loss_Q: [ 2.2   2.23  2.27  1.72  1.23  0.82  0.   10.47] Loss_P: [ 3.28  2.83  2.51  2.4   1.87  1.23  1.08 15.2 ]\n",
      "Loss_Q: [ 2.15  2.22  2.3   1.76  1.21  0.81  0.   10.45] Loss_P: [ 3.31  2.76  2.56  2.44  1.9   1.2   0.99 15.16]\n",
      "Loss_Q: [ 2.14  2.32  2.32  1.79  1.26  0.84  0.   10.67] Loss_P: [ 3.25  2.84  2.58  2.46  1.97  1.23  1.04 15.37]\n",
      "Loss_Q: [ 2.12  2.28  2.31  1.78  1.25  0.81  0.   10.55] Loss_P: [ 3.25  2.77  2.6   2.47  1.99  1.23  1.03 15.35]\n",
      "Loss_Q: [ 2.2   2.29  2.22  1.76  1.29  0.79  0.   10.55] Loss_P: [ 3.25  2.84  2.58  2.34  1.93  1.26  1.02 15.21]\n",
      "Loss_Q: [ 2.12  2.28  2.24  1.8   1.23  0.77  0.   10.44] Loss_P: [ 3.3   2.8   2.51  2.37  1.94  1.25  1.   15.18]\n",
      "Loss_Q: [ 2.11  2.28  2.28  1.84  1.2   0.74  0.   10.45] Loss_P: [ 3.31  2.74  2.5   2.33  2.02  1.22  1.   15.12]\n",
      "Loss_Q: [ 2.16  2.24  2.23  1.79  1.2   0.76  0.   10.38] Loss_P: [ 3.29  2.8   2.59  2.32  1.99  1.19  1.02 15.2 ]\n",
      "Loss_Q: [ 2.07  2.33  2.17  1.8   1.11  0.73  0.   10.21] Loss_P: [ 3.25  2.78  2.67  2.34  2.01  1.13  0.98 15.15]\n",
      "Loss_Q: [ 2.21  2.35  2.22  1.78  1.16  0.73  0.   10.45] Loss_P: [ 3.22  2.9   2.69  2.3   1.93  1.09  0.93 15.06]\n",
      "Loss_Q: [ 2.18  2.31  2.25  1.74  1.12  0.69  0.   10.28] Loss_P: [ 3.22  2.92  2.68  2.35  1.91  1.08  0.92 15.09]\n",
      "Loss_Q: [ 2.14  2.29  2.23  1.73  1.1   0.71  0.   10.2 ] Loss_P: [ 3.25  2.84  2.58  2.4   1.89  1.1   0.91 14.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 2.1   2.17  2.33  1.75  1.06  0.7   0.   10.12] Loss_P: [ 3.24  2.86  2.52  2.48  1.93  1.09  0.91 15.03]\n",
      "Loss_Q: [ 2.1   2.25  2.26  1.73  1.15  0.73  0.   10.21] Loss_P: [ 3.24  2.85  2.55  2.36  1.95  1.08  0.94 14.97]\n",
      "Loss_Q: [2.07 2.19 2.19 1.79 1.08 0.64 0.   9.96] Loss_P: [ 3.21  2.72  2.48  2.3   1.93  1.07  0.88 14.58]\n",
      "Loss_Q: [2.07 2.18 2.24 1.78 1.06 0.64 0.   9.97] Loss_P: [ 3.23  2.77  2.53  2.37  1.97  1.04  0.87 14.77]\n",
      "Loss_Q: [ 2.14  2.25  2.22  1.83  1.03  0.68  0.   10.16] Loss_P: [ 3.25  2.81  2.62  2.39  2.    1.07  0.9  15.05]\n",
      "Loss_Q: [ 2.19  2.33  2.21  1.84  1.09  0.71  0.   10.37] Loss_P: [ 3.23  2.87  2.65  2.28  1.95  1.05  0.89 14.92]\n",
      "Loss_Q: [ 2.21  2.31  2.14  1.85  1.05  0.68  0.   10.22] Loss_P: [ 3.23  2.91  2.62  2.26  2.05  0.99  0.87 14.92]\n",
      "Loss_Q: [ 2.16  2.31  2.11  1.84  1.07  0.7   0.   10.19] Loss_P: [ 3.23  2.89  2.62  2.24  2.05  1.05  0.91 15.  ]\n",
      "Loss_Q: [ 2.19  2.26  2.13  1.82  1.05  0.65  0.   10.09] Loss_P: [ 3.22  2.85  2.61  2.22  1.94  1.05  0.91 14.8 ]\n",
      "Loss_Q: [ 2.09  2.24  2.16  1.77  1.06  0.68  0.   10.01] Loss_P: [ 3.21  2.77  2.53  2.25  1.9   1.02  0.89 14.57]\n",
      "Loss_Q: [1.96 2.15 2.13 1.77 1.08 0.67 0.   9.76] Loss_P: [ 3.25  2.75  2.52  2.27  1.9   1.03  0.91 14.63]\n",
      "Loss_Q: [1.98 2.27 2.08 1.77 1.05 0.67 0.   9.81] Loss_P: [ 3.25  2.67  2.6   2.13  1.89  1.01  0.86 14.41]\n",
      "Loss_Q: [2.04 2.34 2.09 1.78 1.02 0.65 0.   9.92] Loss_P: [ 3.26  2.78  2.66  2.15  1.9   0.99  0.82 14.57]\n",
      "Loss_Q: [2.   2.33 2.1  1.83 1.01 0.64 0.   9.91] Loss_P: [ 3.19  2.82  2.61  2.21  1.91  1.03  0.85 14.63]\n",
      "Loss_Q: [1.97 2.26 2.06 1.74 0.98 0.64 0.   9.65] Loss_P: [ 3.21  2.69  2.57  2.18  1.95  1.    0.83 14.44]\n",
      "Loss_Q: [2.07 2.24 2.15 1.76 1.06 0.64 0.   9.93] Loss_P: [ 3.2   2.81  2.53  2.24  1.93  1.06  0.85 14.62]\n",
      "Loss_Q: [2.07 2.27 2.15 1.78 1.08 0.64 0.   9.99] Loss_P: [ 3.25  2.8   2.58  2.28  1.95  1.03  0.84 14.73]\n",
      "Loss_Q: [2.04 2.32 2.13 1.75 1.06 0.61 0.   9.91] Loss_P: [ 3.24  2.74  2.59  2.21  1.9   0.98  0.83 14.5 ]\n",
      "Loss_Q: [2.1  2.31 2.14 1.71 0.98 0.61 0.   9.86] Loss_P: [ 3.24  2.77  2.66  2.22  1.86  0.98  0.82 14.56]\n",
      "Loss_Q: [2.14 2.25 2.21 1.72 1.01 0.59 0.   9.92] Loss_P: [ 3.21  2.82  2.63  2.3   1.89  0.97  0.73 14.55]\n",
      "Loss_Q: [2.19 2.31 2.2  1.76 0.93 0.59 0.   9.99] Loss_P: [ 3.22  2.82  2.61  2.37  1.92  0.96  0.77 14.67]\n",
      "Loss_Q: [ 2.15  2.34  2.19  1.78  0.97  0.63  0.   10.07] Loss_P: [ 3.22  2.82  2.6   2.29  1.94  0.93  0.79 14.6 ]\n",
      "Loss_Q: [ 2.17  2.24  2.2   1.8   0.99  0.65  0.   10.06] Loss_P: [ 3.23  2.88  2.51  2.28  1.94  0.97  0.8  14.62]\n",
      "Loss_Q: [ 2.13  2.27  2.24  1.82  0.95  0.63  0.   10.03] Loss_P: [ 3.25  2.92  2.55  2.32  1.93  0.97  0.83 14.77]\n",
      "Loss_Q: [2.09 2.13 2.16 1.78 0.94 0.56 0.   9.66] Loss_P: [ 3.26  2.84  2.52  2.24  1.89  0.98  0.78 14.5 ]\n",
      "Loss_Q: [2.06 2.15 2.   1.72 0.92 0.55 0.   9.41] Loss_P: [ 3.29  2.81  2.42  2.08  1.85  0.95  0.72 14.11]\n",
      "Loss_Q: [2.03 2.18 1.93 1.71 0.87 0.56 0.   9.28] Loss_P: [ 3.28  2.79  2.48  2.07  1.8   0.89  0.67 13.98]\n",
      "Loss_Q: [2.08 2.11 2.02 1.71 0.84 0.55 0.   9.3 ] Loss_P: [ 3.2   2.84  2.51  2.06  1.82  0.86  0.65 13.96]\n",
      "Loss_Q: [2.05 1.99 1.99 1.75 0.89 0.57 0.   9.23] Loss_P: [ 3.26  2.88  2.33  2.05  1.85  0.9   0.66 13.93]\n",
      "Loss_Q: [2.09 2.04 1.94 1.6  0.88 0.53 0.   9.08] Loss_P: [ 3.23  2.87  2.35  2.13  1.73  0.9   0.63 13.83]\n",
      "Loss_Q: [2.07 2.01 2.06 1.58 0.84 0.49 0.   9.05] Loss_P: [ 3.2   2.85  2.34  2.22  1.73  0.9   0.66 13.9 ]\n",
      "Loss_Q: [2.08 1.98 2.08 1.63 0.86 0.49 0.   9.13] Loss_P: [ 3.25  2.81  2.31  2.2   1.7   0.86  0.6  13.74]\n",
      "Loss_Q: [2.15 1.9  2.14 1.65 0.85 0.48 0.   9.17] Loss_P: [ 3.24  2.87  2.16  2.25  1.78  0.89  0.62 13.8 ]\n",
      "Loss_Q: [2.14 1.96 2.12 1.7  0.89 0.53 0.   9.35] Loss_P: [ 3.22  2.88  2.24  2.23  1.9   0.94  0.65 14.06]\n",
      "Loss_Q: [2.07 1.94 2.11 1.68 0.87 0.5  0.   9.17] Loss_P: [ 3.3   2.81  2.2   2.23  1.85  0.92  0.64 13.94]\n",
      "Loss_Q: [2.07 1.93 2.12 1.75 0.9  0.53 0.   9.3 ] Loss_P: [ 3.26  2.83  2.21  2.23  1.91  0.91  0.71 14.06]\n",
      "Loss_Q: [2.03 2.   2.12 1.66 0.92 0.54 0.   9.27] Loss_P: [ 3.25  2.75  2.28  2.26  1.86  0.94  0.71 14.05]\n",
      "Loss_Q: [1.99 2.06 2.2  1.74 0.91 0.56 0.   9.47] Loss_P: [ 3.27  2.72  2.24  2.27  1.9   0.92  0.7  14.03]\n",
      "Loss_Q: [2.04 1.99 2.22 1.74 0.92 0.59 0.   9.5 ] Loss_P: [ 3.29  2.72  2.25  2.25  1.89  0.94  0.72 14.06]\n",
      "Loss_Q: [2.01 1.96 2.01 1.66 0.91 0.61 0.   9.16] Loss_P: [ 3.24  2.79  2.3   2.16  1.87  0.97  0.7  14.03]\n",
      "Loss_Q: [2.05 1.87 2.02 1.68 0.96 0.61 0.   9.19] Loss_P: [ 3.28  2.7   2.14  2.13  1.86  0.98  0.79 13.89]\n",
      "Loss_Q: [1.94 1.93 2.09 1.67 0.97 0.62 0.   9.23] Loss_P: [ 3.27  2.72  2.15  2.24  1.84  0.99  0.83 14.05]\n",
      "Loss_Q: [2.05 1.96 2.03 1.6  0.95 0.63 0.   9.22] Loss_P: [ 3.31  2.69  2.18  2.17  1.75  0.95  0.84 13.89]\n",
      "Loss_Q: [2.17 2.01 2.03 1.66 0.97 0.64 0.   9.48] Loss_P: [ 3.29  2.84  2.23  2.11  1.75  1.    0.84 14.06]\n",
      "Loss_Q: [2.16 1.94 1.94 1.61 0.93 0.66 0.   9.25] Loss_P: [ 3.31  2.79  2.19  2.07  1.73  0.97  0.86 13.93]\n",
      "Loss_Q: [2.08 1.97 1.92 1.61 1.02 0.64 0.   9.24] Loss_P: [ 3.31  2.77  2.25  2.02  1.74  0.93  0.81 13.83]\n",
      "Loss_Q: [2.06 2.03 1.93 1.69 1.01 0.64 0.   9.36] Loss_P: [ 3.32  2.68  2.39  2.04  1.79  0.99  0.8  14.02]\n",
      "Loss_Q: [1.97 2.06 1.95 1.73 1.   0.6  0.   9.31] Loss_P: [ 3.29  2.69  2.4   2.04  1.87  1.02  0.79 14.1 ]\n",
      "Loss_Q: [2.06 2.06 1.96 1.62 0.97 0.6  0.   9.27] Loss_P: [ 3.28  2.68  2.42  2.07  1.82  0.99  0.74 13.99]\n",
      "Loss_Q: [2.03 2.01 1.97 1.64 1.01 0.58 0.   9.24] Loss_P: [ 3.27  2.71  2.38  2.08  1.82  1.03  0.72 14.01]\n",
      "Loss_Q: [2.06 2.17 1.96 1.74 1.07 0.63 0.   9.63] Loss_P: [ 3.29  2.75  2.42  2.09  1.89  1.07  0.8  14.31]\n",
      "Loss_Q: [2.09 2.17 2.02 1.73 1.08 0.61 0.   9.7 ] Loss_P: [ 3.31  2.73  2.51  2.18  1.93  1.1   0.78 14.52]\n",
      "Loss_Q: [2.09 2.17 2.05 1.73 1.09 0.65 0.   9.79] Loss_P: [ 3.28  2.76  2.45  2.18  1.96  1.08  0.8  14.51]\n",
      "Loss_Q: [2.14 2.19 2.08 1.75 1.11 0.7  0.   9.97] Loss_P: [ 3.31  2.8   2.52  2.17  1.96  1.11  0.89 14.77]\n",
      "Loss_Q: [ 2.25  2.32  2.07  1.78  1.09  0.68  0.   10.19] Loss_P: [ 3.3   2.82  2.62  2.24  1.97  1.15  0.86 14.96]\n",
      "Loss_Q: [ 2.16  2.27  2.16  1.83  1.14  0.71  0.   10.26] Loss_P: [ 3.32  2.83  2.61  2.28  2.03  1.08  0.87 15.01]\n",
      "Loss_Q: [ 2.19  2.32  2.18  1.81  1.15  0.67  0.   10.32] Loss_P: [ 3.27  2.91  2.63  2.26  2.05  1.13  0.9  15.13]\n",
      "Loss_Q: [ 2.13  2.35  2.1   1.76  1.05  0.68  0.   10.08] Loss_P: [ 3.26  2.85  2.62  2.25  1.94  1.1   0.86 14.86]\n",
      "Loss_Q: [2.09 2.3  1.99 1.67 1.1  0.71 0.   9.86] Loss_P: [ 3.27  2.77  2.58  2.18  1.8   1.08  0.88 14.55]\n",
      "Loss_Q: [2.08 2.33 1.93 1.66 1.12 0.68 0.   9.8 ] Loss_P: [ 3.34  2.8   2.62  2.06  1.84  1.11  0.89 14.66]\n",
      "Loss_Q: [2.14 2.27 1.88 1.71 1.1  0.63 0.   9.73] Loss_P: [ 3.29  2.83  2.56  1.99  1.89  1.12  0.84 14.51]\n",
      "Loss_Q: [2.07 2.29 1.91 1.68 1.08 0.66 0.   9.69] Loss_P: [ 3.29  2.71  2.6   2.    1.82  1.11  0.88 14.41]\n",
      "Loss_Q: [ 2.02  2.36  2.03  1.84  1.2   0.74  0.   10.17] Loss_P: [ 3.26  2.71  2.62  2.08  1.95  1.14  0.9  14.66]\n",
      "Loss_Q: [1.94 2.3  1.9  1.81 1.1  0.66 0.   9.71] Loss_P: [ 3.32  2.62  2.49  2.03  1.95  1.12  0.85 14.39]\n",
      "Loss_Q: [2.05 2.27 1.98 1.74 1.18 0.67 0.   9.89] Loss_P: [ 3.33  2.64  2.55  2.05  1.93  1.13  0.87 14.51]\n",
      "Loss_Q: [ 2.09  2.32  1.98  1.77  1.2   0.66  0.   10.02] Loss_P: [ 3.26  2.82  2.63  2.09  1.96  1.16  0.9  14.83]\n",
      "Loss_Q: [2.03 2.32 1.92 1.68 1.21 0.71 0.   9.88] Loss_P: [ 3.33  2.77  2.59  2.08  1.86  1.19  0.95 14.77]\n",
      "Loss_Q: [ 2.09  2.25  1.99  1.76  1.27  0.72  0.   10.08] Loss_P: [ 3.32  2.77  2.52  2.07  1.92  1.2   0.94 14.74]\n",
      "Loss_Q: [ 2.14  2.2   2.03  1.75  1.26  0.76  0.   10.14] Loss_P: [ 3.32  2.79  2.51  2.06  1.95  1.18  0.99 14.79]\n",
      "Loss_Q: [ 2.19  2.2   2.19  1.79  1.27  0.73  0.   10.36] Loss_P: [ 3.29  2.83  2.49  2.25  2.02  1.2   0.97 15.05]\n",
      "Loss_Q: [ 2.2   2.24  2.14  1.81  1.21  0.73  0.   10.32] Loss_P: [ 3.25  2.86  2.56  2.26  2.03  1.2   0.95 15.11]\n",
      "Loss_Q: [ 2.18  2.36  2.22  1.8   1.23  0.71  0.   10.5 ] Loss_P: [ 3.3   2.83  2.61  2.37  2.01  1.13  0.96 15.22]\n",
      "Loss_Q: [ 2.19  2.37  2.21  1.79  1.18  0.71  0.   10.44] Loss_P: [ 3.3   2.89  2.62  2.35  2.02  1.2   0.98 15.36]\n",
      "Loss_Q: [ 2.16  2.42  2.22  1.8   1.19  0.74  0.   10.53] Loss_P: [ 3.32  2.83  2.68  2.42  1.96  1.17  1.   15.38]\n",
      "Loss_Q: [ 2.11  2.47  2.26  1.79  1.23  0.76  0.   10.62] Loss_P: [ 3.24  2.8   2.73  2.44  1.97  1.18  1.01 15.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 2.13  2.47  2.24  1.73  1.17  0.73  0.   10.48] Loss_P: [ 3.33  2.81  2.73  2.42  1.94  1.13  0.97 15.33]\n",
      "Loss_Q: [ 2.11  2.57  2.26  1.82  1.19  0.71  0.   10.65] Loss_P: [ 3.28  2.71  2.82  2.46  1.95  1.17  0.98 15.37]\n",
      "Loss_Q: [ 1.9   2.6   2.26  1.76  1.16  0.79  0.   10.47] Loss_P: [ 3.29  2.63  2.88  2.43  1.92  1.16  1.   15.31]\n",
      "Loss_Q: [ 2.09  2.47  2.25  1.77  1.18  0.76  0.   10.52] Loss_P: [ 3.31  2.69  2.81  2.45  1.91  1.15  1.01 15.33]\n",
      "Loss_Q: [ 2.08  2.52  2.34  1.84  1.25  0.74  0.   10.76] Loss_P: [ 3.29  2.81  2.81  2.46  2.02  1.2   0.99 15.57]\n",
      "Loss_Q: [ 2.18  2.5   2.29  1.85  1.26  0.75  0.   10.82] Loss_P: [ 3.23  2.89  2.74  2.49  1.99  1.26  0.96 15.56]\n",
      "Loss_Q: [ 2.11  2.38  2.26  1.75  1.28  0.71  0.   10.49] Loss_P: [ 3.26  2.86  2.65  2.44  1.89  1.26  0.96 15.31]\n",
      "Loss_Q: [ 2.12  2.35  2.32  1.78  1.28  0.75  0.   10.61] Loss_P: [ 3.27  2.75  2.69  2.52  1.9   1.18  0.98 15.28]\n",
      "Loss_Q: [ 2.09  2.39  2.26  1.72  1.26  0.82  0.   10.54] Loss_P: [ 3.28  2.76  2.64  2.46  1.89  1.19  1.02 15.25]\n",
      "Loss_Q: [ 2.08  2.39  2.28  1.82  1.3   0.79  0.   10.66] Loss_P: [ 3.3   2.69  2.68  2.47  2.    1.24  1.05 15.43]\n",
      "Loss_Q: [ 2.03  2.4   2.22  1.81  1.34  0.8   0.   10.6 ] Loss_P: [ 3.3   2.73  2.74  2.38  1.98  1.3   1.07 15.5 ]\n",
      "Loss_Q: [ 2.13  2.41  2.24  1.83  1.29  0.83  0.   10.74] Loss_P: [ 3.24  2.84  2.66  2.4   2.02  1.25  1.09 15.5 ]\n",
      "Loss_Q: [ 2.05  2.43  2.28  1.87  1.31  0.88  0.   10.83] Loss_P: [ 3.27  2.79  2.64  2.49  2.03  1.23  1.11 15.56]\n",
      "Loss_Q: [ 2.11  2.38  2.17  1.84  1.3   0.88  0.   10.68] Loss_P: [ 3.24  2.82  2.64  2.34  1.98  1.21  1.08 15.32]\n",
      "Loss_Q: [ 2.13  2.38  2.17  1.86  1.22  0.79  0.   10.55] Loss_P: [ 3.25  2.9   2.65  2.39  1.98  1.19  1.03 15.38]\n",
      "Loss_Q: [ 2.03  2.4   2.28  1.79  1.26  0.8   0.   10.56] Loss_P: [ 3.24  2.8   2.63  2.41  1.95  1.2   1.05 15.27]\n",
      "Loss_Q: [ 2.17  2.41  2.36  1.87  1.27  0.77  0.   10.85] Loss_P: [ 3.27  2.84  2.65  2.48  2.01  1.22  1.03 15.49]\n",
      "Loss_Q: [ 2.15  2.42  2.26  1.83  1.28  0.78  0.   10.72] Loss_P: [ 3.24  2.89  2.66  2.44  1.99  1.24  1.01 15.49]\n",
      "Loss_Q: [ 2.13  2.41  2.16  1.83  1.23  0.75  0.   10.51] Loss_P: [ 3.27  2.77  2.63  2.37  1.96  1.17  0.98 15.13]\n",
      "Loss_Q: [ 2.12  2.33  2.08  1.79  1.09  0.73  0.   10.15] Loss_P: [ 3.29  2.82  2.58  2.29  1.96  1.08  0.99 15.01]\n",
      "Loss_Q: [ 2.01  2.36  2.24  1.83  1.19  0.75  0.   10.37] Loss_P: [ 3.28  2.73  2.64  2.32  2.03  1.09  0.99 15.08]\n",
      "Loss_Q: [ 2.1   2.36  2.14  1.82  1.11  0.72  0.   10.25] Loss_P: [ 3.28  2.73  2.68  2.31  2.03  1.11  0.98 15.13]\n",
      "Loss_Q: [ 2.13  2.42  2.23  1.87  1.05  0.76  0.   10.46] Loss_P: [ 3.3   2.76  2.63  2.37  2.03  1.05  1.   15.13]\n",
      "Loss_Q: [ 2.03  2.39  2.22  1.87  1.09  0.77  0.   10.37] Loss_P: [ 3.34  2.68  2.63  2.35  1.98  1.02  0.97 14.97]\n",
      "Loss_Q: [ 1.94  2.31  2.22  1.8   1.1   0.8   0.   10.18] Loss_P: [ 3.3   2.62  2.59  2.39  1.99  1.05  1.04 14.98]\n",
      "Loss_Q: [ 2.02  2.26  2.14  1.84  1.11  0.8   0.   10.17] Loss_P: [ 3.32  2.72  2.57  2.3   1.97  1.09  1.05 15.02]\n",
      "Loss_Q: [ 1.94  2.29  2.12  1.83  1.14  0.75  0.   10.06] Loss_P: [ 3.34  2.62  2.54  2.32  1.98  1.07  1.   14.86]\n",
      "Loss_Q: [ 1.93  2.36  2.19  1.91  1.21  0.76  0.   10.35] Loss_P: [ 3.3   2.59  2.64  2.4   2.06  1.15  1.01 15.15]\n",
      "Loss_Q: [ 1.91  2.27  2.19  1.89  1.2   0.77  0.   10.22] Loss_P: [ 3.3   2.58  2.62  2.37  2.03  1.17  1.   15.08]\n",
      "Loss_Q: [1.9  2.22 2.03 1.78 1.22 0.77 0.   9.93] Loss_P: [ 3.27  2.57  2.49  2.24  1.9   1.08  1.03 14.59]\n",
      "Loss_Q: [ 2.06  2.3   2.17  1.76  1.17  0.79  0.   10.26] Loss_P: [ 3.25  2.73  2.58  2.31  1.89  1.09  1.04 14.89]\n",
      "Loss_Q: [ 2.    2.32  2.24  1.76  1.12  0.78  0.   10.22] Loss_P: [ 3.3   2.7   2.58  2.38  1.94  1.05  1.02 14.98]\n",
      "Loss_Q: [ 2.06  2.24  2.26  1.78  1.09  0.74  0.   10.16] Loss_P: [ 3.3   2.67  2.57  2.36  1.96  1.03  1.02 14.91]\n",
      "Loss_Q: [ 2.01  2.31  2.25  1.85  1.13  0.8   0.   10.36] Loss_P: [ 3.28  2.68  2.58  2.38  2.01  1.03  1.03 14.99]\n",
      "Loss_Q: [ 1.98  2.34  2.19  1.83  1.11  0.75  0.   10.21] Loss_P: [ 3.28  2.64  2.6   2.39  2.02  1.06  1.   14.99]\n",
      "Loss_Q: [ 1.89  2.38  2.21  1.84  1.1   0.79  0.   10.19] Loss_P: [ 3.27  2.66  2.61  2.4   1.97  1.04  1.04 14.99]\n",
      "Loss_Q: [ 1.9   2.36  2.11  1.86  1.13  0.81  0.   10.17] Loss_P: [ 3.25  2.6   2.68  2.28  1.97  1.06  1.06 14.91]\n",
      "Loss_Q: [ 2.    2.3   2.15  1.79  1.09  0.78  0.   10.11] Loss_P: [ 3.33  2.62  2.59  2.31  1.91  1.06  1.08 14.91]\n",
      "Loss_Q: [ 2.08  2.3   2.16  1.8   1.06  0.81  0.   10.2 ] Loss_P: [ 3.29  2.72  2.59  2.32  1.91  1.01  1.07 14.9 ]\n",
      "Loss_Q: [ 2.    2.38  2.03  1.84  1.09  0.84  0.   10.17] Loss_P: [ 3.25  2.78  2.71  2.31  1.93  1.04  1.11 15.13]\n",
      "Loss_Q: [ 2.11  2.28  2.04  1.74  1.05  0.83  0.   10.05] Loss_P: [ 3.32  2.7   2.64  2.23  1.88  1.03  1.08 14.88]\n",
      "Loss_Q: [ 2.12  2.2   2.1   1.77  1.14  0.79  0.   10.12] Loss_P: [ 3.28  2.75  2.52  2.25  1.93  1.06  1.09 14.88]\n",
      "Loss_Q: [ 2.07  2.21  2.1   1.84  1.14  0.81  0.   10.17] Loss_P: [ 3.28  2.74  2.56  2.26  1.93  1.08  1.05 14.9 ]\n",
      "Loss_Q: [ 2.05  2.29  2.16  1.81  1.18  0.81  0.   10.3 ] Loss_P: [ 3.25  2.71  2.57  2.35  1.94  1.1   1.04 14.98]\n",
      "Loss_Q: [ 1.9   2.29  2.19  1.81  1.13  0.76  0.   10.07] Loss_P: [ 3.31  2.61  2.56  2.29  1.94  1.05  1.05 14.8 ]\n",
      "Loss_Q: [2.01 2.2  2.08 1.76 1.06 0.75 0.   9.87] Loss_P: [ 3.27  2.64  2.51  2.18  1.92  1.03  1.02 14.56]\n",
      "Loss_Q: [1.96 2.21 2.02 1.74 1.07 0.72 0.   9.72] Loss_P: [ 3.28  2.66  2.55  2.2   1.86  1.04  0.97 14.56]\n",
      "Loss_Q: [1.97 2.11 1.93 1.67 1.08 0.73 0.   9.51] Loss_P: [ 3.27  2.69  2.53  2.04  1.82  1.03  0.98 14.35]\n",
      "Loss_Q: [2.03 2.14 2.02 1.61 1.07 0.69 0.   9.56] Loss_P: [ 3.32  2.69  2.45  2.12  1.71  1.01  0.95 14.25]\n",
      "Loss_Q: [1.94 2.04 1.98 1.6  1.04 0.72 0.   9.33] Loss_P: [ 3.3   2.63  2.36  2.06  1.79  0.99  0.97 14.09]\n",
      "Loss_Q: [1.97 2.03 1.98 1.7  1.12 0.74 0.   9.53] Loss_P: [ 3.26  2.62  2.41  2.05  1.84  1.01  0.97 14.16]\n",
      "Loss_Q: [1.97 2.12 1.9  1.72 1.13 0.72 0.   9.56] Loss_P: [ 3.26  2.62  2.5   1.99  1.88  1.05  0.97 14.26]\n",
      "Loss_Q: [2.   2.2  2.   1.74 1.12 0.7  0.   9.75] Loss_P: [ 3.28  2.65  2.56  2.16  1.9   1.06  0.94 14.55]\n",
      "Loss_Q: [ 1.98  2.29  2.11  1.81  1.21  0.69  0.   10.09] Loss_P: [ 3.26  2.61  2.61  2.22  1.93  1.16  0.96 14.76]\n",
      "Loss_Q: [ 2.07  2.27  2.1   1.86  1.25  0.69  0.   10.24] Loss_P: [ 3.25  2.69  2.62  2.22  1.95  1.2   0.95 14.88]\n",
      "Loss_Q: [ 2.1   2.25  2.11  1.75  1.18  0.68  0.   10.08] Loss_P: [ 3.23  2.77  2.58  2.24  1.93  1.14  0.94 14.81]\n",
      "Loss_Q: [ 2.02  2.24  2.11  1.82  1.17  0.65  0.   10.01] Loss_P: [ 3.22  2.69  2.53  2.19  1.97  1.11  0.92 14.62]\n",
      "Loss_Q: [2.   2.24 2.08 1.76 1.16 0.63 0.   9.87] Loss_P: [ 3.31  2.69  2.51  2.18  1.96  1.13  0.88 14.67]\n",
      "Loss_Q: [2.04 2.27 2.09 1.81 1.17 0.61 0.   9.99] Loss_P: [ 3.31  2.61  2.55  2.23  1.96  1.15  0.91 14.72]\n",
      "Loss_Q: [ 1.95  2.18  2.17  1.83  1.18  0.7   0.   10.  ] Loss_P: [ 3.32  2.65  2.58  2.26  1.93  1.08  0.92 14.75]\n",
      "Loss_Q: [ 2.02  2.23  2.24  1.73  1.14  0.68  0.   10.05] Loss_P: [ 3.29  2.64  2.6   2.32  1.97  1.08  0.92 14.82]\n",
      "Loss_Q: [ 1.95  2.25  2.21  1.72  1.22  0.73  0.   10.08] Loss_P: [ 3.27  2.65  2.69  2.3   1.89  1.12  0.96 14.87]\n",
      "Loss_Q: [ 1.98  2.3   2.26  1.73  1.17  0.69  0.   10.14] Loss_P: [ 3.32  2.58  2.62  2.36  1.92  1.14  0.92 14.85]\n",
      "Loss_Q: [ 1.92  2.29  2.26  1.77  1.15  0.7   0.   10.08] Loss_P: [ 3.3   2.58  2.62  2.45  2.    1.13  0.95 15.03]\n",
      "Loss_Q: [ 1.94  2.27  2.21  1.75  1.21  0.68  0.   10.06] Loss_P: [ 3.29  2.59  2.61  2.37  1.92  1.18  0.91 14.86]\n",
      "Loss_Q: [1.89 2.29 2.14 1.75 1.19 0.66 0.   9.94] Loss_P: [ 3.31  2.56  2.64  2.22  1.87  1.13  0.88 14.62]\n",
      "Loss_Q: [1.96 2.26 2.13 1.72 1.23 0.69 0.   9.99] Loss_P: [ 3.24  2.58  2.61  2.2   1.86  1.13  0.92 14.54]\n",
      "Loss_Q: [ 1.97  2.28  2.17  1.8   1.2   0.68  0.   10.1 ] Loss_P: [ 3.29  2.56  2.61  2.31  2.    1.16  0.93 14.86]\n",
      "Loss_Q: [1.88 2.19 2.01 1.81 1.19 0.66 0.   9.74] Loss_P: [ 3.29  2.41  2.58  2.22  2.02  1.17  0.88 14.57]\n",
      "Loss_Q: [ 1.84  2.25  2.2   1.85  1.19  0.69  0.   10.02] Loss_P: [ 3.33  2.45  2.65  2.3   2.    1.18  0.89 14.79]\n",
      "Loss_Q: [ 1.86  2.29  2.2   1.91  1.26  0.67  0.   10.2 ] Loss_P: [ 3.27  2.52  2.7   2.38  2.07  1.19  0.92 15.06]\n",
      "Loss_Q: [ 2.01  2.27  2.2   1.9   1.22  0.69  0.   10.29] Loss_P: [ 3.31  2.62  2.58  2.33  2.03  1.19  0.91 14.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 1.98  2.27  2.15  1.9   1.18  0.68  0.   10.16] Loss_P: [ 3.32  2.56  2.62  2.35  2.01  1.18  0.94 14.99]\n",
      "Loss_Q: [ 1.92  2.29  2.17  1.87  1.25  0.69  0.   10.18] Loss_P: [ 3.3   2.49  2.7   2.33  1.99  1.21  0.98 15.01]\n",
      "Loss_Q: [ 1.99  2.3   2.18  1.92  1.25  0.77  0.   10.4 ] Loss_P: [ 3.27  2.58  2.74  2.34  2.02  1.15  1.03 15.14]\n",
      "Loss_Q: [ 1.97  2.29  2.25  1.94  1.27  0.72  0.   10.44] Loss_P: [ 3.3   2.66  2.64  2.35  2.03  1.19  1.03 15.21]\n",
      "Loss_Q: [ 2.01  2.28  2.17  1.89  1.27  0.75  0.   10.37] Loss_P: [ 3.29  2.65  2.63  2.29  2.06  1.18  1.   15.11]\n",
      "Loss_Q: [ 1.95  2.2   2.08  1.91  1.27  0.77  0.   10.17] Loss_P: [ 3.34  2.55  2.55  2.2   2.04  1.19  1.03 14.9 ]\n",
      "Loss_Q: [1.82 2.2  2.03 1.87 1.29 0.73 0.   9.93] Loss_P: [ 3.34  2.55  2.59  2.09  1.99  1.17  1.04 14.77]\n",
      "Loss_Q: [ 1.91  2.23  2.01  1.89  1.25  0.73  0.   10.02] Loss_P: [ 3.32  2.52  2.68  2.14  2.    1.12  1.04 14.82]\n",
      "Loss_Q: [ 1.83  2.28  2.04  1.87  1.22  0.76  0.   10.01] Loss_P: [ 3.38  2.43  2.62  2.18  1.98  1.12  1.09 14.81]\n",
      "Loss_Q: [1.8  2.27 2.15 1.86 1.15 0.74 0.   9.96] Loss_P: [ 3.35  2.46  2.67  2.24  2.    1.05  1.04 14.81]\n",
      "Loss_Q: [1.8  2.18 2.06 1.81 1.18 0.74 0.   9.78] Loss_P: [ 3.33  2.37  2.57  2.16  1.98  1.11  1.01 14.53]\n",
      "Loss_Q: [1.8  2.17 1.97 1.79 1.16 0.77 0.   9.65] Loss_P: [ 3.29  2.47  2.55  2.14  1.9   1.13  1.05 14.53]\n",
      "Loss_Q: [1.92 2.21 2.14 1.75 1.21 0.75 0.   9.96] Loss_P: [ 3.35  2.56  2.62  2.3   1.89  1.12  1.05 14.89]\n",
      "Loss_Q: [ 1.98  2.29  2.06  1.78  1.21  0.75  0.   10.08] Loss_P: [ 3.33  2.62  2.67  2.16  1.89  1.11  1.02 14.79]\n",
      "Loss_Q: [ 1.97  2.29  2.05  1.76  1.18  0.76  0.   10.01] Loss_P: [ 3.29  2.62  2.62  2.15  1.9   1.03  1.03 14.65]\n",
      "Loss_Q: [ 2.    2.31  2.09  1.8   1.18  0.78  0.   10.16] Loss_P: [ 3.33  2.68  2.62  2.16  1.97  1.09  1.07 14.92]\n",
      "Loss_Q: [ 2.11  2.32  2.14  1.85  1.21  0.84  0.   10.47] Loss_P: [ 3.25  2.68  2.67  2.29  1.98  1.1   1.07 15.03]\n",
      "Loss_Q: [ 2.2   2.28  2.12  1.85  1.18  0.81  0.   10.44] Loss_P: [ 3.28  2.8   2.72  2.25  1.98  1.1   1.14 15.26]\n",
      "Loss_Q: [ 2.09  2.24  2.04  1.91  1.25  0.81  0.   10.33] Loss_P: [ 3.26  2.79  2.68  2.15  1.95  1.17  1.1  15.1 ]\n",
      "Loss_Q: [ 2.18  2.15  1.96  1.88  1.22  0.79  0.   10.19] Loss_P: [ 3.25  2.83  2.63  2.12  1.95  1.17  1.05 14.99]\n",
      "Loss_Q: [ 2.18  2.12  2.03  1.87  1.21  0.83  0.   10.25] Loss_P: [ 3.27  2.83  2.62  2.17  1.9   1.17  1.08 15.04]\n",
      "Loss_Q: [ 2.17  2.13  2.03  1.82  1.17  0.85  0.   10.17] Loss_P: [ 3.24  2.73  2.63  2.15  1.92  1.09  1.08 14.84]\n",
      "Loss_Q: [ 2.1   2.2   2.13  1.77  1.1   0.83  0.   10.14] Loss_P: [ 3.28  2.76  2.64  2.28  1.89  1.08  1.07 15.01]\n",
      "Loss_Q: [ 2.1   2.19  2.26  1.86  1.09  0.81  0.   10.31] Loss_P: [ 3.29  2.7   2.63  2.39  1.91  1.06  1.09 15.07]\n",
      "Loss_Q: [ 2.03  2.15  2.29  1.86  1.07  0.81  0.   10.2 ] Loss_P: [ 3.29  2.66  2.53  2.39  1.95  1.09  1.07 14.97]\n",
      "Loss_Q: [ 2.08  2.11  2.35  1.82  1.13  0.83  0.   10.33] Loss_P: [ 3.27  2.72  2.5   2.45  1.94  1.08  1.14 15.1 ]\n",
      "Loss_Q: [ 2.05  2.11  2.3   1.76  1.12  0.86  0.   10.2 ] Loss_P: [ 3.31  2.65  2.51  2.39  1.94  1.09  1.12 15.02]\n",
      "Loss_Q: [ 2.06  2.2   2.36  1.81  1.07  0.84  0.   10.33] Loss_P: [ 3.28  2.68  2.53  2.45  1.91  1.07  1.09 15.01]\n",
      "Loss_Q: [ 2.04  2.15  2.31  1.77  1.08  0.86  0.   10.2 ] Loss_P: [ 3.28  2.65  2.48  2.4   1.87  1.1   1.11 14.89]\n",
      "Loss_Q: [ 2.05  2.18  2.32  1.76  1.06  0.84  0.   10.22] Loss_P: [ 3.23  2.68  2.53  2.39  1.89  1.01  1.15 14.87]\n",
      "Loss_Q: [2.05 2.07 2.24 1.77 1.01 0.85 0.   9.97] Loss_P: [ 3.29  2.76  2.49  2.37  1.84  0.99  1.12 14.86]\n",
      "Loss_Q: [1.96 2.06 2.27 1.79 1.02 0.84 0.   9.93] Loss_P: [ 3.26  2.62  2.5   2.42  1.83  1.    1.1  14.72]\n",
      "Loss_Q: [ 2.01  2.12  2.34  1.74  1.03  0.85  0.   10.11] Loss_P: [ 3.32  2.64  2.53  2.48  1.78  0.99  1.1  14.86]\n",
      "Loss_Q: [ 2.08  2.13  2.31  1.73  1.11  0.83  0.   10.18] Loss_P: [ 3.33  2.66  2.52  2.41  1.78  1.11  1.12 14.92]\n",
      "Loss_Q: [ 2.09  2.08  2.24  1.72  1.13  0.82  0.   10.08] Loss_P: [ 3.31  2.69  2.5   2.37  1.76  1.08  1.09 14.8 ]\n",
      "Loss_Q: [ 2.17  2.11  2.25  1.72  1.09  0.81  0.   10.16] Loss_P: [ 3.29  2.75  2.52  2.37  1.78  1.07  1.06 14.84]\n",
      "Loss_Q: [2.03 2.08 2.24 1.72 1.08 0.8  0.   9.95] Loss_P: [ 3.27  2.64  2.47  2.29  1.77  1.11  1.07 14.61]\n",
      "Loss_Q: [1.96 2.08 2.19 1.69 1.05 0.78 0.   9.76] Loss_P: [ 3.3   2.54  2.52  2.3   1.74  1.01  1.06 14.47]\n",
      "Loss_Q: [1.95 2.16 2.26 1.71 1.05 0.77 0.   9.89] Loss_P: [ 3.31  2.63  2.54  2.46  1.81  0.99  1.06 14.81]\n",
      "Loss_Q: [ 2.07  2.18  2.35  1.76  1.03  0.76  0.   10.15] Loss_P: [ 3.29  2.68  2.5   2.49  1.75  0.99  1.   14.71]\n",
      "Loss_Q: [2.02 2.2  2.29 1.64 1.01 0.74 0.   9.9 ] Loss_P: [ 3.29  2.62  2.56  2.43  1.79  1.01  1.   14.68]\n",
      "Loss_Q: [ 2.05  2.18  2.28  1.74  1.05  0.7   0.   10.  ] Loss_P: [ 3.3   2.65  2.57  2.53  1.79  1.    1.01 14.84]\n",
      "Loss_Q: [ 2.05  2.14  2.33  1.74  1.06  0.71  0.   10.03] Loss_P: [ 3.26  2.71  2.52  2.45  1.82  0.99  0.99 14.74]\n",
      "Loss_Q: [2.08 2.2  2.31 1.66 0.98 0.73 0.   9.96] Loss_P: [ 3.29  2.72  2.5   2.4   1.8   0.95  0.97 14.63]\n",
      "Loss_Q: [1.99 2.12 2.29 1.68 1.01 0.7  0.   9.8 ] Loss_P: [ 3.29  2.67  2.5   2.32  1.8   0.9   0.97 14.45]\n",
      "Loss_Q: [2.02 2.11 2.26 1.76 1.03 0.71 0.   9.89] Loss_P: [ 3.3   2.7   2.46  2.37  1.84  0.95  0.98 14.59]\n",
      "Loss_Q: [1.98 2.14 2.23 1.74 0.97 0.7  0.   9.76] Loss_P: [ 3.33  2.61  2.56  2.33  1.84  0.92  0.97 14.55]\n",
      "Loss_Q: [1.95 2.17 2.36 1.78 1.04 0.7  0.   9.99] Loss_P: [ 3.29  2.56  2.45  2.45  1.89  0.95  1.01 14.62]\n",
      "Loss_Q: [1.89 2.04 2.38 1.68 1.   0.7  0.   9.69] Loss_P: [ 3.35  2.55  2.46  2.52  1.81  0.93  0.99 14.61]\n",
      "Loss_Q: [1.88 1.97 2.29 1.58 0.98 0.71 0.   9.41] Loss_P: [ 3.34  2.48  2.38  2.39  1.68  0.91  0.96 14.15]\n",
      "Loss_Q: [1.82 2.06 2.33 1.56 0.97 0.66 0.   9.39] Loss_P: [ 3.28  2.55  2.48  2.48  1.59  0.89  0.94 14.2 ]\n",
      "Loss_Q: [1.82 2.08 2.27 1.56 0.96 0.65 0.   9.34] Loss_P: [ 3.31  2.46  2.45  2.41  1.64  0.86  0.93 14.05]\n",
      "Loss_Q: [1.94 2.04 2.44 1.63 1.   0.66 0.   9.71] Loss_P: [ 3.3   2.6   2.48  2.48  1.72  0.86  0.92 14.35]\n",
      "Loss_Q: [2.05 2.05 2.36 1.61 0.99 0.7  0.   9.75] Loss_P: [ 3.29  2.7   2.45  2.5   1.74  0.87  0.95 14.5 ]\n",
      "Loss_Q: [2.08 2.03 2.33 1.53 0.91 0.7  0.   9.58] Loss_P: [ 3.29  2.7   2.43  2.41  1.64  0.84  0.94 14.25]\n",
      "Loss_Q: [2.05 2.09 2.3  1.59 0.96 0.66 0.   9.65] Loss_P: [ 3.26  2.67  2.51  2.44  1.72  0.87  0.92 14.39]\n",
      "Loss_Q: [2.09 2.17 2.29 1.54 0.91 0.67 0.   9.67] Loss_P: [ 3.25  2.7   2.55  2.44  1.75  0.87  0.92 14.47]\n",
      "Loss_Q: [2.09 2.23 2.25 1.67 0.89 0.68 0.   9.81] Loss_P: [ 3.27  2.74  2.58  2.41  1.72  0.85  0.91 14.49]\n",
      "Loss_Q: [1.95 2.21 2.22 1.66 0.92 0.69 0.   9.65] Loss_P: [ 3.33  2.6   2.6   2.4   1.69  0.81  0.95 14.38]\n",
      "Loss_Q: [2.   2.22 2.23 1.62 0.86 0.69 0.   9.62] Loss_P: [ 3.33  2.68  2.59  2.37  1.7   0.77  0.93 14.37]\n",
      "Loss_Q: [1.99 2.18 2.15 1.64 0.89 0.68 0.   9.53] Loss_P: [ 3.31  2.69  2.53  2.32  1.75  0.87  0.93 14.4 ]\n",
      "Loss_Q: [2.04 2.21 2.12 1.7  0.86 0.67 0.   9.6 ] Loss_P: [ 3.29  2.64  2.58  2.3   1.75  0.81  0.92 14.28]\n",
      "Loss_Q: [1.98 2.12 2.06 1.66 0.91 0.68 0.   9.41] Loss_P: [ 3.3   2.62  2.49  2.24  1.72  0.82  0.96 14.14]\n",
      "Loss_Q: [2.01 2.06 1.97 1.58 0.86 0.7  0.   9.18] Loss_P: [ 3.3   2.59  2.48  2.08  1.68  0.82  0.97 13.94]\n",
      "Loss_Q: [2.   2.04 1.99 1.61 0.89 0.73 0.   9.26] Loss_P: [ 3.28  2.68  2.45  2.04  1.68  0.8   0.96 13.89]\n",
      "Loss_Q: [2.05 2.13 2.06 1.61 0.91 0.72 0.   9.49] Loss_P: [ 3.32  2.7   2.5   2.14  1.72  0.81  0.96 14.16]\n",
      "Loss_Q: [2.03 2.1  2.21 1.59 0.95 0.72 0.   9.62] Loss_P: [ 3.35  2.68  2.5   2.33  1.71  0.88  0.97 14.43]\n",
      "Loss_Q: [2.03 2.21 2.33 1.63 0.93 0.76 0.   9.89] Loss_P: [ 3.31  2.67  2.56  2.49  1.69  0.87  1.01 14.61]\n",
      "Loss_Q: [ 2.05  2.26  2.29  1.7   0.95  0.76  0.   10.02] Loss_P: [ 3.3   2.72  2.6   2.5   1.75  0.91  1.03 14.82]\n",
      "Loss_Q: [2.02 2.2  2.31 1.7  0.93 0.73 0.   9.88] Loss_P: [ 3.35  2.63  2.54  2.47  1.76  0.88  0.99 14.63]\n",
      "Loss_Q: [2.01 2.2  2.32 1.74 0.88 0.73 0.   9.88] Loss_P: [ 3.26  2.71  2.52  2.49  1.81  0.85  1.   14.64]\n",
      "Loss_Q: [1.98 2.15 2.21 1.78 0.87 0.73 0.   9.73] Loss_P: [ 3.27  2.62  2.61  2.35  1.86  0.76  0.97 14.46]\n",
      "Loss_Q: [1.94 2.14 2.15 1.8  0.82 0.75 0.   9.6 ] Loss_P: [ 3.28  2.57  2.64  2.34  1.86  0.84  1.   14.54]\n",
      "Loss_Q: [1.98 2.25 2.23 1.82 0.85 0.76 0.   9.89] Loss_P: [ 3.31  2.69  2.69  2.41  1.92  0.81  1.04 14.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.08 2.16 2.16 1.78 0.88 0.78 0.   9.83] Loss_P: [ 3.26  2.72  2.53  2.32  1.84  0.84  1.04 14.55]\n",
      "Loss_Q: [2.   2.1  2.21 1.69 0.88 0.79 0.   9.67] Loss_P: [ 3.34  2.63  2.43  2.33  1.8   0.81  1.   14.34]\n",
      "Loss_Q: [1.92 2.   2.25 1.65 0.82 0.71 0.   9.35] Loss_P: [ 3.28  2.62  2.39  2.39  1.8   0.83  0.95 14.27]\n",
      "Loss_Q: [2.02 1.98 2.36 1.66 0.85 0.72 0.   9.59] Loss_P: [ 3.27  2.67  2.39  2.43  1.78  0.77  0.93 14.24]\n",
      "Loss_Q: [2.04 2.09 2.33 1.63 0.77 0.7  0.   9.56] Loss_P: [ 3.31  2.74  2.45  2.47  1.74  0.72  0.89 14.33]\n",
      "Loss_Q: [1.96 2.07 2.36 1.66 0.81 0.72 0.   9.59] Loss_P: [ 3.35  2.62  2.52  2.51  1.81  0.77  0.92 14.52]\n",
      "Loss_Q: [1.99 2.11 2.39 1.75 0.94 0.72 0.   9.9 ] Loss_P: [ 3.29  2.62  2.44  2.47  1.87  0.89  0.96 14.52]\n",
      "Loss_Q: [1.95 1.97 2.32 1.7  0.93 0.75 0.   9.62] Loss_P: [ 3.3   2.64  2.47  2.48  1.82  0.89  0.96 14.57]\n",
      "Loss_Q: [1.98 1.97 2.28 1.83 0.94 0.76 0.   9.76] Loss_P: [ 3.29  2.65  2.35  2.42  1.88  0.91  1.   14.5 ]\n",
      "Loss_Q: [1.97 1.95 2.28 1.81 0.94 0.77 0.   9.72] Loss_P: [ 3.29  2.63  2.34  2.46  1.96  0.91  0.99 14.59]\n",
      "Loss_Q: [1.9  2.06 2.31 1.83 0.92 0.74 0.   9.77] Loss_P: [ 3.3   2.59  2.38  2.41  1.95  0.89  0.99 14.51]\n",
      "Loss_Q: [1.89 2.12 2.23 1.86 0.95 0.73 0.   9.77] Loss_P: [ 3.35  2.5   2.51  2.46  1.95  0.86  0.97 14.6 ]\n",
      "Loss_Q: [1.93 2.13 2.29 1.83 0.96 0.75 0.   9.9 ] Loss_P: [ 3.33  2.55  2.51  2.48  1.95  0.89  1.   14.72]\n",
      "Loss_Q: [1.84 2.09 2.24 1.86 0.96 0.78 0.   9.78] Loss_P: [ 3.36  2.47  2.45  2.47  1.98  0.94  0.99 14.65]\n",
      "Loss_Q: [1.87 2.17 2.31 1.87 0.94 0.78 0.   9.94] Loss_P: [ 3.31  2.47  2.52  2.5   1.97  0.88  1.04 14.69]\n",
      "Loss_Q: [ 1.91  2.23  2.33  1.81  0.94  0.8   0.   10.02] Loss_P: [ 3.29  2.59  2.57  2.58  1.97  0.88  1.06 14.93]\n",
      "Loss_Q: [ 1.94  2.27  2.45  1.91  1.01  0.79  0.   10.37] Loss_P: [ 3.24  2.65  2.64  2.72  1.97  0.89  1.06 15.17]\n",
      "Loss_Q: [ 1.98  2.28  2.4   1.89  0.98  0.74  0.   10.26] Loss_P: [ 3.28  2.6   2.66  2.63  1.99  0.92  1.04 15.11]\n",
      "Loss_Q: [ 1.93  2.3   2.35  1.95  0.96  0.75  0.   10.25] Loss_P: [ 3.3   2.58  2.66  2.6   2.01  0.89  1.01 15.05]\n",
      "Loss_Q: [ 1.99  2.39  2.42  1.87  0.94  0.75  0.   10.37] Loss_P: [ 3.26  2.68  2.63  2.72  1.96  0.83  0.98 15.06]\n",
      "Loss_Q: [ 2.03  2.28  2.45  1.83  0.91  0.69  0.   10.19] Loss_P: [ 3.25  2.62  2.57  2.66  1.98  0.83  0.92 14.84]\n",
      "Loss_Q: [1.94 2.29 2.37 1.81 0.85 0.73 0.   9.99] Loss_P: [ 3.3   2.6   2.57  2.65  1.9   0.83  0.92 14.77]\n",
      "Loss_Q: [1.86 2.23 2.35 1.83 0.86 0.71 0.   9.85] Loss_P: [ 3.28  2.51  2.53  2.61  1.93  0.82  0.95 14.63]\n",
      "Loss_Q: [1.9  2.23 2.3  1.85 0.89 0.72 0.   9.89] Loss_P: [ 3.32  2.45  2.52  2.5   1.95  0.84  0.93 14.51]\n",
      "Loss_Q: [1.82 2.23 2.34 1.86 0.91 0.69 0.   9.84] Loss_P: [ 3.3   2.46  2.53  2.57  1.96  0.86  0.93 14.61]\n",
      "Loss_Q: [1.82 2.24 2.37 1.86 0.9  0.69 0.   9.89] Loss_P: [ 3.28  2.43  2.64  2.6   1.94  0.83  0.89 14.61]\n",
      "Loss_Q: [1.95 2.25 2.27 1.9  0.87 0.66 0.   9.89] Loss_P: [ 3.3   2.53  2.61  2.6   1.97  0.83  0.88 14.72]\n",
      "Loss_Q: [1.97 2.2  2.33 1.9  0.88 0.63 0.   9.91] Loss_P: [ 3.33  2.58  2.54  2.61  2.    0.83  0.85 14.74]\n",
      "Loss_Q: [ 1.98  2.21  2.45  1.91  0.9   0.61  0.   10.08] Loss_P: [ 3.33  2.62  2.55  2.68  2.    0.87  0.84 14.88]\n",
      "Loss_Q: [ 2.03  2.24  2.43  1.91  0.92  0.62  0.   10.16] Loss_P: [ 3.3   2.66  2.59  2.68  2.02  0.88  0.86 14.99]\n",
      "Loss_Q: [ 2.02  2.31  2.39  1.91  0.94  0.61  0.   10.17] Loss_P: [ 3.26  2.68  2.63  2.65  2.01  0.88  0.79 14.89]\n",
      "Loss_Q: [ 2.05  2.29  2.4   1.96  0.98  0.61  0.   10.29] Loss_P: [ 3.28  2.65  2.66  2.67  2.02  0.93  0.84 15.04]\n",
      "Loss_Q: [ 2.03  2.27  2.42  1.91  1.04  0.6   0.   10.27] Loss_P: [ 3.31  2.66  2.66  2.68  2.02  0.95  0.87 15.15]\n",
      "Loss_Q: [ 2.03  2.28  2.38  1.92  0.99  0.65  0.   10.24] Loss_P: [ 3.23  2.64  2.68  2.59  2.03  0.93  0.89 14.98]\n",
      "Loss_Q: [ 1.96  2.18  2.32  1.93  0.95  0.65  0.   10.  ] Loss_P: [ 3.29  2.66  2.56  2.52  1.99  0.82  0.89 14.72]\n",
      "Loss_Q: [2.   2.13 2.25 1.79 0.93 0.61 0.   9.7 ] Loss_P: [ 3.28  2.62  2.55  2.38  1.88  0.87  0.87 14.45]\n",
      "Loss_Q: [1.9  2.15 2.24 1.81 0.95 0.59 0.   9.65] Loss_P: [ 3.28  2.57  2.54  2.46  1.92  0.86  0.84 14.47]\n",
      "Loss_Q: [1.99 2.13 2.17 1.78 1.   0.65 0.   9.72] Loss_P: [ 3.27  2.63  2.51  2.34  1.94  0.89  0.87 14.45]\n",
      "Loss_Q: [1.95 2.11 2.11 1.88 0.91 0.62 0.   9.59] Loss_P: [ 3.27  2.6   2.46  2.32  1.99  0.85  0.83 14.32]\n",
      "Loss_Q: [1.85 2.04 2.23 1.87 1.   0.68 0.   9.66] Loss_P: [ 3.28  2.51  2.43  2.46  1.94  0.93  0.88 14.43]\n",
      "Loss_Q: [1.82 1.95 2.21 1.89 1.07 0.7  0.   9.64] Loss_P: [ 3.24  2.48  2.39  2.4   1.95  1.01  0.94 14.4 ]\n",
      "Loss_Q: [1.89 1.9  2.21 1.91 1.02 0.68 0.   9.62] Loss_P: [ 3.32  2.56  2.36  2.41  1.99  0.98  0.91 14.53]\n",
      "Loss_Q: [1.92 1.87 2.13 1.87 1.   0.66 0.   9.45] Loss_P: [ 3.32  2.55  2.31  2.29  1.95  0.98  0.91 14.31]\n",
      "Loss_Q: [1.9  1.92 2.07 1.88 1.06 0.66 0.   9.48] Loss_P: [ 3.27  2.54  2.35  2.31  1.95  0.95  0.88 14.26]\n",
      "Loss_Q: [1.95 1.96 2.19 1.88 1.02 0.64 0.   9.64] Loss_P: [ 3.26  2.53  2.41  2.33  1.93  0.93  0.85 14.23]\n",
      "Loss_Q: [1.91 2.   2.25 1.87 1.07 0.64 0.   9.74] Loss_P: [ 3.27  2.57  2.38  2.42  1.93  1.02  0.88 14.46]\n",
      "Loss_Q: [1.9  1.96 2.28 1.9  1.03 0.63 0.   9.69] Loss_P: [ 3.24  2.61  2.38  2.5   1.96  0.97  0.85 14.51]\n",
      "Loss_Q: [1.92 1.91 2.38 1.87 1.08 0.64 0.   9.8 ] Loss_P: [ 3.26  2.64  2.29  2.5   1.92  0.99  0.88 14.49]\n",
      "Loss_Q: [1.84 1.97 2.36 1.9  1.03 0.64 0.   9.73] Loss_P: [ 3.25  2.55  2.34  2.55  1.95  0.97  0.86 14.47]\n",
      "Loss_Q: [ 1.92  2.09  2.31  1.93  1.12  0.66  0.   10.03] Loss_P: [ 3.24  2.54  2.46  2.53  1.96  1.09  0.89 14.71]\n",
      "Loss_Q: [1.85 2.05 2.34 1.92 1.12 0.67 0.   9.95] Loss_P: [ 3.25  2.51  2.43  2.51  1.94  1.05  0.91 14.6 ]\n",
      "Loss_Q: [ 1.8   2.2   2.39  1.9   1.15  0.7   0.   10.14] Loss_P: [ 3.28  2.39  2.46  2.62  1.99  1.11  0.94 14.78]\n",
      "Loss_Q: [ 1.8   2.11  2.33  1.88  1.2   0.72  0.   10.04] Loss_P: [ 3.27  2.46  2.44  2.58  1.92  1.13  0.96 14.77]\n",
      "Loss_Q: [ 1.9   2.2   2.37  1.9   1.19  0.68  0.   10.25] Loss_P: [ 3.22  2.59  2.57  2.62  1.98  1.14  0.92 15.03]\n",
      "Loss_Q: [ 1.89  2.19  2.41  1.93  1.21  0.69  0.   10.33] Loss_P: [ 3.22  2.58  2.5   2.61  1.99  1.18  0.95 15.03]\n",
      "Loss_Q: [ 1.95  2.11  2.39  1.89  1.2   0.7   0.   10.24] Loss_P: [ 3.21  2.58  2.49  2.58  1.97  1.14  0.96 14.95]\n",
      "Loss_Q: [ 1.95  2.14  2.31  1.92  1.19  0.73  0.   10.24] Loss_P: [ 3.21  2.64  2.46  2.57  2.    1.15  0.98 14.99]\n",
      "Loss_Q: [ 1.89  2.07  2.32  1.91  1.16  0.71  0.   10.07] Loss_P: [ 3.22  2.57  2.43  2.55  2.    1.14  0.97 14.89]\n",
      "Loss_Q: [1.81 2.04 2.29 1.85 1.12 0.75 0.   9.86] Loss_P: [ 3.26  2.52  2.41  2.52  1.99  1.08  0.99 14.77]\n",
      "Loss_Q: [ 1.89  2.14  2.38  1.92  1.09  0.73  0.   10.15] Loss_P: [ 3.26  2.54  2.5   2.57  2.01  1.05  1.   14.92]\n",
      "Loss_Q: [1.79 2.08 2.34 1.93 1.04 0.72 0.   9.9 ] Loss_P: [ 3.22  2.57  2.39  2.59  2.05  1.    0.97 14.78]\n",
      "Loss_Q: [ 1.95  2.09  2.39  1.96  1.09  0.68  0.   10.17] Loss_P: [ 3.2   2.59  2.43  2.6   2.04  1.03  0.92 14.81]\n",
      "Loss_Q: [1.82 2.02 2.37 1.96 1.09 0.68 0.   9.94] Loss_P: [ 3.26  2.51  2.35  2.59  2.04  1.01  0.89 14.65]\n",
      "Loss_Q: [1.83 2.05 2.44 1.89 1.07 0.69 0.   9.97] Loss_P: [ 3.25  2.47  2.42  2.65  1.99  1.09  0.92 14.79]\n",
      "Loss_Q: [1.81 2.05 2.37 1.89 1.06 0.72 0.   9.91] Loss_P: [ 3.24  2.49  2.42  2.67  1.98  1.08  0.93 14.8 ]\n",
      "Loss_Q: [1.83 2.01 2.41 1.88 1.07 0.73 0.   9.91] Loss_P: [ 3.3   2.47  2.27  2.6   1.96  1.02  0.96 14.58]\n",
      "Loss_Q: [1.8  1.98 2.3  1.83 1.06 0.75 0.   9.72] Loss_P: [ 3.26  2.49  2.29  2.53  1.91  0.93  0.94 14.35]\n",
      "Loss_Q: [1.82 1.99 2.36 1.83 1.05 0.72 0.   9.78] Loss_P: [ 3.22  2.49  2.33  2.63  1.94  1.02  0.99 14.62]\n",
      "Loss_Q: [1.84 2.03 2.24 1.8  1.05 0.73 0.   9.69] Loss_P: [ 3.32  2.47  2.36  2.57  1.82  0.98  0.99 14.51]\n",
      "Loss_Q: [1.87 1.99 2.3  1.77 1.09 0.74 0.   9.77] Loss_P: [ 3.26  2.58  2.3   2.48  1.8   1.    1.   14.42]\n",
      "Loss_Q: [1.92 1.93 2.34 1.82 1.12 0.74 0.   9.85] Loss_P: [ 3.25  2.64  2.28  2.55  1.86  1.08  1.02 14.69]\n",
      "Loss_Q: [1.92 1.88 2.31 1.87 1.18 0.79 0.   9.95] Loss_P: [ 3.27  2.61  2.18  2.56  1.91  1.1   1.09 14.71]\n",
      "Loss_Q: [1.95 1.87 2.32 1.82 1.19 0.84 0.   9.97] Loss_P: [ 3.28  2.59  2.22  2.53  1.88  1.1   1.09 14.69]\n",
      "Loss_Q: [2.02 1.93 2.32 1.71 1.21 0.79 0.   9.99] Loss_P: [ 3.27  2.7   2.28  2.56  1.79  1.12  1.12 14.83]\n",
      "Loss_Q: [ 2.04  2.    2.33  1.79  1.2   0.85  0.   10.22] Loss_P: [ 3.27  2.71  2.34  2.55  1.8   1.11  1.15 14.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [ 2.1   2.08  2.3   1.81  1.24  0.83  0.   10.35] Loss_P: [ 3.26  2.63  2.4   2.52  1.84  1.14  1.15 14.94]\n",
      "Loss_Q: [ 2.    2.17  2.37  1.81  1.25  0.86  0.   10.46] Loss_P: [ 3.25  2.66  2.48  2.57  1.84  1.17  1.21 15.19]\n",
      "Loss_Q: [ 2.06  2.18  2.37  1.84  1.23  0.87  0.   10.55] Loss_P: [ 3.28  2.67  2.45  2.58  1.84  1.15  1.16 15.12]\n",
      "Loss_Q: [ 2.05  2.18  2.37  1.78  1.18  0.84  0.   10.39] Loss_P: [ 3.26  2.67  2.54  2.6   1.85  1.07  1.14 15.13]\n",
      "Loss_Q: [ 2.03  2.17  2.38  1.8   1.15  0.83  0.   10.36] Loss_P: [ 3.31  2.63  2.5   2.54  1.8   1.08  1.13 14.99]\n",
      "Loss_Q: [ 2.06  2.13  2.45  1.81  1.18  0.77  0.   10.41] Loss_P: [ 3.28  2.64  2.45  2.57  1.85  1.07  1.11 14.97]\n",
      "Loss_Q: [ 2.02  2.11  2.42  1.79  1.17  0.77  0.   10.28] Loss_P: [ 3.23  2.63  2.49  2.6   1.9   1.06  1.05 14.96]\n",
      "Loss_Q: [ 2.02  2.08  2.37  1.83  1.17  0.74  0.   10.22] Loss_P: [ 3.28  2.61  2.47  2.6   1.87  1.11  1.06 15.01]\n",
      "Loss_Q: [ 2.03  2.1   2.35  1.83  1.14  0.79  0.   10.25] Loss_P: [ 3.27  2.66  2.45  2.6   1.87  1.12  1.07 15.05]\n",
      "Loss_Q: [ 1.93  2.12  2.37  1.9   1.17  0.81  0.   10.31] Loss_P: [ 3.29  2.6   2.51  2.54  1.94  1.11  1.12 15.12]\n",
      "Loss_Q: [ 1.92  2.17  2.25  1.86  1.18  0.85  0.   10.23] Loss_P: [ 3.31  2.55  2.48  2.5   1.95  1.11  1.15 15.04]\n",
      "Loss_Q: [ 1.97  2.21  2.26  1.89  1.24  0.76  0.   10.34] Loss_P: [ 3.32  2.57  2.56  2.47  1.92  1.15  1.13 15.12]\n",
      "Loss_Q: [ 1.95  2.22  2.29  1.89  1.25  0.79  0.   10.39] Loss_P: [ 3.31  2.62  2.55  2.5   1.91  1.2   1.09 15.18]\n",
      "Loss_Q: [ 2.01  2.26  2.31  1.82  1.25  0.73  0.   10.37] Loss_P: [ 3.3   2.6   2.5   2.5   1.84  1.15  1.05 14.94]\n",
      "Loss_Q: [ 1.9   2.26  2.32  1.84  1.23  0.73  0.   10.28] Loss_P: [ 3.32  2.57  2.57  2.5   1.86  1.16  1.03 15.  ]\n",
      "Loss_Q: [ 2.03  2.29  2.33  1.82  1.23  0.72  0.   10.41] Loss_P: [ 3.29  2.64  2.6   2.52  1.87  1.17  1.01 15.1 ]\n",
      "Loss_Q: [ 2.03  2.3   2.34  1.87  1.2   0.71  0.   10.45] Loss_P: [ 3.29  2.71  2.62  2.47  1.92  1.12  1.01 15.13]\n",
      "Loss_Q: [ 1.97  2.26  2.26  1.89  1.2   0.71  0.   10.3 ] Loss_P: [ 3.29  2.66  2.64  2.42  1.93  1.11  1.04 15.08]\n",
      "Loss_Q: [ 1.96  2.22  2.25  1.88  1.2   0.73  0.   10.24] Loss_P: [ 3.27  2.64  2.65  2.48  1.9   1.11  1.04 15.1 ]\n",
      "Loss_Q: [1.87 2.12 2.22 1.83 1.2  0.72 0.   9.96] Loss_P: [ 3.31  2.53  2.44  2.4   1.9   1.1   1.01 14.68]\n",
      "Loss_Q: [1.99 2.04 2.14 1.79 1.24 0.69 0.   9.88] Loss_P: [ 3.32  2.6   2.44  2.26  1.86  1.1   1.02 14.59]\n",
      "Loss_Q: [2.   2.05 2.15 1.71 1.25 0.65 0.   9.8 ] Loss_P: [ 3.32  2.59  2.44  2.28  1.78  1.14  0.94 14.49]\n",
      "Loss_Q: [1.99 2.06 2.15 1.76 1.2  0.62 0.   9.78] Loss_P: [ 3.35  2.53  2.45  2.33  1.82  1.13  0.92 14.54]\n",
      "Loss_Q: [1.97 2.06 2.15 1.73 1.24 0.64 0.   9.8 ] Loss_P: [ 3.35  2.51  2.51  2.33  1.82  1.12  0.91 14.55]\n",
      "Loss_Q: [1.91 2.12 2.21 1.73 1.21 0.65 0.   9.83] Loss_P: [ 3.3   2.53  2.5   2.39  1.81  1.11  0.91 14.54]\n",
      "Loss_Q: [1.98 2.08 2.29 1.72 1.18 0.66 0.   9.9 ] Loss_P: [ 3.32  2.59  2.53  2.45  1.84  1.11  0.9  14.76]\n",
      "Loss_Q: [ 1.97  2.17  2.32  1.77  1.2   0.67  0.   10.1 ] Loss_P: [ 3.32  2.56  2.54  2.54  1.88  1.08  0.9  14.83]\n",
      "Loss_Q: [ 2.02  2.26  2.42  1.76  1.26  0.63  0.   10.34] Loss_P: [ 3.27  2.63  2.53  2.59  1.89  1.15  0.89 14.95]\n",
      "Loss_Q: [ 2.08  2.17  2.4   1.8   1.23  0.62  0.   10.3 ] Loss_P: [ 3.29  2.69  2.56  2.56  1.9   1.14  0.9  15.04]\n",
      "Loss_Q: [ 2.11  2.22  2.37  1.82  1.25  0.61  0.   10.38] Loss_P: [ 3.29  2.74  2.52  2.56  1.89  1.18  0.89 15.08]\n",
      "Loss_Q: [ 2.08  2.14  2.37  1.75  1.23  0.61  0.   10.18] Loss_P: [ 3.33  2.71  2.49  2.59  1.84  1.1   0.9  14.96]\n",
      "Loss_Q: [ 2.08  2.16  2.31  1.76  1.2   0.63  0.   10.14] Loss_P: [ 3.33  2.72  2.57  2.51  1.81  1.11  0.89 14.94]\n",
      "Loss_Q: [ 2.09  2.17  2.35  1.78  1.24  0.65  0.   10.29] Loss_P: [ 3.29  2.74  2.58  2.47  1.84  1.17  0.9  14.99]\n",
      "Loss_Q: [ 2.16  2.17  2.34  1.77  1.23  0.64  0.   10.32] Loss_P: [ 3.33  2.69  2.56  2.52  1.84  1.14  0.89 14.98]\n",
      "Loss_Q: [ 2.04  2.16  2.41  1.8   1.24  0.67  0.   10.33] Loss_P: [ 3.32  2.61  2.54  2.57  1.89  1.2   0.92 15.05]\n",
      "Loss_Q: [ 1.98  2.14  2.37  1.79  1.28  0.63  0.   10.19] Loss_P: [ 3.31  2.57  2.53  2.52  1.88  1.19  0.88 14.88]\n",
      "Loss_Q: [1.92 2.1  2.2  1.81 1.23 0.6  0.   9.86] Loss_P: [ 3.35  2.53  2.54  2.38  1.9   1.15  0.85 14.71]\n",
      "Loss_Q: [1.85 2.14 2.17 1.72 1.18 0.63 0.   9.69] Loss_P: [ 3.31  2.46  2.47  2.35  1.86  1.1   0.88 14.43]\n",
      "Loss_Q: [1.87 2.1  2.2  1.82 1.22 0.63 0.   9.84] Loss_P: [ 3.34  2.58  2.57  2.35  1.89  1.11  0.87 14.7 ]\n",
      "Loss_Q: [1.98 2.21 2.16 1.77 1.19 0.65 0.   9.96] Loss_P: [ 3.28  2.49  2.62  2.34  1.87  1.1   0.9  14.61]\n",
      "Loss_Q: [ 1.96  2.23  2.23  1.79  1.17  0.62  0.   10.  ] Loss_P: [ 3.31  2.6   2.58  2.39  1.86  1.1   0.88 14.72]\n",
      "Loss_Q: [ 1.98  2.26  2.22  1.76  1.12  0.65  0.   10.  ] Loss_P: [ 3.27  2.64  2.67  2.45  1.86  1.01  0.86 14.75]\n",
      "Loss_Q: [1.98 2.22 2.19 1.78 1.13 0.62 0.   9.93] Loss_P: [ 3.29  2.64  2.66  2.41  1.93  1.07  0.86 14.86]\n",
      "Loss_Q: [ 2.02  2.22  2.3   1.83  1.16  0.61  0.   10.15] Loss_P: [ 3.24  2.69  2.66  2.41  1.92  1.08  0.87 14.86]\n",
      "Loss_Q: [ 2.01  2.17  2.26  1.84  1.17  0.58  0.   10.03] Loss_P: [ 3.28  2.67  2.65  2.44  1.93  1.16  0.84 14.97]\n",
      "Loss_Q: [1.97 2.13 2.12 1.77 1.17 0.59 0.   9.76] Loss_P: [ 3.27  2.61  2.57  2.26  1.9   1.11  0.79 14.52]\n",
      "Loss_Q: [1.96 2.16 2.1  1.83 1.14 0.56 0.   9.75] Loss_P: [ 3.27  2.59  2.65  2.3   1.92  1.15  0.83 14.71]\n",
      "Loss_Q: [2.02 2.21 2.15 1.78 1.22 0.58 0.   9.96] Loss_P: [ 3.23  2.65  2.66  2.28  1.87  1.21  0.8  14.7 ]\n",
      "Loss_Q: [ 2.03  2.32  2.15  1.74  1.24  0.59  0.   10.08] Loss_P: [ 3.28  2.7   2.75  2.24  1.82  1.21  0.7  14.69]\n",
      "Loss_Q: [ 2.04  2.27  2.16  1.8   1.25  0.56  0.   10.07] Loss_P: [ 3.22  2.69  2.73  2.32  1.9   1.2   0.73 14.79]\n",
      "Loss_Q: [ 2.06  2.32  2.1   1.77  1.22  0.58  0.   10.05] Loss_P: [ 3.22  2.7   2.86  2.24  1.86  1.18  0.79 14.85]\n",
      "Loss_Q: [ 2.09  2.29  2.16  1.73  1.22  0.57  0.   10.06] Loss_P: [ 3.25  2.65  2.75  2.28  1.88  1.2   0.76 14.76]\n",
      "Loss_Q: [1.92 2.3  2.07 1.76 1.24 0.56 0.   9.86] Loss_P: [ 3.27  2.51  2.81  2.24  1.88  1.17  0.76 14.63]\n",
      "Loss_Q: [1.93 2.29 2.09 1.79 1.25 0.57 0.   9.92] Loss_P: [ 3.26  2.58  2.78  2.21  1.84  1.2   0.76 14.63]\n",
      "Loss_Q: [1.88 2.25 2.02 1.85 1.23 0.59 0.   9.82] Loss_P: [ 3.29  2.55  2.74  2.17  1.9   1.2   0.82 14.66]\n",
      "Loss_Q: [ 1.89  2.32  2.13  1.83  1.27  0.57  0.   10.02] Loss_P: [ 3.27  2.5   2.73  2.22  1.95  1.18  0.81 14.65]\n",
      "Loss_Q: [1.9  2.24 2.14 1.77 1.3  0.57 0.   9.93] Loss_P: [ 3.27  2.48  2.66  2.31  1.84  1.19  0.8  14.54]\n",
      "Loss_Q: [1.85 2.23 2.   1.78 1.27 0.57 0.   9.7 ] Loss_P: [ 3.33  2.46  2.64  2.24  1.83  1.21  0.83 14.54]\n",
      "Loss_Q: [1.91 2.18 1.99 1.76 1.25 0.62 0.   9.72] Loss_P: [ 3.28  2.58  2.6   2.16  1.85  1.18  0.8  14.44]\n",
      "Loss_Q: [1.9  2.1  1.95 1.75 1.28 0.58 0.   9.56] Loss_P: [ 3.29  2.59  2.57  2.19  1.84  1.18  0.82 14.47]\n",
      "Loss_Q: [1.98 2.12 2.17 1.79 1.26 0.57 0.   9.9 ] Loss_P: [ 3.27  2.67  2.58  2.33  1.88  1.21  0.8  14.74]\n",
      "Loss_Q: [ 2.01  2.2   2.2   1.75  1.3   0.57  0.   10.03] Loss_P: [ 3.25  2.62  2.57  2.44  1.78  1.24  0.76 14.66]\n",
      "Loss_Q: [ 2.01  2.24  2.21  1.8   1.35  0.58  0.   10.19] Loss_P: [ 3.24  2.64  2.6   2.38  1.87  1.22  0.79 14.74]\n",
      "Loss_Q: [ 1.98  2.21  2.2   1.79  1.34  0.61  0.   10.12] Loss_P: [ 3.22  2.66  2.62  2.41  1.91  1.22  0.82 14.87]\n",
      "Loss_Q: [ 2.09  2.14  2.16  1.78  1.32  0.61  0.   10.09] Loss_P: [ 3.25  2.7   2.58  2.29  1.9   1.19  0.84 14.75]\n",
      "Loss_Q: [1.98 2.08 2.08 1.75 1.24 0.6  0.   9.74] Loss_P: [ 3.24  2.68  2.48  2.22  1.86  1.17  0.82 14.47]\n",
      "Loss_Q: [1.91 1.96 2.06 1.73 1.27 0.58 0.   9.51] Loss_P: [ 3.25  2.65  2.43  2.24  1.84  1.15  0.83 14.39]\n",
      "Loss_Q: [1.93 2.02 2.2  1.75 1.26 0.59 0.   9.74] Loss_P: [ 3.28  2.59  2.45  2.33  1.82  1.19  0.81 14.48]\n",
      "Loss_Q: [1.88 2.03 2.25 1.83 1.3  0.58 0.   9.86] Loss_P: [ 3.24  2.57  2.48  2.44  1.89  1.17  0.84 14.65]\n",
      "Loss_Q: [1.94 2.01 2.27 1.78 1.24 0.56 0.   9.8 ] Loss_P: [ 3.26  2.52  2.47  2.47  1.93  1.15  0.81 14.59]\n",
      "Loss_Q: [1.89 2.02 2.27 1.85 1.27 0.57 0.   9.87] Loss_P: [ 3.21  2.6   2.48  2.42  1.91  1.18  0.78 14.58]\n",
      "Loss_Q: [1.99 2.05 2.26 1.81 1.25 0.55 0.   9.91] Loss_P: [ 3.23  2.63  2.52  2.4   1.88  1.17  0.76 14.58]\n",
      "Loss_Q: [1.94 2.07 2.25 1.77 1.25 0.54 0.   9.83] Loss_P: [ 3.21  2.68  2.49  2.39  1.89  1.23  0.73 14.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [2.03 2.02 2.17 1.77 1.27 0.55 0.   9.8 ] Loss_P: [ 3.26  2.7   2.47  2.28  1.88  1.22  0.74 14.56]\n",
      "Loss_Q: [2.1  2.02 2.22 1.78 1.28 0.54 0.   9.94] Loss_P: [ 3.22  2.84  2.48  2.32  1.87  1.17  0.72 14.61]\n",
      "Loss_Q: [2.05 2.01 2.15 1.76 1.3  0.51 0.   9.78] Loss_P: [ 3.23  2.73  2.51  2.29  1.86  1.22  0.7  14.53]\n",
      "Loss_Q: [2.14 2.01 2.18 1.73 1.29 0.52 0.   9.87] Loss_P: [ 3.21  2.77  2.53  2.33  1.89  1.21  0.71 14.66]\n",
      "Loss_Q: [2.02 2.01 2.16 1.8  1.28 0.51 0.   9.78] Loss_P: [ 3.25  2.64  2.51  2.35  1.92  1.18  0.75 14.6 ]\n",
      "Loss_Q: [2.03 1.97 2.09 1.76 1.27 0.53 0.   9.65] Loss_P: [ 3.22  2.72  2.42  2.22  1.82  1.18  0.69 14.28]\n",
      "Loss_Q: [1.92 1.98 2.03 1.74 1.27 0.56 0.   9.5 ] Loss_P: [ 3.25  2.65  2.4   2.22  1.82  1.19  0.72 14.24]\n",
      "Loss_Q: [1.9  1.94 2.01 1.75 1.29 0.54 0.   9.43] Loss_P: [ 3.24  2.63  2.39  2.17  1.86  1.16  0.76 14.2 ]\n",
      "Loss_Q: [2.04 2.03 1.99 1.77 1.23 0.6  0.   9.66] Loss_P: [ 3.26  2.66  2.47  2.15  1.9   1.13  0.8  14.37]\n",
      "Loss_Q: [2.02 2.06 1.96 1.83 1.29 0.6  0.   9.76] Loss_P: [ 3.21  2.67  2.57  2.05  1.93  1.2   0.82 14.44]\n",
      "Loss_Q: [1.97 2.12 2.07 1.83 1.23 0.57 0.   9.79] Loss_P: [ 3.22  2.69  2.59  2.24  1.95  1.17  0.79 14.64]\n",
      "Loss_Q: [1.9  2.12 2.15 1.86 1.21 0.59 0.   9.83] Loss_P: [ 3.21  2.65  2.55  2.3   1.95  1.17  0.77 14.59]\n",
      "Loss_Q: [ 2.03  2.18  2.09  1.88  1.23  0.62  0.   10.03] Loss_P: [ 3.19  2.73  2.7   2.3   1.99  1.17  0.82 14.9 ]\n",
      "Loss_Q: [ 2.04  2.17  2.22  1.88  1.18  0.57  0.   10.06] Loss_P: [ 3.21  2.7   2.6   2.3   2.02  1.15  0.79 14.76]\n",
      "Loss_Q: [1.94 2.04 2.2  1.82 1.14 0.58 0.   9.72] Loss_P: [ 3.24  2.67  2.49  2.33  2.01  1.15  0.81 14.7 ]\n",
      "Loss_Q: [1.97 1.98 2.11 1.86 1.19 0.59 0.   9.71] Loss_P: [ 3.29  2.65  2.41  2.24  2.01  1.13  0.78 14.51]\n",
      "Loss_Q: [1.95 2.06 2.09 1.8  1.16 0.56 0.   9.61] Loss_P: [ 3.26  2.65  2.44  2.23  2.    1.09  0.78 14.45]\n",
      "Loss_Q: [1.98 2.06 2.15 1.85 1.16 0.55 0.   9.75] Loss_P: [ 3.28  2.63  2.52  2.22  1.99  1.06  0.77 14.48]\n",
      "Loss_Q: [1.98 2.05 2.08 1.81 1.18 0.56 0.   9.66] Loss_P: [ 3.28  2.63  2.47  2.2   2.01  1.1   0.8  14.5 ]\n",
      "Loss_Q: [2.   2.1  2.1  1.88 1.2  0.61 0.   9.88] Loss_P: [ 3.32  2.61  2.51  2.18  2.01  1.1   0.8  14.54]\n",
      "Loss_Q: [1.97 2.07 2.14 1.86 1.12 0.56 0.   9.72] Loss_P: [ 3.36  2.55  2.52  2.29  2.02  1.05  0.81 14.62]\n",
      "Loss_Q: [1.88 2.13 2.16 1.9  1.2  0.59 0.   9.85] Loss_P: [ 3.29  2.51  2.57  2.38  2.01  1.11  0.81 14.7 ]\n",
      "Loss_Q: [1.92 2.11 2.23 1.91 1.18 0.61 0.   9.97] Loss_P: [ 3.26  2.57  2.56  2.41  2.04  1.09  0.83 14.75]\n",
      "Loss_Q: [1.98 2.09 2.19 1.88 1.09 0.58 0.   9.8 ] Loss_P: [ 3.26  2.7   2.52  2.39  2.03  1.04  0.83 14.77]\n",
      "Loss_Q: [1.92 2.11 2.2  1.91 1.09 0.58 0.   9.81] Loss_P: [ 3.22  2.6   2.52  2.3   2.03  0.98  0.81 14.47]\n",
      "Loss_Q: [1.97 2.14 2.18 1.88 1.12 0.55 0.   9.84] Loss_P: [ 3.22  2.62  2.51  2.33  2.08  1.03  0.8  14.59]\n",
      "Loss_Q: [2.02 2.22 2.12 1.9  1.1  0.56 0.   9.92] Loss_P: [ 3.25  2.65  2.56  2.29  2.04  1.01  0.82 14.61]\n",
      "Loss_Q: [2.   2.11 2.17 1.89 1.1  0.58 0.   9.86] Loss_P: [ 3.21  2.68  2.54  2.36  2.03  1.    0.81 14.63]\n",
      "Loss_Q: [2.1  2.15 2.21 1.86 1.1  0.56 0.   9.99] Loss_P: [ 3.28  2.75  2.54  2.4   2.01  0.95  0.81 14.74]\n",
      "Loss_Q: [2.08 2.22 2.17 1.88 1.06 0.56 0.   9.96] Loss_P: [ 3.22  2.7   2.61  2.36  2.01  0.99  0.79 14.68]\n",
      "Loss_Q: [1.98 2.23 2.18 1.87 1.09 0.57 0.   9.93] Loss_P: [ 3.25  2.71  2.61  2.42  2.01  1.07  0.79 14.86]\n",
      "Loss_Q: [ 2.05  2.24  2.24  1.84  1.12  0.55  0.   10.05] Loss_P: [ 3.26  2.69  2.64  2.39  2.03  1.05  0.79 14.85]\n",
      "Loss_Q: [ 2.02  2.24  2.22  1.87  1.1   0.58  0.   10.03] Loss_P: [ 3.23  2.71  2.65  2.37  2.03  1.02  0.81 14.81]\n",
      "Loss_Q: [2.01 2.19 2.22 1.8  1.1  0.58 0.   9.9 ] Loss_P: [ 3.24  2.66  2.58  2.44  1.99  1.03  0.79 14.72]\n",
      "Loss_Q: [1.93 2.13 2.24 1.83 1.07 0.58 0.   9.78] Loss_P: [ 3.27  2.64  2.57  2.38  1.97  0.99  0.8  14.62]\n",
      "Loss_Q: [1.92 2.07 2.18 1.83 1.02 0.6  0.   9.62] Loss_P: [ 3.3   2.57  2.51  2.38  2.02  1.    0.81 14.58]\n",
      "Loss_Q: [1.97 2.11 2.15 1.79 1.03 0.62 0.   9.66] Loss_P: [ 3.29  2.51  2.52  2.32  1.99  0.99  0.82 14.44]\n",
      "Loss_Q: [1.88 2.26 2.12 1.79 1.09 0.62 0.   9.75] Loss_P: [ 3.28  2.52  2.61  2.29  1.98  1.04  0.85 14.58]\n",
      "Loss_Q: [2.06 2.17 2.18 1.79 1.11 0.6  0.   9.92] Loss_P: [ 3.27  2.66  2.56  2.33  1.95  1.07  0.85 14.69]\n",
      "Loss_Q: [ 1.98  2.18  2.37  1.73  1.16  0.61  0.   10.04] Loss_P: [ 3.21  2.73  2.62  2.55  1.92  1.11  0.85 14.98]\n",
      "Loss_Q: [ 2.03  2.2   2.38  1.81  1.26  0.64  0.   10.32] Loss_P: [ 3.21  2.68  2.59  2.59  1.94  1.18  0.82 15.02]\n",
      "Loss_Q: [ 2.05  2.19  2.36  1.77  1.25  0.63  0.   10.26] Loss_P: [ 3.24  2.65  2.6   2.51  1.93  1.14  0.84 14.91]\n",
      "Loss_Q: [1.98 2.13 2.29 1.76 1.18 0.59 0.   9.93] Loss_P: [ 3.27  2.62  2.49  2.52  1.94  1.11  0.85 14.78]\n",
      "Loss_Q: [1.89 2.11 2.31 1.81 1.14 0.57 0.   9.83] Loss_P: [ 3.24  2.59  2.49  2.45  1.91  1.09  0.82 14.59]\n",
      "Loss_Q: [1.92 2.13 2.34 1.81 1.15 0.59 0.   9.95] Loss_P: [ 3.24  2.64  2.56  2.51  2.04  1.13  0.83 14.94]\n",
      "Loss_Q: [1.92 2.09 2.34 1.79 1.2  0.61 0.   9.95] Loss_P: [ 3.27  2.57  2.46  2.5   1.99  1.15  0.8  14.75]\n",
      "Loss_Q: [1.85 2.08 2.17 1.87 1.15 0.61 0.   9.74] Loss_P: [ 3.24  2.47  2.49  2.35  1.99  1.14  0.85 14.54]\n",
      "Loss_Q: [1.87 2.09 2.25 1.84 1.17 0.61 0.   9.84] Loss_P: [ 3.26  2.55  2.49  2.41  1.99  1.11  0.82 14.63]\n",
      "Loss_Q: [1.82 2.06 2.23 1.77 1.13 0.62 0.   9.64] Loss_P: [ 3.25  2.47  2.46  2.42  1.87  1.09  0.81 14.37]\n",
      "Loss_Q: [1.75 1.94 2.26 1.74 1.14 0.61 0.   9.44] Loss_P: [ 3.23  2.44  2.35  2.38  1.86  1.11  0.82 14.2 ]\n",
      "Loss_Q: [1.81 1.98 2.27 1.79 1.16 0.56 0.   9.58] Loss_P: [ 3.26  2.43  2.39  2.42  1.94  1.14  0.77 14.36]\n",
      "Loss_Q: [1.82 2.   2.33 1.79 1.12 0.57 0.   9.62] Loss_P: [ 3.24  2.45  2.34  2.41  1.85  1.11  0.8  14.2 ]\n",
      "Loss_Q: [1.72 1.94 2.32 1.73 1.17 0.59 0.   9.48] Loss_P: [ 3.29  2.39  2.35  2.47  1.86  1.09  0.8  14.25]\n",
      "Loss_Q: [1.73 1.87 2.19 1.69 1.12 0.6  0.   9.2 ] Loss_P: [ 3.23  2.36  2.27  2.37  1.85  1.07  0.78 13.94]\n",
      "Loss_Q: [1.71 1.85 2.23 1.67 1.11 0.56 0.   9.13] Loss_P: [ 3.23  2.42  2.25  2.38  1.77  1.1   0.77 13.91]\n",
      "Loss_Q: [1.76 1.94 2.27 1.65 1.11 0.58 0.   9.31] Loss_P: [ 3.22  2.39  2.23  2.36  1.74  1.1   0.79 13.84]\n",
      "Loss_Q: [1.7  1.93 2.14 1.69 1.15 0.58 0.   9.19] Loss_P: [ 3.26  2.36  2.29  2.29  1.8   1.11  0.78 13.89]\n",
      "Loss_Q: [1.78 1.92 2.1  1.73 1.15 0.58 0.   9.26] Loss_P: [ 3.31  2.44  2.33  2.28  1.84  1.15  0.78 14.12]\n",
      "Loss_Q: [1.69 1.92 2.08 1.7  1.11 0.57 0.   9.06] Loss_P: [ 3.34  2.36  2.3   2.18  1.8   1.11  0.76 13.84]\n",
      "Loss_Q: [1.71 1.88 2.08 1.71 1.06 0.57 0.   9.  ] Loss_P: [ 3.31  2.35  2.23  2.21  1.75  1.04  0.77 13.65]\n",
      "Loss_Q: [1.69 1.93 2.11 1.68 1.04 0.54 0.   9.  ] Loss_P: [ 3.24  2.47  2.31  2.26  1.82  1.07  0.78 13.94]\n",
      "Loss_Q: [1.79 1.94 2.17 1.67 1.1  0.56 0.   9.23] Loss_P: [ 3.25  2.46  2.31  2.27  1.74  1.1   0.77 13.89]\n",
      "Loss_Q: [1.71 1.94 2.2  1.59 1.11 0.57 0.   9.11] Loss_P: [ 3.27  2.43  2.37  2.37  1.69  1.09  0.77 13.99]\n",
      "Loss_Q: [1.59 1.96 2.1  1.61 1.11 0.56 0.   8.94] Loss_P: [ 3.28  2.27  2.36  2.27  1.66  1.13  0.76 13.72]\n",
      "Loss_Q: [1.51 1.86 2.13 1.56 1.09 0.57 0.   8.72] Loss_P: [ 3.25  2.24  2.29  2.22  1.62  1.09  0.76 13.47]\n",
      "Loss_Q: [1.49 1.83 2.06 1.53 1.14 0.57 0.   8.63] Loss_P: [ 3.25  2.24  2.23  2.21  1.58  1.09  0.74 13.33]\n",
      "Loss_Q: [1.58 1.86 2.22 1.45 1.08 0.54 0.   8.72] Loss_P: [ 3.29  2.23  2.28  2.34  1.48  1.05  0.75 13.43]\n",
      "Loss_Q: [1.42 1.79 2.17 1.43 1.02 0.57 0.   8.4 ] Loss_P: [ 3.31  2.16  2.24  2.26  1.45  0.99  0.76 13.17]\n",
      "Loss_Q: [1.5  1.82 2.15 1.51 1.04 0.54 0.   8.56] Loss_P: [ 3.29  2.15  2.26  2.23  1.56  0.99  0.75 13.23]\n",
      "Loss_Q: [1.53 1.78 2.04 1.44 0.96 0.54 0.   8.3 ] Loss_P: [ 3.32  2.24  2.23  2.19  1.48  0.97  0.71 13.15]\n",
      "Loss_Q: [1.45 1.78 1.96 1.37 0.91 0.54 0.   8.01] Loss_P: [ 3.3   2.19  2.21  2.08  1.43  0.88  0.71 12.8 ]\n",
      "Loss_Q: [1.55 1.8  1.97 1.34 0.92 0.53 0.   8.12] Loss_P: [ 3.35  2.21  2.23  2.14  1.45  0.86  0.71 12.95]\n",
      "Loss_Q: [1.61 1.82 1.96 1.42 1.01 0.53 0.   8.35] Loss_P: [ 3.31  2.32  2.25  2.14  1.46  0.93  0.72 13.14]\n",
      "Loss_Q: [1.67 1.88 1.92 1.39 1.01 0.55 0.   8.42] Loss_P: [ 3.33  2.33  2.28  2.08  1.43  0.93  0.7  13.1 ]\n",
      "Loss_Q: [1.77 1.83 2.03 1.48 1.02 0.52 0.   8.64] Loss_P: [ 3.27  2.45  2.28  2.1   1.56  0.96  0.71 13.33]\n",
      "Loss_Q: [1.74 1.82 2.03 1.55 1.05 0.55 0.   8.75] Loss_P: [ 3.26  2.39  2.25  2.15  1.6   1.06  0.74 13.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.71 1.87 2.16 1.55 1.07 0.54 0.   8.91] Loss_P: [ 3.29  2.36  2.26  2.28  1.63  1.02  0.73 13.57]\n",
      "Loss_Q: [1.71 1.81 2.13 1.54 1.12 0.54 0.   8.85] Loss_P: [ 3.29  2.36  2.21  2.3   1.58  1.01  0.72 13.47]\n",
      "Loss_Q: [1.71 1.74 2.06 1.5  1.11 0.55 0.   8.67] Loss_P: [ 3.33  2.34  2.17  2.19  1.54  1.04  0.77 13.37]\n",
      "Loss_Q: [1.65 1.73 2.07 1.51 1.15 0.55 0.   8.67] Loss_P: [ 3.33  2.31  2.1   2.27  1.57  1.12  0.79 13.5 ]\n",
      "Loss_Q: [1.66 1.82 2.09 1.5  1.14 0.57 0.   8.78] Loss_P: [ 3.28  2.28  2.17  2.29  1.56  1.14  0.78 13.5 ]\n",
      "Loss_Q: [1.62 1.81 2.15 1.56 1.12 0.62 0.   8.88] Loss_P: [ 3.36  2.26  2.15  2.31  1.62  1.07  0.8  13.57]\n",
      "Loss_Q: [1.68 1.86 2.18 1.57 1.1  0.65 0.   9.04] Loss_P: [ 3.32  2.32  2.24  2.37  1.63  1.03  0.82 13.74]\n",
      "Loss_Q: [1.68 1.84 2.12 1.57 1.08 0.61 0.   8.9 ] Loss_P: [ 3.32  2.31  2.25  2.28  1.63  1.07  0.82 13.67]\n",
      "Loss_Q: [1.66 1.9  2.11 1.55 1.14 0.6  0.   8.96] Loss_P: [ 3.29  2.29  2.22  2.3   1.61  1.05  0.81 13.57]\n",
      "Loss_Q: [1.64 1.79 2.05 1.49 1.03 0.6  0.   8.61] Loss_P: [ 3.33  2.3   2.24  2.17  1.6   1.02  0.8  13.45]\n",
      "Loss_Q: [1.68 1.85 2.08 1.46 1.04 0.6  0.   8.72] Loss_P: [ 3.3   2.23  2.27  2.25  1.56  1.03  0.77 13.4 ]\n",
      "Loss_Q: [1.63 1.86 2.14 1.45 1.06 0.6  0.   8.74] Loss_P: [ 3.31  2.22  2.26  2.32  1.53  1.03  0.77 13.44]\n",
      "Loss_Q: [1.65 1.8  2.14 1.55 1.12 0.6  0.   8.86] Loss_P: [ 3.26  2.29  2.24  2.36  1.59  1.09  0.77 13.59]\n",
      "Loss_Q: [1.67 1.84 2.14 1.46 1.05 0.59 0.   8.75] Loss_P: [ 3.33  2.28  2.18  2.28  1.57  1.05  0.77 13.45]\n",
      "Loss_Q: [1.74 1.83 2.13 1.45 1.12 0.62 0.   8.89] Loss_P: [ 3.31  2.37  2.13  2.23  1.48  1.05  0.76 13.33]\n",
      "Loss_Q: [1.66 1.76 2.07 1.53 1.12 0.63 0.   8.77] Loss_P: [ 3.27  2.35  2.19  2.25  1.6   1.15  0.78 13.6 ]\n",
      "Loss_Q: [1.67 1.8  2.08 1.54 1.16 0.63 0.   8.89] Loss_P: [ 3.31  2.33  2.2   2.2   1.61  1.11  0.77 13.52]\n",
      "Loss_Q: [1.68 1.88 2.07 1.57 1.09 0.63 0.   8.92] Loss_P: [ 3.32  2.23  2.21  2.22  1.66  1.05  0.84 13.51]\n",
      "Loss_Q: [1.67 1.83 2.17 1.59 1.08 0.67 0.   9.01] Loss_P: [ 3.34  2.28  2.22  2.3   1.69  1.05  0.83 13.71]\n",
      "Loss_Q: [1.68 1.82 2.17 1.6  1.07 0.67 0.   9.  ] Loss_P: [ 3.32  2.34  2.22  2.37  1.71  1.02  0.86 13.85]\n",
      "Loss_Q: [1.65 1.71 2.13 1.56 1.03 0.68 0.   8.76] Loss_P: [ 3.33  2.31  2.19  2.28  1.67  1.07  0.87 13.72]\n",
      "Loss_Q: [1.64 1.87 2.19 1.59 1.12 0.67 0.   9.08] Loss_P: [ 3.3   2.3   2.23  2.34  1.65  1.11  0.85 13.78]\n",
      "Loss_Q: [1.67 1.89 2.18 1.65 1.16 0.65 0.   9.21] Loss_P: [ 3.32  2.29  2.3   2.35  1.71  1.17  0.86 14.01]\n",
      "Loss_Q: [1.73 1.82 2.1  1.61 1.17 0.68 0.   9.1 ] Loss_P: [ 3.33  2.34  2.27  2.23  1.61  1.1   0.85 13.72]\n",
      "Loss_Q: [1.76 1.82 2.16 1.59 1.12 0.63 0.   9.08] Loss_P: [ 3.27  2.45  2.28  2.29  1.61  1.07  0.86 13.83]\n",
      "Loss_Q: [1.73 1.8  2.18 1.59 1.06 0.6  0.   8.96] Loss_P: [ 3.31  2.34  2.28  2.38  1.64  1.05  0.8  13.81]\n",
      "Loss_Q: [1.65 1.81 2.27 1.61 1.08 0.61 0.   9.03] Loss_P: [ 3.3   2.29  2.23  2.4   1.69  1.06  0.82 13.79]\n",
      "Loss_Q: [1.62 1.88 2.27 1.61 1.08 0.61 0.   9.07] Loss_P: [ 3.34  2.23  2.27  2.4   1.64  1.05  0.81 13.76]\n",
      "Loss_Q: [1.58 1.83 2.29 1.68 1.08 0.65 0.   9.11] Loss_P: [ 3.32  2.21  2.28  2.44  1.73  1.06  0.85 13.88]\n",
      "Loss_Q: [1.65 1.94 2.18 1.64 1.12 0.65 0.   9.19] Loss_P: [ 3.3   2.28  2.29  2.37  1.73  1.12  0.83 13.91]\n",
      "Loss_Q: [1.59 1.94 2.1  1.72 1.16 0.66 0.   9.17] Loss_P: [ 3.31  2.27  2.36  2.23  1.8   1.12  0.86 13.95]\n",
      "Loss_Q: [1.57 1.88 2.08 1.69 1.14 0.65 0.   9.02] Loss_P: [ 3.33  2.26  2.38  2.26  1.74  1.15  0.87 13.99]\n",
      "Loss_Q: [1.62 1.87 2.11 1.64 1.15 0.72 0.   9.11] Loss_P: [ 3.27  2.26  2.28  2.3   1.69  1.13  0.88 13.82]\n",
      "Loss_Q: [1.62 1.91 2.12 1.62 1.12 0.74 0.   9.12] Loss_P: [ 3.28  2.28  2.32  2.32  1.68  1.09  0.92 13.87]\n",
      "Loss_Q: [1.67 1.86 2.26 1.63 1.04 0.69 0.   9.15] Loss_P: [ 3.3   2.28  2.29  2.41  1.64  1.05  0.91 13.88]\n",
      "Loss_Q: [1.63 1.91 2.21 1.6  1.03 0.69 0.   9.07] Loss_P: [ 3.29  2.31  2.35  2.42  1.72  1.03  0.91 14.02]\n",
      "Loss_Q: [1.61 1.84 2.26 1.66 1.05 0.74 0.   9.16] Loss_P: [ 3.26  2.32  2.22  2.39  1.7   1.01  0.96 13.86]\n",
      "Loss_Q: [1.65 1.81 2.24 1.66 1.06 0.76 0.   9.18] Loss_P: [ 3.29  2.3   2.22  2.36  1.72  0.99  0.97 13.85]\n",
      "Loss_Q: [1.76 1.88 2.18 1.61 1.08 0.71 0.   9.22] Loss_P: [ 3.32  2.34  2.24  2.33  1.65  1.05  0.92 13.85]\n",
      "Loss_Q: [1.67 1.77 2.11 1.56 1.07 0.71 0.   8.89] Loss_P: [ 3.3   2.32  2.18  2.26  1.65  1.04  0.93 13.68]\n",
      "Loss_Q: [1.63 1.78 2.09 1.6  1.07 0.71 0.   8.88] Loss_P: [ 3.33  2.27  2.23  2.31  1.62  1.02  0.92 13.7 ]\n",
      "Loss_Q: [1.64 1.84 2.18 1.61 1.06 0.7  0.   9.04] Loss_P: [ 3.32  2.28  2.28  2.38  1.66  1.06  0.89 13.86]\n",
      "Loss_Q: [1.69 1.89 2.19 1.6  1.1  0.68 0.   9.15] Loss_P: [ 3.29  2.31  2.28  2.33  1.55  1.09  0.88 13.74]\n",
      "Loss_Q: [1.61 1.66 2.12 1.56 1.1  0.67 0.   8.73] Loss_P: [ 3.34  2.29  2.18  2.3   1.55  1.07  0.88 13.62]\n",
      "Loss_Q: [1.71 1.73 2.16 1.51 1.09 0.69 0.   8.89] Loss_P: [ 3.3   2.27  2.21  2.41  1.58  1.    0.89 13.66]\n",
      "Loss_Q: [1.81 1.8  2.16 1.59 1.09 0.73 0.   9.17] Loss_P: [ 3.28  2.36  2.22  2.3   1.58  1.02  0.93 13.7 ]\n",
      "Loss_Q: [1.83 1.72 2.24 1.61 1.13 0.66 0.   9.19] Loss_P: [ 3.29  2.46  2.2   2.36  1.61  1.04  0.92 13.87]\n",
      "Loss_Q: [1.8  1.62 2.09 1.57 1.14 0.66 0.   8.88] Loss_P: [ 3.3   2.42  2.11  2.28  1.6   1.12  0.92 13.75]\n",
      "Loss_Q: [1.77 1.67 2.   1.54 1.18 0.67 0.   8.85] Loss_P: [ 3.24  2.48  2.18  2.24  1.56  1.18  0.9  13.78]\n",
      "Loss_Q: [1.72 1.64 2.04 1.55 1.13 0.68 0.   8.76] Loss_P: [ 3.31  2.29  2.1   2.24  1.54  1.07  0.9  13.46]\n",
      "Loss_Q: [1.68 1.71 2.03 1.51 1.16 0.69 0.   8.79] Loss_P: [ 3.33  2.3   2.14  2.22  1.54  1.1   0.93 13.55]\n",
      "Loss_Q: [1.69 1.69 2.03 1.54 1.1  0.7  0.   8.75] Loss_P: [ 3.29  2.28  2.17  2.19  1.53  1.02  0.9  13.38]\n",
      "Loss_Q: [1.71 1.67 1.95 1.5  1.03 0.67 0.   8.52] Loss_P: [ 3.32  2.31  2.14  2.15  1.53  0.98  0.89 13.31]\n",
      "Loss_Q: [1.71 1.69 1.98 1.54 1.02 0.67 0.   8.62] Loss_P: [ 3.35  2.32  2.14  2.19  1.58  0.99  0.9  13.47]\n"
     ]
    }
   ],
   "source": [
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3c84f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "\n",
    "distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,dataset)\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "71875d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f8ad071fd0>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - values_d.size))] = values_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "1910a72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.8083,\n",
       " 'FN': array([ 38,  41,  74,  93, 123, 159, 163, 171, 183, 233, 241, 297, 383,\n",
       "        395, 450, 475, 479, 481, 493, 494, 508, 513, 514, 515, 536, 555,\n",
       "        560, 562, 564, 567, 572, 574, 594, 598, 600, 615, 618]),\n",
       " 'n_fn': 37,\n",
       " 'FP': array([[ 627,  629,  631, ..., 4093, 4094, 4095],\n",
       "        [   1,    2,    1, ...,    1,    3,    5]], dtype=int64),\n",
       " 'n_fp': 1192}"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "30a9c6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31989347"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60885f5b",
   "metadata": {},
   "source": [
    "The result is too good. Maybe by self-organization we find a subspace of the entire space that fits the given Helmholtz machine structure best, so that it converges easily to this dataset. In the next notebook I will look closer to the internal structure of this model and see if there is an immediate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124667c",
   "metadata": {},
   "source": [
    "For self-organizing system, we are not restricted to a fixed dataset. We can use pure samples as data in each updation, which forgets the original dataset as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "996478f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 627)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = np.load('self_org_parameters.npy',allow_pickle=True).item()\n",
    "Phi = para['Phi']\n",
    "Theta = para['Theta']\n",
    "dataset = np.load('self_org_dataset.npy')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "5452d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 10\n",
    "n = n_dz[0,0]\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "e81188c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_rate = 100 # after how many epochs we update evidence\n",
    "n_new_sample = 880 # how many samples are chosen as evidence at each update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "b0f6b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sampled: 20 dataset size: 880 Loss_Q: [1.73 2.15 1.98 1.66 1.43 0.9  0.   9.85] Loss_P: [ 2.94  2.53  2.38  2.21  1.87  1.41  1.13 14.49]\n",
      "New sampled: 8 dataset size: 880 Loss_Q: [ 1.74  2.2   2.05  1.71  1.49  0.97  0.   10.16] Loss_P: [ 3.01  2.57  2.45  2.31  1.92  1.49  1.21 14.95]\n",
      "New sampled: 5 dataset size: 880 Loss_Q: [ 1.66  2.2   2.11  1.76  1.55  0.93  0.   10.21] Loss_P: [ 3.01  2.49  2.44  2.4   1.96  1.53  1.15 14.98]\n",
      "New sampled: 1 dataset size: 880 Loss_Q: [1.6  2.19 2.11 1.71 1.47 0.87 0.   9.96] Loss_P: [ 2.99  2.44  2.46  2.42  1.85  1.48  1.08 14.72]\n",
      "New sampled: 1 dataset size: 880 Loss_Q: [1.46 2.18 1.93 1.7  1.34 0.85 0.   9.47] Loss_P: [ 3.02  2.29  2.45  2.22  1.89  1.35  1.02 14.23]\n",
      "New sampled: 0 dataset size: 880 Loss_Q: [1.5  2.1  2.06 1.68 1.38 1.05 0.   9.76] Loss_P: [ 3.    2.33  2.36  2.34  1.83  1.45  1.24 14.55]\n",
      "New sampled: 0 dataset size: 880 Loss_Q: [1.61 1.96 1.93 1.62 1.42 0.98 0.   9.51] Loss_P: [ 2.95  2.45  2.23  2.2   1.83  1.46  1.18 14.3 ]\n",
      "New sampled: 0 dataset size: 880 Loss_Q: [1.67 1.98 1.97 1.57 1.39 0.94 0.   9.51] Loss_P: [ 3.    2.46  2.27  2.21  1.79  1.44  1.13 14.3 ]\n",
      "New sampled: 0 dataset size: 880 Loss_Q: [1.75 2.05 2.07 1.59 1.43 1.03 0.   9.92] Loss_P: [ 2.97  2.57  2.32  2.31  1.8   1.49  1.24 14.7 ]\n",
      "New sampled: 0 dataset size: 880 Loss_Q: [1.81 2.02 2.07 1.62 1.45 0.95 0.   9.91] Loss_P: [ 2.96  2.63  2.28  2.32  1.81  1.5   1.2  14.7 ]\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    new_sampled = np.zeros((n,n_data*update_rate))\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for r in range (update_rate):\n",
    "        index = np.random.permutation(n_data)\n",
    "        for i in range(n_data):\n",
    "            d0 = dataset[:,index[i]:index[i]+1]\n",
    "            Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "            Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "            Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "            Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "\n",
    "            new_sampled[:,i+n_data*r:i+n_data*r+1] = Alpha_P['z0']\n",
    "\n",
    "            Loss_Q_total += Loss_Q\n",
    "            Loss_P_total += Loss_P\n",
    "\n",
    "    values,counts = np.unique(new_sampled,axis=1,return_counts = True)\n",
    "    new_samples = values[:,np.argsort(counts)[-n_new_sample:]]\n",
    "    \n",
    "    if counts.size < n_new_sample:\n",
    "        print('Number of samples reached system maximum')\n",
    "        \n",
    "    n_new_total = np.unique(np.append(dataset,new_samples,axis=1),axis=1).shape[1] - n_data\n",
    "    dataset = new_samples  # renew dataset\n",
    "    n_data = dataset.shape[1]\n",
    "\n",
    "    Loss_Q_total = Loss_Q_total/(n_data*update_rate)\n",
    "    Loss_P_total = Loss_P_total/(n_data*update_rate)\n",
    "    print('New sampled: ' + str(n_new_total),'dataset size: ' + str(n_data),'Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "c0576d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[np.argsort(counts)].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "b5f218a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if counts.size < n_new_sample:\n",
    "    print('Number of samples reached system maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "7d9a5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "reordered_set = ut.reorder_all_comb(entire_set,dataset)\n",
    "distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,dataset)\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "9b841d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f8bcc41fd0>"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - values_d.size))] = values_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "79971c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.982,\n",
       " 'FN': array([], dtype=int32),\n",
       " 'n_fn': 0,\n",
       " 'FP': array([[ 886,  893,  907,  953,  997, 1013, 1073, 1126, 1190, 1193, 1200,\n",
       "         1207, 1246, 1260, 1267, 1311, 1325, 1373, 1387, 1447, 1486, 1553,\n",
       "         1560, 1567, 1619, 1627, 1666, 1678, 1687, 1731, 1740, 1747, 1800,\n",
       "         1807, 2068, 2069, 2137, 2181, 2289, 2327, 2422, 2427, 2464, 2489,\n",
       "         2510, 2513, 2514, 2547, 2560, 2597, 2601, 2607, 2608, 2649, 2656,\n",
       "         2687, 2747, 2803, 2859, 2866, 2871, 2911, 2912, 2918, 2922, 2923,\n",
       "         2963, 2965, 2973, 3014, 3016, 3024, 3025, 3069, 3077, 3079, 3135,\n",
       "         3191, 3235, 3247, 3291, 3333, 3343, 3344, 3355, 3395, 3396, 3405,\n",
       "         3446, 3447, 3449, 3454, 3501, 3509, 3511, 3613, 3698, 3707, 3738,\n",
       "         3744, 3745, 3784, 3791, 3833, 3872, 3879, 3950, 3957, 3968, 4012,\n",
       "         4019, 4034, 4037, 4039, 4052, 4054, 4066, 4068, 4071, 4076, 4077,\n",
       "         4083, 4084, 4087, 4092, 4095],\n",
       "        [   1,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    3,    1,    1,    2,    2,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    3,    1,    2,    1,    1,\n",
       "            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            2,    1,    1,    1,    1,    1,    2,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    2,    2,    1,    1,    1,\n",
       "            2,    1,    1,    1,    1,    1,    1,    1,    1,    2,    1,\n",
       "            1,    1,    1,    2,    1,    1,    1,    1,    2,    1,    2,\n",
       "            1,    2,    1,    1,    1,    4,    1,    4,    2,    2,    3,\n",
       "            2,    1,    5,    3,    1,    5,    1,    1,    4,    1,    3,\n",
       "            2,    1,    2,    1,    3,    1,    2,    1,    1,    1,    1,\n",
       "            1,    2,    3,    2,    1]], dtype=int64),\n",
       " 'n_fp': 126}"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "088f8f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1298224"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "3aecd396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_dataset = np.load('self_org_dataset.npy')\n",
    "union = np.unique(np.append(dataset,org_dataset,axis=1),axis=1).shape[1]\n",
    "intersection = dataset.shape[1] + org_dataset.shape[1] - union\n",
    "intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8410f",
   "metadata": {},
   "source": [
    "percent: 0.9461, 0.9711,0.9784,0.9792,0.98,0.98,0.982\n",
    "\n",
    "n_fn: 0,0,0,0,0,0,0\n",
    "\n",
    "n_fp: 333,211,176,155,126,111,126\n",
    "\n",
    "MSE: 0.1459,0.1451,0.1248,0.1175,0.1431,0.1254,0.1298\n",
    "\n",
    "intersection (max 672): 532,537,535,535,536,536,537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "12bde376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "aef232d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'Phi': Phi, 'Theta': Theta}\n",
    "np.save('self_org_dataset_98.npy',dataset)\n",
    "np.save('self_org_parameters_98.npy',parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb54f16",
   "metadata": {},
   "source": [
    "For self-organizing system, we not only can extend upon an original samll subspace, as time goes by, some undersampled instances in the original set could also be discarded. By shifting from one space to another (with overlap), we find a better niche to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6721e1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 627)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = np.load('self_org_parameters.npy',allow_pickle=True).item()\n",
    "Phi = para['Phi']\n",
    "Theta = para['Theta']\n",
    "dataset = np.load('self_org_dataset.npy')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d3151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_dataset = np.load('self_org_dataset.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96541b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 10\n",
    "n = n_dz[0,0]\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c102a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_rate = 100 # after how many epochs we update evidence\n",
    "n_add_sample = 700\n",
    "n_del_sample = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc37b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new sampled: 2258 dataset size: 784 delete samples: 0\n",
      "Total new sampled: 2231 dataset size: 789 delete samples: 1\n",
      "Total new sampled: 2226 dataset size: 793 delete samples: 1\n",
      "Total new sampled: 2188 dataset size: 799 delete samples: 1\n",
      "Total new sampled: 2190 dataset size: 801 delete samples: 0\n",
      "Total new sampled: 2198 dataset size: 801 delete samples: 0\n",
      "Total new sampled: 2135 dataset size: 804 delete samples: 0\n",
      "Total new sampled: 2192 dataset size: 808 delete samples: 0\n",
      "Total new sampled: 2169 dataset size: 811 delete samples: 0\n",
      "Total new sampled: 2151 dataset size: 814 delete samples: 0\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    new_sampled = np.zeros((n,n_data*update_rate))\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for r in range (update_rate):\n",
    "        index = np.random.permutation(n_data)\n",
    "        for i in range(n_data):\n",
    "            d0 = dataset[:,index[i]:index[i]+1]\n",
    "            Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "            Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "            Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "            Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "\n",
    "            new_sampled[:,i+n_data*r:i+n_data*r+1] = Alpha_P['z0']\n",
    "\n",
    "            Loss_Q_total += Loss_Q\n",
    "            Loss_P_total += Loss_P\n",
    "\n",
    "    values,counts = np.unique(new_sampled,axis=1,return_counts = True)\n",
    "    n_new_total = np.unique(np.append(dataset,values,axis=1),axis=1).shape[1] - n_data\n",
    "    \n",
    "    new_samples = values[:,np.argsort(counts)[-n_add_sample:]]\n",
    "    del_samples = values[:,np.argsort(counts)[n_del_sample:]]\n",
    "    \n",
    "    index = []\n",
    "    for j in range(n_del_sample):\n",
    "        for k in range(n_data):\n",
    "            if np.array_equal(del_samples[:,j], dataset[:,k]):\n",
    "                index.append(k)\n",
    "                break\n",
    "    dataset = np.delete(dataset,index,axis=1) # remove undersampled data points\n",
    "    dataset = np.unique(np.append(dataset,new_samples,axis=1),axis=1)  # expand dataset        \n",
    "    n_data = dataset.shape[1]\n",
    "\n",
    "    Loss_Q_total = Loss_Q_total/(n_data*update_rate)\n",
    "    Loss_P_total = Loss_P_total/(n_data*update_rate)\n",
    "    print('Total new sampled: ' + str(n_new_total),'dataset size: ' + str(n_data),'delete samples: ' + str(len(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75868ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "reordered_set = ut.reorder_all_comb(entire_set,np.append(dataset,org_dataset,axis=1))\n",
    "distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,dataset)\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91a50d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x161892dfe90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - values_d.size))] = values_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3df8652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.8323,\n",
       " 'FN': array([ 18,  23,  46,  53,  86, 148, 153, 194, 209, 225, 228, 279, 281,\n",
       "        346, 453, 465, 555, 562, 578, 581, 602, 621, 635, 636, 637, 640,\n",
       "        642, 657, 662, 675, 687, 688, 693, 700, 704, 705, 706, 707, 708,\n",
       "        717, 720, 721, 722, 739, 743, 745, 767, 768, 769]),\n",
       " 'n_fn': 49,\n",
       " 'FP': array([[ 771,  772,  773, ..., 4093, 4094, 4095],\n",
       "        [   5,    8,   14, ...,    4,    1,    2]], dtype=int64),\n",
       " 'n_fp': 929}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1addc2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28591193000000004"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d63cf",
   "metadata": {},
   "source": [
    "'percent': 0.8111; 0.8234\n",
    "\n",
    "n_fn': 33; 35\n",
    "\n",
    "'n_fp': 1184; 1058\n",
    "\n",
    "MSE: 0.2873; 0.3159"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
