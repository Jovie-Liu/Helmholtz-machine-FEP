{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96a5d01",
   "metadata": {},
   "source": [
    "***\n",
    "*Project:* Helmholtz Machine on Niche Construction\n",
    "\n",
    "*Author:* Jingwei Liu, Computer Music Ph.D., UC San Diego\n",
    "***\n",
    "\n",
    "# <span style=\"background-color:darkorange; color:white; padding:2px 6px\">Experiment 4_4</span> \n",
    "\n",
    "# Active Sampling on Excluding False Instances\n",
    "\n",
    "\n",
    "*Created:* December 26, 2023\n",
    "\n",
    "*Updated:* December 26, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e099c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20847db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 10,  8,  7,  5,  4,  3,  1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure = [[12,10,8,7,5,4,3,1]]\n",
    "n_dz = np.array(structure)\n",
    "n_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d5e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_set = [1,0]\n",
    "activation_type = \"tanh\"\n",
    "bias = [False,False,True] # [instantiation bias, MLP bias,data bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633ee2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi, Theta = ut.parameter_initialization(\"zero\",n_dz)  # \"zero\" or \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d548310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 627)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.load('self_org_dataset.npy')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b3f93",
   "metadata": {},
   "source": [
    "We take this dataset as well-formed bound, which presents all valid instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6786e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_dz[0,0]\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abb17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_set = ut.all_comb(n, value_set)\n",
    "reordered_set = ut.reorder_all_comb(entire_set,dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad192f6a",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2d6f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.008\n",
    "epoch = 1000\n",
    "n_data = dataset.shape[1]\n",
    "n_layer = n_dz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6303c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.77 1.19 2.25 1.39 1.16 1.   0.   8.76] Loss_P: [ 3.16  2.52  1.75  2.33  1.53  1.2   1.1  13.6 ]\n",
      "Loss_Q: [1.79 1.18 2.33 1.38 1.17 1.   0.   8.85] Loss_P: [ 3.17  2.53  1.7   2.31  1.48  1.21  1.09 13.48]\n",
      "Loss_Q: [1.76 1.15 2.25 1.3  1.13 0.99 0.   8.58] Loss_P: [ 3.15  2.51  1.69  2.29  1.4   1.19  1.12 13.34]\n",
      "Loss_Q: [1.8  1.1  2.23 1.32 1.12 1.01 0.   8.58] Loss_P: [ 3.15  2.49  1.68  2.25  1.37  1.16  1.11 13.21]\n",
      "Loss_Q: [1.84 1.14 2.25 1.31 1.13 1.01 0.   8.67] Loss_P: [ 3.15  2.52  1.7   2.3   1.39  1.19  1.13 13.39]\n",
      "Loss_Q: [1.81 1.16 2.21 1.34 1.12 1.04 0.   8.68] Loss_P: [ 3.16  2.56  1.68  2.26  1.36  1.17  1.14 13.32]\n",
      "Loss_Q: [1.79 1.09 2.22 1.3  1.09 1.04 0.   8.53] Loss_P: [ 3.15  2.53  1.6   2.24  1.42  1.22  1.14 13.29]\n",
      "Loss_Q: [1.85 1.14 2.2  1.34 1.09 1.02 0.   8.65] Loss_P: [ 3.16  2.54  1.67  2.25  1.45  1.18  1.11 13.37]\n",
      "Loss_Q: [1.85 1.09 2.22 1.35 1.11 1.02 0.   8.63] Loss_P: [ 3.13  2.56  1.69  2.24  1.4   1.16  1.11 13.28]\n",
      "Loss_Q: [1.86 1.14 2.18 1.37 1.1  1.03 0.   8.67] Loss_P: [ 3.18  2.53  1.7   2.27  1.43  1.15  1.12 13.37]\n",
      "Loss_Q: [1.8  1.13 2.22 1.31 1.07 1.01 0.   8.54] Loss_P: [ 3.14  2.52  1.68  2.29  1.43  1.13  1.12 13.32]\n",
      "Loss_Q: [1.8  1.11 2.28 1.35 1.09 1.03 0.   8.65] Loss_P: [ 3.18  2.48  1.67  2.28  1.39  1.18  1.11 13.3 ]\n",
      "Loss_Q: [1.81 1.06 2.23 1.38 1.09 1.03 0.   8.6 ] Loss_P: [ 3.17  2.48  1.65  2.26  1.44  1.19  1.1  13.3 ]\n",
      "Loss_Q: [1.9  1.11 2.21 1.36 1.08 1.01 0.   8.67] Loss_P: [ 3.18  2.5   1.64  2.27  1.48  1.21  1.13 13.4 ]\n",
      "Loss_Q: [1.77 1.09 2.26 1.29 1.07 1.   0.   8.47] Loss_P: [ 3.16  2.49  1.66  2.29  1.39  1.14  1.09 13.22]\n",
      "Loss_Q: [1.8  1.16 2.28 1.36 1.09 1.01 0.   8.7 ] Loss_P: [ 3.19  2.48  1.65  2.3   1.41  1.16  1.1  13.3 ]\n",
      "Loss_Q: [1.8  1.16 2.33 1.37 1.13 1.   0.   8.79] Loss_P: [ 3.18  2.48  1.69  2.31  1.42  1.26  1.1  13.44]\n",
      "Loss_Q: [1.8  1.14 2.29 1.32 1.14 1.03 0.   8.73] Loss_P: [ 3.15  2.53  1.68  2.36  1.41  1.25  1.11 13.49]\n",
      "Loss_Q: [1.78 1.15 2.26 1.31 1.18 1.01 0.   8.7 ] Loss_P: [ 3.22  2.46  1.63  2.32  1.39  1.24  1.11 13.36]\n",
      "Loss_Q: [1.79 1.12 2.28 1.3  1.14 1.01 0.   8.63] Loss_P: [ 3.14  2.45  1.65  2.29  1.44  1.2   1.1  13.26]\n",
      "Loss_Q: [1.75 1.1  2.27 1.35 1.15 1.   0.   8.64] Loss_P: [ 3.21  2.45  1.66  2.31  1.49  1.25  1.13 13.51]\n",
      "Loss_Q: [1.73 1.19 2.29 1.38 1.1  1.   0.   8.67] Loss_P: [ 3.22  2.43  1.62  2.39  1.45  1.2   1.1  13.41]\n",
      "Loss_Q: [1.89 1.14 2.28 1.35 1.15 1.01 0.   8.83] Loss_P: [ 3.18  2.56  1.65  2.34  1.41  1.21  1.1  13.44]\n",
      "Loss_Q: [1.8  1.12 2.27 1.36 1.17 1.01 0.   8.73] Loss_P: [ 3.15  2.53  1.67  2.34  1.45  1.22  1.07 13.44]\n",
      "Loss_Q: [1.71 1.12 2.26 1.35 1.17 0.99 0.   8.61] Loss_P: [ 3.17  2.46  1.65  2.28  1.45  1.2   1.1  13.31]\n",
      "Loss_Q: [1.74 1.12 2.23 1.35 1.12 1.   0.   8.57] Loss_P: [ 3.2   2.36  1.64  2.3   1.46  1.23  1.1  13.3 ]\n",
      "Loss_Q: [1.72 1.1  2.29 1.37 1.17 0.99 0.   8.65] Loss_P: [ 3.15  2.41  1.64  2.33  1.48  1.22  1.08 13.31]\n",
      "Loss_Q: [1.78 1.1  2.25 1.36 1.12 1.01 0.   8.63] Loss_P: [ 3.21  2.45  1.7   2.37  1.49  1.22  1.09 13.54]\n",
      "Loss_Q: [1.8  1.07 2.27 1.36 1.1  1.01 0.   8.62] Loss_P: [ 3.18  2.45  1.61  2.32  1.5   1.2   1.09 13.36]\n",
      "Loss_Q: [1.72 1.12 2.19 1.37 1.13 1.   0.   8.53] Loss_P: [ 3.21  2.45  1.69  2.29  1.38  1.22  1.09 13.34]\n",
      "Loss_Q: [1.73 1.11 2.25 1.33 1.16 0.99 0.   8.58] Loss_P: [ 3.18  2.43  1.69  2.32  1.42  1.23  1.09 13.35]\n",
      "Loss_Q: [1.81 1.13 2.21 1.37 1.15 0.98 0.   8.65] Loss_P: [ 3.15  2.43  1.68  2.27  1.41  1.2   1.1  13.22]\n",
      "Loss_Q: [1.74 1.15 2.19 1.35 1.18 1.01 0.   8.62] Loss_P: [ 3.17  2.43  1.69  2.3   1.45  1.23  1.11 13.38]\n",
      "Loss_Q: [1.66 1.13 2.22 1.36 1.19 1.01 0.   8.57] Loss_P: [ 3.19  2.32  1.7   2.28  1.39  1.24  1.11 13.24]\n",
      "Loss_Q: [1.66 1.18 2.21 1.35 1.16 1.01 0.   8.59] Loss_P: [ 3.2   2.38  1.66  2.26  1.44  1.23  1.13 13.29]\n",
      "Loss_Q: [1.71 1.19 2.16 1.28 1.13 1.01 0.   8.48] Loss_P: [ 3.16  2.37  1.66  2.21  1.38  1.22  1.11 13.12]\n",
      "Loss_Q: [1.66 1.11 2.14 1.31 1.14 1.02 0.   8.39] Loss_P: [ 3.19  2.35  1.62  2.21  1.38  1.21  1.11 13.06]\n",
      "Loss_Q: [1.68 1.16 2.13 1.26 1.17 1.   0.   8.4 ] Loss_P: [ 3.15  2.37  1.73  2.24  1.45  1.22  1.13 13.29]\n",
      "Loss_Q: [1.64 1.15 2.16 1.32 1.16 1.02 0.   8.44] Loss_P: [ 3.23  2.29  1.69  2.3   1.4   1.22  1.11 13.24]\n",
      "Loss_Q: [1.61 1.18 2.14 1.25 1.12 1.02 0.   8.32] Loss_P: [ 3.21  2.28  1.69  2.21  1.31  1.2   1.12 13.03]\n",
      "Loss_Q: [1.58 1.11 2.1  1.31 1.16 1.04 0.   8.3 ] Loss_P: [ 3.21  2.27  1.67  2.23  1.4   1.22  1.11 13.12]\n",
      "Loss_Q: [1.62 1.15 2.14 1.27 1.15 1.   0.   8.33] Loss_P: [ 3.19  2.27  1.62  2.22  1.38  1.24  1.1  13.01]\n",
      "Loss_Q: [1.63 1.21 2.09 1.27 1.13 1.   0.   8.33] Loss_P: [ 3.17  2.3   1.65  2.16  1.34  1.21  1.08 12.89]\n",
      "Loss_Q: [1.67 1.09 2.01 1.28 1.07 1.01 0.   8.13] Loss_P: [ 3.19  2.29  1.63  2.13  1.43  1.14  1.09 12.89]\n",
      "Loss_Q: [1.66 1.12 2.07 1.34 1.06 1.   0.   8.25] Loss_P: [ 3.18  2.29  1.66  2.21  1.44  1.17  1.08 13.02]\n",
      "Loss_Q: [1.6  1.08 2.11 1.31 1.05 0.99 0.   8.13] Loss_P: [ 3.21  2.41  1.6   2.2   1.42  1.14  1.08 13.06]\n",
      "Loss_Q: [1.68 1.09 2.13 1.32 1.08 0.99 0.   8.3 ] Loss_P: [ 3.19  2.34  1.59  2.19  1.38  1.17  1.08 12.95]\n",
      "Loss_Q: [1.7  1.14 2.15 1.32 1.11 0.98 0.   8.41] Loss_P: [ 3.2   2.33  1.64  2.25  1.41  1.16  1.09 13.09]\n",
      "Loss_Q: [1.65 1.19 2.18 1.34 1.07 0.98 0.   8.41] Loss_P: [ 3.21  2.39  1.72  2.23  1.46  1.18  1.09 13.26]\n",
      "Loss_Q: [1.69 1.15 2.18 1.32 1.05 1.   0.   8.39] Loss_P: [ 3.2   2.38  1.67  2.22  1.43  1.14  1.12 13.17]\n",
      "Loss_Q: [1.75 1.14 2.1  1.28 1.1  0.99 0.   8.36] Loss_P: [ 3.21  2.36  1.62  2.2   1.36  1.13  1.09 12.96]\n",
      "Loss_Q: [1.7  1.18 2.13 1.34 1.09 0.99 0.   8.41] Loss_P: [ 3.19  2.35  1.69  2.18  1.41  1.15  1.11 13.06]\n",
      "Loss_Q: [1.76 1.15 2.09 1.28 1.05 1.   0.   8.34] Loss_P: [ 3.19  2.45  1.64  2.13  1.33  1.1   1.09 12.93]\n",
      "Loss_Q: [1.75 1.15 2.07 1.28 1.06 0.99 0.   8.3 ] Loss_P: [ 3.23  2.37  1.62  2.1   1.33  1.08  1.1  12.82]\n",
      "Loss_Q: [1.67 1.12 2.08 1.24 0.98 0.99 0.   8.07] Loss_P: [ 3.18  2.36  1.6   2.12  1.32  1.08  1.11 12.78]\n",
      "Loss_Q: [1.61 1.07 1.99 1.21 0.97 0.98 0.   7.84] Loss_P: [ 3.18  2.37  1.62  2.14  1.27  1.06  1.11 12.76]\n",
      "Loss_Q: [1.63 1.15 2.05 1.29 0.99 0.99 0.   8.09] Loss_P: [ 3.19  2.39  1.62  2.08  1.34  1.01  1.08 12.72]\n",
      "Loss_Q: [1.63 1.11 2.   1.25 1.01 0.96 0.   7.96] Loss_P: [ 3.17  2.32  1.62  2.08  1.31  1.05  1.09 12.63]\n",
      "Loss_Q: [1.62 1.1  2.02 1.23 0.99 1.01 0.   7.97] Loss_P: [ 3.2   2.35  1.64  2.13  1.31  1.07  1.07 12.78]\n",
      "Loss_Q: [1.63 1.08 2.01 1.29 0.99 0.99 0.   8.  ] Loss_P: [ 3.22  2.3   1.59  2.15  1.35  1.07  1.07 12.75]\n",
      "Loss_Q: [1.66 1.12 2.04 1.23 0.97 0.99 0.   8.01] Loss_P: [ 3.2   2.38  1.58  2.11  1.31  1.04  1.08 12.69]\n",
      "Loss_Q: [1.66 1.11 1.99 1.22 1.04 0.98 0.   7.99] Loss_P: [ 3.22  2.37  1.61  2.07  1.31  1.13  1.05 12.77]\n",
      "Loss_Q: [1.67 1.12 2.03 1.26 1.05 0.99 0.   8.11] Loss_P: [ 3.15  2.41  1.59  2.11  1.33  1.16  1.08 12.83]\n",
      "Loss_Q: [1.72 1.1  2.02 1.25 1.03 0.98 0.   8.09] Loss_P: [ 3.2   2.39  1.58  2.11  1.3   1.12  1.06 12.76]\n",
      "Loss_Q: [1.8  1.04 2.07 1.18 1.06 0.97 0.   8.13] Loss_P: [ 3.19  2.42  1.59  2.15  1.32  1.13  1.05 12.85]\n",
      "Loss_Q: [1.72 1.11 2.08 1.25 1.06 0.98 0.   8.21] Loss_P: [ 3.18  2.43  1.57  2.14  1.3   1.12  1.06 12.81]\n",
      "Loss_Q: [1.71 1.04 2.08 1.16 1.06 0.98 0.   8.03] Loss_P: [ 3.22  2.44  1.61  2.18  1.28  1.16  1.08 12.97]\n",
      "Loss_Q: [1.72 1.08 2.07 1.21 1.08 0.99 0.   8.16] Loss_P: [ 3.21  2.37  1.56  2.18  1.26  1.16  1.06 12.8 ]\n",
      "Loss_Q: [1.66 1.09 2.09 1.26 1.13 0.97 0.   8.19] Loss_P: [ 3.23  2.33  1.59  2.19  1.29  1.2   1.06 12.9 ]\n",
      "Loss_Q: [1.78 1.15 2.08 1.26 1.1  0.98 0.   8.36] Loss_P: [ 3.22  2.35  1.59  2.22  1.26  1.17  1.08 12.89]\n",
      "Loss_Q: [1.69 1.1  2.09 1.18 1.11 0.98 0.   8.16] Loss_P: [ 3.25  2.33  1.61  2.2   1.22  1.18  1.07 12.86]\n",
      "Loss_Q: [1.65 1.08 2.03 1.16 1.06 1.   0.   7.99] Loss_P: [ 3.2   2.39  1.59  2.2   1.23  1.14  1.09 12.83]\n",
      "Loss_Q: [1.74 1.12 2.09 1.23 1.05 0.98 0.   8.22] Loss_P: [ 3.18  2.35  1.6   2.2   1.26  1.13  1.08 12.8 ]\n",
      "Loss_Q: [1.69 1.13 2.16 1.19 1.04 0.98 0.   8.19] Loss_P: [ 3.17  2.39  1.59  2.26  1.24  1.12  1.08 12.84]\n",
      "Loss_Q: [1.7  1.18 2.16 1.22 1.04 0.99 0.   8.29] Loss_P: [ 3.22  2.28  1.63  2.24  1.3   1.19  1.11 12.96]\n",
      "Loss_Q: [1.7  1.1  2.1  1.2  1.02 0.97 0.   8.09] Loss_P: [ 3.19  2.36  1.62  2.19  1.29  1.15  1.08 12.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.81 1.15 2.04 1.21 1.07 0.98 0.   8.25] Loss_P: [ 3.2   2.36  1.65  2.12  1.24  1.15  1.07 12.79]\n",
      "Loss_Q: [1.69 1.08 1.99 1.17 1.05 0.97 0.   7.96] Loss_P: [ 3.19  2.38  1.59  2.07  1.27  1.15  1.07 12.73]\n",
      "Loss_Q: [1.73 1.05 1.98 1.13 1.03 0.95 0.   7.88] Loss_P: [ 3.19  2.41  1.64  2.04  1.17  1.08  1.07 12.61]\n",
      "Loss_Q: [1.82 1.06 1.97 1.16 1.02 0.96 0.   7.99] Loss_P: [ 3.19  2.42  1.61  2.03  1.19  1.1   1.09 12.63]\n",
      "Loss_Q: [1.76 1.02 1.97 1.07 1.02 0.97 0.   7.81] Loss_P: [ 3.2   2.43  1.58  2.09  1.19  1.12  1.1  12.71]\n",
      "Loss_Q: [1.76 1.09 1.97 1.11 1.09 0.98 0.   8.01] Loss_P: [ 3.18  2.44  1.61  2.06  1.12  1.13  1.08 12.64]\n",
      "Loss_Q: [1.79 1.11 1.97 1.08 1.04 0.97 0.   7.96] Loss_P: [ 3.18  2.48  1.61  2.04  1.16  1.11  1.1  12.69]\n",
      "Loss_Q: [1.72 1.06 1.92 1.11 1.03 0.96 0.   7.81] Loss_P: [ 3.23  2.35  1.57  1.97  1.19  1.11  1.09 12.51]\n",
      "Loss_Q: [1.65 1.03 1.86 1.12 1.05 0.95 0.   7.66] Loss_P: [ 3.25  2.37  1.58  1.96  1.25  1.15  1.08 12.64]\n",
      "Loss_Q: [1.78 1.16 1.92 1.21 1.09 0.97 0.   8.13] Loss_P: [ 3.14  2.47  1.59  1.95  1.22  1.12  1.06 12.55]\n",
      "Loss_Q: [1.78 1.13 1.93 1.16 1.04 0.95 0.   7.99] Loss_P: [ 3.16  2.45  1.66  2.02  1.23  1.14  1.06 12.73]\n",
      "Loss_Q: [1.78 1.11 1.93 1.13 1.13 0.96 0.   8.04] Loss_P: [ 3.17  2.47  1.59  1.96  1.21  1.15  1.08 12.63]\n",
      "Loss_Q: [1.78 1.1  1.88 1.19 1.09 0.94 0.   7.99] Loss_P: [ 3.16  2.42  1.63  1.99  1.26  1.16  1.07 12.68]\n",
      "Loss_Q: [1.74 1.12 1.89 1.19 1.08 0.97 0.   7.98] Loss_P: [ 3.16  2.41  1.64  1.96  1.23  1.13  1.09 12.63]\n",
      "Loss_Q: [1.8  1.09 1.95 1.2  1.09 0.98 0.   8.13] Loss_P: [ 3.2   2.4   1.62  1.95  1.26  1.18  1.09 12.7 ]\n",
      "Loss_Q: [1.68 1.17 1.91 1.2  1.12 0.95 0.   8.03] Loss_P: [ 3.19  2.45  1.65  2.01  1.26  1.23  1.09 12.88]\n",
      "Loss_Q: [1.66 1.13 1.91 1.12 1.15 0.95 0.   7.92] Loss_P: [ 3.21  2.33  1.61  1.97  1.21  1.19  1.08 12.59]\n",
      "Loss_Q: [1.64 1.13 1.94 1.17 1.17 0.95 0.   7.99] Loss_P: [ 3.2   2.31  1.66  1.96  1.17  1.21  1.06 12.57]\n",
      "Loss_Q: [1.66 1.12 1.93 1.2  1.18 0.98 0.   8.06] Loss_P: [ 3.19  2.28  1.64  2.    1.25  1.25  1.08 12.69]\n",
      "Loss_Q: [1.7  1.16 1.98 1.22 1.15 0.95 0.   8.16] Loss_P: [ 3.18  2.28  1.69  1.98  1.29  1.25  1.08 12.76]\n",
      "Loss_Q: [1.64 1.12 2.02 1.22 1.13 0.95 0.   8.09] Loss_P: [ 3.18  2.28  1.63  1.99  1.28  1.19  1.06 12.6 ]\n",
      "Loss_Q: [1.72 1.13 1.99 1.2  1.11 0.96 0.   8.1 ] Loss_P: [ 3.2   2.36  1.66  2.04  1.3   1.21  1.06 12.84]\n",
      "Loss_Q: [1.7  1.13 2.04 1.24 1.19 0.92 0.   8.22] Loss_P: [ 3.22  2.41  1.65  2.08  1.34  1.25  1.05 12.99]\n",
      "Loss_Q: [1.65 1.13 2.02 1.25 1.17 0.92 0.   8.14] Loss_P: [ 3.22  2.32  1.64  2.03  1.29  1.24  1.03 12.77]\n",
      "Loss_Q: [1.62 1.1  1.96 1.23 1.13 0.95 0.   7.98] Loss_P: [ 3.24  2.3   1.65  2.07  1.31  1.24  1.06 12.86]\n",
      "Loss_Q: [1.63 1.11 2.01 1.27 1.16 0.93 0.   8.1 ] Loss_P: [ 3.23  2.27  1.59  1.99  1.31  1.23  1.05 12.67]\n",
      "Loss_Q: [1.58 1.07 1.99 1.24 1.17 0.94 0.   7.98] Loss_P: [ 3.26  2.22  1.61  2.01  1.36  1.23  1.04 12.74]\n",
      "Loss_Q: [1.59 1.11 1.98 1.3  1.14 0.95 0.   8.06] Loss_P: [ 3.23  2.23  1.62  2.07  1.39  1.23  1.04 12.81]\n",
      "Loss_Q: [1.65 1.05 1.97 1.29 1.11 0.97 0.   8.04] Loss_P: [ 3.24  2.3   1.56  2.02  1.33  1.21  1.06 12.71]\n",
      "Loss_Q: [1.66 1.07 2.01 1.31 1.14 0.94 0.   8.12] Loss_P: [ 3.25  2.29  1.61  2.08  1.32  1.19  1.07 12.81]\n",
      "Loss_Q: [1.69 1.07 2.03 1.24 1.13 0.94 0.   8.1 ] Loss_P: [ 3.26  2.31  1.6   2.05  1.3   1.18  1.05 12.74]\n",
      "Loss_Q: [1.7  1.04 1.98 1.2  1.1  0.96 0.   7.97] Loss_P: [ 3.23  2.34  1.56  2.    1.26  1.08  1.04 12.52]\n",
      "Loss_Q: [1.7  1.09 2.02 1.27 1.15 0.96 0.   8.19] Loss_P: [ 3.17  2.41  1.57  2.04  1.29  1.17  1.07 12.72]\n",
      "Loss_Q: [1.78 1.13 1.97 1.2  1.09 0.96 0.   8.12] Loss_P: [ 3.19  2.41  1.61  2.02  1.3   1.14  1.07 12.73]\n",
      "Loss_Q: [1.76 1.13 2.04 1.34 1.12 0.96 0.   8.35] Loss_P: [ 3.19  2.4   1.6   2.11  1.38  1.16  1.08 12.93]\n",
      "Loss_Q: [1.69 1.09 1.98 1.33 1.05 0.95 0.   8.09] Loss_P: [ 3.22  2.33  1.61  2.05  1.41  1.17  1.09 12.86]\n",
      "Loss_Q: [1.77 1.12 2.   1.25 1.05 0.95 0.   8.15] Loss_P: [ 3.23  2.34  1.58  2.06  1.33  1.12  1.08 12.74]\n",
      "Loss_Q: [1.67 1.1  1.97 1.29 1.02 0.97 0.   8.02] Loss_P: [ 3.23  2.39  1.58  2.02  1.33  1.15  1.07 12.78]\n",
      "Loss_Q: [1.66 1.13 1.97 1.31 1.1  0.96 0.   8.12] Loss_P: [ 3.23  2.41  1.62  2.1   1.41  1.19  1.08 13.04]\n",
      "Loss_Q: [1.65 1.11 1.98 1.38 1.16 0.96 0.   8.24] Loss_P: [ 3.23  2.34  1.58  2.06  1.46  1.2   1.09 12.95]\n",
      "Loss_Q: [1.72 1.12 2.03 1.39 1.18 0.95 0.   8.39] Loss_P: [ 3.18  2.37  1.57  2.07  1.48  1.27  1.1  13.05]\n",
      "Loss_Q: [1.73 1.11 2.   1.41 1.16 0.97 0.   8.38] Loss_P: [ 3.21  2.37  1.63  2.07  1.45  1.23  1.11 13.06]\n",
      "Loss_Q: [1.73 1.15 2.06 1.43 1.21 0.95 0.   8.54] Loss_P: [ 3.2   2.41  1.64  2.09  1.46  1.27  1.1  13.16]\n",
      "Loss_Q: [1.75 1.19 2.06 1.42 1.18 0.97 0.   8.56] Loss_P: [ 3.21  2.5   1.65  2.1   1.44  1.24  1.11 13.25]\n",
      "Loss_Q: [1.83 1.12 1.96 1.35 1.14 0.96 0.   8.36] Loss_P: [ 3.23  2.45  1.64  2.07  1.42  1.21  1.1  13.11]\n",
      "Loss_Q: [1.79 1.21 2.07 1.41 1.18 0.96 0.   8.62] Loss_P: [ 3.14  2.5   1.68  2.09  1.48  1.24  1.09 13.22]\n",
      "Loss_Q: [1.84 1.18 2.03 1.39 1.17 0.96 0.   8.58] Loss_P: [ 3.19  2.52  1.65  2.07  1.46  1.23  1.08 13.2 ]\n",
      "Loss_Q: [1.87 1.1  2.04 1.37 1.14 0.97 0.   8.49] Loss_P: [ 3.18  2.44  1.64  2.07  1.46  1.19  1.1  13.09]\n",
      "Loss_Q: [1.74 1.14 2.05 1.32 1.17 0.99 0.   8.41] Loss_P: [ 3.16  2.49  1.65  2.1   1.42  1.22  1.09 13.13]\n",
      "Loss_Q: [1.76 1.2  2.05 1.38 1.14 0.96 0.   8.49] Loss_P: [ 3.17  2.48  1.63  2.07  1.43  1.21  1.09 13.1 ]\n",
      "Loss_Q: [1.74 1.15 2.03 1.31 1.17 0.97 0.   8.38] Loss_P: [ 3.2   2.43  1.62  2.03  1.41  1.23  1.09 13.  ]\n",
      "Loss_Q: [1.74 1.08 1.95 1.29 1.13 0.97 0.   8.17] Loss_P: [ 3.15  2.46  1.65  2.04  1.41  1.24  1.1  13.05]\n",
      "Loss_Q: [1.8  1.11 1.99 1.32 1.12 0.99 0.   8.34] Loss_P: [ 3.17  2.48  1.65  2.    1.33  1.23  1.07 12.94]\n",
      "Loss_Q: [1.72 1.12 1.92 1.22 1.11 0.99 0.   8.07] Loss_P: [ 3.13  2.44  1.65  1.98  1.31  1.19  1.1  12.8 ]\n",
      "Loss_Q: [1.69 1.12 1.93 1.26 1.07 0.97 0.   8.04] Loss_P: [ 3.16  2.41  1.63  1.98  1.3   1.18  1.1  12.76]\n",
      "Loss_Q: [1.65 1.08 1.88 1.21 1.09 0.96 0.   7.88] Loss_P: [ 3.17  2.39  1.6   1.94  1.32  1.19  1.07 12.68]\n",
      "Loss_Q: [1.71 1.1  1.92 1.2  1.12 0.93 0.   7.97] Loss_P: [ 3.22  2.36  1.6   1.93  1.21  1.19  1.06 12.57]\n",
      "Loss_Q: [1.61 1.11 1.9  1.22 1.11 0.94 0.   7.9 ] Loss_P: [ 3.22  2.33  1.63  1.96  1.24  1.16  1.05 12.59]\n",
      "Loss_Q: [1.6  1.1  1.94 1.24 1.11 0.93 0.   7.92] Loss_P: [ 3.2   2.29  1.64  1.98  1.35  1.21  1.04 12.72]\n",
      "Loss_Q: [1.62 1.1  1.97 1.24 1.12 0.92 0.   7.98] Loss_P: [ 3.2   2.29  1.62  1.95  1.35  1.21  1.04 12.67]\n",
      "Loss_Q: [1.66 1.12 1.94 1.28 1.11 0.95 0.   8.07] Loss_P: [ 3.22  2.33  1.65  1.96  1.3   1.14  1.04 12.64]\n",
      "Loss_Q: [1.69 1.08 2.   1.26 1.14 0.94 0.   8.11] Loss_P: [ 3.21  2.36  1.65  1.99  1.33  1.17  1.04 12.76]\n",
      "Loss_Q: [1.68 1.11 1.94 1.29 1.1  0.92 0.   8.06] Loss_P: [ 3.18  2.34  1.67  1.96  1.38  1.2   1.03 12.76]\n",
      "Loss_Q: [1.69 1.14 1.89 1.28 1.1  0.95 0.   8.06] Loss_P: [ 3.21  2.36  1.64  1.92  1.31  1.21  1.05 12.7 ]\n",
      "Loss_Q: [1.67 1.08 1.86 1.2  1.06 0.95 0.   7.83] Loss_P: [ 3.17  2.36  1.62  1.9   1.26  1.19  1.05 12.56]\n",
      "Loss_Q: [1.74 1.09 1.88 1.18 1.07 0.94 0.   7.9 ] Loss_P: [ 3.18  2.35  1.63  1.96  1.27  1.18  1.05 12.63]\n",
      "Loss_Q: [1.7  1.03 1.81 1.12 1.1  0.92 0.   7.69] Loss_P: [ 3.18  2.3   1.62  1.93  1.17  1.14  1.04 12.38]\n",
      "Loss_Q: [1.67 1.08 1.81 1.2  1.14 0.91 0.   7.82] Loss_P: [ 3.18  2.34  1.6   1.88  1.25  1.19  1.   12.44]\n",
      "Loss_Q: [1.65 1.08 1.88 1.25 1.13 0.89 0.   7.88] Loss_P: [ 3.17  2.33  1.64  1.96  1.31  1.2   0.98 12.6 ]\n",
      "Loss_Q: [1.65 1.1  1.85 1.28 1.1  0.94 0.   7.93] Loss_P: [ 3.19  2.3   1.61  1.95  1.37  1.15  1.02 12.59]\n",
      "Loss_Q: [1.58 1.1  1.9  1.33 1.13 0.92 0.   7.96] Loss_P: [ 3.18  2.3   1.61  2.    1.38  1.2   1.03 12.69]\n",
      "Loss_Q: [1.57 1.06 1.88 1.33 1.12 0.91 0.   7.86] Loss_P: [ 3.17  2.27  1.59  1.98  1.43  1.23  0.99 12.65]\n",
      "Loss_Q: [1.61 1.09 1.87 1.36 1.16 0.92 0.   8.01] Loss_P: [ 3.19  2.29  1.61  1.94  1.42  1.24  1.01 12.71]\n",
      "Loss_Q: [1.72 1.08 1.89 1.3  1.14 0.9  0.   8.03] Loss_P: [ 3.18  2.38  1.63  1.96  1.42  1.25  1.   12.82]\n",
      "Loss_Q: [1.67 1.11 1.91 1.29 1.13 0.89 0.   8.01] Loss_P: [ 3.23  2.4   1.63  1.92  1.35  1.19  1.   12.71]\n",
      "Loss_Q: [1.66 1.07 1.94 1.35 1.15 0.88 0.   8.05] Loss_P: [ 3.2   2.35  1.62  1.99  1.39  1.23  0.98 12.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.66 1.04 1.88 1.34 1.14 0.89 0.   7.97] Loss_P: [ 3.19  2.34  1.67  1.96  1.35  1.24  0.99 12.75]\n",
      "Loss_Q: [1.66 1.07 1.9  1.28 1.16 0.89 0.   7.94] Loss_P: [ 3.22  2.31  1.61  1.93  1.33  1.27  0.99 12.66]\n",
      "Loss_Q: [1.65 1.1  1.92 1.3  1.18 0.89 0.   8.04] Loss_P: [ 3.15  2.35  1.64  1.96  1.34  1.25  0.96 12.64]\n",
      "Loss_Q: [1.61 1.09 1.87 1.22 1.15 0.88 0.   7.83] Loss_P: [ 3.15  2.32  1.67  1.97  1.37  1.26  0.97 12.71]\n",
      "Loss_Q: [1.63 1.16 1.96 1.31 1.14 0.87 0.   8.07] Loss_P: [ 3.23  2.28  1.7   1.97  1.43  1.24  0.98 12.82]\n",
      "Loss_Q: [1.55 1.14 1.93 1.36 1.18 0.88 0.   8.05] Loss_P: [ 3.18  2.32  1.64  1.93  1.43  1.29  0.98 12.77]\n",
      "Loss_Q: [1.7  1.15 1.99 1.38 1.17 0.87 0.   8.25] Loss_P: [ 3.2   2.29  1.68  1.97  1.43  1.23  0.98 12.78]\n",
      "Loss_Q: [1.63 1.11 1.97 1.38 1.16 0.89 0.   8.14] Loss_P: [ 3.18  2.32  1.64  1.99  1.43  1.25  0.96 12.79]\n",
      "Loss_Q: [1.57 1.18 1.97 1.37 1.16 0.83 0.   8.08] Loss_P: [ 3.17  2.28  1.66  1.98  1.43  1.23  0.93 12.68]\n",
      "Loss_Q: [1.66 1.16 1.91 1.34 1.16 0.85 0.   8.08] Loss_P: [ 3.17  2.31  1.68  1.96  1.4   1.29  0.92 12.73]\n",
      "Loss_Q: [1.69 1.17 1.95 1.39 1.18 0.85 0.   8.22] Loss_P: [ 3.19  2.38  1.7   1.95  1.42  1.25  0.93 12.82]\n",
      "Loss_Q: [1.76 1.15 1.93 1.35 1.13 0.84 0.   8.17] Loss_P: [ 3.15  2.43  1.73  1.98  1.41  1.19  0.95 12.85]\n",
      "Loss_Q: [1.65 1.13 1.94 1.32 1.12 0.85 0.   8.02] Loss_P: [ 3.22  2.35  1.69  1.96  1.38  1.22  0.9  12.71]\n",
      "Loss_Q: [1.7  1.14 1.93 1.26 1.14 0.82 0.   8.  ] Loss_P: [ 3.19  2.3   1.67  2.02  1.31  1.24  0.92 12.64]\n",
      "Loss_Q: [1.65 1.09 1.89 1.25 1.11 0.83 0.   7.81] Loss_P: [ 3.18  2.29  1.64  1.91  1.31  1.18  0.94 12.45]\n",
      "Loss_Q: [1.6  1.07 1.92 1.23 1.12 0.84 0.   7.77] Loss_P: [ 3.17  2.35  1.62  1.96  1.31  1.19  0.92 12.53]\n",
      "Loss_Q: [1.64 1.1  1.91 1.21 1.12 0.82 0.   7.8 ] Loss_P: [ 3.18  2.39  1.64  1.97  1.28  1.2   0.9  12.57]\n",
      "Loss_Q: [1.63 1.07 1.9  1.13 1.12 0.82 0.   7.67] Loss_P: [ 3.17  2.31  1.63  1.97  1.2   1.2   0.89 12.37]\n",
      "Loss_Q: [1.62 1.07 1.9  1.15 1.13 0.83 0.   7.71] Loss_P: [ 3.22  2.29  1.61  2.01  1.23  1.19  0.91 12.47]\n",
      "Loss_Q: [1.6  1.09 2.   1.14 1.17 0.83 0.   7.82] Loss_P: [ 3.19  2.28  1.61  2.03  1.2   1.21  0.92 12.44]\n",
      "Loss_Q: [1.64 1.   1.97 1.11 1.1  0.82 0.   7.63] Loss_P: [ 3.14  2.33  1.6   2.03  1.21  1.2   0.92 12.44]\n",
      "Loss_Q: [1.63 1.08 2.   1.14 1.1  0.8  0.   7.75] Loss_P: [ 3.17  2.39  1.61  2.07  1.22  1.19  0.92 12.58]\n",
      "Loss_Q: [1.66 1.06 1.98 1.17 1.08 0.83 0.   7.79] Loss_P: [ 3.18  2.42  1.59  2.04  1.24  1.16  0.92 12.55]\n",
      "Loss_Q: [1.7  1.08 1.99 1.22 1.11 0.83 0.   7.93] Loss_P: [ 3.18  2.33  1.57  2.02  1.22  1.19  0.93 12.44]\n",
      "Loss_Q: [1.64 1.04 2.04 1.19 1.08 0.85 0.   7.86] Loss_P: [ 3.18  2.35  1.57  2.06  1.29  1.2   0.94 12.59]\n",
      "Loss_Q: [1.66 1.11 2.04 1.2  1.11 0.8  0.   7.93] Loss_P: [ 3.22  2.39  1.57  2.06  1.25  1.2   0.92 12.62]\n",
      "Loss_Q: [1.66 1.11 2.12 1.25 1.14 0.83 0.   8.11] Loss_P: [ 3.2   2.34  1.64  2.04  1.3   1.22  0.9  12.63]\n",
      "Loss_Q: [1.59 1.09 2.06 1.3  1.17 0.84 0.   8.05] Loss_P: [ 3.18  2.31  1.61  2.07  1.34  1.26  0.94 12.72]\n",
      "Loss_Q: [1.63 1.07 2.03 1.24 1.13 0.86 0.   7.97] Loss_P: [ 3.22  2.38  1.6   2.06  1.33  1.26  0.94 12.79]\n",
      "Loss_Q: [1.69 1.02 2.04 1.27 1.16 0.87 0.   8.06] Loss_P: [ 3.18  2.31  1.59  2.03  1.32  1.24  0.92 12.58]\n",
      "Loss_Q: [1.61 1.06 2.03 1.32 1.19 0.85 0.   8.06] Loss_P: [ 3.2   2.31  1.63  2.04  1.31  1.26  0.97 12.73]\n",
      "Loss_Q: [1.67 1.11 2.04 1.27 1.19 0.87 0.   8.15] Loss_P: [ 3.16  2.36  1.65  2.08  1.33  1.25  0.96 12.8 ]\n",
      "Loss_Q: [1.62 1.04 2.02 1.27 1.15 0.89 0.   7.99] Loss_P: [ 3.21  2.32  1.58  2.04  1.29  1.24  0.97 12.66]\n",
      "Loss_Q: [1.64 1.05 2.06 1.19 1.15 0.87 0.   7.97] Loss_P: [ 3.18  2.35  1.58  2.08  1.26  1.24  0.99 12.67]\n",
      "Loss_Q: [1.75 1.14 2.02 1.17 1.14 0.88 0.   8.09] Loss_P: [ 3.17  2.47  1.63  2.03  1.21  1.25  0.98 12.74]\n",
      "Loss_Q: [1.79 1.14 1.97 1.13 1.15 0.88 0.   8.06] Loss_P: [ 3.2   2.4   1.64  2.04  1.2   1.28  0.98 12.74]\n",
      "Loss_Q: [1.71 1.02 1.98 1.14 1.16 0.9  0.   7.91] Loss_P: [ 3.2   2.38  1.66  2.02  1.25  1.23  1.01 12.74]\n",
      "Loss_Q: [1.79 1.11 2.04 1.19 1.17 0.89 0.   8.19] Loss_P: [ 3.18  2.45  1.71  2.07  1.24  1.29  1.02 12.96]\n",
      "Loss_Q: [1.75 1.14 2.01 1.15 1.18 0.91 0.   8.15] Loss_P: [ 3.2   2.4   1.69  2.07  1.21  1.25  1.02 12.85]\n",
      "Loss_Q: [1.7  1.08 2.03 1.2  1.21 0.92 0.   8.13] Loss_P: [ 3.19  2.36  1.64  2.02  1.23  1.27  1.   12.71]\n",
      "Loss_Q: [1.69 1.05 2.   1.13 1.24 0.92 0.   8.02] Loss_P: [ 3.18  2.39  1.57  2.03  1.21  1.34  1.01 12.73]\n",
      "Loss_Q: [1.73 1.1  1.97 1.11 1.22 0.9  0.   8.02] Loss_P: [ 3.21  2.38  1.64  1.98  1.14  1.29  1.   12.62]\n",
      "Loss_Q: [1.76 1.15 2.   1.11 1.21 0.92 0.   8.15] Loss_P: [ 3.17  2.41  1.64  2.01  1.16  1.27  1.02 12.67]\n",
      "Loss_Q: [1.7  1.1  2.01 1.1  1.15 0.92 0.   7.96] Loss_P: [ 3.21  2.41  1.65  2.06  1.19  1.28  1.04 12.84]\n",
      "Loss_Q: [1.73 1.11 2.02 1.04 1.14 0.92 0.   7.95] Loss_P: [ 3.17  2.44  1.65  2.01  1.07  1.22  1.03 12.59]\n",
      "Loss_Q: [1.78 1.14 2.05 1.05 1.12 0.92 0.   8.05] Loss_P: [ 3.19  2.42  1.7   2.05  1.12  1.21  1.06 12.76]\n",
      "Loss_Q: [1.66 1.08 1.97 1.06 1.13 0.94 0.   7.84] Loss_P: [ 3.17  2.42  1.66  2.03  1.09  1.21  1.05 12.62]\n",
      "Loss_Q: [1.72 1.11 1.99 1.03 1.09 0.94 0.   7.88] Loss_P: [ 3.17  2.47  1.64  1.94  1.07  1.18  1.01 12.48]\n",
      "Loss_Q: [1.76 1.09 1.95 1.04 1.09 0.92 0.   7.85] Loss_P: [ 3.17  2.49  1.69  2.03  1.08  1.21  1.03 12.7 ]\n",
      "Loss_Q: [1.73 1.07 1.94 1.05 1.13 0.95 0.   7.87] Loss_P: [ 3.21  2.46  1.66  1.95  1.07  1.19  1.04 12.57]\n",
      "Loss_Q: [1.76 1.13 1.89 1.01 1.16 0.95 0.   7.89] Loss_P: [ 3.22  2.45  1.65  1.9   1.04  1.21  1.05 12.52]\n",
      "Loss_Q: [1.75 1.14 1.91 1.07 1.12 0.93 0.   7.92] Loss_P: [ 3.22  2.45  1.69  1.94  1.07  1.21  1.03 12.62]\n",
      "Loss_Q: [1.74 1.17 1.92 1.03 1.11 0.91 0.   7.88] Loss_P: [ 3.18  2.45  1.67  1.92  1.1   1.23  1.   12.55]\n",
      "Loss_Q: [1.76 1.1  1.9  1.02 1.13 0.9  0.   7.81] Loss_P: [ 3.15  2.49  1.63  1.93  1.05  1.22  1.01 12.49]\n",
      "Loss_Q: [1.76 1.12 1.91 1.05 1.14 0.91 0.   7.89] Loss_P: [ 3.11  2.56  1.65  2.    1.1   1.19  1.02 12.63]\n",
      "Loss_Q: [1.8  1.12 1.98 1.02 1.14 0.9  0.   7.96] Loss_P: [ 3.18  2.5   1.63  2.02  1.06  1.23  1.01 12.62]\n",
      "Loss_Q: [1.84 1.17 1.97 1.01 1.09 0.92 0.   8.  ] Loss_P: [ 3.17  2.51  1.65  2.04  1.04  1.24  0.98 12.63]\n",
      "Loss_Q: [1.79 1.12 1.99 1.02 1.15 0.9  0.   7.96] Loss_P: [ 3.15  2.5   1.72  2.06  1.07  1.23  0.99 12.73]\n",
      "Loss_Q: [1.67 1.04 1.9  1.03 1.11 0.88 0.   7.64] Loss_P: [ 3.16  2.46  1.64  2.    1.06  1.23  0.98 12.53]\n",
      "Loss_Q: [1.71 1.1  1.92 1.05 1.14 0.9  0.   7.82] Loss_P: [ 3.18  2.46  1.62  1.93  1.07  1.21  0.97 12.44]\n",
      "Loss_Q: [1.69 1.11 1.86 1.03 1.17 0.89 0.   7.74] Loss_P: [ 3.17  2.46  1.63  1.93  1.11  1.23  1.   12.54]\n",
      "Loss_Q: [1.84 1.12 1.92 0.99 1.12 0.89 0.   7.87] Loss_P: [ 3.2   2.49  1.65  1.9   1.03  1.19  0.99 12.45]\n",
      "Loss_Q: [1.82 1.16 1.87 1.02 1.06 0.87 0.   7.8 ] Loss_P: [ 3.16  2.51  1.65  1.94  1.08  1.21  0.96 12.51]\n",
      "Loss_Q: [1.83 1.09 1.87 1.06 1.11 0.9  0.   7.86] Loss_P: [ 3.18  2.45  1.67  1.93  1.09  1.2   0.97 12.49]\n",
      "Loss_Q: [1.75 1.13 1.9  1.04 1.14 0.89 0.   7.85] Loss_P: [ 3.17  2.42  1.66  1.92  1.06  1.2   0.96 12.4 ]\n",
      "Loss_Q: [1.76 1.07 1.89 1.11 1.17 0.87 0.   7.87] Loss_P: [ 3.2   2.4   1.64  1.91  1.1   1.21  0.94 12.41]\n",
      "Loss_Q: [1.76 1.11 1.9  1.08 1.14 0.86 0.   7.83] Loss_P: [ 3.17  2.46  1.7   1.94  1.19  1.26  0.94 12.66]\n",
      "Loss_Q: [1.72 1.09 1.92 1.09 1.12 0.87 0.   7.81] Loss_P: [ 3.19  2.45  1.63  1.9   1.13  1.25  0.96 12.51]\n",
      "Loss_Q: [1.75 1.05 1.89 1.05 1.17 0.85 0.   7.78] Loss_P: [ 3.21  2.42  1.6   1.92  1.16  1.27  0.95 12.53]\n",
      "Loss_Q: [1.75 1.15 1.91 1.1  1.14 0.86 0.   7.91] Loss_P: [ 3.22  2.47  1.61  1.95  1.14  1.25  0.98 12.63]\n",
      "Loss_Q: [1.75 1.12 1.92 1.02 1.12 0.84 0.   7.77] Loss_P: [ 3.19  2.5   1.65  1.94  1.06  1.2   0.93 12.47]\n",
      "Loss_Q: [1.76 1.12 1.88 1.06 1.13 0.87 0.   7.81] Loss_P: [ 3.19  2.48  1.65  1.97  1.11  1.21  0.94 12.55]\n",
      "Loss_Q: [1.78 1.14 1.98 1.02 1.12 0.9  0.   7.93] Loss_P: [ 3.17  2.5   1.63  1.99  1.07  1.2   0.95 12.51]\n",
      "Loss_Q: [1.82 1.12 1.94 1.04 1.14 0.89 0.   7.96] Loss_P: [ 3.16  2.52  1.67  1.97  1.08  1.2   0.98 12.57]\n",
      "Loss_Q: [1.82 1.15 1.9  1.02 1.11 0.87 0.   7.88] Loss_P: [ 3.18  2.58  1.68  1.99  1.08  1.22  0.95 12.69]\n",
      "Loss_Q: [1.86 1.16 1.93 1.04 1.12 0.87 0.   7.98] Loss_P: [ 3.15  2.57  1.67  1.94  1.1   1.24  0.97 12.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.79 1.11 1.85 1.07 1.16 0.9  0.   7.87] Loss_P: [ 3.18  2.54  1.68  1.86  1.13  1.24  1.   12.62]\n",
      "Loss_Q: [1.88 1.18 1.89 1.06 1.16 0.89 0.   8.05] Loss_P: [ 3.17  2.49  1.72  1.92  1.14  1.25  0.98 12.66]\n",
      "Loss_Q: [1.79 1.15 1.86 1.06 1.17 0.87 0.   7.91] Loss_P: [ 3.13  2.57  1.67  1.91  1.1   1.26  0.99 12.62]\n",
      "Loss_Q: [1.84 1.15 1.89 0.98 1.17 0.85 0.   7.89] Loss_P: [ 3.14  2.56  1.68  1.9   1.03  1.24  0.98 12.53]\n",
      "Loss_Q: [1.81 1.07 1.9  1.04 1.21 0.87 0.   7.9 ] Loss_P: [ 3.18  2.56  1.72  1.96  1.11  1.3   0.96 12.78]\n",
      "Loss_Q: [1.84 1.15 1.91 1.04 1.22 0.86 0.   8.02] Loss_P: [ 3.13  2.55  1.7   1.95  1.13  1.33  0.96 12.74]\n",
      "Loss_Q: [1.82 1.19 1.91 1.12 1.23 0.89 0.   8.16] Loss_P: [ 3.18  2.53  1.73  1.9   1.14  1.35  0.97 12.79]\n",
      "Loss_Q: [1.78 1.17 1.91 1.03 1.22 0.86 0.   7.99] Loss_P: [ 3.2   2.46  1.74  1.92  1.11  1.32  0.97 12.74]\n",
      "Loss_Q: [1.72 1.18 1.88 1.02 1.27 0.88 0.   7.94] Loss_P: [ 3.18  2.41  1.69  1.93  1.13  1.33  0.96 12.64]\n",
      "Loss_Q: [1.67 1.16 1.87 1.07 1.23 0.92 0.   7.92] Loss_P: [ 3.2   2.34  1.67  1.91  1.13  1.35  0.96 12.56]\n",
      "Loss_Q: [1.66 1.13 1.84 1.03 1.24 0.9  0.   7.8 ] Loss_P: [ 3.21  2.33  1.72  1.88  1.13  1.3   0.95 12.52]\n",
      "Loss_Q: [1.71 1.19 1.87 1.04 1.21 0.87 0.   7.9 ] Loss_P: [ 3.21  2.35  1.73  1.93  1.13  1.32  0.98 12.66]\n",
      "Loss_Q: [1.62 1.18 1.9  1.07 1.2  0.88 0.   7.84] Loss_P: [ 3.24  2.32  1.72  1.93  1.12  1.31  0.96 12.6 ]\n",
      "Loss_Q: [1.7  1.21 1.98 1.08 1.25 0.85 0.   8.07] Loss_P: [ 3.22  2.33  1.7   1.96  1.14  1.3   0.93 12.58]\n",
      "Loss_Q: [1.7  1.23 1.87 1.05 1.19 0.84 0.   7.89] Loss_P: [ 3.19  2.38  1.72  1.95  1.12  1.28  0.93 12.57]\n",
      "Loss_Q: [1.67 1.18 1.93 1.09 1.25 0.83 0.   7.95] Loss_P: [ 3.19  2.34  1.74  1.97  1.2   1.33  0.92 12.69]\n",
      "Loss_Q: [1.6  1.14 1.9  1.11 1.25 0.83 0.   7.82] Loss_P: [ 3.23  2.28  1.73  1.98  1.13  1.33  0.94 12.62]\n",
      "Loss_Q: [1.61 1.18 1.95 1.11 1.23 0.86 0.   7.94] Loss_P: [ 3.21  2.29  1.69  1.99  1.19  1.33  0.93 12.64]\n",
      "Loss_Q: [1.63 1.18 2.05 1.1  1.22 0.88 0.   8.07] Loss_P: [ 3.22  2.28  1.67  2.04  1.16  1.28  0.99 12.64]\n",
      "Loss_Q: [1.59 1.14 1.98 1.07 1.2  0.87 0.   7.84] Loss_P: [ 3.23  2.26  1.67  2.03  1.18  1.29  0.99 12.64]\n",
      "Loss_Q: [1.58 1.14 1.95 1.1  1.17 0.87 0.   7.81] Loss_P: [ 3.21  2.33  1.66  2.04  1.19  1.36  0.96 12.75]\n",
      "Loss_Q: [1.65 1.16 1.96 1.12 1.18 0.89 0.   7.96] Loss_P: [ 3.18  2.34  1.69  2.02  1.18  1.28  1.   12.68]\n",
      "Loss_Q: [1.73 1.17 1.97 1.13 1.2  0.88 0.   8.08] Loss_P: [ 3.2   2.34  1.72  1.99  1.22  1.28  0.95 12.71]\n",
      "Loss_Q: [1.63 1.23 1.96 1.13 1.17 0.84 0.   7.95] Loss_P: [ 3.24  2.29  1.77  1.99  1.23  1.29  0.94 12.74]\n",
      "Loss_Q: [1.6  1.2  1.97 1.19 1.18 0.85 0.   8.  ] Loss_P: [ 3.21  2.34  1.75  2.02  1.22  1.3   0.92 12.76]\n",
      "Loss_Q: [1.6  1.24 1.99 1.19 1.22 0.84 0.   8.08] Loss_P: [ 3.23  2.23  1.73  2.04  1.25  1.36  0.92 12.76]\n",
      "Loss_Q: [1.58 1.19 1.96 1.21 1.22 0.84 0.   8.  ] Loss_P: [ 3.23  2.25  1.74  2.04  1.28  1.34  0.93 12.81]\n",
      "Loss_Q: [1.6  1.21 1.97 1.23 1.18 0.83 0.   8.03] Loss_P: [ 3.17  2.31  1.64  2.02  1.28  1.28  0.9  12.61]\n",
      "Loss_Q: [1.56 1.11 1.99 1.2  1.19 0.82 0.   7.87] Loss_P: [ 3.17  2.32  1.65  2.    1.23  1.31  0.9  12.58]\n",
      "Loss_Q: [1.59 1.16 2.   1.18 1.18 0.83 0.   7.94] Loss_P: [ 3.2   2.19  1.68  2.03  1.3   1.32  0.9  12.61]\n",
      "Loss_Q: [1.58 1.19 2.   1.23 1.19 0.83 0.   8.01] Loss_P: [ 3.25  2.21  1.66  2.05  1.3   1.28  0.92 12.67]\n",
      "Loss_Q: [1.59 1.16 1.99 1.19 1.2  0.83 0.   7.96] Loss_P: [ 3.23  2.21  1.65  2.02  1.3   1.32  0.93 12.66]\n",
      "Loss_Q: [1.52 1.17 1.99 1.22 1.2  0.84 0.   7.94] Loss_P: [ 3.23  2.2   1.66  2.04  1.33  1.32  0.9  12.68]\n",
      "Loss_Q: [1.6  1.15 2.04 1.19 1.18 0.82 0.   7.98] Loss_P: [ 3.16  2.28  1.64  2.07  1.31  1.3   0.94 12.69]\n",
      "Loss_Q: [1.56 1.19 2.06 1.26 1.21 0.83 0.   8.12] Loss_P: [ 3.26  2.23  1.66  2.03  1.28  1.34  0.94 12.75]\n",
      "Loss_Q: [1.51 1.21 2.04 1.22 1.25 0.83 0.   8.05] Loss_P: [ 3.24  2.22  1.7   2.03  1.31  1.37  0.93 12.79]\n",
      "Loss_Q: [1.55 1.21 2.09 1.26 1.27 0.83 0.   8.21] Loss_P: [ 3.28  2.21  1.68  2.11  1.33  1.38  0.93 12.92]\n",
      "Loss_Q: [1.62 1.19 2.09 1.27 1.25 0.85 0.   8.27] Loss_P: [ 3.23  2.22  1.68  2.14  1.35  1.38  0.91 12.91]\n",
      "Loss_Q: [1.51 1.22 2.07 1.22 1.22 0.83 0.   8.08] Loss_P: [ 3.2   2.23  1.69  2.1   1.35  1.35  0.92 12.84]\n",
      "Loss_Q: [1.62 1.2  2.11 1.21 1.24 0.83 0.   8.2 ] Loss_P: [ 3.27  2.23  1.67  2.1   1.37  1.33  0.93 12.92]\n",
      "Loss_Q: [1.55 1.19 2.08 1.28 1.25 0.87 0.   8.22] Loss_P: [ 3.25  2.22  1.65  2.1   1.39  1.33  0.97 12.89]\n",
      "Loss_Q: [1.55 1.17 2.1  1.22 1.27 0.88 0.   8.2 ] Loss_P: [ 3.26  2.2   1.68  2.15  1.38  1.37  0.99 13.03]\n",
      "Loss_Q: [1.63 1.15 2.1  1.27 1.26 0.87 0.   8.28] Loss_P: [ 3.21  2.25  1.63  2.12  1.34  1.38  0.94 12.87]\n",
      "Loss_Q: [1.55 1.12 2.03 1.26 1.27 0.86 0.   8.1 ] Loss_P: [ 3.2   2.23  1.66  2.08  1.33  1.39  0.94 12.83]\n",
      "Loss_Q: [1.52 1.18 2.03 1.2  1.25 0.84 0.   8.03] Loss_P: [ 3.23  2.19  1.63  2.04  1.33  1.37  0.93 12.71]\n",
      "Loss_Q: [1.48 1.17 2.04 1.26 1.26 0.83 0.   8.03] Loss_P: [ 3.21  2.19  1.65  2.07  1.34  1.36  0.95 12.78]\n",
      "Loss_Q: [1.51 1.14 2.07 1.23 1.23 0.84 0.   8.02] Loss_P: [ 3.22  2.23  1.61  2.06  1.3   1.38  0.92 12.71]\n",
      "Loss_Q: [1.54 1.04 2.04 1.2  1.26 0.82 0.   7.89] Loss_P: [ 3.21  2.14  1.58  2.05  1.32  1.38  0.93 12.61]\n",
      "Loss_Q: [1.56 1.11 2.06 1.21 1.28 0.84 0.   8.06] Loss_P: [ 3.2   2.2   1.53  2.08  1.29  1.39  0.95 12.63]\n",
      "Loss_Q: [1.51 1.1  2.09 1.18 1.24 0.88 0.   8.  ] Loss_P: [ 3.24  2.22  1.63  2.05  1.32  1.38  0.97 12.81]\n",
      "Loss_Q: [1.53 1.11 2.07 1.16 1.29 0.87 0.   8.04] Loss_P: [ 3.19  2.22  1.59  2.03  1.35  1.37  0.99 12.74]\n",
      "Loss_Q: [1.49 1.08 2.07 1.2  1.27 0.89 0.   8.  ] Loss_P: [ 3.2   2.2   1.54  2.06  1.31  1.36  0.98 12.65]\n",
      "Loss_Q: [1.52 1.08 2.01 1.16 1.27 0.88 0.   7.92] Loss_P: [ 3.21  2.25  1.61  2.09  1.28  1.39  0.98 12.8 ]\n",
      "Loss_Q: [1.57 1.11 2.06 1.17 1.31 0.89 0.   8.11] Loss_P: [ 3.2   2.2   1.55  2.02  1.28  1.37  1.   12.62]\n",
      "Loss_Q: [1.49 1.1  1.99 1.14 1.31 0.91 0.   7.95] Loss_P: [ 3.2   2.19  1.59  1.97  1.27  1.41  1.03 12.64]\n",
      "Loss_Q: [1.63 1.13 2.01 1.13 1.27 0.92 0.   8.09] Loss_P: [ 3.21  2.26  1.6   1.96  1.23  1.37  1.02 12.63]\n",
      "Loss_Q: [1.57 1.09 1.9  1.11 1.26 0.9  0.   7.84] Loss_P: [ 3.2   2.27  1.64  1.97  1.16  1.36  0.98 12.59]\n",
      "Loss_Q: [1.59 1.09 1.95 1.04 1.27 0.91 0.   7.86] Loss_P: [ 3.24  2.27  1.63  1.97  1.15  1.37  1.02 12.65]\n",
      "Loss_Q: [1.58 1.13 1.94 1.05 1.22 0.89 0.   7.81] Loss_P: [ 3.2   2.28  1.6   1.97  1.13  1.34  0.99 12.5 ]\n",
      "Loss_Q: [1.59 1.14 1.92 1.09 1.26 0.9  0.   7.89] Loss_P: [ 3.23  2.23  1.61  1.97  1.1   1.34  0.99 12.47]\n",
      "Loss_Q: [1.58 1.12 1.95 1.06 1.23 0.93 0.   7.87] Loss_P: [ 3.19  2.22  1.63  1.97  1.16  1.36  0.98 12.51]\n",
      "Loss_Q: [1.62 1.12 1.99 1.1  1.29 0.9  0.   8.02] Loss_P: [ 3.17  2.31  1.64  1.95  1.16  1.39  0.99 12.63]\n",
      "Loss_Q: [1.71 1.12 2.03 1.08 1.33 0.89 0.   8.16] Loss_P: [ 3.14  2.32  1.66  1.98  1.2   1.39  0.99 12.67]\n",
      "Loss_Q: [1.65 1.21 1.95 1.11 1.31 0.9  0.   8.12] Loss_P: [ 3.18  2.33  1.66  1.98  1.19  1.39  1.   12.73]\n",
      "Loss_Q: [1.65 1.17 1.97 1.1  1.27 0.9  0.   8.05] Loss_P: [ 3.16  2.33  1.62  1.94  1.2   1.4   1.03 12.67]\n",
      "Loss_Q: [1.63 1.1  1.99 1.15 1.28 0.94 0.   8.1 ] Loss_P: [ 3.19  2.31  1.64  1.97  1.26  1.39  1.03 12.79]\n",
      "Loss_Q: [1.64 1.16 1.96 1.13 1.26 0.93 0.   8.08] Loss_P: [ 3.15  2.31  1.66  1.97  1.21  1.34  1.01 12.66]\n",
      "Loss_Q: [1.58 1.15 2.   1.11 1.28 0.91 0.   8.03] Loss_P: [ 3.19  2.24  1.62  1.98  1.28  1.39  1.   12.7 ]\n",
      "Loss_Q: [1.57 1.17 2.   1.13 1.25 0.91 0.   8.03] Loss_P: [ 3.2   2.25  1.65  1.99  1.23  1.41  0.99 12.71]\n",
      "Loss_Q: [1.57 1.13 1.99 1.12 1.28 0.88 0.   7.97] Loss_P: [ 3.17  2.26  1.65  2.03  1.26  1.43  0.97 12.76]\n",
      "Loss_Q: [1.57 1.16 2.02 1.15 1.31 0.87 0.   8.08] Loss_P: [ 3.2   2.22  1.67  2.08  1.23  1.42  0.94 12.76]\n",
      "Loss_Q: [1.59 1.19 2.03 1.17 1.29 0.87 0.   8.14] Loss_P: [ 3.19  2.27  1.64  2.03  1.29  1.43  0.97 12.82]\n",
      "Loss_Q: [1.58 1.19 2.04 1.13 1.32 0.84 0.   8.1 ] Loss_P: [ 3.14  2.31  1.66  2.06  1.24  1.43  0.92 12.76]\n",
      "Loss_Q: [1.59 1.17 2.07 1.13 1.32 0.81 0.   8.09] Loss_P: [ 3.17  2.34  1.67  2.06  1.23  1.44  0.9  12.81]\n",
      "Loss_Q: [1.57 1.13 2.08 1.12 1.31 0.79 0.   8.  ] Loss_P: [ 3.16  2.32  1.67  2.09  1.26  1.44  0.89 12.85]\n",
      "Loss_Q: [1.67 1.18 2.08 1.14 1.3  0.79 0.   8.17] Loss_P: [ 3.19  2.31  1.66  2.14  1.29  1.42  0.9  12.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.59 1.19 2.08 1.22 1.28 0.82 0.   8.18] Loss_P: [ 3.19  2.22  1.63  2.09  1.27  1.44  0.93 12.76]\n",
      "Loss_Q: [1.55 1.14 2.07 1.19 1.33 0.81 0.   8.08] Loss_P: [ 3.22  2.22  1.67  2.12  1.28  1.44  0.9  12.85]\n",
      "Loss_Q: [1.59 1.15 2.07 1.18 1.33 0.81 0.   8.13] Loss_P: [ 3.25  2.19  1.66  2.1   1.22  1.42  0.88 12.73]\n",
      "Loss_Q: [1.53 1.15 2.02 1.19 1.33 0.81 0.   8.04] Loss_P: [ 3.21  2.15  1.62  2.07  1.31  1.4   0.93 12.68]\n",
      "Loss_Q: [1.56 1.18 2.06 1.17 1.35 0.82 0.   8.13] Loss_P: [ 3.2   2.19  1.62  2.08  1.25  1.48  0.9  12.71]\n",
      "Loss_Q: [1.57 1.23 2.1  1.19 1.32 0.79 0.   8.21] Loss_P: [ 3.2   2.18  1.71  2.05  1.25  1.42  0.88 12.7 ]\n",
      "Loss_Q: [1.44 1.15 2.04 1.15 1.3  0.79 0.   7.87] Loss_P: [ 3.22  2.17  1.73  2.05  1.28  1.45  0.91 12.81]\n",
      "Loss_Q: [1.49 1.13 2.04 1.09 1.29 0.77 0.   7.81] Loss_P: [ 3.18  2.09  1.63  2.05  1.21  1.42  0.84 12.41]\n",
      "Loss_Q: [1.49 1.16 2.03 1.2  1.3  0.78 0.   7.95] Loss_P: [ 3.18  2.14  1.66  2.03  1.23  1.38  0.84 12.45]\n",
      "Loss_Q: [1.52 1.19 1.99 1.18 1.3  0.76 0.   7.95] Loss_P: [ 3.23  2.2   1.6   1.99  1.21  1.41  0.81 12.45]\n",
      "Loss_Q: [1.55 1.11 1.99 1.18 1.3  0.78 0.   7.9 ] Loss_P: [ 3.19  2.23  1.69  1.98  1.3   1.46  0.85 12.7 ]\n",
      "Loss_Q: [1.5  1.22 1.99 1.19 1.28 0.81 0.   7.98] Loss_P: [ 3.23  2.22  1.67  1.98  1.31  1.42  0.86 12.69]\n",
      "Loss_Q: [1.51 1.2  2.03 1.15 1.3  0.8  0.   7.99] Loss_P: [ 3.19  2.18  1.64  2.05  1.3   1.42  0.88 12.67]\n",
      "Loss_Q: [1.51 1.14 2.06 1.23 1.32 0.83 0.   8.09] Loss_P: [ 3.2   2.13  1.67  1.99  1.27  1.41  0.86 12.52]\n",
      "Loss_Q: [1.57 1.16 2.05 1.23 1.32 0.79 0.   8.12] Loss_P: [ 3.21  2.22  1.65  2.04  1.35  1.46  0.91 12.84]\n",
      "Loss_Q: [1.44 1.12 2.03 1.23 1.31 0.88 0.   8.02] Loss_P: [ 3.22  2.17  1.67  2.07  1.35  1.43  0.94 12.85]\n",
      "Loss_Q: [1.53 1.15 2.06 1.27 1.28 0.9  0.   8.18] Loss_P: [ 3.21  2.14  1.63  2.02  1.41  1.41  0.96 12.78]\n",
      "Loss_Q: [1.45 1.14 2.05 1.24 1.3  0.86 0.   8.03] Loss_P: [ 3.23  2.08  1.64  2.    1.38  1.4   0.95 12.69]\n",
      "Loss_Q: [1.44 1.11 2.08 1.28 1.3  0.89 0.   8.11] Loss_P: [ 3.23  2.06  1.6   1.98  1.36  1.4   0.94 12.58]\n",
      "Loss_Q: [1.37 1.11 2.05 1.3  1.28 0.86 0.   7.96] Loss_P: [ 3.21  2.06  1.63  2.02  1.4   1.42  0.94 12.68]\n",
      "Loss_Q: [1.44 1.21 2.06 1.28 1.29 0.85 0.   8.13] Loss_P: [ 3.21  2.18  1.62  2.    1.36  1.41  0.95 12.72]\n",
      "Loss_Q: [1.49 1.15 2.   1.22 1.3  0.83 0.   7.99] Loss_P: [ 3.2   2.16  1.68  2.06  1.33  1.39  0.97 12.78]\n",
      "Loss_Q: [1.5  1.2  1.97 1.23 1.29 0.88 0.   8.07] Loss_P: [ 3.2   2.12  1.7   1.95  1.34  1.41  0.96 12.68]\n",
      "Loss_Q: [1.57 1.21 2.05 1.24 1.28 0.85 0.   8.2 ] Loss_P: [ 3.18  2.18  1.66  2.01  1.33  1.37  0.95 12.67]\n",
      "Loss_Q: [1.5  1.17 2.04 1.2  1.28 0.82 0.   8.02] Loss_P: [ 3.22  2.17  1.68  2.06  1.28  1.39  0.93 12.73]\n",
      "Loss_Q: [1.52 1.16 2.07 1.19 1.31 0.87 0.   8.12] Loss_P: [ 3.21  2.14  1.63  2.    1.34  1.39  0.94 12.65]\n",
      "Loss_Q: [1.54 1.17 2.04 1.15 1.19 0.83 0.   7.92] Loss_P: [ 3.22  2.17  1.71  2.06  1.35  1.39  0.95 12.84]\n",
      "Loss_Q: [1.5  1.17 2.05 1.19 1.25 0.87 0.   8.04] Loss_P: [ 3.18  2.18  1.66  2.02  1.27  1.39  0.95 12.66]\n",
      "Loss_Q: [1.55 1.22 2.02 1.17 1.29 0.86 0.   8.11] Loss_P: [ 3.18  2.23  1.67  2.02  1.34  1.39  0.96 12.77]\n",
      "Loss_Q: [1.59 1.2  2.04 1.23 1.3  0.88 0.   8.25] Loss_P: [ 3.17  2.26  1.73  1.99  1.36  1.4   0.95 12.87]\n",
      "Loss_Q: [1.53 1.17 2.   1.17 1.33 0.88 0.   8.08] Loss_P: [ 3.17  2.23  1.67  2.03  1.32  1.42  0.95 12.79]\n",
      "Loss_Q: [1.58 1.22 2.03 1.2  1.31 0.84 0.   8.17] Loss_P: [ 3.19  2.26  1.66  2.04  1.31  1.39  0.94 12.79]\n",
      "Loss_Q: [1.55 1.18 1.99 1.19 1.28 0.83 0.   8.02] Loss_P: [ 3.19  2.25  1.66  1.96  1.31  1.37  0.96 12.7 ]\n",
      "Loss_Q: [1.5  1.15 2.04 1.17 1.24 0.84 0.   7.94] Loss_P: [ 3.18  2.27  1.69  1.99  1.31  1.38  0.92 12.74]\n",
      "Loss_Q: [1.61 1.18 2.03 1.15 1.28 0.82 0.   8.07] Loss_P: [ 3.19  2.2   1.63  2.04  1.3   1.38  0.94 12.67]\n",
      "Loss_Q: [1.57 1.27 2.05 1.25 1.3  0.87 0.   8.3 ] Loss_P: [ 3.15  2.26  1.67  2.04  1.33  1.43  0.94 12.82]\n",
      "Loss_Q: [1.63 1.23 2.07 1.25 1.29 0.81 0.   8.28] Loss_P: [ 3.17  2.25  1.68  2.    1.31  1.42  0.94 12.77]\n",
      "Loss_Q: [1.63 1.13 2.04 1.19 1.3  0.88 0.   8.16] Loss_P: [ 3.16  2.26  1.66  1.96  1.39  1.44  0.96 12.84]\n",
      "Loss_Q: [1.52 1.2  1.89 1.25 1.32 0.85 0.   8.02] Loss_P: [ 3.2   2.24  1.69  1.97  1.33  1.41  0.95 12.79]\n",
      "Loss_Q: [1.57 1.13 1.92 1.19 1.29 0.83 0.   7.93] Loss_P: [ 3.15  2.25  1.63  1.97  1.33  1.4   0.98 12.71]\n",
      "Loss_Q: [1.64 1.17 1.95 1.17 1.28 0.84 0.   8.05] Loss_P: [ 3.19  2.26  1.67  1.93  1.31  1.43  0.95 12.74]\n",
      "Loss_Q: [1.59 1.18 1.98 1.28 1.35 0.9  0.   8.28] Loss_P: [ 3.16  2.29  1.62  1.97  1.35  1.45  1.   12.83]\n",
      "Loss_Q: [1.54 1.14 1.99 1.2  1.32 0.88 0.   8.06] Loss_P: [ 3.16  2.29  1.63  1.97  1.31  1.43  1.   12.79]\n",
      "Loss_Q: [1.57 1.1  1.98 1.19 1.34 0.84 0.   8.02] Loss_P: [ 3.15  2.24  1.63  2.04  1.31  1.42  0.98 12.77]\n",
      "Loss_Q: [1.58 1.19 1.96 1.15 1.33 0.88 0.   8.1 ] Loss_P: [ 3.16  2.3   1.63  1.97  1.29  1.41  0.97 12.74]\n",
      "Loss_Q: [1.55 1.16 1.95 1.18 1.35 0.85 0.   8.04] Loss_P: [ 3.15  2.35  1.63  2.    1.3   1.44  0.99 12.86]\n",
      "Loss_Q: [1.64 1.16 1.97 1.19 1.32 0.83 0.   8.11] Loss_P: [ 3.17  2.26  1.61  1.96  1.27  1.4   0.97 12.64]\n",
      "Loss_Q: [1.64 1.23 1.96 1.24 1.3  0.85 0.   8.23] Loss_P: [ 3.16  2.28  1.7   1.97  1.33  1.35  0.95 12.73]\n",
      "Loss_Q: [1.58 1.17 1.98 1.22 1.28 0.83 0.   8.06] Loss_P: [ 3.2   2.25  1.63  1.98  1.3   1.36  0.96 12.67]\n",
      "Loss_Q: [1.55 1.24 1.97 1.19 1.29 0.89 0.   8.12] Loss_P: [ 3.18  2.26  1.65  1.96  1.35  1.36  0.99 12.75]\n",
      "Loss_Q: [1.58 1.25 1.95 1.23 1.27 0.88 0.   8.15] Loss_P: [ 3.16  2.28  1.64  1.97  1.34  1.4   1.02 12.8 ]\n",
      "Loss_Q: [1.6  1.24 1.92 1.15 1.28 0.9  0.   8.09] Loss_P: [ 3.17  2.28  1.63  1.96  1.29  1.33  0.99 12.66]\n",
      "Loss_Q: [1.58 1.18 1.97 1.18 1.26 0.85 0.   8.03] Loss_P: [ 3.16  2.32  1.67  1.96  1.33  1.37  1.02 12.84]\n",
      "Loss_Q: [1.58 1.17 2.03 1.21 1.25 0.87 0.   8.11] Loss_P: [ 3.21  2.28  1.62  2.    1.28  1.35  0.99 12.74]\n",
      "Loss_Q: [1.62 1.14 2.04 1.21 1.24 0.87 0.   8.12] Loss_P: [ 3.25  2.23  1.64  1.97  1.31  1.3   1.   12.7 ]\n",
      "Loss_Q: [1.62 1.14 2.01 1.3  1.22 0.89 0.   8.18] Loss_P: [ 3.24  2.21  1.64  2.01  1.35  1.28  1.01 12.74]\n",
      "Loss_Q: [1.54 1.2  2.01 1.22 1.26 0.92 0.   8.15] Loss_P: [ 3.24  2.2   1.65  2.02  1.4   1.34  1.04 12.89]\n",
      "Loss_Q: [1.54 1.14 2.04 1.19 1.26 0.91 0.   8.1 ] Loss_P: [ 3.2   2.2   1.69  2.03  1.33  1.3   1.02 12.77]\n",
      "Loss_Q: [1.53 1.16 2.03 1.2  1.26 0.91 0.   8.09] Loss_P: [ 3.24  2.13  1.66  2.    1.35  1.35  1.02 12.74]\n",
      "Loss_Q: [1.48 1.16 2.04 1.26 1.23 0.89 0.   8.05] Loss_P: [ 3.25  2.16  1.61  2.01  1.35  1.33  0.98 12.69]\n",
      "Loss_Q: [1.51 1.15 2.03 1.25 1.23 0.88 0.   8.04] Loss_P: [ 3.21  2.16  1.65  2.01  1.42  1.34  0.99 12.78]\n",
      "Loss_Q: [1.62 1.15 2.04 1.33 1.22 0.87 0.   8.23] Loss_P: [ 3.19  2.18  1.62  2.03  1.45  1.33  0.98 12.77]\n",
      "Loss_Q: [1.61 1.15 2.1  1.25 1.25 0.9  0.   8.25] Loss_P: [ 3.19  2.2   1.57  2.08  1.42  1.35  1.01 12.82]\n",
      "Loss_Q: [1.55 1.09 2.06 1.22 1.23 0.85 0.   8.01] Loss_P: [ 3.2   2.24  1.65  2.1   1.42  1.36  1.02 12.99]\n",
      "Loss_Q: [1.65 1.16 2.01 1.24 1.24 0.89 0.   8.19] Loss_P: [ 3.2   2.26  1.64  2.01  1.39  1.35  0.96 12.82]\n",
      "Loss_Q: [1.56 1.1  2.05 1.24 1.24 0.86 0.   8.05] Loss_P: [ 3.23  2.23  1.62  2.03  1.34  1.36  0.95 12.76]\n",
      "Loss_Q: [1.52 1.11 2.04 1.26 1.21 0.87 0.   8.01] Loss_P: [ 3.24  2.25  1.6   1.99  1.37  1.37  0.95 12.77]\n",
      "Loss_Q: [1.52 1.15 2.06 1.24 1.29 0.84 0.   8.11] Loss_P: [ 3.18  2.23  1.58  2.05  1.33  1.37  0.95 12.69]\n",
      "Loss_Q: [1.57 1.12 2.04 1.22 1.3  0.85 0.   8.1 ] Loss_P: [ 3.22  2.19  1.62  2.03  1.4   1.38  0.96 12.8 ]\n",
      "Loss_Q: [1.6  1.12 2.04 1.17 1.27 0.86 0.   8.05] Loss_P: [ 3.18  2.22  1.67  2.04  1.36  1.39  0.93 12.79]\n",
      "Loss_Q: [1.63 1.18 2.07 1.22 1.32 0.87 0.   8.29] Loss_P: [ 3.21  2.23  1.67  2.05  1.33  1.4   0.98 12.87]\n",
      "Loss_Q: [1.57 1.22 2.08 1.22 1.27 0.84 0.   8.2 ] Loss_P: [ 3.22  2.29  1.68  2.05  1.38  1.38  0.94 12.93]\n",
      "Loss_Q: [1.64 1.21 2.09 1.26 1.29 0.84 0.   8.33] Loss_P: [ 3.18  2.24  1.61  2.12  1.34  1.4   0.92 12.81]\n",
      "Loss_Q: [1.61 1.17 2.11 1.16 1.28 0.83 0.   8.16] Loss_P: [ 3.18  2.27  1.66  2.08  1.29  1.41  0.95 12.84]\n",
      "Loss_Q: [1.64 1.15 2.14 1.2  1.26 0.85 0.   8.24] Loss_P: [ 3.16  2.35  1.68  2.08  1.31  1.33  0.91 12.82]\n",
      "Loss_Q: [1.68 1.16 2.07 1.22 1.24 0.82 0.   8.19] Loss_P: [ 3.18  2.34  1.69  2.09  1.33  1.37  0.95 12.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.74 1.12 2.08 1.18 1.27 0.84 0.   8.24] Loss_P: [ 3.14  2.44  1.7   2.1   1.31  1.35  0.93 12.97]\n",
      "Loss_Q: [1.7  1.13 2.11 1.19 1.31 0.82 0.   8.26] Loss_P: [ 3.14  2.42  1.62  2.09  1.29  1.4   0.93 12.88]\n",
      "Loss_Q: [1.74 1.15 2.13 1.16 1.33 0.86 0.   8.37] Loss_P: [ 3.18  2.46  1.66  2.1   1.28  1.41  0.91 13.  ]\n",
      "Loss_Q: [1.8  1.16 2.14 1.18 1.35 0.86 0.   8.5 ] Loss_P: [ 3.2   2.42  1.61  2.17  1.35  1.44  0.97 13.16]\n",
      "Loss_Q: [1.73 1.09 2.18 1.19 1.36 0.85 0.   8.4 ] Loss_P: [ 3.15  2.41  1.66  2.18  1.31  1.45  0.95 13.11]\n",
      "Loss_Q: [1.71 1.09 2.2  1.22 1.35 0.88 0.   8.45] Loss_P: [ 3.2   2.4   1.64  2.18  1.31  1.45  0.96 13.14]\n",
      "Loss_Q: [1.7  1.1  2.15 1.21 1.35 0.88 0.   8.39] Loss_P: [ 3.16  2.37  1.63  2.18  1.37  1.44  0.99 13.14]\n",
      "Loss_Q: [1.76 1.17 2.13 1.2  1.32 0.87 0.   8.44] Loss_P: [ 3.14  2.47  1.64  2.11  1.34  1.44  0.97 13.11]\n",
      "Loss_Q: [1.81 1.15 2.11 1.28 1.34 0.9  0.   8.59] Loss_P: [ 3.14  2.47  1.68  2.1   1.37  1.45  1.   13.21]\n",
      "Loss_Q: [1.72 1.13 2.13 1.24 1.35 0.92 0.   8.48] Loss_P: [ 3.16  2.4   1.67  2.1   1.36  1.43  1.   13.14]\n",
      "Loss_Q: [1.73 1.16 2.17 1.26 1.31 0.88 0.   8.52] Loss_P: [ 3.15  2.43  1.71  2.17  1.35  1.43  1.03 13.27]\n",
      "Loss_Q: [1.77 1.17 2.16 1.28 1.32 0.91 0.   8.62] Loss_P: [ 3.2   2.45  1.69  2.18  1.4   1.43  1.07 13.42]\n",
      "Loss_Q: [1.74 1.18 2.14 1.26 1.28 0.9  0.   8.49] Loss_P: [ 3.13  2.43  1.69  2.15  1.45  1.44  1.02 13.31]\n",
      "Loss_Q: [1.78 1.18 2.14 1.32 1.32 0.88 0.   8.62] Loss_P: [ 3.13  2.42  1.74  2.13  1.45  1.45  0.98 13.3 ]\n",
      "Loss_Q: [1.77 1.22 2.15 1.36 1.31 0.88 0.   8.68] Loss_P: [ 3.15  2.47  1.7   2.17  1.47  1.43  1.02 13.4 ]\n",
      "Loss_Q: [1.77 1.22 2.14 1.3  1.31 0.91 0.   8.65] Loss_P: [ 3.16  2.43  1.69  2.17  1.4   1.42  1.02 13.29]\n",
      "Loss_Q: [1.76 1.2  2.09 1.28 1.33 0.91 0.   8.57] Loss_P: [ 3.16  2.37  1.72  2.13  1.38  1.42  1.03 13.22]\n",
      "Loss_Q: [1.72 1.23 2.13 1.28 1.29 0.91 0.   8.56] Loss_P: [ 3.18  2.38  1.76  2.08  1.4   1.4   1.02 13.22]\n",
      "Loss_Q: [1.69 1.18 2.15 1.26 1.28 0.91 0.   8.48] Loss_P: [ 3.18  2.34  1.72  2.14  1.36  1.44  1.04 13.23]\n",
      "Loss_Q: [1.69 1.22 2.09 1.26 1.26 0.87 0.   8.39] Loss_P: [ 3.16  2.33  1.76  2.13  1.34  1.41  1.01 13.15]\n",
      "Loss_Q: [1.63 1.19 2.14 1.25 1.26 0.88 0.   8.34] Loss_P: [ 3.16  2.32  1.69  2.11  1.33  1.38  1.06 13.04]\n",
      "Loss_Q: [1.75 1.18 2.11 1.22 1.23 0.86 0.   8.35] Loss_P: [ 3.16  2.36  1.69  2.07  1.31  1.36  0.99 12.94]\n",
      "Loss_Q: [1.62 1.18 2.1  1.29 1.3  0.91 0.   8.4 ] Loss_P: [ 3.19  2.38  1.72  2.09  1.37  1.37  0.99 13.12]\n",
      "Loss_Q: [1.7  1.18 2.08 1.32 1.26 0.93 0.   8.46] Loss_P: [ 3.14  2.36  1.78  2.06  1.38  1.38  1.02 13.11]\n",
      "Loss_Q: [1.67 1.26 2.05 1.35 1.27 0.97 0.   8.57] Loss_P: [ 3.14  2.41  1.72  2.08  1.47  1.4   1.07 13.3 ]\n",
      "Loss_Q: [1.69 1.25 2.11 1.29 1.28 0.92 0.   8.55] Loss_P: [ 3.12  2.45  1.74  2.1   1.39  1.42  1.07 13.29]\n",
      "Loss_Q: [1.74 1.23 2.16 1.25 1.27 0.95 0.   8.61] Loss_P: [ 3.17  2.44  1.77  2.14  1.35  1.41  1.05 13.33]\n",
      "Loss_Q: [1.76 1.22 2.13 1.29 1.27 0.94 0.   8.61] Loss_P: [ 3.18  2.38  1.74  2.15  1.29  1.41  1.03 13.19]\n",
      "Loss_Q: [1.82 1.2  2.14 1.3  1.29 0.94 0.   8.68] Loss_P: [ 3.18  2.49  1.78  2.17  1.34  1.4   1.08 13.45]\n",
      "Loss_Q: [1.73 1.24 2.13 1.29 1.3  0.91 0.   8.6 ] Loss_P: [ 3.19  2.48  1.74  2.13  1.38  1.41  1.04 13.38]\n",
      "Loss_Q: [1.83 1.27 2.05 1.31 1.3  0.9  0.   8.66] Loss_P: [ 3.17  2.44  1.73  2.15  1.35  1.43  1.   13.28]\n",
      "Loss_Q: [1.87 1.19 2.09 1.3  1.28 0.88 0.   8.61] Loss_P: [ 3.15  2.5   1.77  2.13  1.41  1.44  0.99 13.4 ]\n",
      "Loss_Q: [1.78 1.29 2.1  1.27 1.32 0.92 0.   8.68] Loss_P: [ 3.16  2.47  1.75  2.08  1.38  1.39  1.02 13.24]\n",
      "Loss_Q: [1.72 1.29 2.1  1.27 1.27 0.94 0.   8.59] Loss_P: [ 3.16  2.44  1.74  2.12  1.38  1.41  1.04 13.28]\n",
      "Loss_Q: [1.76 1.25 2.1  1.28 1.32 0.95 0.   8.65] Loss_P: [ 3.19  2.45  1.73  2.18  1.38  1.41  1.06 13.39]\n",
      "Loss_Q: [1.76 1.17 2.12 1.34 1.3  0.97 0.   8.66] Loss_P: [ 3.2   2.4   1.75  2.09  1.39  1.39  1.07 13.3 ]\n",
      "Loss_Q: [1.73 1.21 2.17 1.3  1.28 0.98 0.   8.68] Loss_P: [ 3.2   2.37  1.77  2.18  1.36  1.42  1.08 13.39]\n",
      "Loss_Q: [1.68 1.2  2.17 1.33 1.28 0.93 0.   8.59] Loss_P: [ 3.2   2.34  1.75  2.12  1.44  1.41  1.08 13.34]\n",
      "Loss_Q: [1.69 1.24 2.13 1.36 1.32 0.94 0.   8.67] Loss_P: [ 3.19  2.36  1.71  2.19  1.41  1.41  1.09 13.35]\n",
      "Loss_Q: [1.7  1.22 2.15 1.36 1.32 0.92 0.   8.65] Loss_P: [ 3.19  2.4   1.73  2.18  1.46  1.39  1.08 13.43]\n",
      "Loss_Q: [1.74 1.27 2.09 1.31 1.32 0.92 0.   8.64] Loss_P: [ 3.19  2.35  1.79  2.13  1.47  1.42  1.09 13.42]\n",
      "Loss_Q: [1.75 1.27 2.05 1.34 1.32 0.96 0.   8.68] Loss_P: [ 3.21  2.36  1.73  2.14  1.42  1.38  1.06 13.3 ]\n",
      "Loss_Q: [1.75 1.24 2.05 1.37 1.27 0.95 0.   8.62] Loss_P: [ 3.23  2.32  1.71  2.11  1.48  1.4   1.04 13.3 ]\n",
      "Loss_Q: [1.63 1.18 2.07 1.39 1.3  0.95 0.   8.52] Loss_P: [ 3.18  2.33  1.73  2.12  1.5   1.44  1.06 13.36]\n",
      "Loss_Q: [1.78 1.21 2.09 1.43 1.28 0.96 0.   8.75] Loss_P: [ 3.18  2.34  1.76  2.06  1.49  1.37  1.05 13.25]\n",
      "Loss_Q: [1.75 1.23 2.09 1.38 1.24 0.94 0.   8.63] Loss_P: [ 3.14  2.37  1.73  2.15  1.47  1.36  1.08 13.3 ]\n",
      "Loss_Q: [1.81 1.22 2.1  1.39 1.26 0.96 0.   8.73] Loss_P: [ 3.2   2.37  1.69  2.14  1.45  1.34  1.07 13.26]\n",
      "Loss_Q: [1.7  1.17 2.09 1.4  1.24 0.96 0.   8.55] Loss_P: [ 3.16  2.33  1.69  2.1   1.48  1.38  1.09 13.22]\n",
      "Loss_Q: [1.64 1.23 2.11 1.37 1.28 0.99 0.   8.62] Loss_P: [ 3.21  2.3   1.7   2.09  1.49  1.38  1.1  13.28]\n",
      "Loss_Q: [1.65 1.23 2.16 1.43 1.3  0.98 0.   8.75] Loss_P: [ 3.18  2.32  1.73  2.11  1.51  1.44  1.15 13.44]\n",
      "Loss_Q: [1.7  1.22 2.15 1.42 1.26 0.97 0.   8.72] Loss_P: [ 3.21  2.34  1.74  2.12  1.51  1.39  1.15 13.46]\n",
      "Loss_Q: [1.7  1.21 2.16 1.42 1.23 0.96 0.   8.67] Loss_P: [ 3.2   2.34  1.77  2.13  1.5   1.36  1.12 13.43]\n",
      "Loss_Q: [1.66 1.23 2.08 1.38 1.25 0.98 0.   8.59] Loss_P: [ 3.16  2.4   1.72  2.15  1.45  1.36  1.12 13.37]\n",
      "Loss_Q: [1.71 1.22 2.09 1.33 1.2  0.98 0.   8.53] Loss_P: [ 3.22  2.36  1.69  2.14  1.44  1.38  1.12 13.36]\n",
      "Loss_Q: [1.68 1.24 2.12 1.37 1.25 0.95 0.   8.61] Loss_P: [ 3.16  2.34  1.72  2.1   1.47  1.4   1.11 13.29]\n",
      "Loss_Q: [1.64 1.28 2.08 1.36 1.25 0.97 0.   8.58] Loss_P: [ 3.21  2.34  1.74  2.16  1.46  1.36  1.1  13.36]\n",
      "Loss_Q: [1.65 1.21 2.12 1.43 1.29 0.99 0.   8.69] Loss_P: [ 3.2   2.38  1.68  2.11  1.49  1.39  1.09 13.34]\n",
      "Loss_Q: [1.69 1.22 2.14 1.41 1.23 0.96 0.   8.65] Loss_P: [ 3.17  2.34  1.77  2.14  1.53  1.39  1.06 13.4 ]\n",
      "Loss_Q: [1.61 1.25 2.11 1.44 1.23 0.92 0.   8.55] Loss_P: [ 3.23  2.26  1.71  2.15  1.53  1.34  1.03 13.25]\n",
      "Loss_Q: [1.61 1.24 2.08 1.37 1.26 0.92 0.   8.48] Loss_P: [ 3.22  2.29  1.73  2.1   1.52  1.39  1.01 13.26]\n",
      "Loss_Q: [1.59 1.23 2.08 1.38 1.23 0.92 0.   8.43] Loss_P: [ 3.25  2.21  1.69  2.06  1.52  1.34  1.01 13.07]\n",
      "Loss_Q: [1.54 1.16 2.07 1.39 1.22 0.88 0.   8.26] Loss_P: [ 3.2   2.24  1.73  2.09  1.55  1.36  1.   13.17]\n",
      "Loss_Q: [1.62 1.18 2.06 1.39 1.19 0.91 0.   8.36] Loss_P: [ 3.17  2.26  1.68  2.    1.52  1.35  1.03 13.02]\n",
      "Loss_Q: [1.57 1.21 2.02 1.44 1.25 0.9  0.   8.38] Loss_P: [ 3.22  2.28  1.7   2.11  1.56  1.37  1.03 13.26]\n",
      "Loss_Q: [1.63 1.21 2.03 1.38 1.27 0.9  0.   8.42] Loss_P: [ 3.23  2.29  1.64  2.03  1.51  1.37  1.   13.06]\n",
      "Loss_Q: [1.65 1.19 2.04 1.39 1.24 0.88 0.   8.39] Loss_P: [ 3.15  2.27  1.7   2.06  1.47  1.35  0.99 13.  ]\n",
      "Loss_Q: [1.63 1.19 2.03 1.4  1.23 0.92 0.   8.41] Loss_P: [ 3.17  2.32  1.7   2.08  1.51  1.38  1.04 13.19]\n",
      "Loss_Q: [1.59 1.26 2.05 1.42 1.26 0.95 0.   8.53] Loss_P: [ 3.19  2.25  1.68  2.08  1.53  1.34  1.03 13.1 ]\n",
      "Loss_Q: [1.6  1.25 2.06 1.4  1.22 0.88 0.   8.42] Loss_P: [ 3.19  2.27  1.76  2.14  1.52  1.36  1.02 13.26]\n",
      "Loss_Q: [1.7  1.21 2.04 1.36 1.24 0.92 0.   8.48] Loss_P: [ 3.21  2.21  1.7   2.06  1.53  1.36  1.   13.06]\n",
      "Loss_Q: [1.54 1.23 2.01 1.43 1.22 0.88 0.   8.3 ] Loss_P: [ 3.23  2.24  1.67  2.09  1.56  1.38  1.02 13.2 ]\n",
      "Loss_Q: [1.53 1.2  2.03 1.47 1.23 0.91 0.   8.37] Loss_P: [ 3.19  2.22  1.6   2.1   1.55  1.42  1.01 13.08]\n",
      "Loss_Q: [1.6  1.25 2.08 1.45 1.29 0.92 0.   8.59] Loss_P: [ 3.23  2.22  1.6   2.08  1.56  1.44  1.03 13.15]\n",
      "Loss_Q: [1.57 1.19 2.02 1.42 1.27 0.9  0.   8.37] Loss_P: [ 3.22  2.26  1.63  2.05  1.55  1.42  1.02 13.13]\n",
      "Loss_Q: [1.6  1.2  2.06 1.45 1.27 0.87 0.   8.45] Loss_P: [ 3.18  2.15  1.65  2.07  1.58  1.43  1.04 13.09]\n",
      "Loss_Q: [1.52 1.23 2.04 1.5  1.25 0.94 0.   8.47] Loss_P: [ 3.22  2.19  1.71  2.08  1.59  1.37  1.04 13.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.64 1.26 2.08 1.52 1.28 0.89 0.   8.67] Loss_P: [ 3.21  2.17  1.69  2.09  1.62  1.35  1.03 13.16]\n",
      "Loss_Q: [1.57 1.17 2.04 1.44 1.28 0.93 0.   8.41] Loss_P: [ 3.21  2.21  1.67  2.03  1.54  1.4   1.04 13.1 ]\n",
      "Loss_Q: [1.6  1.27 2.02 1.5  1.31 0.92 0.   8.62] Loss_P: [ 3.19  2.23  1.69  2.03  1.57  1.36  1.04 13.11]\n",
      "Loss_Q: [1.62 1.17 2.03 1.41 1.3  0.93 0.   8.46] Loss_P: [ 3.2   2.24  1.72  2.03  1.54  1.34  1.04 13.12]\n",
      "Loss_Q: [1.62 1.18 2.02 1.4  1.25 0.91 0.   8.38] Loss_P: [ 3.19  2.2   1.68  2.06  1.62  1.42  1.06 13.23]\n",
      "Loss_Q: [1.58 1.17 2.03 1.41 1.25 0.9  0.   8.35] Loss_P: [ 3.15  2.27  1.69  2.09  1.54  1.41  1.05 13.21]\n",
      "Loss_Q: [1.64 1.25 2.07 1.45 1.25 0.93 0.   8.59] Loss_P: [ 3.18  2.26  1.64  2.12  1.56  1.4   1.08 13.24]\n",
      "Loss_Q: [1.57 1.16 2.06 1.41 1.32 0.92 0.   8.44] Loss_P: [ 3.15  2.27  1.65  2.12  1.52  1.41  1.08 13.2 ]\n",
      "Loss_Q: [1.65 1.18 2.1  1.42 1.3  0.99 0.   8.62] Loss_P: [ 3.18  2.25  1.62  2.1   1.53  1.43  1.11 13.21]\n",
      "Loss_Q: [1.6  1.18 2.01 1.42 1.3  0.94 0.   8.44] Loss_P: [ 3.21  2.24  1.66  2.08  1.53  1.44  1.11 13.27]\n",
      "Loss_Q: [1.66 1.16 2.08 1.44 1.27 0.96 0.   8.57] Loss_P: [ 3.2   2.24  1.69  2.05  1.53  1.43  1.09 13.23]\n",
      "Loss_Q: [1.57 1.22 2.   1.43 1.29 0.91 0.   8.43] Loss_P: [ 3.2   2.23  1.74  2.09  1.54  1.4   1.06 13.26]\n",
      "Loss_Q: [1.59 1.16 2.06 1.41 1.32 0.92 0.   8.47] Loss_P: [ 3.16  2.23  1.67  2.07  1.54  1.4   1.06 13.14]\n",
      "Loss_Q: [1.55 1.16 2.03 1.45 1.28 0.94 0.   8.41] Loss_P: [ 3.21  2.21  1.67  2.05  1.55  1.4   1.07 13.16]\n",
      "Loss_Q: [1.52 1.14 2.   1.42 1.27 0.95 0.   8.3 ] Loss_P: [ 3.23  2.2   1.7   2.05  1.53  1.38  1.07 13.15]\n",
      "Loss_Q: [1.6  1.18 2.04 1.46 1.33 0.95 0.   8.54] Loss_P: [ 3.15  2.25  1.68  2.06  1.57  1.42  1.1  13.22]\n",
      "Loss_Q: [1.59 1.19 2.01 1.47 1.29 0.92 0.   8.48] Loss_P: [ 3.18  2.18  1.63  2.03  1.55  1.41  1.07 13.06]\n",
      "Loss_Q: [1.5  1.16 2.03 1.43 1.29 0.92 0.   8.32] Loss_P: [ 3.2   2.12  1.64  2.02  1.54  1.39  1.1  13.  ]\n",
      "Loss_Q: [1.38 1.11 2.04 1.42 1.29 0.92 0.   8.15] Loss_P: [ 3.22  2.08  1.68  2.04  1.56  1.43  1.05 13.05]\n",
      "Loss_Q: [1.51 1.19 2.04 1.43 1.29 0.93 0.   8.39] Loss_P: [ 3.25  2.03  1.65  1.99  1.54  1.39  1.05 12.91]\n",
      "Loss_Q: [1.42 1.16 1.98 1.44 1.31 0.94 0.   8.26] Loss_P: [ 3.24  2.13  1.69  2.05  1.56  1.41  1.03 13.11]\n",
      "Loss_Q: [1.48 1.19 2.02 1.44 1.31 0.91 0.   8.35] Loss_P: [ 3.21  2.06  1.63  2.02  1.56  1.42  1.05 12.95]\n",
      "Loss_Q: [1.44 1.19 1.94 1.41 1.3  0.9  0.   8.19] Loss_P: [ 3.22  2.03  1.64  1.98  1.46  1.36  1.01 12.69]\n",
      "Loss_Q: [1.46 1.15 1.98 1.4  1.29 0.9  0.   8.18] Loss_P: [ 3.19  2.08  1.65  1.99  1.52  1.37  1.   12.79]\n",
      "Loss_Q: [1.47 1.13 2.03 1.38 1.32 0.85 0.   8.17] Loss_P: [ 3.22  2.12  1.65  2.04  1.49  1.41  1.02 12.96]\n",
      "Loss_Q: [1.52 1.18 2.04 1.43 1.33 0.89 0.   8.38] Loss_P: [ 3.2   2.17  1.64  2.02  1.52  1.41  0.98 12.95]\n",
      "Loss_Q: [1.49 1.14 2.01 1.47 1.28 0.89 0.   8.28] Loss_P: [ 3.17  2.19  1.63  2.01  1.55  1.38  1.04 12.98]\n",
      "Loss_Q: [1.49 1.13 2.03 1.48 1.31 0.87 0.   8.31] Loss_P: [ 3.18  2.19  1.67  2.05  1.53  1.38  1.01 13.02]\n",
      "Loss_Q: [1.61 1.21 2.01 1.41 1.33 0.94 0.   8.5 ] Loss_P: [ 3.19  2.22  1.62  2.09  1.53  1.44  1.03 13.13]\n",
      "Loss_Q: [1.57 1.14 2.04 1.41 1.34 0.93 0.   8.41] Loss_P: [ 3.19  2.27  1.62  2.05  1.52  1.44  1.08 13.16]\n",
      "Loss_Q: [1.6  1.19 2.02 1.42 1.31 0.91 0.   8.45] Loss_P: [ 3.2   2.29  1.69  2.08  1.52  1.42  1.04 13.24]\n",
      "Loss_Q: [1.59 1.17 2.04 1.42 1.33 0.9  0.   8.44] Loss_P: [ 3.22  2.3   1.66  2.11  1.53  1.44  1.03 13.28]\n",
      "Loss_Q: [1.66 1.21 2.08 1.48 1.32 0.87 0.   8.62] Loss_P: [ 3.18  2.31  1.66  2.08  1.54  1.4   0.97 13.14]\n",
      "Loss_Q: [1.66 1.12 2.06 1.48 1.29 0.85 0.   8.46] Loss_P: [ 3.18  2.36  1.64  2.09  1.6   1.42  0.95 13.23]\n",
      "Loss_Q: [1.63 1.14 2.04 1.39 1.27 0.83 0.   8.29] Loss_P: [ 3.19  2.33  1.64  2.04  1.56  1.37  0.99 13.13]\n",
      "Loss_Q: [1.71 1.19 2.11 1.52 1.32 0.89 0.   8.73] Loss_P: [ 3.21  2.37  1.67  2.08  1.58  1.4   1.   13.3 ]\n",
      "Loss_Q: [1.77 1.12 2.05 1.43 1.33 0.9  0.   8.6 ] Loss_P: [ 3.2   2.42  1.62  2.07  1.6   1.41  1.02 13.34]\n",
      "Loss_Q: [1.7  1.15 1.99 1.46 1.28 0.88 0.   8.46] Loss_P: [ 3.17  2.43  1.66  2.04  1.6   1.41  1.04 13.34]\n",
      "Loss_Q: [1.72 1.15 2.03 1.44 1.3  0.91 0.   8.55] Loss_P: [ 3.2   2.36  1.59  2.01  1.51  1.39  1.02 13.08]\n",
      "Loss_Q: [1.69 1.18 2.   1.43 1.34 0.91 0.   8.56] Loss_P: [ 3.17  2.44  1.64  2.04  1.57  1.39  1.08 13.34]\n",
      "Loss_Q: [1.7  1.13 2.01 1.45 1.32 0.89 0.   8.5 ] Loss_P: [ 3.17  2.38  1.61  2.    1.53  1.4   1.02 13.11]\n",
      "Loss_Q: [1.77 1.16 2.03 1.42 1.34 0.88 0.   8.62] Loss_P: [ 3.18  2.43  1.64  2.05  1.59  1.4   1.02 13.31]\n",
      "Loss_Q: [1.7  1.22 1.97 1.46 1.3  0.86 0.   8.51] Loss_P: [ 3.18  2.46  1.64  2.04  1.59  1.41  1.01 13.33]\n",
      "Loss_Q: [1.7  1.15 1.96 1.42 1.3  0.87 0.   8.39] Loss_P: [ 3.18  2.44  1.66  2.01  1.52  1.37  1.   13.19]\n",
      "Loss_Q: [1.78 1.2  1.99 1.43 1.3  0.86 0.   8.55] Loss_P: [ 3.2   2.46  1.64  2.04  1.58  1.41  1.02 13.35]\n",
      "Loss_Q: [1.82 1.2  2.03 1.45 1.29 0.87 0.   8.66] Loss_P: [ 3.15  2.52  1.64  2.02  1.58  1.38  1.   13.29]\n",
      "Loss_Q: [1.77 1.15 2.01 1.46 1.27 0.88 0.   8.54] Loss_P: [ 3.19  2.42  1.65  1.99  1.5   1.36  0.94 13.06]\n",
      "Loss_Q: [1.77 1.18 2.05 1.49 1.27 0.85 0.   8.61] Loss_P: [ 3.2   2.42  1.68  2.07  1.58  1.36  1.01 13.31]\n",
      "Loss_Q: [1.73 1.21 2.01 1.46 1.27 0.88 0.   8.56] Loss_P: [ 3.21  2.45  1.64  2.03  1.57  1.36  1.   13.26]\n",
      "Loss_Q: [1.78 1.2  2.04 1.45 1.27 0.81 0.   8.56] Loss_P: [ 3.14  2.45  1.68  2.07  1.55  1.34  0.98 13.2 ]\n",
      "Loss_Q: [1.79 1.12 2.02 1.43 1.22 0.83 0.   8.4 ] Loss_P: [ 3.19  2.45  1.62  2.08  1.54  1.34  0.98 13.19]\n",
      "Loss_Q: [1.75 1.19 1.96 1.46 1.28 0.85 0.   8.49] Loss_P: [ 3.19  2.38  1.62  2.04  1.52  1.34  0.97 13.06]\n",
      "Loss_Q: [1.76 1.18 1.96 1.44 1.27 0.89 0.   8.5 ] Loss_P: [ 3.17  2.41  1.62  2.03  1.48  1.34  0.99 13.04]\n",
      "Loss_Q: [1.75 1.15 1.98 1.41 1.27 0.82 0.   8.38] Loss_P: [ 3.18  2.41  1.62  2.05  1.52  1.37  0.97 13.12]\n",
      "Loss_Q: [1.75 1.14 1.98 1.43 1.26 0.83 0.   8.39] Loss_P: [ 3.2   2.42  1.63  2.05  1.51  1.39  0.95 13.14]\n",
      "Loss_Q: [1.73 1.14 1.98 1.43 1.29 0.83 0.   8.4 ] Loss_P: [ 3.17  2.44  1.61  1.97  1.51  1.37  0.96 13.04]\n",
      "Loss_Q: [1.76 1.18 1.98 1.5  1.29 0.86 0.   8.57] Loss_P: [ 3.2   2.44  1.65  2.    1.55  1.36  0.98 13.18]\n",
      "Loss_Q: [1.73 1.14 1.97 1.47 1.21 0.84 0.   8.36] Loss_P: [ 3.16  2.46  1.65  1.98  1.58  1.36  0.99 13.17]\n",
      "Loss_Q: [1.76 1.21 1.96 1.45 1.24 0.86 0.   8.49] Loss_P: [ 3.15  2.42  1.71  2.    1.53  1.33  0.97 13.12]\n",
      "Loss_Q: [1.78 1.14 1.97 1.44 1.23 0.87 0.   8.43] Loss_P: [ 3.15  2.47  1.71  2.02  1.5   1.33  0.96 13.13]\n",
      "Loss_Q: [1.8  1.24 2.01 1.46 1.24 0.89 0.   8.63] Loss_P: [ 3.15  2.53  1.75  2.01  1.51  1.3   1.03 13.26]\n",
      "Loss_Q: [1.71 1.29 2.03 1.48 1.25 0.89 0.   8.64] Loss_P: [ 3.17  2.47  1.75  2.07  1.51  1.31  1.02 13.31]\n",
      "Loss_Q: [1.75 1.17 1.94 1.42 1.19 0.85 0.   8.32] Loss_P: [ 3.13  2.48  1.74  2.03  1.51  1.32  0.97 13.18]\n",
      "Loss_Q: [1.76 1.27 2.   1.47 1.2  0.84 0.   8.53] Loss_P: [ 3.11  2.46  1.77  2.12  1.58  1.3   0.99 13.34]\n",
      "Loss_Q: [1.72 1.24 2.   1.49 1.21 0.84 0.   8.51] Loss_P: [ 3.18  2.49  1.79  2.1   1.58  1.25  0.97 13.37]\n",
      "Loss_Q: [1.77 1.2  1.96 1.5  1.15 0.85 0.   8.44] Loss_P: [ 3.18  2.5   1.69  2.04  1.55  1.22  0.96 13.13]\n",
      "Loss_Q: [1.82 1.22 1.97 1.49 1.16 0.85 0.   8.51] Loss_P: [ 3.17  2.52  1.74  2.02  1.59  1.24  0.98 13.25]\n",
      "Loss_Q: [1.8  1.2  1.92 1.51 1.13 0.88 0.   8.44] Loss_P: [ 3.13  2.55  1.76  2.02  1.58  1.27  0.96 13.28]\n",
      "Loss_Q: [1.81 1.29 2.01 1.51 1.15 0.82 0.   8.6 ] Loss_P: [ 3.17  2.53  1.73  2.03  1.55  1.25  0.94 13.21]\n",
      "Loss_Q: [1.8  1.32 1.99 1.52 1.18 0.8  0.   8.61] Loss_P: [ 3.16  2.48  1.81  2.03  1.56  1.26  0.9  13.2 ]\n",
      "Loss_Q: [1.75 1.23 1.98 1.5  1.16 0.78 0.   8.4 ] Loss_P: [ 3.15  2.41  1.82  2.04  1.58  1.28  0.91 13.19]\n",
      "Loss_Q: [1.79 1.28 1.91 1.47 1.17 0.79 0.   8.42] Loss_P: [ 3.15  2.44  1.83  2.    1.56  1.23  0.91 13.12]\n",
      "Loss_Q: [1.77 1.2  1.95 1.4  1.18 0.77 0.   8.27] Loss_P: [ 3.15  2.49  1.8   1.99  1.53  1.28  0.93 13.17]\n",
      "Loss_Q: [1.84 1.27 1.93 1.48 1.16 0.82 0.   8.49] Loss_P: [ 3.16  2.53  1.77  1.99  1.5   1.24  0.89 13.08]\n",
      "Loss_Q: [1.74 1.23 1.94 1.48 1.17 0.76 0.   8.32] Loss_P: [ 3.15  2.43  1.78  2.05  1.54  1.26  0.91 13.11]\n",
      "Loss_Q: [1.79 1.22 1.91 1.43 1.16 0.78 0.   8.29] Loss_P: [ 3.2   2.43  1.75  1.94  1.5   1.22  0.93 12.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.78 1.2  1.94 1.46 1.15 0.77 0.   8.3 ] Loss_P: [ 3.17  2.46  1.75  2.02  1.51  1.24  0.93 13.09]\n",
      "Loss_Q: [1.71 1.25 1.96 1.46 1.2  0.78 0.   8.35] Loss_P: [ 3.19  2.43  1.74  2.01  1.52  1.26  0.9  13.05]\n",
      "Loss_Q: [1.78 1.21 1.96 1.46 1.14 0.77 0.   8.32] Loss_P: [ 3.2   2.37  1.75  2.02  1.51  1.25  0.89 12.98]\n",
      "Loss_Q: [1.78 1.22 1.99 1.5  1.16 0.81 0.   8.46] Loss_P: [ 3.14  2.42  1.77  2.02  1.57  1.26  0.95 13.12]\n",
      "Loss_Q: [1.75 1.2  2.   1.51 1.16 0.8  0.   8.4 ] Loss_P: [ 3.14  2.45  1.74  2.04  1.56  1.22  0.9  13.05]\n",
      "Loss_Q: [1.71 1.16 1.95 1.51 1.15 0.81 0.   8.29] Loss_P: [ 3.12  2.44  1.72  2.02  1.58  1.24  0.93 13.05]\n",
      "Loss_Q: [1.73 1.13 1.91 1.49 1.18 0.75 0.   8.19] Loss_P: [ 3.2   2.4   1.61  1.95  1.52  1.22  0.91 12.81]\n",
      "Loss_Q: [1.76 1.13 1.91 1.49 1.1  0.78 0.   8.17] Loss_P: [ 3.17  2.37  1.7   1.98  1.55  1.2   0.9  12.86]\n",
      "Loss_Q: [1.71 1.12 1.94 1.47 1.16 0.79 0.   8.2 ] Loss_P: [ 3.14  2.47  1.64  2.    1.53  1.2   0.93 12.91]\n",
      "Loss_Q: [1.75 1.09 1.94 1.48 1.16 0.78 0.   8.21] Loss_P: [ 3.17  2.44  1.66  2.01  1.53  1.26  0.88 12.96]\n",
      "Loss_Q: [1.74 1.14 1.92 1.45 1.16 0.77 0.   8.18] Loss_P: [ 3.18  2.44  1.62  1.95  1.48  1.18  0.91 12.76]\n",
      "Loss_Q: [1.8  1.17 1.89 1.43 1.16 0.79 0.   8.24] Loss_P: [ 3.19  2.45  1.61  1.97  1.49  1.21  0.93 12.85]\n",
      "Loss_Q: [1.82 1.09 1.92 1.42 1.16 0.77 0.   8.18] Loss_P: [ 3.19  2.47  1.7   2.03  1.45  1.23  0.89 12.95]\n",
      "Loss_Q: [1.8  1.15 1.94 1.41 1.2  0.8  0.   8.3 ] Loss_P: [ 3.17  2.48  1.65  2.02  1.47  1.23  0.95 12.98]\n",
      "Loss_Q: [1.78 1.1  1.95 1.37 1.16 0.81 0.   8.16] Loss_P: [ 3.18  2.41  1.6   1.99  1.41  1.23  0.94 12.75]\n",
      "Loss_Q: [1.8  1.1  1.87 1.34 1.2  0.8  0.   8.12] Loss_P: [ 3.16  2.48  1.59  1.94  1.43  1.2   0.94 12.74]\n",
      "Loss_Q: [1.82 1.13 1.95 1.37 1.18 0.83 0.   8.26] Loss_P: [ 3.19  2.45  1.59  1.98  1.42  1.22  0.96 12.8 ]\n",
      "Loss_Q: [1.82 1.15 1.91 1.38 1.15 0.83 0.   8.24] Loss_P: [ 3.19  2.47  1.66  2.    1.45  1.23  0.97 12.97]\n",
      "Loss_Q: [1.71 1.13 1.94 1.37 1.18 0.85 0.   8.18] Loss_P: [ 3.23  2.44  1.61  2.05  1.47  1.22  0.95 12.96]\n",
      "Loss_Q: [1.68 1.2  1.98 1.42 1.22 0.84 0.   8.34] Loss_P: [ 3.21  2.35  1.63  2.02  1.47  1.22  0.97 12.87]\n",
      "Loss_Q: [1.75 1.18 1.98 1.37 1.15 0.89 0.   8.33] Loss_P: [ 3.21  2.43  1.67  2.06  1.5   1.24  1.   13.11]\n",
      "Loss_Q: [1.72 1.14 1.96 1.44 1.23 0.86 0.   8.36] Loss_P: [ 3.17  2.44  1.6   2.06  1.49  1.25  0.99 12.99]\n",
      "Loss_Q: [1.81 1.2  1.96 1.44 1.14 0.84 0.   8.39] Loss_P: [ 3.19  2.41  1.64  2.05  1.49  1.21  0.98 12.98]\n",
      "Loss_Q: [1.73 1.13 1.88 1.41 1.16 0.86 0.   8.18] Loss_P: [ 3.17  2.44  1.66  2.01  1.49  1.27  0.99 13.03]\n",
      "Loss_Q: [1.72 1.13 1.91 1.36 1.18 0.82 0.   8.12] Loss_P: [ 3.22  2.42  1.67  1.99  1.46  1.24  0.95 12.96]\n",
      "Loss_Q: [1.79 1.15 1.95 1.42 1.19 0.84 0.   8.34] Loss_P: [ 3.22  2.36  1.67  2.01  1.45  1.25  0.92 12.86]\n",
      "Loss_Q: [1.73 1.12 1.97 1.34 1.2  0.81 0.   8.18] Loss_P: [ 3.18  2.45  1.64  2.03  1.45  1.24  0.94 12.93]\n",
      "Loss_Q: [1.72 1.14 1.99 1.4  1.2  0.8  0.   8.24] Loss_P: [ 3.17  2.47  1.66  2.06  1.45  1.27  0.88 12.96]\n",
      "Loss_Q: [1.76 1.15 2.04 1.42 1.21 0.81 0.   8.38] Loss_P: [ 3.21  2.44  1.66  2.08  1.47  1.27  0.91 13.03]\n",
      "Loss_Q: [1.78 1.15 2.01 1.4  1.15 0.78 0.   8.27] Loss_P: [ 3.21  2.42  1.61  2.06  1.52  1.29  0.87 12.97]\n",
      "Loss_Q: [1.74 1.19 2.04 1.43 1.22 0.77 0.   8.39] Loss_P: [ 3.18  2.38  1.72  2.13  1.48  1.29  0.9  13.09]\n",
      "Loss_Q: [1.74 1.17 2.   1.41 1.25 0.78 0.   8.36] Loss_P: [ 3.16  2.37  1.7   2.13  1.5   1.33  0.92 13.11]\n",
      "Loss_Q: [1.77 1.2  2.01 1.39 1.19 0.8  0.   8.36] Loss_P: [ 3.2   2.38  1.7   2.01  1.47  1.29  0.89 12.93]\n",
      "Loss_Q: [1.69 1.25 2.   1.39 1.22 0.79 0.   8.34] Loss_P: [ 3.2   2.4   1.71  2.12  1.4   1.27  0.88 12.98]\n",
      "Loss_Q: [1.7  1.21 2.06 1.35 1.21 0.74 0.   8.26] Loss_P: [ 3.17  2.44  1.69  2.11  1.41  1.29  0.83 12.95]\n",
      "Loss_Q: [1.77 1.23 1.98 1.33 1.2  0.74 0.   8.26] Loss_P: [ 3.21  2.44  1.68  2.07  1.4   1.27  0.88 12.94]\n",
      "Loss_Q: [1.74 1.25 2.05 1.34 1.21 0.78 0.   8.38] Loss_P: [ 3.15  2.43  1.7   2.09  1.46  1.32  0.89 13.03]\n",
      "Loss_Q: [1.71 1.19 2.07 1.42 1.21 0.79 0.   8.38] Loss_P: [ 3.16  2.39  1.62  2.1   1.45  1.31  0.89 12.92]\n",
      "Loss_Q: [1.68 1.18 2.01 1.35 1.22 0.76 0.   8.21] Loss_P: [ 3.21  2.41  1.66  2.17  1.48  1.31  0.85 13.1 ]\n",
      "Loss_Q: [1.79 1.22 2.05 1.36 1.27 0.74 0.   8.44] Loss_P: [ 3.19  2.42  1.68  2.11  1.46  1.37  0.83 13.06]\n",
      "Loss_Q: [1.76 1.19 2.02 1.36 1.23 0.73 0.   8.31] Loss_P: [ 3.18  2.34  1.72  2.16  1.47  1.36  0.82 13.06]\n",
      "Loss_Q: [1.69 1.17 2.07 1.4  1.24 0.73 0.   8.31] Loss_P: [ 3.17  2.39  1.6   2.16  1.47  1.32  0.9  13.01]\n",
      "Loss_Q: [1.77 1.21 2.04 1.41 1.29 0.76 0.   8.47] Loss_P: [ 3.19  2.43  1.64  2.1   1.45  1.36  0.87 13.03]\n",
      "Loss_Q: [1.71 1.1  2.05 1.32 1.25 0.75 0.   8.17] Loss_P: [ 3.15  2.43  1.62  2.11  1.44  1.33  0.88 12.96]\n",
      "Loss_Q: [1.59 1.14 1.97 1.37 1.29 0.76 0.   8.12] Loss_P: [ 3.18  2.38  1.59  2.11  1.44  1.37  0.84 12.91]\n",
      "Loss_Q: [1.66 1.19 2.   1.4  1.26 0.77 0.   8.28] Loss_P: [ 3.19  2.39  1.62  2.04  1.46  1.34  0.87 12.91]\n",
      "Loss_Q: [1.63 1.14 1.98 1.34 1.24 0.77 0.   8.1 ] Loss_P: [ 3.18  2.33  1.66  2.04  1.43  1.34  0.86 12.84]\n",
      "Loss_Q: [1.68 1.2  2.01 1.41 1.27 0.79 0.   8.37] Loss_P: [ 3.18  2.26  1.66  2.05  1.41  1.32  0.88 12.76]\n",
      "Loss_Q: [1.64 1.18 1.98 1.41 1.25 0.77 0.   8.22] Loss_P: [ 3.2   2.32  1.64  2.08  1.47  1.35  0.91 12.98]\n",
      "Loss_Q: [1.64 1.18 1.98 1.42 1.28 0.83 0.   8.34] Loss_P: [ 3.2   2.27  1.75  2.12  1.48  1.29  0.94 13.06]\n",
      "Loss_Q: [1.6  1.23 2.   1.4  1.23 0.79 0.   8.25] Loss_P: [ 3.22  2.28  1.69  2.09  1.47  1.33  0.89 12.96]\n",
      "Loss_Q: [1.56 1.19 1.93 1.37 1.23 0.79 0.   8.07] Loss_P: [ 3.21  2.24  1.68  2.03  1.44  1.28  0.92 12.79]\n",
      "Loss_Q: [1.56 1.2  1.95 1.34 1.22 0.79 0.   8.06] Loss_P: [ 3.17  2.31  1.67  2.08  1.45  1.3   0.88 12.88]\n",
      "Loss_Q: [1.7  1.2  2.01 1.42 1.24 0.77 0.   8.34] Loss_P: [ 3.21  2.33  1.63  2.06  1.5   1.32  0.88 12.92]\n",
      "Loss_Q: [1.66 1.15 2.   1.42 1.24 0.77 0.   8.24] Loss_P: [ 3.17  2.32  1.64  2.03  1.57  1.32  0.89 12.95]\n",
      "Loss_Q: [1.65 1.11 1.94 1.39 1.25 0.8  0.   8.16] Loss_P: [ 3.18  2.31  1.64  2.02  1.46  1.33  0.88 12.82]\n",
      "Loss_Q: [1.56 1.18 1.87 1.36 1.21 0.73 0.   7.91] Loss_P: [ 3.25  2.27  1.64  1.98  1.45  1.31  0.87 12.78]\n",
      "Loss_Q: [1.59 1.13 1.87 1.37 1.23 0.78 0.   7.97] Loss_P: [ 3.2   2.23  1.64  1.95  1.44  1.32  0.88 12.66]\n",
      "Loss_Q: [1.62 1.19 1.92 1.47 1.22 0.78 0.   8.19] Loss_P: [ 3.19  2.3   1.67  2.    1.51  1.35  0.9  12.91]\n",
      "Loss_Q: [1.57 1.17 1.91 1.39 1.22 0.79 0.   8.05] Loss_P: [ 3.19  2.28  1.66  1.98  1.49  1.28  0.85 12.74]\n",
      "Loss_Q: [1.59 1.16 1.91 1.42 1.22 0.82 0.   8.12] Loss_P: [ 3.17  2.26  1.65  1.96  1.49  1.31  0.88 12.73]\n",
      "Loss_Q: [1.62 1.2  1.9  1.45 1.24 0.77 0.   8.19] Loss_P: [ 3.22  2.3   1.69  1.99  1.54  1.33  0.88 12.94]\n",
      "Loss_Q: [1.57 1.18 1.87 1.47 1.21 0.79 0.   8.09] Loss_P: [ 3.18  2.29  1.68  1.94  1.56  1.3   0.86 12.81]\n",
      "Loss_Q: [1.61 1.16 1.91 1.42 1.23 0.78 0.   8.11] Loss_P: [ 3.24  2.28  1.66  2.    1.56  1.29  0.9  12.94]\n",
      "Loss_Q: [1.65 1.18 1.9  1.52 1.21 0.82 0.   8.28] Loss_P: [ 3.22  2.29  1.7   2.01  1.59  1.29  0.87 12.98]\n",
      "Loss_Q: [1.62 1.2  1.92 1.49 1.19 0.78 0.   8.19] Loss_P: [ 3.23  2.2   1.63  1.98  1.55  1.28  0.89 12.78]\n",
      "Loss_Q: [1.59 1.15 1.96 1.53 1.2  0.84 0.   8.26] Loss_P: [ 3.22  2.24  1.68  2.02  1.6   1.31  0.93 13.  ]\n",
      "Loss_Q: [1.62 1.21 1.98 1.55 1.21 0.82 0.   8.39] Loss_P: [ 3.22  2.28  1.68  2.08  1.64  1.29  0.95 13.14]\n",
      "Loss_Q: [1.53 1.17 1.94 1.52 1.21 0.81 0.   8.18] Loss_P: [ 3.25  2.21  1.67  2.04  1.62  1.29  0.94 13.01]\n",
      "Loss_Q: [1.58 1.22 1.94 1.54 1.2  0.87 0.   8.35] Loss_P: [ 3.24  2.26  1.66  2.03  1.63  1.31  0.96 13.08]\n",
      "Loss_Q: [1.58 1.21 1.93 1.54 1.25 0.85 0.   8.35] Loss_P: [ 3.19  2.26  1.68  2.05  1.61  1.32  0.98 13.1 ]\n",
      "Loss_Q: [1.66 1.29 1.92 1.55 1.26 0.89 0.   8.58] Loss_P: [ 3.21  2.29  1.63  1.99  1.63  1.33  0.99 13.07]\n",
      "Loss_Q: [1.59 1.2  2.01 1.55 1.24 0.92 0.   8.52] Loss_P: [ 3.2   2.26  1.6   2.    1.58  1.33  1.06 13.04]\n",
      "Loss_Q: [1.56 1.15 1.97 1.56 1.25 0.92 0.   8.41] Loss_P: [ 3.22  2.24  1.68  2.06  1.62  1.29  1.03 13.14]\n",
      "Loss_Q: [1.62 1.16 1.98 1.54 1.23 0.91 0.   8.46] Loss_P: [ 3.2   2.28  1.66  2.05  1.62  1.32  1.04 13.15]\n",
      "Loss_Q: [1.59 1.21 1.94 1.52 1.19 0.94 0.   8.4 ] Loss_P: [ 3.25  2.28  1.65  2.04  1.61  1.33  1.04 13.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.64 1.19 2.06 1.58 1.19 0.96 0.   8.62] Loss_P: [ 3.23  2.34  1.66  2.08  1.64  1.35  1.05 13.34]\n",
      "Loss_Q: [1.58 1.21 1.99 1.6  1.23 0.9  0.   8.52] Loss_P: [ 3.22  2.27  1.71  2.14  1.69  1.34  1.05 13.43]\n",
      "Loss_Q: [1.65 1.24 2.01 1.58 1.27 0.91 0.   8.66] Loss_P: [ 3.23  2.28  1.68  2.1   1.62  1.36  1.01 13.28]\n",
      "Loss_Q: [1.7  1.27 2.01 1.62 1.24 0.87 0.   8.71] Loss_P: [ 3.23  2.3   1.69  2.05  1.68  1.37  1.01 13.33]\n",
      "Loss_Q: [1.66 1.25 1.95 1.54 1.19 0.83 0.   8.42] Loss_P: [ 3.21  2.38  1.7   2.11  1.63  1.32  0.98 13.33]\n",
      "Loss_Q: [1.64 1.21 2.01 1.6  1.29 0.85 0.   8.6 ] Loss_P: [ 3.19  2.35  1.71  2.05  1.66  1.39  0.99 13.33]\n",
      "Loss_Q: [1.71 1.2  1.95 1.56 1.27 0.88 0.   8.58] Loss_P: [ 3.21  2.38  1.72  2.02  1.62  1.33  0.99 13.28]\n",
      "Loss_Q: [1.7  1.23 1.96 1.56 1.24 0.87 0.   8.56] Loss_P: [ 3.2   2.43  1.68  2.03  1.61  1.35  0.98 13.27]\n",
      "Loss_Q: [1.7  1.22 1.96 1.52 1.27 0.87 0.   8.55] Loss_P: [ 3.22  2.37  1.64  2.03  1.55  1.33  0.95 13.1 ]\n",
      "Loss_Q: [1.68 1.2  1.96 1.51 1.2  0.87 0.   8.43] Loss_P: [ 3.19  2.38  1.68  2.    1.57  1.28  0.99 13.08]\n",
      "Loss_Q: [1.68 1.21 1.97 1.55 1.16 0.84 0.   8.4 ] Loss_P: [ 3.21  2.34  1.68  2.08  1.62  1.25  0.94 13.11]\n",
      "Loss_Q: [1.73 1.24 1.92 1.53 1.19 0.82 0.   8.43] Loss_P: [ 3.19  2.37  1.76  2.09  1.65  1.31  0.97 13.36]\n",
      "Loss_Q: [1.68 1.23 1.98 1.58 1.22 0.86 0.   8.55] Loss_P: [ 3.22  2.32  1.74  2.05  1.66  1.32  0.97 13.27]\n",
      "Loss_Q: [1.74 1.19 1.98 1.56 1.24 0.83 0.   8.54] Loss_P: [ 3.19  2.38  1.67  2.06  1.7   1.33  0.94 13.26]\n",
      "Loss_Q: [1.77 1.19 1.98 1.55 1.26 0.84 0.   8.58] Loss_P: [ 3.23  2.37  1.65  2.02  1.64  1.31  0.94 13.15]\n",
      "Loss_Q: [1.7  1.24 1.99 1.53 1.19 0.85 0.   8.5 ] Loss_P: [ 3.16  2.41  1.71  2.04  1.61  1.28  0.97 13.17]\n",
      "Loss_Q: [1.72 1.22 1.95 1.52 1.22 0.87 0.   8.49] Loss_P: [ 3.2   2.42  1.7   2.03  1.64  1.33  0.96 13.29]\n",
      "Loss_Q: [1.66 1.11 1.9  1.51 1.23 0.84 0.   8.25] Loss_P: [ 3.19  2.41  1.7   2.02  1.67  1.31  0.94 13.23]\n",
      "Loss_Q: [1.68 1.21 1.91 1.53 1.23 0.85 0.   8.4 ] Loss_P: [ 3.18  2.35  1.63  1.96  1.61  1.34  0.97 13.04]\n",
      "Loss_Q: [1.67 1.2  1.96 1.51 1.26 0.82 0.   8.42] Loss_P: [ 3.2   2.29  1.67  1.99  1.61  1.33  0.91 13.01]\n",
      "Loss_Q: [1.65 1.16 1.94 1.49 1.28 0.8  0.   8.32] Loss_P: [ 3.17  2.33  1.66  1.98  1.61  1.37  0.9  13.02]\n",
      "Loss_Q: [1.64 1.2  1.92 1.48 1.28 0.82 0.   8.34] Loss_P: [ 3.2   2.36  1.67  1.97  1.58  1.34  0.95 13.08]\n",
      "Loss_Q: [1.71 1.19 1.97 1.48 1.29 0.86 0.   8.49] Loss_P: [ 3.2   2.36  1.72  2.05  1.57  1.35  0.9  13.14]\n",
      "Loss_Q: [1.68 1.19 1.96 1.48 1.31 0.82 0.   8.44] Loss_P: [ 3.2   2.37  1.68  2.05  1.57  1.37  0.95 13.19]\n",
      "Loss_Q: [1.7  1.26 2.   1.55 1.26 0.87 0.   8.64] Loss_P: [ 3.18  2.39  1.73  2.02  1.64  1.34  0.98 13.27]\n",
      "Loss_Q: [1.72 1.22 2.   1.5  1.26 0.86 0.   8.56] Loss_P: [ 3.21  2.31  1.66  1.99  1.56  1.32  0.98 13.02]\n",
      "Loss_Q: [1.74 1.2  2.01 1.47 1.23 0.85 0.   8.49] Loss_P: [ 3.2   2.36  1.69  2.07  1.61  1.29  0.96 13.18]\n",
      "Loss_Q: [1.68 1.23 2.   1.52 1.23 0.88 0.   8.52] Loss_P: [ 3.19  2.38  1.72  2.06  1.55  1.3   0.98 13.18]\n",
      "Loss_Q: [1.65 1.15 2.   1.5  1.25 0.87 0.   8.42] Loss_P: [ 3.22  2.34  1.63  2.05  1.54  1.31  0.95 13.04]\n",
      "Loss_Q: [1.73 1.2  2.02 1.48 1.18 0.86 0.   8.47] Loss_P: [ 3.22  2.37  1.76  2.05  1.57  1.32  1.01 13.3 ]\n",
      "Loss_Q: [1.65 1.14 1.95 1.53 1.25 0.94 0.   8.45] Loss_P: [ 3.19  2.35  1.63  2.02  1.58  1.29  1.04 13.1 ]\n",
      "Loss_Q: [1.7  1.19 1.94 1.47 1.24 0.92 0.   8.46] Loss_P: [ 3.21  2.35  1.69  2.02  1.58  1.32  1.04 13.21]\n",
      "Loss_Q: [1.71 1.21 1.98 1.47 1.24 0.92 0.   8.52] Loss_P: [ 3.19  2.35  1.72  2.04  1.61  1.31  1.04 13.26]\n",
      "Loss_Q: [1.75 1.24 1.99 1.51 1.25 0.94 0.   8.68] Loss_P: [ 3.18  2.37  1.63  2.04  1.62  1.3   1.05 13.19]\n",
      "Loss_Q: [1.68 1.27 1.99 1.52 1.24 0.93 0.   8.64] Loss_P: [ 3.2   2.37  1.74  2.01  1.61  1.3   1.04 13.27]\n",
      "Loss_Q: [1.73 1.26 1.98 1.54 1.19 0.93 0.   8.62] Loss_P: [ 3.19  2.44  1.74  2.04  1.64  1.32  1.07 13.45]\n",
      "Loss_Q: [1.7  1.25 1.99 1.55 1.24 0.91 0.   8.64] Loss_P: [ 3.23  2.36  1.74  2.02  1.64  1.35  1.07 13.4 ]\n",
      "Loss_Q: [1.68 1.26 1.93 1.6  1.21 0.89 0.   8.56] Loss_P: [ 3.21  2.34  1.75  2.01  1.68  1.33  1.01 13.33]\n",
      "Loss_Q: [1.71 1.3  1.94 1.59 1.22 0.89 0.   8.66] Loss_P: [ 3.23  2.37  1.75  2.01  1.66  1.32  0.98 13.32]\n",
      "Loss_Q: [1.67 1.28 1.94 1.52 1.28 0.87 0.   8.57] Loss_P: [ 3.22  2.37  1.78  2.02  1.67  1.3   0.97 13.33]\n",
      "Loss_Q: [1.71 1.23 2.01 1.58 1.24 0.87 0.   8.64] Loss_P: [ 3.23  2.32  1.73  2.04  1.68  1.32  0.97 13.28]\n",
      "Loss_Q: [1.74 1.24 2.05 1.57 1.19 0.85 0.   8.63] Loss_P: [ 3.22  2.37  1.73  2.08  1.71  1.28  0.98 13.36]\n",
      "Loss_Q: [1.65 1.18 2.02 1.54 1.18 0.88 0.   8.45] Loss_P: [ 3.21  2.37  1.76  2.07  1.68  1.28  1.01 13.38]\n",
      "Loss_Q: [1.7  1.27 2.03 1.59 1.23 0.87 0.   8.67] Loss_P: [ 3.17  2.35  1.69  2.07  1.68  1.28  1.   13.22]\n",
      "Loss_Q: [1.66 1.25 2.06 1.54 1.22 0.88 0.   8.62] Loss_P: [ 3.23  2.38  1.74  2.12  1.66  1.28  1.01 13.41]\n",
      "Loss_Q: [1.69 1.27 2.02 1.62 1.21 0.91 0.   8.71] Loss_P: [ 3.2   2.35  1.75  2.07  1.71  1.3   1.03 13.41]\n",
      "Loss_Q: [1.54 1.28 2.05 1.58 1.21 0.93 0.   8.58] Loss_P: [ 3.22  2.25  1.72  2.08  1.7   1.3   1.05 13.32]\n",
      "Loss_Q: [1.65 1.23 2.04 1.59 1.25 0.9  0.   8.66] Loss_P: [ 3.21  2.32  1.76  2.12  1.75  1.29  1.05 13.48]\n",
      "Loss_Q: [1.65 1.27 2.05 1.57 1.22 0.9  0.   8.66] Loss_P: [ 3.18  2.36  1.68  2.07  1.71  1.3   0.99 13.29]\n",
      "Loss_Q: [1.68 1.25 2.04 1.61 1.24 0.85 0.   8.67] Loss_P: [ 3.16  2.36  1.7   2.11  1.73  1.34  1.   13.39]\n",
      "Loss_Q: [1.67 1.22 2.07 1.59 1.25 0.87 0.   8.68] Loss_P: [ 3.18  2.34  1.74  2.11  1.74  1.33  1.04 13.49]\n",
      "Loss_Q: [1.66 1.19 2.04 1.61 1.26 0.91 0.   8.67] Loss_P: [ 3.18  2.37  1.71  2.09  1.72  1.33  1.02 13.43]\n",
      "Loss_Q: [1.68 1.19 2.03 1.57 1.21 0.9  0.   8.58] Loss_P: [ 3.25  2.32  1.7   2.07  1.73  1.32  1.   13.39]\n",
      "Loss_Q: [1.68 1.21 2.02 1.61 1.23 0.89 0.   8.64] Loss_P: [ 3.17  2.35  1.74  2.08  1.7   1.32  1.03 13.39]\n",
      "Loss_Q: [1.64 1.24 2.04 1.62 1.26 0.91 0.   8.7 ] Loss_P: [ 3.15  2.36  1.78  2.1   1.76  1.33  1.04 13.52]\n",
      "Loss_Q: [1.66 1.22 2.04 1.58 1.25 0.88 0.   8.63] Loss_P: [ 3.22  2.3   1.63  2.1   1.76  1.33  1.02 13.36]\n",
      "Loss_Q: [1.62 1.22 2.06 1.55 1.26 0.88 0.   8.59] Loss_P: [ 3.21  2.33  1.7   2.06  1.71  1.28  0.99 13.29]\n",
      "Loss_Q: [1.66 1.22 2.05 1.58 1.28 0.91 0.   8.72] Loss_P: [ 3.18  2.31  1.73  2.02  1.7   1.32  1.06 13.32]\n",
      "Loss_Q: [1.64 1.25 2.01 1.57 1.28 0.87 0.   8.62] Loss_P: [ 3.21  2.36  1.73  2.03  1.74  1.32  1.04 13.42]\n",
      "Loss_Q: [1.68 1.28 2.01 1.58 1.2  0.89 0.   8.64] Loss_P: [ 3.2   2.37  1.74  2.08  1.75  1.32  1.01 13.47]\n",
      "Loss_Q: [1.68 1.21 2.08 1.59 1.28 0.87 0.   8.72] Loss_P: [ 3.16  2.38  1.74  2.07  1.72  1.29  1.01 13.36]\n",
      "Loss_Q: [1.72 1.17 2.03 1.59 1.27 0.84 0.   8.61] Loss_P: [ 3.16  2.39  1.67  2.02  1.73  1.32  0.98 13.27]\n",
      "Loss_Q: [1.71 1.22 2.07 1.62 1.21 0.86 0.   8.69] Loss_P: [ 3.22  2.47  1.73  2.09  1.79  1.33  0.96 13.59]\n",
      "Loss_Q: [1.74 1.25 2.03 1.62 1.27 0.85 0.   8.75] Loss_P: [ 3.24  2.38  1.72  2.06  1.8   1.3   0.93 13.44]\n",
      "Loss_Q: [1.75 1.16 2.06 1.63 1.29 0.84 0.   8.72] Loss_P: [ 3.18  2.46  1.69  2.1   1.76  1.35  0.97 13.51]\n",
      "Loss_Q: [1.74 1.23 2.06 1.64 1.3  0.89 0.   8.86] Loss_P: [ 3.22  2.46  1.7   2.1   1.79  1.37  0.96 13.6 ]\n",
      "Loss_Q: [1.72 1.22 2.06 1.63 1.29 0.87 0.   8.79] Loss_P: [ 3.2   2.42  1.7   2.13  1.8   1.35  0.99 13.59]\n",
      "Loss_Q: [1.71 1.2  2.09 1.64 1.34 0.89 0.   8.88] Loss_P: [ 3.23  2.49  1.66  2.13  1.78  1.37  1.   13.66]\n",
      "Loss_Q: [1.78 1.16 2.06 1.67 1.3  0.86 0.   8.83] Loss_P: [ 3.16  2.43  1.71  2.11  1.84  1.36  1.   13.61]\n",
      "Loss_Q: [1.82 1.25 2.05 1.68 1.33 0.83 0.   8.96] Loss_P: [ 3.2   2.5   1.66  2.19  1.77  1.39  1.   13.71]\n",
      "Loss_Q: [1.79 1.21 2.06 1.69 1.29 0.88 0.   8.92] Loss_P: [ 3.19  2.48  1.78  2.09  1.84  1.34  1.01 13.72]\n",
      "Loss_Q: [1.77 1.22 2.05 1.67 1.33 0.88 0.   8.92] Loss_P: [ 3.2   2.41  1.75  2.13  1.87  1.35  1.04 13.76]\n",
      "Loss_Q: [1.73 1.28 2.05 1.69 1.27 0.92 0.   8.94] Loss_P: [ 3.23  2.39  1.73  2.11  1.84  1.39  1.04 13.73]\n",
      "Loss_Q: [1.71 1.26 2.05 1.66 1.3  0.92 0.   8.9 ] Loss_P: [ 3.18  2.35  1.74  2.12  1.82  1.35  1.04 13.61]\n",
      "Loss_Q: [1.75 1.22 2.09 1.65 1.3  0.9  0.   8.89] Loss_P: [ 3.23  2.33  1.69  2.03  1.79  1.33  1.04 13.44]\n",
      "Loss_Q: [1.67 1.21 2.09 1.69 1.26 0.88 0.   8.8 ] Loss_P: [ 3.22  2.39  1.74  2.12  1.84  1.36  1.03 13.7 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.72 1.24 2.09 1.66 1.28 0.86 0.   8.85] Loss_P: [ 3.18  2.4   1.75  2.11  1.75  1.3   1.   13.48]\n",
      "Loss_Q: [1.73 1.25 2.09 1.68 1.28 0.92 0.   8.95] Loss_P: [ 3.21  2.43  1.75  2.12  1.8   1.33  1.01 13.66]\n",
      "Loss_Q: [1.69 1.28 2.07 1.66 1.29 0.87 0.   8.86] Loss_P: [ 3.23  2.32  1.78  2.13  1.79  1.34  1.   13.58]\n",
      "Loss_Q: [1.71 1.25 2.07 1.67 1.29 0.87 0.   8.85] Loss_P: [ 3.18  2.36  1.79  2.18  1.8   1.35  1.03 13.69]\n",
      "Loss_Q: [1.63 1.3  2.08 1.64 1.32 0.88 0.   8.84] Loss_P: [ 3.21  2.39  1.77  2.13  1.76  1.37  1.02 13.65]\n",
      "Loss_Q: [1.76 1.24 2.13 1.66 1.36 0.89 0.   9.03] Loss_P: [ 3.21  2.39  1.79  2.18  1.81  1.37  1.02 13.76]\n",
      "Loss_Q: [1.78 1.25 2.15 1.7  1.34 0.88 0.   9.11] Loss_P: [ 3.18  2.45  1.73  2.15  1.84  1.38  1.03 13.75]\n",
      "Loss_Q: [1.73 1.22 2.07 1.65 1.35 0.88 0.   8.9 ] Loss_P: [ 3.22  2.36  1.78  2.18  1.8   1.42  1.01 13.77]\n",
      "Loss_Q: [1.68 1.25 2.12 1.67 1.32 0.89 0.   8.93] Loss_P: [ 3.15  2.43  1.82  2.18  1.83  1.41  1.06 13.88]\n",
      "Loss_Q: [1.7  1.28 2.12 1.68 1.32 0.91 0.   9.  ] Loss_P: [ 3.16  2.35  1.79  2.2   1.82  1.4   1.06 13.78]\n",
      "Loss_Q: [1.74 1.32 2.11 1.65 1.31 0.89 0.   9.02] Loss_P: [ 3.16  2.38  1.76  2.19  1.74  1.4   1.05 13.68]\n",
      "Loss_Q: [1.64 1.26 2.12 1.64 1.33 0.9  0.   8.9 ] Loss_P: [ 3.19  2.34  1.77  2.19  1.73  1.39  1.05 13.65]\n",
      "Loss_Q: [1.67 1.24 2.1  1.61 1.31 0.9  0.   8.82] Loss_P: [ 3.17  2.29  1.77  2.22  1.74  1.34  1.03 13.57]\n",
      "Loss_Q: [1.65 1.25 2.07 1.67 1.28 0.89 0.   8.81] Loss_P: [ 3.21  2.33  1.79  2.14  1.81  1.37  1.06 13.71]\n",
      "Loss_Q: [1.66 1.27 2.01 1.66 1.3  0.87 0.   8.77] Loss_P: [ 3.22  2.29  1.77  2.14  1.78  1.37  1.03 13.61]\n",
      "Loss_Q: [1.66 1.33 2.04 1.66 1.34 0.91 0.   8.94] Loss_P: [ 3.17  2.33  1.78  2.1   1.8   1.42  1.05 13.65]\n",
      "Loss_Q: [1.63 1.29 2.05 1.69 1.32 0.91 0.   8.88] Loss_P: [ 3.17  2.32  1.76  2.16  1.81  1.38  1.04 13.65]\n",
      "Loss_Q: [1.63 1.25 2.13 1.69 1.3  0.9  0.   8.89] Loss_P: [ 3.2   2.3   1.8   2.19  1.82  1.34  1.05 13.7 ]\n",
      "Loss_Q: [1.62 1.28 2.11 1.73 1.26 0.91 0.   8.9 ] Loss_P: [ 3.17  2.27  1.77  2.16  1.8   1.35  1.06 13.58]\n",
      "Loss_Q: [1.55 1.22 2.13 1.68 1.23 0.88 0.   8.69] Loss_P: [ 3.24  2.28  1.8   2.19  1.81  1.28  1.04 13.65]\n",
      "Loss_Q: [1.58 1.28 2.09 1.72 1.28 0.92 0.   8.87] Loss_P: [ 3.23  2.26  1.8   2.14  1.88  1.35  1.04 13.7 ]\n",
      "Loss_Q: [1.62 1.25 2.1  1.71 1.26 0.91 0.   8.86] Loss_P: [ 3.19  2.28  1.74  2.16  1.83  1.35  1.05 13.61]\n",
      "Loss_Q: [1.66 1.29 2.05 1.71 1.28 0.88 0.   8.88] Loss_P: [ 3.2   2.3   1.79  2.16  1.84  1.33  1.04 13.67]\n",
      "Loss_Q: [1.72 1.28 2.07 1.71 1.29 0.87 0.   8.94] Loss_P: [ 3.18  2.39  1.77  2.13  1.88  1.35  1.01 13.71]\n",
      "Loss_Q: [1.66 1.25 2.06 1.7  1.3  0.86 0.   8.84] Loss_P: [ 3.18  2.38  1.76  2.11  1.91  1.34  1.02 13.69]\n",
      "Loss_Q: [1.72 1.26 2.   1.75 1.31 0.91 0.   8.95] Loss_P: [ 3.18  2.39  1.78  2.08  1.88  1.33  1.01 13.65]\n",
      "Loss_Q: [1.74 1.3  2.03 1.7  1.29 0.9  0.   8.95] Loss_P: [ 3.23  2.31  1.76  2.15  1.9   1.37  1.03 13.74]\n",
      "Loss_Q: [1.66 1.28 2.05 1.72 1.32 0.9  0.   8.93] Loss_P: [ 3.17  2.32  1.77  2.09  1.88  1.36  1.05 13.64]\n",
      "Loss_Q: [1.77 1.34 2.12 1.74 1.33 0.93 0.   9.23] Loss_P: [ 3.17  2.36  1.78  2.16  1.91  1.35  1.07 13.81]\n",
      "Loss_Q: [1.72 1.27 2.11 1.75 1.37 0.89 0.   9.12] Loss_P: [ 3.18  2.39  1.77  2.13  1.9   1.37  1.03 13.78]\n",
      "Loss_Q: [1.74 1.29 2.03 1.73 1.29 0.88 0.   8.96] Loss_P: [ 3.2   2.4   1.72  2.1   1.89  1.35  1.04 13.71]\n",
      "Loss_Q: [1.75 1.34 2.06 1.73 1.33 0.9  0.   9.11] Loss_P: [ 3.18  2.39  1.73  2.14  1.88  1.34  1.05 13.71]\n",
      "Loss_Q: [1.75 1.28 2.08 1.68 1.3  0.95 0.   9.04] Loss_P: [ 3.18  2.41  1.78  2.18  1.88  1.38  1.05 13.86]\n",
      "Loss_Q: [1.75 1.31 2.06 1.7  1.37 0.97 0.   9.17] Loss_P: [ 3.22  2.41  1.79  2.12  1.87  1.39  1.11 13.9 ]\n",
      "Loss_Q: [1.72 1.27 2.08 1.71 1.38 0.98 0.   9.13] Loss_P: [ 3.17  2.38  1.75  2.1   1.86  1.37  1.12 13.76]\n",
      "Loss_Q: [1.65 1.22 2.04 1.7  1.4  0.94 0.   8.95] Loss_P: [ 3.18  2.32  1.74  2.14  1.86  1.37  1.1  13.71]\n",
      "Loss_Q: [1.69 1.29 2.05 1.68 1.36 0.94 0.   9.  ] Loss_P: [ 3.17  2.37  1.74  2.15  1.85  1.38  1.09 13.74]\n",
      "Loss_Q: [1.71 1.32 2.05 1.7  1.33 0.92 0.   9.03] Loss_P: [ 3.16  2.32  1.75  2.14  1.86  1.4   1.09 13.73]\n",
      "Loss_Q: [1.72 1.3  2.06 1.65 1.35 0.9  0.   8.97] Loss_P: [ 3.17  2.39  1.76  2.13  1.9   1.37  1.05 13.78]\n",
      "Loss_Q: [1.68 1.36 2.07 1.7  1.35 0.93 0.   9.09] Loss_P: [ 3.16  2.35  1.73  2.14  1.89  1.38  1.07 13.72]\n",
      "Loss_Q: [1.74 1.32 2.03 1.72 1.32 0.91 0.   9.04] Loss_P: [ 3.21  2.37  1.76  2.14  1.89  1.39  1.06 13.82]\n",
      "Loss_Q: [1.73 1.25 2.07 1.76 1.36 0.9  0.   9.08] Loss_P: [ 3.18  2.38  1.76  2.12  1.94  1.38  1.04 13.81]\n",
      "Loss_Q: [1.71 1.38 2.08 1.68 1.35 0.92 0.   9.12] Loss_P: [ 3.18  2.35  1.79  2.14  1.88  1.39  1.07 13.79]\n",
      "Loss_Q: [1.69 1.33 2.05 1.75 1.37 0.95 0.   9.14] Loss_P: [ 3.2   2.37  1.78  2.16  1.87  1.41  1.07 13.87]\n",
      "Loss_Q: [1.71 1.36 2.04 1.74 1.39 0.92 0.   9.16] Loss_P: [ 3.21  2.34  1.74  2.16  1.85  1.42  1.06 13.78]\n",
      "Loss_Q: [1.7  1.39 2.08 1.74 1.43 0.96 0.   9.29] Loss_P: [ 3.19  2.39  1.81  2.18  1.87  1.46  1.06 13.96]\n",
      "Loss_Q: [1.64 1.33 2.09 1.71 1.4  0.96 0.   9.12] Loss_P: [ 3.2   2.29  1.77  2.24  1.92  1.46  1.1  13.97]\n",
      "Loss_Q: [1.63 1.35 2.09 1.74 1.43 0.95 0.   9.19] Loss_P: [ 3.15  2.32  1.73  2.2   1.92  1.44  1.1  13.86]\n",
      "Loss_Q: [1.63 1.31 2.11 1.75 1.4  0.93 0.   9.12] Loss_P: [ 3.21  2.25  1.78  2.13  1.94  1.43  1.11 13.86]\n",
      "Loss_Q: [1.61 1.3  2.12 1.75 1.4  1.   0.   9.19] Loss_P: [ 3.21  2.25  1.73  2.15  1.95  1.42  1.14 13.84]\n",
      "Loss_Q: [1.51 1.39 2.08 1.74 1.41 0.95 0.   9.08] Loss_P: [ 3.23  2.2   1.77  2.2   1.9   1.44  1.12 13.86]\n",
      "Loss_Q: [1.61 1.35 2.04 1.74 1.38 0.96 0.   9.08] Loss_P: [ 3.17  2.26  1.73  2.16  1.91  1.43  1.15 13.82]\n",
      "Loss_Q: [1.66 1.32 2.11 1.76 1.39 0.98 0.   9.22] Loss_P: [ 3.23  2.23  1.7   2.16  1.9   1.41  1.17 13.8 ]\n",
      "Loss_Q: [1.61 1.3  2.13 1.67 1.42 0.95 0.   9.08] Loss_P: [ 3.24  2.22  1.73  2.18  1.84  1.46  1.12 13.79]\n",
      "Loss_Q: [1.56 1.36 2.1  1.74 1.41 0.95 0.   9.12] Loss_P: [ 3.21  2.23  1.72  2.2   1.86  1.45  1.13 13.79]\n",
      "Loss_Q: [1.53 1.31 2.06 1.7  1.37 0.96 0.   8.93] Loss_P: [ 3.22  2.19  1.77  2.11  1.87  1.42  1.13 13.7 ]\n",
      "Loss_Q: [1.57 1.3  2.06 1.72 1.39 0.95 0.   8.98] Loss_P: [ 3.18  2.21  1.73  2.14  1.86  1.42  1.09 13.64]\n",
      "Loss_Q: [1.57 1.35 2.08 1.7  1.42 0.93 0.   9.05] Loss_P: [ 3.24  2.27  1.75  2.09  1.83  1.47  1.09 13.74]\n",
      "Loss_Q: [1.54 1.4  1.99 1.71 1.36 0.97 0.   8.97] Loss_P: [ 3.18  2.23  1.79  2.13  1.87  1.46  1.13 13.78]\n",
      "Loss_Q: [1.47 1.31 2.   1.75 1.39 0.98 0.   8.9 ] Loss_P: [ 3.25  2.15  1.79  2.08  1.9   1.41  1.14 13.74]\n",
      "Loss_Q: [1.59 1.32 2.04 1.73 1.36 1.01 0.   9.05] Loss_P: [ 3.2   2.24  1.8   2.1   1.87  1.39  1.15 13.75]\n",
      "Loss_Q: [1.61 1.36 2.01 1.75 1.36 1.02 0.   9.11] Loss_P: [ 3.2   2.21  1.79  2.05  1.91  1.41  1.17 13.73]\n",
      "Loss_Q: [1.62 1.34 2.03 1.76 1.38 1.   0.   9.14] Loss_P: [ 3.21  2.23  1.74  2.06  1.87  1.41  1.18 13.7 ]\n",
      "Loss_Q: [1.63 1.28 1.99 1.69 1.35 1.04 0.   8.98] Loss_P: [ 3.21  2.28  1.71  2.08  1.95  1.4   1.2  13.83]\n",
      "Loss_Q: [1.6  1.28 1.98 1.78 1.34 1.03 0.   9.  ] Loss_P: [ 3.21  2.28  1.73  2.07  1.89  1.42  1.21 13.8 ]\n",
      "Loss_Q: [1.53 1.33 1.98 1.73 1.34 1.06 0.   8.96] Loss_P: [ 3.22  2.22  1.75  2.01  1.87  1.39  1.18 13.64]\n",
      "Loss_Q: [1.57 1.21 1.95 1.74 1.31 1.   0.   8.78] Loss_P: [ 3.23  2.16  1.73  2.1   1.9   1.37  1.18 13.67]\n",
      "Loss_Q: [1.53 1.26 1.96 1.76 1.27 1.01 0.   8.79] Loss_P: [ 3.16  2.25  1.7   2.04  1.93  1.37  1.16 13.6 ]\n",
      "Loss_Q: [1.62 1.25 2.05 1.73 1.29 1.01 0.   8.94] Loss_P: [ 3.19  2.24  1.71  2.02  1.92  1.34  1.15 13.57]\n",
      "Loss_Q: [1.59 1.24 1.94 1.75 1.28 0.99 0.   8.8 ] Loss_P: [ 3.16  2.22  1.67  2.01  1.93  1.35  1.19 13.53]\n",
      "Loss_Q: [1.56 1.23 1.97 1.73 1.27 1.01 0.   8.77] Loss_P: [ 3.16  2.29  1.68  2.    1.89  1.35  1.17 13.53]\n",
      "Loss_Q: [1.58 1.33 1.95 1.79 1.31 0.99 0.   8.96] Loss_P: [ 3.17  2.33  1.72  2.03  1.95  1.37  1.13 13.69]\n",
      "Loss_Q: [1.56 1.22 1.95 1.76 1.27 0.97 0.   8.74] Loss_P: [ 3.18  2.3   1.69  2.03  1.89  1.32  1.17 13.58]\n",
      "Loss_Q: [1.5  1.25 1.9  1.78 1.27 1.   0.   8.7 ] Loss_P: [ 3.17  2.23  1.72  1.97  1.91  1.35  1.15 13.52]\n",
      "Loss_Q: [1.63 1.19 1.92 1.77 1.24 1.02 0.   8.76] Loss_P: [ 3.17  2.3   1.66  1.97  1.95  1.3   1.17 13.52]\n",
      "Loss_Q: [1.61 1.26 1.95 1.74 1.27 1.01 0.   8.83] Loss_P: [ 3.19  2.28  1.68  1.95  1.93  1.3   1.17 13.5 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.58 1.26 1.93 1.8  1.27 1.01 0.   8.83] Loss_P: [ 3.16  2.33  1.71  1.95  1.93  1.32  1.16 13.57]\n",
      "Loss_Q: [1.58 1.33 1.92 1.78 1.26 1.   0.   8.87] Loss_P: [ 3.18  2.28  1.73  1.99  1.96  1.31  1.13 13.58]\n",
      "Loss_Q: [1.58 1.27 1.91 1.81 1.3  0.98 0.   8.86] Loss_P: [ 3.19  2.31  1.72  2.03  1.92  1.31  1.12 13.61]\n",
      "Loss_Q: [1.58 1.32 1.97 1.8  1.26 1.01 0.   8.95] Loss_P: [ 3.18  2.24  1.73  1.99  1.95  1.36  1.16 13.61]\n",
      "Loss_Q: [1.6  1.33 1.9  1.81 1.3  1.02 0.   8.97] Loss_P: [ 3.19  2.3   1.75  2.01  1.95  1.32  1.15 13.69]\n",
      "Loss_Q: [1.61 1.35 1.94 1.81 1.27 1.01 0.   8.99] Loss_P: [ 3.23  2.25  1.75  2.01  1.97  1.38  1.17 13.76]\n",
      "Loss_Q: [1.6  1.41 1.97 1.77 1.33 1.01 0.   9.08] Loss_P: [ 3.17  2.26  1.79  2.04  1.95  1.38  1.18 13.78]\n",
      "Loss_Q: [1.57 1.31 1.96 1.76 1.28 0.97 0.   8.86] Loss_P: [ 3.2   2.26  1.72  2.02  1.92  1.36  1.13 13.61]\n",
      "Loss_Q: [1.63 1.22 1.97 1.79 1.32 1.   0.   8.93] Loss_P: [ 3.18  2.25  1.69  1.99  1.93  1.34  1.15 13.53]\n",
      "Loss_Q: [1.59 1.32 1.96 1.78 1.32 0.97 0.   8.95] Loss_P: [ 3.18  2.28  1.71  2.01  1.95  1.39  1.12 13.64]\n",
      "Loss_Q: [1.64 1.23 1.96 1.72 1.33 0.98 0.   8.86] Loss_P: [ 3.16  2.25  1.66  2.    1.93  1.37  1.14 13.51]\n",
      "Loss_Q: [1.53 1.33 1.97 1.73 1.3  0.95 0.   8.82] Loss_P: [ 3.19  2.28  1.67  2.07  1.92  1.36  1.1  13.6 ]\n",
      "Loss_Q: [1.55 1.3  2.02 1.75 1.28 0.95 0.   8.84] Loss_P: [ 3.19  2.32  1.73  2.08  1.91  1.36  1.11 13.69]\n",
      "Loss_Q: [1.61 1.28 1.99 1.77 1.24 0.95 0.   8.84] Loss_P: [ 3.18  2.26  1.72  2.07  1.89  1.35  1.12 13.58]\n",
      "Loss_Q: [1.63 1.3  2.02 1.75 1.28 0.94 0.   8.92] Loss_P: [ 3.24  2.25  1.75  2.    1.95  1.31  1.07 13.58]\n",
      "Loss_Q: [1.62 1.3  1.93 1.78 1.27 0.94 0.   8.85] Loss_P: [ 3.19  2.27  1.74  2.06  1.94  1.33  1.09 13.6 ]\n",
      "Loss_Q: [1.61 1.28 1.99 1.73 1.26 0.93 0.   8.81] Loss_P: [ 3.18  2.21  1.74  2.06  1.96  1.34  1.06 13.55]\n",
      "Loss_Q: [1.5  1.31 1.91 1.78 1.2  0.94 0.   8.64] Loss_P: [ 3.21  2.22  1.7   2.    1.9   1.27  1.08 13.37]\n",
      "Loss_Q: [1.53 1.28 1.91 1.73 1.21 0.89 0.   8.55] Loss_P: [ 3.18  2.26  1.71  1.98  1.92  1.28  1.07 13.4 ]\n",
      "Loss_Q: [1.41 1.23 1.93 1.75 1.19 0.92 0.   8.43] Loss_P: [ 3.18  2.26  1.7   2.02  1.9   1.27  1.08 13.4 ]\n",
      "Loss_Q: [1.6  1.29 1.93 1.7  1.24 0.93 0.   8.69] Loss_P: [ 3.18  2.3   1.64  1.97  1.9   1.27  1.08 13.34]\n",
      "Loss_Q: [1.6  1.25 1.94 1.75 1.24 0.93 0.   8.7 ] Loss_P: [ 3.17  2.26  1.69  2.01  1.93  1.26  1.08 13.4 ]\n",
      "Loss_Q: [1.6  1.25 1.96 1.76 1.21 0.92 0.   8.7 ] Loss_P: [ 3.16  2.29  1.66  2.03  1.93  1.31  1.1  13.49]\n",
      "Loss_Q: [1.62 1.21 1.97 1.75 1.23 0.99 0.   8.76] Loss_P: [ 3.14  2.29  1.74  2.06  1.93  1.25  1.11 13.52]\n",
      "Loss_Q: [1.57 1.24 1.97 1.76 1.25 0.97 0.   8.75] Loss_P: [ 3.16  2.31  1.67  2.04  1.94  1.26  1.13 13.51]\n",
      "Loss_Q: [1.59 1.24 1.96 1.79 1.21 0.97 0.   8.75] Loss_P: [ 3.19  2.33  1.67  2.    1.9   1.23  1.11 13.43]\n",
      "Loss_Q: [1.59 1.2  1.88 1.75 1.16 1.   0.   8.59] Loss_P: [ 3.16  2.31  1.67  1.93  1.93  1.19  1.14 13.33]\n",
      "Loss_Q: [1.61 1.26 1.87 1.74 1.2  1.03 0.   8.7 ] Loss_P: [ 3.14  2.23  1.66  1.93  1.93  1.24  1.15 13.29]\n",
      "Loss_Q: [1.58 1.29 1.92 1.79 1.22 1.03 0.   8.83] Loss_P: [ 3.19  2.23  1.69  1.94  1.95  1.25  1.14 13.38]\n",
      "Loss_Q: [1.64 1.24 1.93 1.76 1.16 1.04 0.   8.78] Loss_P: [ 3.17  2.35  1.67  1.98  1.96  1.25  1.15 13.53]\n",
      "Loss_Q: [1.63 1.19 1.94 1.76 1.15 1.05 0.   8.71] Loss_P: [ 3.16  2.34  1.61  2.01  1.94  1.2   1.18 13.44]\n",
      "Loss_Q: [1.66 1.26 1.91 1.78 1.14 1.03 0.   8.79] Loss_P: [ 3.16  2.37  1.7   1.98  1.96  1.16  1.18 13.5 ]\n",
      "Loss_Q: [1.69 1.28 1.93 1.77 1.13 1.07 0.   8.87] Loss_P: [ 3.18  2.39  1.7   1.95  1.95  1.18  1.18 13.52]\n",
      "Loss_Q: [1.62 1.25 1.91 1.8  1.17 1.05 0.   8.8 ] Loss_P: [ 3.18  2.35  1.66  2.01  1.97  1.17  1.23 13.57]\n",
      "Loss_Q: [1.66 1.23 1.9  1.8  1.19 1.06 0.   8.83] Loss_P: [ 3.14  2.34  1.64  1.95  1.95  1.24  1.2  13.46]\n",
      "Loss_Q: [1.56 1.22 1.93 1.76 1.17 1.04 0.   8.67] Loss_P: [ 3.12  2.3   1.65  1.96  1.95  1.22  1.19 13.4 ]\n",
      "Loss_Q: [1.62 1.23 1.92 1.76 1.16 1.02 0.   8.71] Loss_P: [ 3.17  2.33  1.62  1.95  1.91  1.14  1.2  13.33]\n",
      "Loss_Q: [1.56 1.2  1.9  1.75 1.12 1.07 0.   8.6 ] Loss_P: [ 3.15  2.26  1.63  1.96  1.94  1.21  1.25 13.4 ]\n",
      "Loss_Q: [1.6  1.19 1.94 1.78 1.2  1.05 0.   8.77] Loss_P: [ 3.13  2.26  1.69  2.01  1.96  1.18  1.23 13.46]\n",
      "Loss_Q: [1.53 1.28 1.9  1.75 1.19 1.08 0.   8.74] Loss_P: [ 3.16  2.19  1.66  1.99  2.04  1.23  1.25 13.51]\n",
      "Loss_Q: [1.5  1.23 1.91 1.73 1.23 1.08 0.   8.69] Loss_P: [ 3.21  2.21  1.66  2.02  1.94  1.24  1.22 13.51]\n",
      "Loss_Q: [1.46 1.16 1.89 1.76 1.17 1.08 0.   8.52] Loss_P: [ 3.2   2.18  1.62  1.98  1.99  1.23  1.24 13.44]\n",
      "Loss_Q: [1.6  1.27 1.95 1.77 1.2  1.1  0.   8.89] Loss_P: [ 3.21  2.19  1.63  2.    1.95  1.23  1.24 13.47]\n",
      "Loss_Q: [1.57 1.2  1.92 1.76 1.21 1.07 0.   8.73] Loss_P: [ 3.2   2.3   1.66  1.97  1.99  1.23  1.25 13.6 ]\n",
      "Loss_Q: [1.65 1.2  1.94 1.8  1.16 1.1  0.   8.85] Loss_P: [ 3.19  2.32  1.66  1.95  2.01  1.21  1.28 13.62]\n",
      "Loss_Q: [1.61 1.19 1.85 1.76 1.14 1.1  0.   8.64] Loss_P: [ 3.15  2.32  1.74  1.98  2.    1.18  1.29 13.65]\n",
      "Loss_Q: [1.68 1.21 1.93 1.79 1.21 1.11 0.   8.92] Loss_P: [ 3.16  2.32  1.67  1.99  2.02  1.21  1.29 13.65]\n",
      "Loss_Q: [1.6  1.2  1.91 1.84 1.21 1.12 0.   8.89] Loss_P: [ 3.15  2.36  1.67  2.02  2.03  1.23  1.29 13.75]\n",
      "Loss_Q: [1.62 1.21 1.91 1.86 1.19 1.1  0.   8.89] Loss_P: [ 3.17  2.36  1.67  1.97  2.05  1.24  1.28 13.74]\n",
      "Loss_Q: [1.67 1.25 1.87 1.86 1.18 1.1  0.   8.94] Loss_P: [ 3.13  2.36  1.65  1.92  2.05  1.23  1.28 13.62]\n",
      "Loss_Q: [1.7  1.27 1.87 1.87 1.23 1.11 0.   9.05] Loss_P: [ 3.18  2.39  1.68  1.96  2.06  1.19  1.26 13.71]\n",
      "Loss_Q: [1.74 1.28 1.9  1.83 1.25 1.06 0.   9.07] Loss_P: [ 3.13  2.47  1.69  1.93  2.06  1.25  1.24 13.77]\n",
      "Loss_Q: [1.76 1.23 1.89 1.84 1.24 1.05 0.   9.01] Loss_P: [ 3.12  2.52  1.65  1.94  2.02  1.25  1.21 13.7 ]\n",
      "Loss_Q: [1.8  1.27 1.94 1.84 1.21 1.03 0.   9.11] Loss_P: [ 3.15  2.44  1.68  1.98  2.04  1.23  1.2  13.72]\n",
      "Loss_Q: [1.76 1.28 1.9  1.86 1.17 1.05 0.   9.02] Loss_P: [ 3.12  2.5   1.69  1.97  2.07  1.28  1.18 13.8 ]\n",
      "Loss_Q: [1.76 1.22 1.89 1.87 1.28 1.05 0.   9.07] Loss_P: [ 3.12  2.42  1.66  1.95  2.05  1.27  1.21 13.68]\n",
      "Loss_Q: [1.72 1.24 1.89 1.87 1.24 1.04 0.   8.99] Loss_P: [ 3.16  2.44  1.66  1.93  2.04  1.25  1.22 13.72]\n",
      "Loss_Q: [1.76 1.22 1.87 1.83 1.2  1.04 0.   8.92] Loss_P: [ 3.15  2.47  1.65  1.95  2.02  1.24  1.2  13.69]\n",
      "Loss_Q: [1.69 1.21 1.86 1.86 1.15 1.05 0.   8.83] Loss_P: [ 3.16  2.43  1.6   1.89  1.98  1.21  1.23 13.5 ]\n",
      "Loss_Q: [1.77 1.19 1.87 1.81 1.15 1.03 0.   8.82] Loss_P: [ 3.11  2.45  1.64  1.98  2.    1.2   1.2  13.57]\n",
      "Loss_Q: [1.74 1.17 1.87 1.82 1.22 1.04 0.   8.85] Loss_P: [ 3.13  2.48  1.59  1.97  2.02  1.24  1.22 13.65]\n",
      "Loss_Q: [1.74 1.25 1.87 1.83 1.23 1.05 0.   8.96] Loss_P: [ 3.17  2.43  1.65  1.91  1.99  1.23  1.2  13.59]\n",
      "Loss_Q: [1.74 1.15 1.89 1.8  1.19 1.03 0.   8.8 ] Loss_P: [ 3.13  2.44  1.65  1.92  1.99  1.25  1.2  13.56]\n",
      "Loss_Q: [1.78 1.24 1.93 1.78 1.18 1.04 0.   8.94] Loss_P: [ 3.15  2.47  1.66  1.94  1.94  1.2   1.18 13.54]\n",
      "Loss_Q: [1.82 1.26 1.91 1.78 1.1  1.04 0.   8.92] Loss_P: [ 3.16  2.53  1.68  1.88  1.97  1.18  1.23 13.64]\n",
      "Loss_Q: [1.82 1.24 1.82 1.79 1.1  1.02 0.   8.78] Loss_P: [ 3.13  2.51  1.63  1.92  1.96  1.16  1.19 13.5 ]\n",
      "Loss_Q: [1.82 1.27 1.88 1.79 1.18 1.01 0.   8.95] Loss_P: [ 3.12  2.49  1.69  1.96  1.95  1.19  1.19 13.57]\n",
      "Loss_Q: [1.75 1.28 1.91 1.76 1.18 1.05 0.   8.93] Loss_P: [ 3.17  2.47  1.65  1.98  1.95  1.21  1.19 13.63]\n",
      "Loss_Q: [1.72 1.22 1.84 1.74 1.15 1.03 0.   8.7 ] Loss_P: [ 3.13  2.49  1.71  1.96  1.95  1.19  1.18 13.62]\n",
      "Loss_Q: [1.71 1.24 1.89 1.76 1.22 1.01 0.   8.84] Loss_P: [ 3.14  2.39  1.68  1.91  1.92  1.18  1.16 13.37]\n",
      "Loss_Q: [1.68 1.26 1.9  1.74 1.17 1.04 0.   8.79] Loss_P: [ 3.14  2.4   1.7   1.95  1.93  1.19  1.2  13.52]\n",
      "Loss_Q: [1.6  1.27 1.9  1.74 1.22 1.01 0.   8.74] Loss_P: [ 3.18  2.39  1.64  1.98  1.9   1.24  1.2  13.54]\n",
      "Loss_Q: [1.74 1.24 1.92 1.74 1.26 1.07 0.   8.97] Loss_P: [ 3.19  2.39  1.7   1.9   1.92  1.31  1.23 13.65]\n",
      "Loss_Q: [1.74 1.24 1.89 1.73 1.24 1.09 0.   8.93] Loss_P: [ 3.14  2.42  1.65  1.89  1.93  1.3   1.26 13.59]\n",
      "Loss_Q: [1.72 1.24 1.87 1.82 1.22 1.07 0.   8.94] Loss_P: [ 3.15  2.39  1.63  1.89  1.96  1.27  1.26 13.55]\n",
      "Loss_Q: [1.67 1.25 1.86 1.77 1.22 1.08 0.   8.84] Loss_P: [ 3.11  2.39  1.61  1.9   1.96  1.26  1.25 13.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.65 1.23 1.82 1.78 1.19 1.05 0.   8.72] Loss_P: [ 3.16  2.35  1.67  1.9   1.93  1.24  1.25 13.49]\n",
      "Loss_Q: [1.7  1.28 1.88 1.8  1.22 1.08 0.   8.95] Loss_P: [ 3.11  2.39  1.64  1.97  1.99  1.26  1.27 13.62]\n",
      "Loss_Q: [1.63 1.23 1.87 1.77 1.21 1.07 0.   8.77] Loss_P: [ 3.17  2.28  1.64  1.94  1.95  1.25  1.27 13.49]\n",
      "Loss_Q: [1.67 1.32 1.88 1.71 1.25 1.07 0.   8.9 ] Loss_P: [ 3.15  2.37  1.63  1.97  1.91  1.25  1.26 13.54]\n",
      "Loss_Q: [1.64 1.25 1.81 1.77 1.22 1.1  0.   8.79] Loss_P: [ 3.14  2.39  1.63  1.94  1.95  1.3   1.26 13.62]\n",
      "Loss_Q: [1.68 1.26 1.84 1.79 1.22 1.09 0.   8.88] Loss_P: [ 3.14  2.39  1.72  1.96  1.95  1.3   1.27 13.73]\n",
      "Loss_Q: [1.71 1.29 1.86 1.77 1.22 1.07 0.   8.92] Loss_P: [ 3.15  2.39  1.71  1.98  1.94  1.29  1.24 13.7 ]\n",
      "Loss_Q: [1.73 1.28 1.86 1.77 1.24 1.06 0.   8.95] Loss_P: [ 3.13  2.38  1.69  1.98  1.98  1.25  1.24 13.64]\n",
      "Loss_Q: [1.69 1.25 1.86 1.78 1.22 1.06 0.   8.86] Loss_P: [ 3.16  2.38  1.62  1.94  1.99  1.28  1.23 13.6 ]\n",
      "Loss_Q: [1.69 1.27 1.85 1.76 1.23 1.07 0.   8.87] Loss_P: [ 3.14  2.44  1.65  1.94  1.97  1.26  1.21 13.6 ]\n",
      "Loss_Q: [1.73 1.26 1.88 1.78 1.24 1.07 0.   8.96] Loss_P: [ 3.18  2.44  1.68  1.99  1.93  1.28  1.2  13.71]\n",
      "Loss_Q: [1.68 1.2  1.9  1.74 1.24 1.02 0.   8.77] Loss_P: [ 3.15  2.38  1.64  1.98  1.96  1.25  1.2  13.56]\n",
      "Loss_Q: [1.64 1.28 1.83 1.76 1.2  1.04 0.   8.74] Loss_P: [ 3.19  2.28  1.65  1.95  1.95  1.21  1.2  13.44]\n",
      "Loss_Q: [1.67 1.27 1.86 1.8  1.18 1.02 0.   8.79] Loss_P: [ 3.15  2.34  1.66  1.91  1.93  1.17  1.18 13.35]\n",
      "Loss_Q: [1.69 1.24 1.82 1.75 1.14 1.01 0.   8.66] Loss_P: [ 3.18  2.4   1.66  1.96  1.91  1.19  1.17 13.47]\n",
      "Loss_Q: [1.64 1.27 1.85 1.8  1.17 1.03 0.   8.75] Loss_P: [ 3.22  2.3   1.64  1.89  1.91  1.18  1.15 13.29]\n",
      "Loss_Q: [1.68 1.27 1.79 1.75 1.17 1.01 0.   8.67] Loss_P: [ 3.17  2.3   1.68  1.84  1.91  1.2   1.17 13.26]\n",
      "Loss_Q: [1.6  1.29 1.83 1.71 1.15 1.02 0.   8.59] Loss_P: [ 3.18  2.29  1.66  1.91  1.92  1.19  1.17 13.32]\n",
      "Loss_Q: [1.58 1.26 1.8  1.73 1.16 1.04 0.   8.56] Loss_P: [ 3.17  2.29  1.68  1.88  1.91  1.22  1.19 13.35]\n",
      "Loss_Q: [1.6  1.2  1.8  1.72 1.15 1.03 0.   8.5 ] Loss_P: [ 3.19  2.25  1.64  1.87  1.94  1.18  1.16 13.24]\n",
      "Loss_Q: [1.67 1.29 1.83 1.72 1.15 1.03 0.   8.7 ] Loss_P: [ 3.18  2.25  1.68  1.87  1.92  1.19  1.16 13.26]\n",
      "Loss_Q: [1.67 1.23 1.8  1.75 1.13 0.99 0.   8.58] Loss_P: [ 3.21  2.33  1.68  1.82  1.94  1.18  1.12 13.28]\n",
      "Loss_Q: [1.64 1.22 1.8  1.69 1.11 1.   0.   8.47] Loss_P: [ 3.15  2.35  1.62  1.89  1.91  1.24  1.13 13.29]\n",
      "Loss_Q: [1.61 1.24 1.83 1.71 1.2  0.98 0.   8.57] Loss_P: [ 3.15  2.37  1.63  1.88  1.89  1.22  1.14 13.28]\n",
      "Loss_Q: [1.58 1.13 1.79 1.72 1.18 1.   0.   8.41] Loss_P: [ 3.19  2.3   1.56  1.82  1.89  1.16  1.12 13.05]\n",
      "Loss_Q: [1.58 1.12 1.79 1.75 1.16 0.99 0.   8.39] Loss_P: [ 3.16  2.38  1.62  1.89  1.88  1.22  1.11 13.26]\n",
      "Loss_Q: [1.61 1.17 1.75 1.7  1.13 1.   0.   8.36] Loss_P: [ 3.14  2.35  1.64  1.86  1.86  1.23  1.15 13.23]\n",
      "Loss_Q: [1.6  1.19 1.78 1.7  1.21 1.   0.   8.47] Loss_P: [ 3.18  2.31  1.69  1.79  1.91  1.22  1.14 13.23]\n",
      "Loss_Q: [1.61 1.21 1.75 1.72 1.19 1.02 0.   8.5 ] Loss_P: [ 3.17  2.27  1.65  1.83  1.85  1.2   1.14 13.12]\n",
      "Loss_Q: [1.67 1.2  1.73 1.71 1.2  0.98 0.   8.49] Loss_P: [ 3.2   2.36  1.67  1.8   1.88  1.25  1.13 13.28]\n",
      "Loss_Q: [1.71 1.2  1.72 1.72 1.18 0.98 0.   8.51] Loss_P: [ 3.16  2.28  1.63  1.76  1.88  1.18  1.11 13.02]\n",
      "Loss_Q: [1.68 1.21 1.7  1.72 1.11 0.95 0.   8.36] Loss_P: [ 3.17  2.28  1.67  1.78  1.88  1.22  1.07 13.07]\n",
      "Loss_Q: [1.65 1.17 1.63 1.71 1.13 0.96 0.   8.25] Loss_P: [ 3.18  2.32  1.61  1.69  1.88  1.16  1.08 12.91]\n",
      "Loss_Q: [1.66 1.17 1.73 1.71 1.13 0.94 0.   8.34] Loss_P: [ 3.18  2.35  1.63  1.77  1.9   1.17  1.08 13.09]\n",
      "Loss_Q: [1.66 1.13 1.71 1.7  1.13 0.91 0.   8.24] Loss_P: [ 3.14  2.41  1.64  1.78  1.86  1.15  1.06 13.04]\n",
      "Loss_Q: [1.74 1.16 1.74 1.7  1.1  0.9  0.   8.34] Loss_P: [ 3.17  2.32  1.64  1.81  1.89  1.14  1.01 12.98]\n",
      "Loss_Q: [1.62 1.15 1.79 1.69 1.1  0.89 0.   8.24] Loss_P: [ 3.17  2.38  1.62  1.83  1.86  1.14  1.   13.  ]\n",
      "Loss_Q: [1.63 1.21 1.67 1.7  1.1  0.88 0.   8.18] Loss_P: [ 3.13  2.41  1.64  1.76  1.85  1.15  1.   12.94]\n",
      "Loss_Q: [1.66 1.18 1.68 1.67 1.11 0.88 0.   8.18] Loss_P: [ 3.13  2.4   1.63  1.81  1.88  1.16  0.97 12.98]\n",
      "Loss_Q: [1.65 1.2  1.75 1.66 1.1  0.9  0.   8.26] Loss_P: [ 3.12  2.39  1.64  1.77  1.85  1.12  0.98 12.87]\n",
      "Loss_Q: [1.65 1.23 1.71 1.64 1.07 0.85 0.   8.16] Loss_P: [ 3.12  2.43  1.63  1.82  1.87  1.15  0.96 12.99]\n",
      "Loss_Q: [1.67 1.21 1.73 1.65 1.11 0.89 0.   8.26] Loss_P: [ 3.14  2.38  1.61  1.83  1.83  1.14  0.99 12.91]\n",
      "Loss_Q: [1.66 1.19 1.7  1.66 1.12 0.86 0.   8.19] Loss_P: [ 3.12  2.33  1.61  1.83  1.86  1.13  1.01 12.89]\n",
      "Loss_Q: [1.64 1.17 1.72 1.65 1.09 0.9  0.   8.16] Loss_P: [ 3.14  2.28  1.64  1.76  1.83  1.15  1.04 12.83]\n",
      "Loss_Q: [1.55 1.11 1.63 1.65 1.09 0.9  0.   7.92] Loss_P: [ 3.15  2.31  1.57  1.72  1.79  1.11  1.01 12.66]\n",
      "Loss_Q: [1.63 1.14 1.66 1.61 1.08 0.89 0.   8.01] Loss_P: [ 3.13  2.29  1.63  1.79  1.82  1.15  1.01 12.82]\n",
      "Loss_Q: [1.6  1.12 1.63 1.64 1.09 0.87 0.   7.95] Loss_P: [ 3.2   2.25  1.62  1.78  1.81  1.12  1.   12.78]\n",
      "Loss_Q: [1.64 1.18 1.64 1.66 1.09 0.88 0.   8.09] Loss_P: [ 3.15  2.26  1.65  1.75  1.8   1.13  1.   12.74]\n",
      "Loss_Q: [1.52 1.15 1.68 1.64 1.1  0.87 0.   7.96] Loss_P: [ 3.21  2.21  1.6   1.81  1.82  1.14  1.   12.79]\n",
      "Loss_Q: [1.52 1.15 1.7  1.68 1.06 0.83 0.   7.94] Loss_P: [ 3.15  2.21  1.61  1.71  1.82  1.14  0.98 12.61]\n",
      "Loss_Q: [1.57 1.17 1.63 1.66 1.09 0.86 0.   7.97] Loss_P: [ 3.14  2.17  1.6   1.7   1.83  1.12  0.95 12.5 ]\n",
      "Loss_Q: [1.47 1.12 1.65 1.66 1.08 0.84 0.   7.83] Loss_P: [ 3.16  2.18  1.62  1.75  1.8   1.11  0.95 12.57]\n",
      "Loss_Q: [1.55 1.17 1.68 1.62 1.06 0.81 0.   7.9 ] Loss_P: [ 3.17  2.2   1.59  1.72  1.79  1.11  0.95 12.53]\n",
      "Loss_Q: [1.52 1.23 1.65 1.65 1.14 0.82 0.   8.  ] Loss_P: [ 3.2   2.19  1.65  1.74  1.81  1.14  0.94 12.68]\n",
      "Loss_Q: [1.45 1.18 1.67 1.56 1.09 0.85 0.   7.8 ] Loss_P: [ 3.21  2.18  1.66  1.77  1.76  1.18  0.95 12.7 ]\n",
      "Loss_Q: [1.51 1.22 1.66 1.62 1.14 0.84 0.   7.98] Loss_P: [ 3.2   2.23  1.65  1.78  1.8   1.14  0.92 12.72]\n",
      "Loss_Q: [1.52 1.2  1.69 1.67 1.14 0.81 0.   8.03] Loss_P: [ 3.19  2.25  1.67  1.77  1.82  1.2   0.95 12.85]\n",
      "Loss_Q: [1.5  1.24 1.69 1.66 1.13 0.83 0.   8.05] Loss_P: [ 3.17  2.2   1.65  1.76  1.8   1.17  0.95 12.71]\n",
      "Loss_Q: [1.6  1.25 1.66 1.65 1.11 0.86 0.   8.13] Loss_P: [ 3.17  2.21  1.66  1.7   1.82  1.14  0.99 12.7 ]\n",
      "Loss_Q: [1.55 1.21 1.62 1.63 1.12 0.84 0.   7.98] Loss_P: [ 3.18  2.24  1.69  1.74  1.85  1.1   0.97 12.78]\n",
      "Loss_Q: [1.65 1.18 1.64 1.68 1.13 0.85 0.   8.12] Loss_P: [ 3.19  2.28  1.69  1.76  1.83  1.13  0.94 12.8 ]\n",
      "Loss_Q: [1.58 1.17 1.62 1.63 1.09 0.8  0.   7.9 ] Loss_P: [ 3.18  2.26  1.6   1.72  1.8   1.15  0.95 12.68]\n",
      "Loss_Q: [1.61 1.2  1.72 1.6  1.15 0.84 0.   8.11] Loss_P: [ 3.18  2.3   1.67  1.77  1.79  1.19  0.93 12.84]\n",
      "Loss_Q: [1.66 1.22 1.71 1.66 1.2  0.8  0.   8.25] Loss_P: [ 3.18  2.3   1.58  1.78  1.83  1.19  0.96 12.82]\n",
      "Loss_Q: [1.58 1.2  1.6  1.62 1.14 0.76 0.   7.89] Loss_P: [ 3.17  2.3   1.65  1.74  1.79  1.16  0.92 12.73]\n",
      "Loss_Q: [1.62 1.18 1.66 1.64 1.14 0.78 0.   8.02] Loss_P: [ 3.15  2.26  1.62  1.72  1.78  1.16  0.87 12.55]\n",
      "Loss_Q: [1.6  1.28 1.65 1.65 1.11 0.79 0.   8.08] Loss_P: [ 3.16  2.27  1.65  1.72  1.83  1.14  0.89 12.67]\n",
      "Loss_Q: [1.63 1.15 1.64 1.64 1.11 0.77 0.   7.95] Loss_P: [ 3.15  2.36  1.64  1.76  1.8   1.16  0.93 12.8 ]\n",
      "Loss_Q: [1.61 1.17 1.68 1.6  1.13 0.84 0.   8.03] Loss_P: [ 3.16  2.3   1.62  1.75  1.83  1.21  0.97 12.84]\n",
      "Loss_Q: [1.64 1.22 1.68 1.59 1.12 0.84 0.   8.09] Loss_P: [ 3.22  2.32  1.58  1.79  1.78  1.15  1.01 12.85]\n",
      "Loss_Q: [1.56 1.21 1.71 1.6  1.14 0.88 0.   8.11] Loss_P: [ 3.15  2.29  1.64  1.77  1.75  1.13  1.   12.73]\n",
      "Loss_Q: [1.6  1.14 1.72 1.6  1.15 0.84 0.   8.04] Loss_P: [ 3.25  2.23  1.61  1.83  1.72  1.15  1.   12.79]\n",
      "Loss_Q: [1.66 1.19 1.71 1.59 1.15 0.83 0.   8.13] Loss_P: [ 3.19  2.35  1.62  1.86  1.79  1.18  1.02 13.01]\n",
      "Loss_Q: [1.58 1.16 1.69 1.58 1.18 0.86 0.   8.05] Loss_P: [ 3.15  2.34  1.58  1.83  1.76  1.18  1.01 12.84]\n",
      "Loss_Q: [1.67 1.13 1.72 1.55 1.16 0.87 0.   8.11] Loss_P: [ 3.17  2.33  1.59  1.77  1.77  1.19  0.98 12.79]\n",
      "Loss_Q: [1.7  1.12 1.67 1.54 1.14 0.84 0.   8.02] Loss_P: [ 3.17  2.39  1.58  1.78  1.7   1.21  0.97 12.8 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.68 1.16 1.7  1.57 1.13 0.84 0.   8.09] Loss_P: [ 3.15  2.42  1.58  1.75  1.74  1.12  0.92 12.7 ]\n",
      "Loss_Q: [1.67 1.13 1.68 1.57 1.09 0.85 0.   7.99] Loss_P: [ 3.15  2.37  1.61  1.75  1.72  1.09  0.95 12.64]\n",
      "Loss_Q: [1.7  1.13 1.6  1.6  1.04 0.84 0.   7.91] Loss_P: [ 3.15  2.37  1.6   1.76  1.77  1.1   1.02 12.76]\n",
      "Loss_Q: [1.68 1.18 1.66 1.58 1.13 0.85 0.   8.1 ] Loss_P: [ 3.13  2.38  1.62  1.77  1.76  1.14  1.   12.8 ]\n",
      "Loss_Q: [1.64 1.11 1.62 1.57 1.12 0.85 0.   7.91] Loss_P: [ 3.16  2.39  1.58  1.73  1.75  1.09  0.99 12.69]\n",
      "Loss_Q: [1.69 1.16 1.63 1.58 1.12 0.87 0.   8.06] Loss_P: [ 3.18  2.38  1.59  1.65  1.75  1.11  1.02 12.68]\n",
      "Loss_Q: [1.69 1.14 1.56 1.59 1.1  0.87 0.   7.95] Loss_P: [ 3.15  2.33  1.56  1.69  1.75  1.13  1.   12.63]\n",
      "Loss_Q: [1.73 1.18 1.57 1.59 1.12 0.9  0.   8.08] Loss_P: [ 3.16  2.4   1.57  1.65  1.8   1.11  1.01 12.7 ]\n",
      "Loss_Q: [1.69 1.16 1.56 1.6  1.05 0.86 0.   7.93] Loss_P: [ 3.16  2.37  1.6   1.61  1.79  1.11  0.99 12.63]\n",
      "Loss_Q: [1.68 1.14 1.58 1.63 1.09 0.86 0.   7.98] Loss_P: [ 3.13  2.4   1.66  1.64  1.81  1.09  0.98 12.73]\n",
      "Loss_Q: [1.66 1.17 1.58 1.63 1.1  0.89 0.   8.03] Loss_P: [ 3.17  2.35  1.6   1.68  1.8   1.1   1.03 12.74]\n",
      "Loss_Q: [1.78 1.17 1.66 1.62 1.15 0.88 0.   8.26] Loss_P: [ 3.11  2.38  1.63  1.68  1.81  1.14  1.04 12.8 ]\n",
      "Loss_Q: [1.71 1.2  1.61 1.63 1.09 0.9  0.   8.14] Loss_P: [ 3.13  2.49  1.66  1.78  1.8   1.11  1.05 13.01]\n",
      "Loss_Q: [1.71 1.15 1.57 1.6  1.1  0.91 0.   8.03] Loss_P: [ 3.13  2.43  1.62  1.73  1.79  1.13  1.06 12.88]\n",
      "Loss_Q: [1.75 1.15 1.65 1.61 1.11 0.94 0.   8.21] Loss_P: [ 3.15  2.45  1.61  1.75  1.76  1.1   1.07 12.89]\n",
      "Loss_Q: [1.79 1.2  1.66 1.59 1.1  0.95 0.   8.29] Loss_P: [ 3.14  2.42  1.6   1.72  1.76  1.09  1.1  12.84]\n",
      "Loss_Q: [1.7  1.17 1.64 1.58 1.08 0.95 0.   8.11] Loss_P: [ 3.12  2.46  1.65  1.78  1.74  1.09  1.09 12.93]\n",
      "Loss_Q: [1.76 1.17 1.61 1.57 1.11 0.95 0.   8.18] Loss_P: [ 3.15  2.41  1.61  1.75  1.73  1.14  1.09 12.87]\n",
      "Loss_Q: [1.7  1.24 1.64 1.59 1.08 0.94 0.   8.18] Loss_P: [ 3.2   2.39  1.65  1.69  1.72  1.1   1.12 12.87]\n",
      "Loss_Q: [1.66 1.17 1.65 1.59 1.09 0.96 0.   8.12] Loss_P: [ 3.15  2.42  1.62  1.74  1.76  1.09  1.12 12.92]\n",
      "Loss_Q: [1.77 1.17 1.65 1.6  1.1  0.98 0.   8.26] Loss_P: [ 3.14  2.46  1.59  1.77  1.72  1.1   1.11 12.89]\n",
      "Loss_Q: [1.73 1.16 1.6  1.54 1.02 0.96 0.   8.  ] Loss_P: [ 3.19  2.45  1.61  1.69  1.68  1.04  1.11 12.77]\n",
      "Loss_Q: [1.73 1.2  1.68 1.53 1.08 0.98 0.   8.2 ] Loss_P: [ 3.15  2.44  1.62  1.8   1.7   1.06  1.09 12.86]\n",
      "Loss_Q: [1.75 1.16 1.7  1.54 1.09 0.98 0.   8.22] Loss_P: [ 3.13  2.38  1.61  1.79  1.72  1.08  1.1  12.81]\n",
      "Loss_Q: [1.73 1.17 1.75 1.52 1.09 0.93 0.   8.2 ] Loss_P: [ 3.14  2.41  1.64  1.88  1.68  1.08  1.1  12.93]\n",
      "Loss_Q: [1.74 1.18 1.77 1.5  1.08 0.95 0.   8.2 ] Loss_P: [ 3.13  2.41  1.6   1.84  1.69  1.08  1.09 12.85]\n",
      "Loss_Q: [1.66 1.2  1.72 1.57 1.11 0.96 0.   8.21] Loss_P: [ 3.14  2.39  1.61  1.83  1.71  1.06  1.08 12.82]\n",
      "Loss_Q: [1.73 1.11 1.77 1.53 1.1  0.96 0.   8.2 ] Loss_P: [ 3.16  2.45  1.59  1.86  1.72  1.11  1.1  12.99]\n",
      "Loss_Q: [1.72 1.19 1.72 1.53 1.09 0.94 0.   8.19] Loss_P: [ 3.13  2.43  1.54  1.85  1.7   1.06  1.09 12.8 ]\n",
      "Loss_Q: [1.72 1.15 1.72 1.54 1.04 0.95 0.   8.13] Loss_P: [ 3.14  2.44  1.59  1.87  1.71  1.07  1.08 12.89]\n",
      "Loss_Q: [1.74 1.08 1.69 1.5  1.02 0.95 0.   7.98] Loss_P: [ 3.17  2.39  1.58  1.84  1.69  1.06  1.08 12.79]\n",
      "Loss_Q: [1.69 1.08 1.77 1.55 1.05 0.96 0.   8.09] Loss_P: [ 3.09  2.4   1.56  1.85  1.73  1.01  1.1  12.73]\n",
      "Loss_Q: [1.71 1.12 1.7  1.58 1.04 0.99 0.   8.14] Loss_P: [ 3.17  2.37  1.53  1.78  1.75  1.03  1.1  12.75]\n",
      "Loss_Q: [1.68 1.21 1.78 1.59 1.04 0.97 0.   8.26] Loss_P: [ 3.13  2.37  1.58  1.82  1.78  1.03  1.13 12.84]\n",
      "Loss_Q: [1.66 1.18 1.68 1.59 0.97 0.99 0.   8.08] Loss_P: [ 3.17  2.3   1.57  1.76  1.74  1.04  1.13 12.71]\n",
      "Loss_Q: [1.72 1.12 1.72 1.61 0.99 0.99 0.   8.15] Loss_P: [ 3.15  2.37  1.59  1.79  1.75  1.01  1.14 12.8 ]\n",
      "Loss_Q: [1.68 1.17 1.68 1.59 0.99 0.99 0.   8.1 ] Loss_P: [ 3.18  2.42  1.62  1.76  1.71  0.96  1.13 12.78]\n",
      "Loss_Q: [1.7  1.17 1.69 1.58 0.94 0.98 0.   8.07] Loss_P: [ 3.16  2.4   1.63  1.78  1.73  0.98  1.12 12.8 ]\n",
      "Loss_Q: [1.68 1.16 1.7  1.61 0.97 0.97 0.   8.09] Loss_P: [ 3.19  2.36  1.59  1.79  1.75  0.97  1.12 12.76]\n",
      "Loss_Q: [1.64 1.17 1.71 1.61 0.96 0.98 0.   8.07] Loss_P: [ 3.19  2.29  1.59  1.82  1.77  0.98  1.12 12.75]\n",
      "Loss_Q: [1.64 1.15 1.72 1.55 0.9  0.97 0.   7.92] Loss_P: [ 3.12  2.33  1.59  1.73  1.73  0.96  1.1  12.55]\n",
      "Loss_Q: [1.68 1.14 1.68 1.54 0.92 0.97 0.   7.93] Loss_P: [ 3.12  2.4   1.58  1.87  1.72  0.96  1.12 12.77]\n",
      "Loss_Q: [1.63 1.05 1.64 1.56 0.95 0.96 0.   7.79] Loss_P: [ 3.12  2.38  1.55  1.79  1.72  1.    1.12 12.69]\n",
      "Loss_Q: [1.74 1.17 1.76 1.53 1.01 0.97 0.   8.19] Loss_P: [ 3.1   2.48  1.6   1.8   1.71  1.04  1.11 12.84]\n",
      "Loss_Q: [1.74 1.1  1.74 1.51 1.04 0.96 0.   8.08] Loss_P: [ 3.15  2.44  1.56  1.79  1.66  1.04  1.09 12.72]\n",
      "Loss_Q: [1.73 1.13 1.74 1.51 1.02 0.94 0.   8.06] Loss_P: [ 3.14  2.44  1.61  1.85  1.69  1.05  1.07 12.84]\n",
      "Loss_Q: [1.74 1.11 1.8  1.55 1.04 0.96 0.   8.2 ] Loss_P: [ 3.18  2.45  1.54  1.83  1.75  1.05  1.08 12.87]\n",
      "Loss_Q: [1.69 1.13 1.76 1.54 1.03 0.94 0.   8.09] Loss_P: [ 3.11  2.43  1.61  1.81  1.7   1.04  1.06 12.77]\n",
      "Loss_Q: [1.73 1.17 1.79 1.51 1.03 0.95 0.   8.17] Loss_P: [ 3.15  2.42  1.56  1.92  1.73  1.06  1.07 12.91]\n",
      "Loss_Q: [1.79 1.13 1.8  1.57 1.04 0.96 0.   8.29] Loss_P: [ 3.1   2.51  1.58  1.9   1.68  1.07  1.08 12.93]\n",
      "Loss_Q: [1.8  1.16 1.83 1.61 1.03 0.95 0.   8.38] Loss_P: [ 3.12  2.52  1.58  1.9   1.78  1.08  1.08 13.06]\n",
      "Loss_Q: [1.79 1.13 1.79 1.59 1.   0.99 0.   8.28] Loss_P: [ 3.15  2.5   1.6   1.87  1.79  1.05  1.11 13.07]\n",
      "Loss_Q: [1.73 1.18 1.77 1.64 1.01 0.95 0.   8.28] Loss_P: [ 3.12  2.43  1.56  1.86  1.77  1.05  1.11 12.89]\n",
      "Loss_Q: [1.65 1.13 1.81 1.64 0.99 1.01 0.   8.22] Loss_P: [ 3.14  2.36  1.62  1.91  1.8   1.01  1.1  12.95]\n",
      "Loss_Q: [1.65 1.14 1.84 1.66 1.01 0.99 0.   8.29] Loss_P: [ 3.11  2.4   1.59  2.01  1.88  1.06  1.12 13.16]\n",
      "Loss_Q: [1.61 1.12 1.87 1.62 1.   0.97 0.   8.19] Loss_P: [ 3.19  2.41  1.62  1.98  1.81  1.05  1.11 13.17]\n",
      "Loss_Q: [1.66 1.12 1.84 1.6  0.99 0.99 0.   8.2 ] Loss_P: [ 3.14  2.39  1.62  1.97  1.83  1.04  1.1  13.09]\n",
      "Loss_Q: [1.72 1.12 1.92 1.6  1.03 0.96 0.   8.35] Loss_P: [ 3.15  2.4   1.59  1.99  1.8   1.06  1.09 13.08]\n",
      "Loss_Q: [1.68 1.14 1.93 1.63 1.04 0.96 0.   8.38] Loss_P: [ 3.16  2.34  1.6   2.    1.79  1.08  1.07 13.04]\n",
      "Loss_Q: [1.71 1.2  1.93 1.66 1.07 0.95 0.   8.52] Loss_P: [ 3.19  2.3   1.63  2.01  1.85  1.02  1.06 13.07]\n",
      "Loss_Q: [1.67 1.17 1.88 1.67 1.05 0.95 0.   8.39] Loss_P: [ 3.19  2.37  1.64  1.96  1.85  1.06  1.07 13.14]\n",
      "Loss_Q: [1.62 1.19 1.89 1.62 1.02 0.97 0.   8.3 ] Loss_P: [ 3.17  2.33  1.62  1.96  1.83  1.05  1.09 13.05]\n",
      "Loss_Q: [1.71 1.2  1.89 1.64 1.03 0.96 0.   8.43] Loss_P: [ 3.19  2.31  1.6   1.93  1.8   1.1   1.07 13.  ]\n",
      "Loss_Q: [1.66 1.11 1.85 1.6  1.08 0.95 0.   8.25] Loss_P: [ 3.2   2.34  1.66  1.91  1.84  1.08  1.08 13.12]\n",
      "Loss_Q: [1.64 1.08 1.83 1.53 1.04 0.95 0.   8.08] Loss_P: [ 3.16  2.36  1.61  1.94  1.77  1.1   1.1  13.03]\n",
      "Loss_Q: [1.62 1.16 1.86 1.57 1.06 0.96 0.   8.23] Loss_P: [ 3.2   2.33  1.6   1.89  1.76  1.06  1.09 12.94]\n",
      "Loss_Q: [1.62 1.14 1.82 1.56 1.09 0.96 0.   8.19] Loss_P: [ 3.15  2.32  1.62  1.89  1.77  1.09  1.11 12.95]\n",
      "Loss_Q: [1.68 1.1  1.85 1.61 1.1  0.96 0.   8.3 ] Loss_P: [ 3.18  2.28  1.59  1.93  1.79  1.11  1.11 12.99]\n",
      "Loss_Q: [1.68 1.11 1.84 1.57 1.07 0.99 0.   8.25] Loss_P: [ 3.16  2.39  1.59  1.88  1.79  1.07  1.12 13.  ]\n",
      "Loss_Q: [1.64 1.12 1.81 1.61 1.08 0.98 0.   8.24] Loss_P: [ 3.19  2.29  1.57  1.87  1.75  1.05  1.1  12.82]\n",
      "Loss_Q: [1.65 1.13 1.82 1.6  1.09 0.99 0.   8.27] Loss_P: [ 3.12  2.38  1.57  1.92  1.79  1.09  1.13 13.01]\n",
      "Loss_Q: [1.6  1.13 1.83 1.58 1.08 0.95 0.   8.17] Loss_P: [ 3.13  2.38  1.54  1.92  1.79  1.11  1.11 12.99]\n",
      "Loss_Q: [1.68 1.08 1.87 1.58 1.07 0.97 0.   8.25] Loss_P: [ 3.16  2.33  1.57  1.91  1.77  1.09  1.1  12.93]\n",
      "Loss_Q: [1.6  1.16 1.88 1.59 1.09 0.93 0.   8.25] Loss_P: [ 3.17  2.3   1.61  1.92  1.81  1.09  1.1  12.99]\n",
      "Loss_Q: [1.59 1.13 1.87 1.59 1.08 0.95 0.   8.22] Loss_P: [ 3.11  2.31  1.57  1.96  1.81  1.07  1.07 12.9 ]\n",
      "Loss_Q: [1.58 1.15 1.91 1.59 1.09 0.96 0.   8.28] Loss_P: [ 3.13  2.31  1.66  2.    1.79  1.08  1.06 13.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_Q: [1.63 1.21 1.92 1.61 1.11 0.92 0.   8.39] Loss_P: [ 3.19  2.26  1.65  1.97  1.83  1.07  1.06 13.03]\n",
      "Loss_Q: [1.62 1.14 1.89 1.56 1.06 0.89 0.   8.17] Loss_P: [ 3.14  2.3   1.64  1.95  1.8   1.12  1.04 12.98]\n",
      "Loss_Q: [1.68 1.13 1.87 1.54 1.12 0.92 0.   8.25] Loss_P: [ 3.15  2.34  1.64  1.95  1.8   1.12  1.05 13.06]\n",
      "Loss_Q: [1.65 1.15 1.87 1.63 1.13 0.92 0.   8.35] Loss_P: [ 3.16  2.37  1.6   1.93  1.79  1.1   1.07 13.01]\n",
      "Loss_Q: [1.71 1.19 1.83 1.59 1.14 0.94 0.   8.39] Loss_P: [ 3.12  2.39  1.61  1.97  1.8   1.11  1.05 13.05]\n",
      "Loss_Q: [1.79 1.13 1.85 1.52 1.05 0.92 0.   8.25] Loss_P: [ 3.15  2.42  1.65  1.9   1.75  1.1   1.05 13.02]\n",
      "Loss_Q: [1.73 1.17 1.86 1.57 1.14 0.94 0.   8.42] Loss_P: [ 3.16  2.35  1.6   1.95  1.75  1.11  1.07 12.99]\n",
      "Loss_Q: [1.74 1.12 1.84 1.6  1.1  0.91 0.   8.32] Loss_P: [ 3.14  2.36  1.64  1.98  1.77  1.09  1.02 13.01]\n",
      "Loss_Q: [1.66 1.24 1.9  1.59 1.15 0.92 0.   8.46] Loss_P: [ 3.14  2.37  1.62  1.98  1.8   1.1   1.06 13.07]\n",
      "Loss_Q: [1.68 1.23 1.87 1.62 1.14 0.88 0.   8.41] Loss_P: [ 3.16  2.35  1.62  1.97  1.81  1.14  1.05 13.11]\n",
      "Loss_Q: [1.74 1.13 1.89 1.63 1.14 0.91 0.   8.44] Loss_P: [ 3.24  2.39  1.61  1.97  1.77  1.15  1.01 13.14]\n",
      "Loss_Q: [1.7  1.19 1.86 1.61 1.11 0.87 0.   8.35] Loss_P: [ 3.16  2.37  1.6   1.99  1.83  1.15  1.03 13.11]\n"
     ]
    }
   ],
   "source": [
    "### Training without constraints, accuracy 0.82\n",
    "\n",
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4f1a693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_data,counts_data = np.unique(dataset, axis=1, return_counts = True)\n",
    "counts_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f81d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 8456 Loss_Q: [1.93 0.78 1.76 1.37 0.83 0.66 0.   7.34] Loss_P: [ 2.9   2.47  1.12  1.84  1.55  0.86  0.71 11.45]\n",
      "dataset size: 8519 Loss_Q: [1.95 0.8  1.66 1.34 0.95 0.66 0.   7.37] Loss_P: [ 2.91  2.47  1.12  1.76  1.49  0.99  0.72 11.45]\n",
      "dataset size: 8568 Loss_Q: [1.9  0.81 1.67 1.38 0.92 0.67 0.   7.35] Loss_P: [ 2.94  2.39  1.1   1.78  1.57  0.95  0.72 11.45]\n",
      "dataset size: 8621 Loss_Q: [1.92 0.77 1.67 1.39 0.89 0.7  0.   7.34] Loss_P: [ 2.92  2.43  1.07  1.8   1.54  0.89  0.75 11.41]\n",
      "dataset size: 8676 Loss_Q: [1.84 0.77 1.65 1.4  0.89 0.69 0.   7.24] Loss_P: [ 2.94  2.36  1.08  1.8   1.56  0.9   0.76 11.4 ]\n",
      "dataset size: 8742 Loss_Q: [1.7  0.77 1.56 1.42 0.81 0.69 0.   6.96] Loss_P: [ 2.95  2.23  1.05  1.68  1.58  0.82  0.76 11.08]\n",
      "dataset size: 8786 Loss_Q: [1.67 0.79 1.5  1.49 0.81 0.71 0.   6.96] Loss_P: [ 2.97  2.22  1.06  1.62  1.62  0.8   0.78 11.07]\n",
      "dataset size: 8846 Loss_Q: [1.71 0.83 1.53 1.53 0.84 0.69 0.   7.12] Loss_P: [ 2.96  2.21  1.12  1.67  1.66  0.83  0.77 11.24]\n",
      "dataset size: 8902 Loss_Q: [1.63 0.81 1.52 1.58 0.79 0.63 0.   6.97] Loss_P: [ 2.98  2.11  1.09  1.68  1.72  0.8   0.69 11.06]\n",
      "dataset size: 8960 Loss_Q: [1.58 0.8  1.59 1.45 0.78 0.63 0.   6.83] Loss_P: [ 2.99  2.07  1.1   1.74  1.56  0.78  0.7  10.93]\n"
     ]
    }
   ],
   "source": [
    "### Training with modified data distribution as sampled salience, accuracy 0.97\n",
    "### All datapoints are reserved with positive counts (at least 1); the distribution is modified with a controled dataset scale\n",
    "### \n",
    "\n",
    "lr = 0.01\n",
    "epoch = 10\n",
    "for e in range (epoch):\n",
    "    index = np.random.permutation(n_data)\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    for i in range(n_data):\n",
    "        d0 = dataset[:,index[i]:index[i]+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        gen = Alpha_P['z0']\n",
    "        for j in range(counts_data.size):\n",
    "            if np.array_equal(gen, values_data[:,j:j+1]):\n",
    "                dataset = np.append(dataset,gen,axis=1)\n",
    "                break\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    \n",
    "    values,counts = np.unique(dataset,axis=1,return_counts = True)\n",
    "    dataset = np.repeat(values, (counts/2+0.5).astype(int), axis=1)\n",
    "    n_data = dataset.shape[1]\n",
    "    \n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('dataset size: ' + str(n_data),'Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a6247764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 9906 dataset support: 438 Loss_Q: [1.61 0.52 1.15 0.85 0.67 0.93 0.   5.73] Loss_P: [2.64 1.89 0.83 1.35 0.86 0.68 0.98 9.24]\n",
      "dataset size: 9911 dataset support: 414 Loss_Q: [1.65 0.53 1.17 0.93 0.58 0.95 0.   5.8 ] Loss_P: [2.59 1.94 0.83 1.34 0.95 0.6  0.98 9.24]\n",
      "dataset size: 9935 dataset support: 424 Loss_Q: [1.56 0.54 1.06 0.92 0.57 0.95 0.   5.6 ] Loss_P: [2.61 1.83 0.91 1.16 0.95 0.59 0.98 9.03]\n",
      "dataset size: 9930 dataset support: 415 Loss_Q: [1.58 0.54 0.96 0.79 0.57 0.9  0.   5.33] Loss_P: [2.59 1.83 0.9  1.03 0.81 0.59 0.93 8.68]\n",
      "dataset size: 9940 dataset support: 412 Loss_Q: [1.49 0.54 0.86 0.85 0.52 0.92 0.   5.19] Loss_P: [2.59 1.8  0.87 0.93 0.88 0.51 0.95 8.54]\n",
      "dataset size: 9927 dataset support: 421 Loss_Q: [1.49 0.55 0.88 0.85 0.54 0.95 0.   5.26] Loss_P: [2.61 1.81 0.89 0.95 0.86 0.53 0.99 8.65]\n",
      "dataset size: 9931 dataset support: 418 Loss_Q: [1.63 0.58 0.98 0.84 0.48 0.95 0.   5.45] Loss_P: [2.58 1.95 0.91 1.07 0.88 0.47 0.99 8.85]\n",
      "dataset size: 9934 dataset support: 440 Loss_Q: [1.66 0.6  0.99 0.82 0.51 0.93 0.   5.51] Loss_P: [2.57 2.   0.95 1.09 0.85 0.52 0.97 8.94]\n",
      "dataset size: 9903 dataset support: 432 Loss_Q: [1.77 0.6  1.07 0.93 0.56 0.98 0.   5.91] Loss_P: [2.54 2.1  0.96 1.16 0.97 0.59 1.02 9.34]\n",
      "dataset size: 9929 dataset support: 426 Loss_Q: [1.77 0.61 1.19 0.96 0.61 0.97 0.   6.12] Loss_P: [2.49 2.1  0.95 1.27 1.03 0.63 1.01 9.48]\n",
      "dataset size: 9934 dataset support: 413 Loss_Q: [1.52 0.56 1.14 0.91 0.56 0.97 0.   5.66] Loss_P: [2.49 1.81 0.92 1.2  0.95 0.59 1.   8.95]\n",
      "dataset size: 9930 dataset support: 419 Loss_Q: [1.46 0.55 1.11 0.95 0.43 0.96 0.   5.45] Loss_P: [2.49 1.76 0.91 1.17 1.01 0.45 0.99 8.78]\n",
      "dataset size: 9947 dataset support: 407 Loss_Q: [1.31 0.56 1.14 0.85 0.43 0.9  0.   5.18] Loss_P: [2.49 1.6  0.91 1.21 0.94 0.43 0.93 8.51]\n",
      "dataset size: 9927 dataset support: 406 Loss_Q: [1.47 0.57 1.15 0.92 0.38 0.84 0.   5.34] Loss_P: [2.48 1.74 0.94 1.21 1.02 0.39 0.87 8.64]\n",
      "dataset size: 9918 dataset support: 432 Loss_Q: [1.61 0.59 1.16 0.97 0.37 0.86 0.   5.56] Loss_P: [2.52 1.86 0.97 1.21 1.07 0.38 0.89 8.9 ]\n",
      "dataset size: 9918 dataset support: 417 Loss_Q: [1.46 0.53 1.13 0.96 0.4  0.78 0.   5.27] Loss_P: [2.54 1.72 0.92 1.21 1.03 0.4  0.8  8.63]\n",
      "dataset size: 9938 dataset support: 407 Loss_Q: [1.58 0.53 1.18 0.9  0.39 0.74 0.   5.32] Loss_P: [2.52 1.85 0.9  1.23 0.95 0.39 0.76 8.61]\n",
      "dataset size: 9914 dataset support: 418 Loss_Q: [1.41 0.57 1.22 0.93 0.31 0.78 0.   5.22] Loss_P: [2.49 1.66 0.96 1.29 0.99 0.32 0.82 8.53]\n",
      "dataset size: 9912 dataset support: 413 Loss_Q: [1.35 0.55 1.23 1.   0.26 0.71 0.   5.1 ] Loss_P: [2.49 1.6  0.95 1.29 1.08 0.26 0.74 8.41]\n",
      "dataset size: 9941 dataset support: 411 Loss_Q: [1.4  0.55 1.16 1.06 0.24 0.76 0.   5.18] Loss_P: [2.5  1.66 0.93 1.23 1.14 0.24 0.79 8.49]\n",
      "dataset size: 9940 dataset support: 407 Loss_Q: [1.34 0.57 1.13 0.96 0.24 0.78 0.   5.02] Loss_P: [2.56 1.62 0.9  1.2  1.06 0.24 0.8  8.37]\n",
      "dataset size: 9931 dataset support: 397 Loss_Q: [1.39 0.51 1.09 0.83 0.23 0.74 0.   4.8 ] Loss_P: [2.54 1.64 0.88 1.14 0.9  0.23 0.76 8.09]\n",
      "dataset size: 9931 dataset support: 421 Loss_Q: [1.46 0.51 1.17 0.85 0.23 0.76 0.   4.98] Loss_P: [2.54 1.73 0.9  1.22 0.9  0.23 0.78 8.29]\n",
      "dataset size: 9941 dataset support: 411 Loss_Q: [1.47 0.55 1.25 0.84 0.24 0.78 0.   5.14] Loss_P: [2.54 1.77 0.89 1.32 0.92 0.24 0.8  8.47]\n",
      "dataset size: 9937 dataset support: 401 Loss_Q: [1.53 0.58 1.32 0.84 0.25 0.81 0.   5.33] Loss_P: [2.46 1.82 0.93 1.4  0.95 0.26 0.84 8.64]\n",
      "dataset size: 9948 dataset support: 403 Loss_Q: [1.36 0.6  1.34 0.82 0.27 0.82 0.   5.22] Loss_P: [2.47 1.63 0.92 1.4  0.93 0.28 0.85 8.49]\n",
      "dataset size: 9959 dataset support: 404 Loss_Q: [1.56 0.57 1.32 0.94 0.27 0.8  0.   5.46] Loss_P: [2.5  1.83 0.92 1.38 1.06 0.28 0.82 8.78]\n",
      "dataset size: 9955 dataset support: 395 Loss_Q: [1.7  0.59 1.12 0.98 0.28 0.72 0.   5.4 ] Loss_P: [2.45 1.99 0.92 1.19 1.09 0.3  0.74 8.69]\n",
      "dataset size: 9954 dataset support: 397 Loss_Q: [1.52 0.64 1.08 0.9  0.29 0.73 0.   5.16] Loss_P: [2.44 1.82 0.98 1.15 1.02 0.3  0.75 8.47]\n",
      "dataset size: 9946 dataset support: 384 Loss_Q: [1.57 0.63 1.15 0.82 0.28 0.75 0.   5.19] Loss_P: [2.4  1.84 0.98 1.23 0.92 0.29 0.77 8.43]\n",
      "dataset size: 9942 dataset support: 379 Loss_Q: [1.55 0.6  1.37 0.78 0.25 0.74 0.   5.29] Loss_P: [2.4  1.81 0.96 1.45 0.86 0.25 0.77 8.49]\n",
      "dataset size: 9951 dataset support: 387 Loss_Q: [1.46 0.57 1.34 0.69 0.25 0.85 0.   5.16] Loss_P: [2.44 1.7  0.93 1.4  0.75 0.25 0.87 8.35]\n",
      "dataset size: 9949 dataset support: 395 Loss_Q: [1.44 0.54 1.34 0.66 0.25 0.95 0.   5.18] Loss_P: [2.39 1.7  0.88 1.41 0.72 0.26 0.97 8.33]\n",
      "dataset size: 9954 dataset support: 382 Loss_Q: [1.41 0.52 1.35 0.62 0.25 0.99 0.   5.13] Loss_P: [2.38 1.66 0.89 1.43 0.66 0.25 1.01 8.28]\n",
      "dataset size: 9956 dataset support: 377 Loss_Q: [1.41 0.52 1.29 0.62 0.24 0.96 0.   5.04] Loss_P: [2.34 1.64 0.88 1.37 0.67 0.24 0.99 8.12]\n",
      "dataset size: 9959 dataset support: 368 Loss_Q: [1.32 0.51 1.43 0.6  0.28 0.95 0.   5.09] Loss_P: [2.27 1.59 0.85 1.53 0.63 0.28 0.96 8.11]\n",
      "dataset size: 9952 dataset support: 347 Loss_Q: [1.34 0.47 1.43 0.64 0.31 0.98 0.   5.16] Loss_P: [2.16 1.58 0.79 1.54 0.65 0.31 1.   8.04]\n",
      "dataset size: 9954 dataset support: 337 Loss_Q: [1.12 0.49 1.4  0.64 0.26 0.99 0.   4.91] Loss_P: [2.15 1.38 0.83 1.5  0.67 0.26 1.01 7.79]\n",
      "dataset size: 9954 dataset support: 345 Loss_Q: [1.17 0.5  1.22 0.62 0.24 0.98 0.   4.74] Loss_P: [2.23 1.44 0.84 1.3  0.66 0.26 1.01 7.74]\n",
      "dataset size: 9950 dataset support: 328 Loss_Q: [1.22 0.49 1.32 0.63 0.27 0.98 0.   4.9 ] Loss_P: [2.18 1.48 0.81 1.44 0.64 0.27 1.01 7.83]\n",
      "dataset size: 9968 dataset support: 337 Loss_Q: [1.35 0.47 1.38 0.61 0.3  0.98 0.   5.09] Loss_P: [2.2  1.6  0.79 1.54 0.63 0.29 1.01 8.06]\n",
      "dataset size: 9961 dataset support: 331 Loss_Q: [1.28 0.47 1.48 0.71 0.28 0.96 0.   5.19] Loss_P: [2.17 1.55 0.8  1.59 0.75 0.29 0.98 8.13]\n",
      "dataset size: 9957 dataset support: 347 Loss_Q: [1.49 0.49 1.66 0.84 0.31 0.92 0.   5.71] Loss_P: [2.13 1.75 0.77 1.78 0.91 0.3  0.94 8.59]\n",
      "dataset size: 9959 dataset support: 335 Loss_Q: [1.53 0.43 1.58 0.79 0.32 0.87 0.   5.52] Loss_P: [2.13 1.78 0.75 1.69 0.85 0.33 0.9  8.42]\n",
      "dataset size: 9959 dataset support: 335 Loss_Q: [1.63 0.42 1.59 0.68 0.27 0.81 0.   5.4 ] Loss_P: [2.11 1.89 0.74 1.68 0.74 0.27 0.83 8.27]\n",
      "dataset size: 9960 dataset support: 318 Loss_Q: [1.7  0.39 1.64 0.68 0.23 0.77 0.   5.42] Loss_P: [2.08 2.   0.72 1.73 0.77 0.23 0.79 8.32]\n",
      "dataset size: 9968 dataset support: 333 Loss_Q: [1.55 0.42 1.59 0.71 0.23 0.77 0.   5.27] Loss_P: [2.12 1.83 0.73 1.61 0.81 0.24 0.79 8.15]\n",
      "dataset size: 9973 dataset support: 343 Loss_Q: [1.71 0.43 1.55 0.73 0.23 0.82 0.   5.48] Loss_P: [2.11 2.   0.74 1.64 0.8  0.24 0.84 8.37]\n",
      "dataset size: 9949 dataset support: 346 Loss_Q: [1.69 0.41 1.48 0.72 0.25 0.8  0.   5.34] Loss_P: [2.14 1.99 0.74 1.58 0.78 0.25 0.82 8.31]\n",
      "dataset size: 9956 dataset support: 350 Loss_Q: [1.75 0.42 1.48 0.7  0.26 0.77 0.   5.38] Loss_P: [2.08 2.04 0.75 1.58 0.78 0.26 0.79 8.27]\n",
      "dataset size: 9976 dataset support: 338 Loss_Q: [1.68 0.38 1.36 0.63 0.26 0.78 0.   5.09] Loss_P: [2.08 1.97 0.73 1.44 0.72 0.26 0.79 7.99]\n",
      "dataset size: 9957 dataset support: 345 Loss_Q: [1.74 0.34 1.42 0.57 0.27 0.76 0.   5.1 ] Loss_P: [2.07 2.09 0.7  1.49 0.63 0.27 0.77 8.03]\n",
      "dataset size: 9965 dataset support: 336 Loss_Q: [1.85 0.38 1.56 0.7  0.34 0.67 0.   5.49] Loss_P: [2.07 2.18 0.71 1.66 0.75 0.34 0.69 8.41]\n",
      "dataset size: 9964 dataset support: 309 Loss_Q: [1.82 0.38 1.4  0.67 0.3  0.59 0.   5.17] Loss_P: [2.02 2.18 0.7  1.46 0.74 0.3  0.6  8.01]\n",
      "dataset size: 9967 dataset support: 305 Loss_Q: [1.86 0.38 1.35 0.56 0.29 0.58 0.   5.01] Loss_P: [2.02 2.21 0.69 1.39 0.62 0.3  0.59 7.81]\n",
      "dataset size: 9975 dataset support: 312 Loss_Q: [1.81 0.43 1.35 0.6  0.34 0.63 0.   5.17] Loss_P: [1.99 2.19 0.7  1.39 0.69 0.34 0.65 7.94]\n",
      "dataset size: 9970 dataset support: 319 Loss_Q: [1.83 0.44 1.27 0.57 0.37 0.65 0.   5.14] Loss_P: [2.07 2.16 0.72 1.3  0.65 0.37 0.67 7.95]\n",
      "dataset size: 9977 dataset support: 313 Loss_Q: [1.83 0.44 1.17 0.58 0.4  0.61 0.   5.04] Loss_P: [2.02 2.18 0.7  1.2  0.67 0.4  0.62 7.8 ]\n",
      "dataset size: 9978 dataset support: 311 Loss_Q: [1.71 0.44 1.17 0.56 0.35 0.64 0.   4.88] Loss_P: [2.04 1.99 0.72 1.23 0.66 0.35 0.66 7.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 9969 dataset support: 323 Loss_Q: [1.61 0.39 1.25 0.51 0.34 0.73 0.   4.82] Loss_P: [2.05 1.88 0.69 1.3  0.59 0.34 0.74 7.59]\n",
      "dataset size: 9968 dataset support: 337 Loss_Q: [1.57 0.41 1.1  0.51 0.28 0.74 0.   4.62] Loss_P: [2.06 1.9  0.73 1.16 0.57 0.29 0.75 7.46]\n",
      "dataset size: 9965 dataset support: 334 Loss_Q: [1.56 0.44 1.19 0.55 0.29 0.85 0.   4.88] Loss_P: [2.05 1.88 0.77 1.25 0.63 0.29 0.87 7.74]\n",
      "dataset size: 9977 dataset support: 333 Loss_Q: [1.51 0.42 1.33 0.63 0.31 0.89 0.   5.09] Loss_P: [2.03 1.84 0.75 1.38 0.76 0.32 0.91 7.98]\n",
      "dataset size: 9978 dataset support: 332 Loss_Q: [1.39 0.45 1.21 0.55 0.28 0.88 0.   4.75] Loss_P: [2.07 1.7  0.76 1.25 0.63 0.29 0.91 7.61]\n",
      "dataset size: 9963 dataset support: 339 Loss_Q: [1.46 0.5  1.25 0.55 0.26 0.92 0.   4.93] Loss_P: [2.08 1.8  0.77 1.3  0.67 0.26 0.94 7.82]\n",
      "dataset size: 9963 dataset support: 334 Loss_Q: [1.5  0.52 1.39 0.53 0.27 0.86 0.   5.08] Loss_P: [2.09 1.86 0.78 1.45 0.68 0.27 0.89 8.02]\n",
      "dataset size: 9979 dataset support: 328 Loss_Q: [1.69 0.45 1.24 0.5  0.29 0.88 0.   5.06] Loss_P: [2.09 2.09 0.75 1.28 0.62 0.3  0.9  8.03]\n",
      "dataset size: 9971 dataset support: 322 Loss_Q: [1.72 0.42 1.19 0.47 0.3  0.85 0.   4.95] Loss_P: [2.11 2.1  0.73 1.22 0.58 0.3  0.88 7.92]\n",
      "dataset size: 9975 dataset support: 337 Loss_Q: [1.8  0.42 1.03 0.47 0.27 0.83 0.   4.81] Loss_P: [2.11 2.23 0.7  1.06 0.58 0.27 0.85 7.79]\n",
      "dataset size: 9974 dataset support: 333 Loss_Q: [1.96 0.42 0.95 0.51 0.28 0.84 0.   4.95] Loss_P: [2.09 2.35 0.71 0.97 0.62 0.28 0.86 7.89]\n",
      "dataset size: 9968 dataset support: 327 Loss_Q: [2.01 0.42 0.98 0.47 0.31 0.82 0.   5.01] Loss_P: [2.09 2.32 0.74 1.03 0.58 0.32 0.83 7.91]\n",
      "dataset size: 9976 dataset support: 325 Loss_Q: [2.01 0.43 1.06 0.38 0.4  0.84 0.   5.13] Loss_P: [2.09 2.36 0.78 1.1  0.46 0.41 0.85 8.05]\n",
      "dataset size: 9969 dataset support: 330 Loss_Q: [1.84 0.42 1.   0.39 0.44 0.9  0.   5.  ] Loss_P: [2.08 2.21 0.73 1.04 0.47 0.45 0.91 7.89]\n",
      "dataset size: 9960 dataset support: 330 Loss_Q: [1.97 0.44 0.95 0.41 0.48 0.93 0.   5.19] Loss_P: [2.07 2.36 0.74 0.98 0.51 0.5  0.94 8.1 ]\n",
      "dataset size: 9974 dataset support: 332 Loss_Q: [1.79 0.48 0.94 0.41 0.41 0.87 0.   4.89] Loss_P: [2.05 2.2  0.79 0.98 0.52 0.42 0.87 7.83]\n",
      "dataset size: 9974 dataset support: 318 Loss_Q: [1.68 0.51 1.   0.42 0.39 0.82 0.   4.82] Loss_P: [2.08 2.03 0.82 1.04 0.53 0.39 0.83 7.72]\n",
      "dataset size: 9973 dataset support: 314 Loss_Q: [1.69 0.49 1.06 0.42 0.36 0.8  0.   4.81] Loss_P: [2.08 1.99 0.79 1.12 0.55 0.36 0.81 7.7 ]\n",
      "dataset size: 9975 dataset support: 313 Loss_Q: [1.51 0.44 1.03 0.44 0.47 0.77 0.   4.66] Loss_P: [2.07 1.78 0.76 1.09 0.59 0.48 0.79 7.55]\n",
      "dataset size: 9971 dataset support: 306 Loss_Q: [1.3  0.41 0.9  0.43 0.53 0.83 0.   4.4 ] Loss_P: [2.01 1.6  0.72 0.92 0.55 0.54 0.84 7.18]\n",
      "dataset size: 9969 dataset support: 314 Loss_Q: [1.21 0.38 0.9  0.35 0.52 0.84 0.   4.2 ] Loss_P: [2.   1.51 0.69 0.93 0.44 0.53 0.85 6.96]\n",
      "dataset size: 9976 dataset support: 305 Loss_Q: [1.17 0.45 1.04 0.38 0.46 0.93 0.   4.44] Loss_P: [2.03 1.49 0.73 1.08 0.51 0.46 0.94 7.23]\n",
      "dataset size: 9965 dataset support: 303 Loss_Q: [1.39 0.47 1.07 0.44 0.42 0.96 0.   4.74] Loss_P: [1.96 1.67 0.75 1.11 0.58 0.42 0.97 7.46]\n",
      "dataset size: 9974 dataset support: 282 Loss_Q: [1.55 0.49 1.04 0.48 0.41 0.98 0.   4.94] Loss_P: [1.97 1.79 0.75 1.1  0.62 0.4  1.   7.63]\n",
      "dataset size: 9976 dataset support: 297 Loss_Q: [1.3  0.49 1.14 0.5  0.37 0.98 0.   4.79] Loss_P: [1.96 1.56 0.78 1.21 0.68 0.38 0.99 7.55]\n",
      "dataset size: 9982 dataset support: 292 Loss_Q: [1.24 0.49 1.18 0.5  0.35 0.99 0.   4.75] Loss_P: [1.9  1.51 0.78 1.23 0.65 0.35 1.   7.42]\n",
      "dataset size: 9970 dataset support: 284 Loss_Q: [1.18 0.48 1.18 0.48 0.34 0.96 0.   4.63] Loss_P: [1.91 1.48 0.76 1.25 0.64 0.34 0.97 7.36]\n",
      "dataset size: 9980 dataset support: 304 Loss_Q: [1.19 0.45 1.27 0.46 0.39 0.91 0.   4.68] Loss_P: [1.96 1.46 0.78 1.33 0.62 0.4  0.92 7.46]\n",
      "dataset size: 9973 dataset support: 295 Loss_Q: [1.34 0.46 1.13 0.37 0.36 0.99 0.   4.65] Loss_P: [1.97 1.61 0.84 1.2  0.45 0.37 1.   7.44]\n",
      "dataset size: 9982 dataset support: 278 Loss_Q: [1.33 0.46 1.17 0.39 0.37 0.99 0.   4.71] Loss_P: [1.98 1.56 0.88 1.22 0.46 0.37 1.01 7.48]\n",
      "dataset size: 9983 dataset support: 293 Loss_Q: [1.27 0.45 1.15 0.44 0.35 0.98 0.   4.63] Loss_P: [1.99 1.51 0.82 1.22 0.55 0.34 1.   7.43]\n",
      "dataset size: 9978 dataset support: 283 Loss_Q: [1.28 0.43 1.11 0.51 0.35 0.97 0.   4.65] Loss_P: [1.94 1.53 0.74 1.22 0.67 0.34 0.98 7.44]\n",
      "dataset size: 9986 dataset support: 285 Loss_Q: [1.23 0.43 1.15 0.53 0.27 0.92 0.   4.54] Loss_P: [1.9  1.48 0.75 1.29 0.68 0.28 0.94 7.31]\n",
      "dataset size: 9984 dataset support: 290 Loss_Q: [1.23 0.44 1.19 0.53 0.3  0.85 0.   4.54] Loss_P: [1.91 1.48 0.77 1.31 0.65 0.3  0.88 7.31]\n",
      "dataset size: 9983 dataset support: 277 Loss_Q: [1.24 0.49 1.19 0.48 0.32 0.81 0.   4.52] Loss_P: [1.93 1.51 0.79 1.25 0.65 0.33 0.82 7.28]\n",
      "dataset size: 9976 dataset support: 271 Loss_Q: [1.37 0.48 1.27 0.44 0.27 0.84 0.   4.68] Loss_P: [1.93 1.62 0.81 1.31 0.62 0.27 0.86 7.41]\n",
      "dataset size: 9992 dataset support: 282 Loss_Q: [1.28 0.46 1.14 0.47 0.23 0.88 0.   4.46] Loss_P: [1.91 1.56 0.77 1.21 0.65 0.23 0.9  7.22]\n",
      "dataset size: 9981 dataset support: 282 Loss_Q: [1.08 0.4  1.06 0.46 0.22 0.89 0.   4.1 ] Loss_P: [1.86 1.32 0.73 1.14 0.64 0.23 0.91 6.83]\n",
      "dataset size: 9986 dataset support: 277 Loss_Q: [0.92 0.39 1.03 0.44 0.24 0.88 0.   3.91] Loss_P: [1.86 1.18 0.69 1.14 0.61 0.25 0.9  6.63]\n",
      "dataset size: 9984 dataset support: 274 Loss_Q: [0.93 0.42 1.02 0.42 0.27 0.78 0.   3.83] Loss_P: [1.92 1.21 0.7  1.09 0.58 0.28 0.8  6.58]\n",
      "dataset size: 9978 dataset support: 282 Loss_Q: [1.03 0.42 1.12 0.47 0.26 0.74 0.   4.03] Loss_P: [1.93 1.3  0.72 1.19 0.64 0.26 0.75 6.8 ]\n"
     ]
    }
   ],
   "source": [
    "### Training by sampled valid instances, discarding hardly sampled support in the original set\n",
    "### accuracy 0.998, with 282 valid support (out of 672), outlier 16/10000, basically excluded false generations\n",
    "\n",
    "lr = 0.008\n",
    "epoch = 100\n",
    "for e in range (epoch):\n",
    "    Loss_Q_total = np.zeros(n_layer)\n",
    "    Loss_P_total = np.zeros(n_layer)\n",
    "    \n",
    "    n_sample = 10000\n",
    "    generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "    valid_index = []\n",
    "    for i in range(n_sample):\n",
    "        for j in range(counts_data.size):\n",
    "            if np.array_equal(generation[:,i], values_data[:,j]):\n",
    "                valid_index.append(i)\n",
    "                break\n",
    "    generated_dataset = generation[:,valid_index]\n",
    "    n_data = generated_dataset.shape[1]\n",
    "    n_support = np.unique(generated_dataset,axis=1).shape[1]\n",
    "    \n",
    "    for i in range(n_data):\n",
    "        d0 = generated_dataset[:,i:i+1]\n",
    "        Alpha_Q = ut.wake_sample(n_dz,d0,value_set,Phi,activation_type,bias)\n",
    "        Theta,Loss_P = ut.sleep_update_delta(Theta,Alpha_Q,lr,n_dz,value_set,activation_type,bias)\n",
    "        Alpha_P = ut.sleep_sample(n_dz,value_set,Theta,activation_type,bias)\n",
    "        Phi,Loss_Q = ut.wake_update_delta(Phi,Alpha_P,lr,n_dz,value_set,activation_type,bias)\n",
    "        \n",
    "        Loss_Q_total += Loss_Q\n",
    "        Loss_P_total += Loss_P\n",
    "    Loss_Q_total = Loss_Q_total/n_data\n",
    "    Loss_P_total = Loss_P_total/n_data\n",
    "    print('dataset size: ' + str(n_data), 'dataset support: ' + str(n_support), 'Loss_Q: '+ str(np.around(Loss_Q_total,2)), 'Loss_P: '+ str(np.around(Loss_P_total,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f462414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10000\n",
    "generation = ut.generate(n_sample,n_dz,value_set,Theta,activation_type,bias)\n",
    "# distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,dataset)\n",
    "distribution,data_dist,statistics, MSE, ABS_Error = ut.metrics(generation,reordered_set,values_data) # for generated_dataset\n",
    "values_t, counts_t = np.unique(distribution, return_counts=True)\n",
    "values_d, counts_d  = np.unique(data_dist, return_counts=True)\n",
    "counts_t = counts_t/n_sample*n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "644d7519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22b49876250>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "x_lim = reordered_set.shape[1]\n",
    "n_ticks = 8\n",
    "xtick = np.arange(0,x_lim,int(x_lim/n_ticks/100+0.5)*100)\n",
    "xtick[np.argmin(np.abs(xtick - values_d.size))] = values_d.size\n",
    "xtick[-1] = x_lim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "ax.bar(values_d,counts_d,label = \"Ground Truth\")\n",
    "ax.bar(values_t,counts_t,label = \"Generation\")\n",
    "ax.set(xlim=(0, x_lim), xticks=xtick)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a0438f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent': 0.9981,\n",
       " 'FN': array([  0,   1,   2,   3,   5,  15,  16,  18,  26,  28,  30,  31,  32,\n",
       "         34,  35,  36,  37,  38,  41,  48,  50,  52,  53,  56,  57,  64,\n",
       "         66,  69,  70,  71,  72,  73,  74,  75,  77,  80,  81,  83,  84,\n",
       "         85,  89,  90,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
       "        102, 104, 106, 107, 109, 110, 112, 116, 117, 118, 120, 122, 123,\n",
       "        124, 125, 127, 131, 133, 135, 137, 141, 142, 150, 152, 154, 156,\n",
       "        158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
       "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184,\n",
       "        216, 220, 226, 228, 232, 233, 235, 237, 240, 241, 261, 269, 286,\n",
       "        290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
       "        303, 304, 305, 309, 311, 316, 317, 318, 320, 321, 322, 324, 326,\n",
       "        328, 330, 332, 333, 334, 335, 336, 338, 339, 341, 342, 343, 344,\n",
       "        345, 346, 347, 348, 357, 359, 361, 365, 367, 369, 373, 374, 375,\n",
       "        376, 377, 378, 380, 382, 383, 384, 385, 386, 387, 395, 398, 400,\n",
       "        402, 403, 404, 409, 410, 411, 412, 415, 416, 417, 418, 419, 420,\n",
       "        421, 427, 434, 435, 437, 441, 443, 444, 445, 447, 449, 450, 452,\n",
       "        453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "        466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "        479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "        492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 505,\n",
       "        506, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 520,\n",
       "        522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534,\n",
       "        535, 536, 538, 540, 542, 543, 544, 545, 546, 548, 549, 550, 551,\n",
       "        552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
       "        565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
       "        579, 581, 583, 584, 585, 586, 588, 589, 590, 591, 592, 593, 594,\n",
       "        595, 598, 600, 605, 607, 609, 611, 613, 614, 615, 616, 617, 618,\n",
       "        619, 620, 621, 622, 623, 624, 625, 626]),\n",
       " 'n_fn': 359,\n",
       " 'FP': array([[2463, 2699, 2712, 2870, 2911, 3135, 3191, 3372, 3550, 3556, 3823,\n",
       "         3835, 3988, 4012, 4057, 4058],\n",
       "        [   1,    3,    1,    1,    1,    2,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1]], dtype=int64),\n",
       " 'n_fp': 16}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9bd18712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.81643507"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2797d",
   "metadata": {},
   "source": [
    "percent: (original) 0.8146,0.8227, 0.8231; (salience) 0.9485 (after 45 epochs, datasize=4000), (+10 epochs)0.9534, 0.9648,0.9698, 0.9687,0.9727, 0.9728; (generated dataset) 0.9887,0.989,0.993,0.9981\n",
    "\n",
    "n_fn: 34,29,37; 72,80,83,81,86,80,88; 140,153,186,359\n",
    "\n",
    "n_fp: 1138,1122,1089; 410,371,301,251,275,231,237; 98,93,58,16\n",
    "\n",
    "MSE: 0.3040,0.2839,0.2824; 0.2824,9.1827,12.72,19.74,17.59,18.48,27.11; 230.67,527.85,502.87,15.81"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
